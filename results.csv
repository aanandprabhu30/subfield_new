Title,Abstract,Discipline,Subfield,Confidence_Score,Reasoning
Experimental study of a medical data analysis model based on comparative performance of classification algorithms,"This article centers around the development and analysis of machine learning (ML) and deep learning models aimed at enhancing diabetes diagnosis. In the swiftly evolving landscape of data technologies, it becomes crucial to explore the applications of these methods for accurate predictions and improved medical decision-making. Our research encompasses diverse datasets, leveraging state-of-the-art algorithms and technologies for model training and testing. The primary emphasis lies in evaluating the accuracy, sensitivity, and specificity of models within the realm of diabetes diagnosis. The study results reveal significant advancements in disease prediction, underscoring the potential of ML and deep learning in medical applications. This work introduces fresh perspectives on the utilization of computational methods in healthcare and serves as a foundation for prospective research in this domain.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Determinan Individu Terhadap Penggunaan Tiktok Shop: Pendekatan Technology Acceptance Model (Tam),"TikTok shop is among the mushrooming e-commerce platforms in Indonesia, which-despite providing interesting features-not many users use the them due to better reputation of other e-commerce platforms. As such, it is important to conduct the research to examine the effect of perceived usefulness, perceived ease of use, perceived risk, and using behavior on the intention to use TikTok Shop. This study applies a quantitative approach involving questionnaires distributed online for data collection. The samples include accounting students of Universitas Brawijaya class of 2019-2021 using TikTok Shop. The results of data analysis utilizing SmartPLS 3.2.9 exhibit that perceived usefulness and perceived ease of use have a positive effect on using behavior, and perceived risk has no effect on using behavior. The positive using behavior has a significant effect on the intention to use TikTok Shop. The results of this research are expected to be a reference for the TikTok Shop developer company to improve the existing system service quality. Abstrak Saat ini di Indonesia banyak sekali platform e-commerce yang bermunculan, salah satunya TikTok Shop. Meskipun peluncuran TikTok Shop ini memberikan fitur-fitur yang menarik, tetapi masih banyak pengguna yang belum menggunakan fitur ini karena reputasi yang lebih baik dari e-commerce lain. Oleh karena itu, hal ini menjadi penting untuk diteliti. Penelitian ini dilakukan untuk menguji pengaruh persepsi kegunaan, persepsi kemudahan penggunaan, persepsi risiko, dan sikap penggunaan terhadap minat penggunaan TikTok Shop. Pendekatan yang digunakan dalam penelitian ini adalah pendekatan kuantitatif dengan teknik pengumpulan data kuesioner yang disebarkan secara online. Sampel pada penelitian ini adalah mahasiswa Akuntansi Universitas Brawijaya yang terdiri dari angkatan 2019-2021 yang menggunakan TikTok Shop. Data yang diperoleh diuji dengan aplikasi SmartPLS 3.2.9 dan hasilnya menunjukkan bahwa persepsi kegunaan dan persepsi kemudahan penggunaan berpengaruh positif terhadap sikap penggunaan, kemudian persepsi risiko tidak berpengaruh terhadap sikap penggunaan. Sikap penggunaan yang positif memberikan pengaruh yang signifikan terhadap minat penggunaan seseorang dalam menggunakan TikTok Shop. Hasil dari penelitian ini diharapkan dapat menjadi bahan acuan kontribusi bagi perusahaan pengembang TikTok Shop untuk meningkatkan kualitas layanan sistem yang telah ada.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Investigating Physicians’ Adoption of Telemedicine in Romania Using Technology Acceptance Model (TAM),"This study investigates Romanian physicians’ acceptance of telemedicine using the Technology Acceptance Model. We analyzed 1093 responses to an online survey distributed nationwide to physicians via email by the National Authority of Quality Management in Health, employing the partial least squares algorithm to estimate the relationship between the behavioral intention to adopt telemedicine and its potential determinants. Our findings reveal that the model accounts for 84.6% of the variance in behavioral intention to use telemedicine. Among the two constructs of the TAM model, perceived usefulness is a stronger predictor of behavioral intention than perceived ease of use. Additionally, subjective norms positively and significantly influence physicians’ intention to use telemedicine and their perception of its usefulness. Furthermore, perceived incentives and accessibility to medical records also positively impact the behavioral intention to use telemedicine.",IS,HIS,85,Clear IS paper with focus on Healthcare Information Systems
Recent advances of lab-based diffraction contrast tomography – reconstruction speed benchmark testing and validations,"Abstract Lab-based diffraction contrast tomography (DCT) enables the user to reconstruct 3D grain maps of polycrystalline materials non-destructively. For each grain, the morphology and crystallographic orientation, as well as derived properties such as grain boundaries, can be determined. Through two application examples this paper demonstrates the data acquisition and reconstruction speed of the current implementation and validates the resulting grain maps. Firstly, for a conventional Laue focusing scan of an AlCu sample comprising 340 grains, major performance enhancements of the reconstruction algorithm have reduced the reconstruction time from half a day to half an hour. The second example highlights an advanced scan with projection geometry of an oriented electrical steel sheet containing 7,800 grains. While the data collection time is around a day for each of the data sets, the boosted reconstruction of the advanced acquisition data takes half a day and gives the full grain map without the need for stitching. After the major algorithm speed enhancements, grain map qualities are comparable with misorientations below 0.02° and grain boundary distances less than a voxel for both samples.",CS,CG_VIS,85,Clear CS paper with focus on Computer Graphics & Visualization
Towards Continuous Systematic Literature Review in Software Engineering,"Context: New scientific evidence continuously arises with advances in Software Engineering (SE) research. Conventionally, Systematic Literature Reviews (SLRs) are not updated or updated intermittently, leaving gaps between updates, during which time the SLR may be missing crucial new evidence. Goal: We propose and evaluate a concept and process called Continuous Systematic Literature Review (CSLR) in SE. Method: To elaborate on the CSLR concept and process, we performed a synthesis of evidence by conducting a meta-ethnography, addressing knowledge from varied research areas. Furthermore, we conducted a case study to evaluate the CSLR process. Results: We describe the resulting CSLR process in BPMN format. The case study results provide indications on the importance and feasibility of applying CSLR in practice to continuously update SLR evidence in SE. Conclusion: The CSLR concept and process provide a feasible and systematic way to continuously incorporate new evidence into SLRs, supporting trustworthy and up-to-date evidence for SLRs in SE.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
"A Cross-Company Ethnographic Study on Software Teams for DevOps and Microservices: Organization, Benefits, and Issues","Context: DevOps and microservices are acknowledged to be important new paradigms to tackle contemporary software demands and provide capabilities for rapid and reliable software development. Industrial reports show that they are quickly adopted together in massive software companies. However, because of the technical and organizational requirements, many difficulties against efficient implementation of the both emerge in real software teams. Objectives: This study aims to discover the organization, benefits and issues of software teams using DevOps & microservices from an immersive perspective. Method: An ethnographic study was carried out in three companies with different business, size, products, customers, and degree of globalization. All the three companies claimed their adoption of DevOps and microservices. Seven months (cumulative) of participant observations and nine interviews with practitioners were conducted to collect the data of software teams related to DevOps and microservices. A cross-company empirical investigation using grounded theory was done by analyzing the archive data. Results: The adoption of DevOps and microservices brings benefits to rapid delivery, ability improvements and burden reduction, whilst the high cost and lack of practical guidance were emerged. Moreover, our observations and interviews reflect that in software teams, the relationship between DevOps and microservices is not significant, which differs from the relationship described in the previous studies. Four lessons for practitioners and four implications for researchers were discussed based on our findings. Conclusion: Our findings contribute to the understanding of the organization, benefits and issues of adopting DevOps and microservices from an immersive perspective of software teams.",IS,IT_PM,85,Clear IS paper with focus on IT Project Management
"A Cross-Company Ethnographic Study on Software Teams for DevOps and Microservices: Organization, Benefits, and Issues","Context: DevOps and microservices are acknowledged to be important new paradigms to tackle contemporary software demands and provide capabilities for rapid and reliable software development. Industrial reports show that they are quickly adopted together in massive software companies. However, because of the technical and organizational requirements, many difficulties against efficient implementation of the both emerge in real software teams. Objectives: This study aims to discover the organization, benefits and issues of software teams using DevOps & microservices from an immersive perspective. Method: An ethnographic study was carried out in three companies with different business, size, products, customers, and degree of globalization. All the three companies claimed their adoption of DevOps and microservices. Seven months (cumulative) of participant observations and nine interviews with practitioners were conducted to collect the data of software teams related to DevOps and microservices. A cross-company empirical investigation using grounded theory was done by analyzing the archive data. Results: The virtual software teams were organized for adopting DevOps and microservices under the stubborn organizational structure. The adoption of DevOps and microservices brings benefits to rapid delivery, ability improvements and burden reduction, whilst the high cost and lack of practical guidance were emerged. Two major issues of adopting DevOps and microservices in software teams (i.e. fragmentary DevOps and abuse of microservices) were found common in the companies. Moreover, our observations and interviews reflect that in software teams, the relationship between DevOps and microservices is not significant, which differs from the relationship described in the previous studies. Four lessons for practitioners and four implications for researchers were discussed based on our findings. Conclusion: Our findings contribute to the understanding of the organization, benefits and issues of adopting DevOps and microservices from an immersive perspective of software teams.",IS,DIGITAL_TRANS,85,Clear IS paper with focus on Digital Transformation
Technology acceptance model (TAM) for analysing cloud computing acceptance in higher education institution (HEI),"Abstract Cloud computing is a fast-growing and up-and-coming technology. Cloud computing allows educational users a more flexible, accessible, and wide range of education resources. In this study, the technology acceptance model (TAM) was used as the foundation to explore the effects of perceived usefulness, perceived ease of use with two proposed additional variable: security and learning environment towards using cloud computing. The questionnaire was distributed using convenient sampling and a total of 664 higher education institution members consisting of lecturers, students and staffs participated in this research. The response was analysed using the Partial Least Squares Structural Equation modeling (PLS-SEM), which provides evidence of the reliability and validity of the Technology Acceptance Model (TAM) in this study. The result shows that perceived ease of use and perceived usefulness positively and significantly influence the intention to use cloud computing. It is also seen that security and learning environment also have a positive and significant impact on the intention to use cloud computing.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Analisis Penerimaan Pemustaka pada Layanan Electronic Thesis and Dissertation (ETD) di Perpustakaan Universitas Islam Negeri Syekh Ali Hasan Ahmad Addary Padangsidimpuan Menggunakan Technology Acceptance Model (TAM),"ETD (Electronic Thesis and Dissertation) is one of Padangsidimpuan State Islamic University's Syekh Ali Hasan Ahmad Addary Library's key services for end-user reading. The new service's user satisfaction is unclear. ETD user acceptability at Padangsidimpuan's Syekh Ali Hasan Ahmad Addary Library needs analysis. Analysis is needed to test user acceptance of ETD (Electronic Thesis and Dissertation) services at the Syekh Ali Hasan Ahmad Addary Library of Padangsidimpuan.The services taken in this study are ETD (Electronic Thesis and Dissertation) services and are located at the Library of Sheikh Ali Hasan Ahmad Addary State Islamic University Padangsidimpuan using the TAM (Technology Acceptance Model) approach, therefore the author's reason for knowing and analyzing how users receive and use ETD services at the Syekh Ali Hasan Ahmad Addary Padangsidimpuan State Islamic University Library. The method used in this study is quantitative research with data collection techniques, namely literature techniques, field research (research files), and distributing questionnaires. Researchers intend to use the TAM method to measure client happiness with ETD offerings. Participants are people who have used ETD services at UIN SYAHADA. The results of TAM X1-Y used to study the opinions of system users or respondents showed that on average 32.825% of respondents answered with Strongly Agree, while 55.7% answered with Agree. Multiple regression analysis shows that variables related to information system interface design (X1), user attitudes toward information systems (X3), and user behavioral attitudes toward information systems (X4) all have a significant impact on users' actual situation (Y). The simplicity of the user interface (X2) has nothing to do with this. R2 = 0.352, so it seems pretty close. So, external factors account for a total of 35.2% of the variance in users' real-world ISS conditions. The remaining 64.8%, then, were influenced by factors not included in the model.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
A Case Study of Building Shared Understanding of Non-Functional Requirements in a Remote Software Organization,"Building a shared understanding of non-functional requirements (NFRs) is a known but understudied challenge in requirements engineering, especially in organizations that adopt continuous software engineering (CSE) practices. During the peak of the COVID-19 pandemic, many CSE organizations complied with working remotely due to the imposed health restrictions; some continued to work remotely while implementing business processes to facilitate team communication and productivity. In remote CSE organizations, managing NFRs becomes more challenging due to the limitations to team communication coupled with the incentive to deliver products quickly. While previous research has identified the factors that lead to a lack of shared understanding of NFRs in CSE, we still have a significant gap in understanding how CSE organizations, particularly in remote work, build a shared understanding of NFRs in their software development. We conduct a three-month ethnography-informed case study of a remote CSE organization. Through thematic analysis of our qualitative data from interviews and observations, we identify a number of practices in developing a shared understanding of NFRs. The collaborative workspace the organization uses for remote interaction is Gather, which simulates physical workspaces, and which our findings suggest allows for informal communications instrumental for building shared understanding. As actionable insights, we discuss our findings in light of proactive practices that represent opportunities for software organizations to invest in building a shared understanding of NFRs in their development.",IS,SOC_COMP,85,Clear IS paper with focus on Social Computing & Collaboration
The Effect of Implementing the Technology Acceptance Model (Tam) on Shopping Intention (Behavioral Intention) Using Paylater,"This research analyzes the Technology Acceptance Model theory (TAM) in shopping behavior using Paylater. The purpose of this research is to determine the influence of perceived usefulness and perceived ease of use on shopping interest using Paylater (behavioral intention). Respondents from this research are people who know E-Commerce and know the Paylater payment tool. After testing, the results of this research are that perceived usefulness has a significant positive effect on shopping interest using Paylater (behavioral intention), this is shown by the level of significance value of 0.046. Meanwhile, perceived ease of use does not have a significant effect on shopping interest using Paylater, behavioral intention of Paylater users. The behavioral intention of Paylater users is explained by the variables Perceived usefulness and Perceived ease of use of 7.4%, the rest is explained by other variables outside this research.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Penerapan Technology Acceptance Model (TAM) Dalam Pengujian Model Penerimaan Sistem Informasi Keuangan Daerah,"This study analyzed the factors which influence the acceptance of the Financial Information System for local Government (SIPKD) using the Technology Acceptance Model (TAM). TAM stated that behavioral intension to use Information System are determined by two beliefs: perceived usefulness (POU) and perceived ease of use (PEU). Perceived usefulness (POU) was defined as the extent to people were sure that the use of the system will improve its performance. Perceived ease of use (PEU) was defined as the extent to people were sure that the system is easy to use. This study analyzed the acceptance SIPKD in the context of the acceptance by the user. User are the local government employee have an obligation to run SIPKD. The analysis focused on the user's perception regarding SIPKD they should run. User perception analyzed is user perception about the ease of use SIPKD and user perceptions about the benefits SIPKD in their work. Research was conducted on the SIPKD in the Special Province of Yogyakarta. Respondents of this study 67 from various agencies in the region of Yogyakarta. The results showed that the Perceived usefulness influence the using of SIPKD, while Perceived Ease of Use does not influence",IS,ISM,85,Clear IS paper with focus on Information Systems Management
Timestamp Injected Cryptographic Hash Function to Reduce Fabrication of Hash Collisions,"Cryptographic hash functions are used in many applications. One important application is to ensure data integrity. Although there are many different types of hashing algorithms, MD5 is widely used to ensure data integrity in digital evidence. However, a weakness, where collisions can occur, has been found in the MD5 algorithm. With regards to digital evidence, this is a big issue. The integrity of the digital evidence becomes questionable due to collisions and hence it is not admissible in court. Many methods were used to find collisions, such as the Chosen-Prefix Collision and researchers have been improving collision finding algorithms. This paper concentrates on reducing the chances of collision by chopping the last 16 bits of the MD5 algorithm and injecting timestamp into the chopped parts. Experiments are performed to test this algorithm and the results show that the time taken to find collisions is longer using the MD5 with an injected timestamp. The chopping construction and the timestamp disrupt the iterative property of the hash function thus when dealing with digital evidence, there are less chances of hash collision and therefore the probability of the admissibility of the digital evidence in court is higher.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
A Novel Approach for Testing Benchmark Functions using Biogeography based Optimization (BBO) Algorithm,"Biogeography is the science and study of geographical distribution of biological organisms. BBO is a traditional algorithm that maximises efficiency, based on the mathematical aspects of biogeography. The project aims at sharing the probable features between solutions and fitness values that are represented as immigration and emigration between islands. BBO is similar to biological optimization methods i.e. Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) that carries features which are unique. The proposed algorithm of BBO provides a solution to many that uses GA and PSO. This paper demonstrates a performance of the proposed BBO with a set of well known standard benchmark functions.Â Â",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
"A realistic 2D multi-offset, multi-frequency synthetic GPR data set as a benchmark for testing new algorithms","Abstract We present a 2D multi-offset, multi-frequency synthetic GPR data set specifically designed to evaluate and test processing, analysis and inversion techniques. The data set replicates realistic subsurface conditions at four sections separated by 2 m. We modeled four multi-offset GPR profiles at 50, 100 and 200 MHz frequencies using realistic wavelets. The data set provides a robust framework for validating advanced GPR algorithms and techniques such as pre-stack depth migration, amplitude versus offset analysis and full waveform inversion. Extensive technical validation ensures data reproducibility and affordability. The standardized, realistic synthetic data set can be used as a reliable benchmark for developing and testing new algorithms and methods, thereby advancing the understanding of subsurface imaging and real-world data interpretation.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Structural Equation Modeling with Factors and Composites,"Recent methodological developments building on partial least squares (PLS) techniques and related ideas have significantly contributed to bridging the gap between factor-based and composite-based structural equation modeling (SEM) methods. PLS-SEM is extensively used in the field of e-collaboration, as well as in many other fields where multivariate statistical analyses are employed. The author compares results obtained with four methods: covariance-based SEM with full information maximum likelihood (FIML), factor-based SEM with common factor model assumptions (FSEM1), factor-based SEM building on the PLS Regression algorithm (FSEM2), and PLS-SEM employing the Mode A algorithm (PLSA). The comparison suggests that FSEM1 yields path coefficients and loadings that are very similar to FIML's; and that FSEM2 yields path coefficients that are very similar to FIML's and loadings that are very similar to PLSA's.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Regression Analysis with Scikit-Learn (part 1 - Linear),"This lesson is the first of a two-part lesson focusing on an indispensable set of data analysis methods, logistic and linear regression. It provides an overview of linear regression and walks through running both algorithms in Python (using scikit-learn). The lesson also discusses interpreting the results of a regression model and some common pitfalls to avoid.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Adoption of E-commerce in Bangladesh: Probit Regression and Principal Component Analysis Approach,"Web based services have turned into an integral part of peoples’ life. Ease of use and several advantages lead to the popularity of adopting electronic commerce (e-commerce). Though a big number of customers visit and register on the commerce sites daily, the selling rate is low for lack of trust and loyalty. Trust is the main connector of companies and customers. Loyal customers are the important assets for the companies. This study aims at identifying the factors that are associated with the trust and risk perception of the customers on e-commerce. A primary dataset is collected on this occasion. Likert scale type questionnaire is involved in the questionnaire and data reliability is checked through Cronbach's alpha. Probit regression and principal component analysis are used as statistical analysis. The study finds the variables age, internet using purpose, market orientation, and technology as significantly related to trustworthiness of the customers on e-commerce, while the variables experienced using the internet, internet using purpose, and market orientation are potential determinants of risk perception. The study results suggest the e-commerce companies emphasize technologies for keeping accurate information, performing the outmost of the customers’ benefit, and collecting customers’ information. In addition, they should be conscious of the customers’ opinion and pleasant experience, along with maintaining the mentioned delivery time.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Exploratory Students’ Behavior towards Massive Open Online Courses: A Structural Equation Modeling Approach,"Since the evolution of massive online open courses (MOOCs) as an Ed-Tech solution to various educational problems, learners have registered themselves for various MOOCs offered by various universities and MOOC platforms. However, it has been observed that many learners who register need to complete the course and progress further. Thus, the present research aims to study the learners’ lack of continuance of MOOCs. The research is based upon a quantitative research design in which a conceptual model is developed and tested empirically by employing a survey questionnaire as a tool for data collection. The data was collected from 377 respondents who were university students from Saudi Arabia studying at Jazan University, and partial least square-structural equation modeling (PLS-SEM) was used as a tool for data analysis. The results of PLS-SEM show that learner continuance behavior depends on three elements: perceived career success, perceived training opportunity, and satisfaction with MOOCs. The results further show that content vividness, interactivity, and intellectual curiosity are antecedents of satisfaction with MOOCs. The present research has argued that user gratification will yield continuance with technology products. It argues that rational aspects such as career success and acquisition of tangible skills can also yield continuance with technology products and services.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Modeling the Socialization of Artificial Intelligence Technologies in Accounting Using Structural Equation Modeling,"The aim of this study is to model the socialization of artificial intelligence technologies in accounting using structural equation modeling. The research approach follows a grounded theory methodology combined with structural equation modeling. In this context, interviews were conducted with 12 participants, including engineers educated in artificial intelligence, board members, financial managers, accountants, and auditors, until theoretical saturation was reached. The results from the secondary coding process were categorized into 11 main themes, 35 sub-themes, and 6 categories: causal factors, core factors, strategies, mediators, actions, and outcomes. The target population for the quantitative part of this study consisted of all accountants. The questionnaire was distributed among the members of the target population via direct (in-person), electronic, virtual spaces, emails, and mail. Ultimately, 342 individuals responded to the questionnaire. To test the research hypotheses, correlation techniques using structural equation modeling were applied. The results of the analysis of findings from 28 hypotheses showed that training and awareness significantly and positively affect effective communication, the ability to collaborate with artificial intelligence, and the prevention of threats. Moreover, the interaction between artificial intelligence and humans significantly and positively impacts effective communication, the ability to collaborate with artificial intelligence, software compatibility, and threat prevention. Continuous evaluation and improvement also significantly and positively affect effective communication, the ability to collaborate with artificial intelligence, and the prevention of threats. Additionally, support from financial tools positively and significantly influences effective communication, collaboration with artificial intelligence, and the prevention of threats. Effective communication has a significant and positive effect on comprehensive and accurate financial reporting, intelligent financial management, and diversity and inclusion. The ability to collaborate with artificial intelligence has a significant and positive effect on comprehensive and accurate financial reporting, intelligent financial management, and diversity and inclusion. Software compatibility positively and significantly impacts intelligent financial management and diversity and inclusion. Finally, the prevention of threats significantly and positively influences comprehensive and accurate financial reporting, intelligent financial management, and diversity and inclusion. The results indicate that there are fundamental infrastructures and technologies that can assist in the socialization of artificial intelligence in the field of accounting. Furthermore, accountants can play a crucial role in the effective use of artificial intelligence by providing relevant financial information to financial stakeholders and supporting financial information.",IS,SOC_COMP,85,Clear IS paper with focus on Social Computing & Collaboration
Likelihood of AI Tools Adoption and Interest in Professional Development Opportunities in Higher Education: An Ordinal Logistic Regression Analysis,"This study explored the factors influencing academic staff’s readiness to use artificial intelligence(AI) tools and participate in AI-related professional development, utilizing a quantitative approach. Data from95 academic staff members of the University of Vlora “Ismail Qemali” were gathered via an online survey. Theanalysis, conducted using univariate ordinal logistic regression, pinpointed key predictors of educational AItools adoption likelihood and interest in attending AI professional development opportunities. Rigorousevaluation of model fit, influence diagnostics, and cross-validation was conducted to ensure the findings’reliability and accuracy. Results highlight the critical role of interest in AI educational tools development,technological proficiency, and past use of AI educational tools in determining the likelihood of adoptingeducational AI tools, underscoring the pivotal importance of fostering a genuine interest in AI. Furthermore, theresearch identifies gender as a significant factor influencing interest in attending AI professional developmentopportunities, while negative perceptions of AI’s role in education tend to reduce such interest. These findingsstress the need for targeted efforts to enhance educators’ readiness for AI, mitigate gender disparities, andcorrect misconceptions about AI. By revealing the complex factors affecting educators’ willingness to adopt AItechnologies, this study advocates for a holistic strategy encompassing a broader range of influences. It providesactionable insights for educational policymakers, curriculum developers, and AI tool creators to create anenvironment conducive to AI adoption in higher education. Although limited by its use of convenience samplingand focus on a single institution, this research offers essential insights into the dynamics of AI adoption ineducation. It lays a foundation for strategies that encourage innovation, inclusivity, and a forward-thinkingapproach to integrating AI into future teaching and learning.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Determinants of adoption of household water treatment in Haiti using two analysis methods: logistic regression and machine learning,"ABSTRACT Household water treatment (HWT) is recommended when safe drinking water is limited. To understand determinants of HWT adoption, we conducted a cross-sectional survey with 650 households across different regions in Haiti. Data were collected on 71 demographic and psychosocial factors and 2 outcomes (self-reported and confirmed HWT use). Data were transformed into 169 possible determinants of adoption across nine categories. We assessed determinants using logistic regression and, as machine learning methods are increasingly used, random forest analyses. Overall, 376 (58%) respondents self-reported treating or purchasing water, and 123 (19%) respondents had residual chlorine in stored household water. Both logistic regression and machine learning analyses had high accuracy (area under the receiver operating characteristic curve (AUC): 0.77–0.82), and the strongest determinants in models were in the demographics and socioeconomics, risk belief, and WASH practice categories. Determinants that can be influenced inform HWT promotion in Haiti. It is recommended to increase access to HWT products, provide cash and education on water treatment to emergency-impacted populations, and focus future surveys on known determinants of adoption. We found both regression and machine learning methods need informed, thoughtful, and trained analysts to ensure meaningful results and discuss the benefits/drawbacks of analysis methods herein.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
"Comparison and analysis of the accuracy of Lasso regression, Ridge regression and Elastic Net regression models in predicting students' teaching quality achievement","In this paper, the ""Student Performance Data Set"" data set of Kaggle competition is used to conduct correlation analysis on multiple attributes such as students' personal information, school information and students' school performance, and a correlation heat map is drawn. The results show that there is a strong positive correlation among the attributes. We then divided the data set into training, validation, and test sets in a 6:2:2 ratio, and used the Lasso regression model, the Elastic Net model, and the Ridge regression model to make predictions. After training 50 epochs, we evaluated and compared the models. The results show that Lasso regression model has the lowest prediction error and the best error effect. Elastic Net was the next best predictor, while Ridge regression model had the largest prediction error and was the worst. To sum up, Lasso regression model has the best Performance in grade prediction based on the ""Student Performance Data Set"" dataset. This conclusion is of great significance for schools and educational institutions, as it can help them better understand students' learning and improve the quality and effectiveness of teaching. At the same time, this conclusion is also valuable for data scientists and machine learning researchers, because it can guide them to choose the most appropriate model and algorithm on similar data sets, improving the accuracy and effectiveness of predictions. In general, this paper analyzes and discusses the problem of Student achievement prediction, puts forward the Lasso regression model based on the ""Student Performance Data Set"" data set, which has the best performance prediction effect, and analyzes the principles of the three models. This conclusion has practical applications for schools and educational institutions, as well as providing a reference for data scientists and machine learning researchers.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Gene Network Inference via Structural Equation Modeling in Genetical Genomics Experiments,"AbstractOur goal is gene network inference in genetical genomics or systems genetics experiments. For species where sequence information is available, we first perform expression quantitative trait locus (eQTL) mapping by jointly utilizing cis-, cis–trans-, and trans-regulation. After using local structural models to identify regulator–target pairs for each eQTL, we construct an encompassing directed network (EDN) by assembling all retained regulator–target relationships. The EDN has nodes corresponding to expressed genes and eQTL and directed edges from eQTL to cis-regulated target genes, from cis-regulated genes to cis–trans-regulated target genes, from trans-regulator genes to target genes, and from trans-eQTL to target genes. For network inference within the strongly constrained search space defined by the EDN, we propose structural equation modeling (SEM), because it can model cyclic networks and the EDN indeed contains feedback relationships. On the basis of a factorization of the likelihood and the constrained search space, our SEM algorithm infers networks involving several hundred genes and eQTL. Structure inference is based on a penalized likelihood ratio and an adaptation of Occam's window model selection. The SEM algorithm was evaluated using data simulated with nonlinear ordinary differential equations and known cyclic network topologies and was applied to a real yeast data set.",CS,BIOINFO,85,Clear CS paper with focus on Bioinformatics & Computational Biology
Optimization Methodologies and Testing on Standard Benchmark Functions of Load Frequency Control for Interconnected Multi Area Power System in Smart Grids,"In the recent era, the need for modern smart grid system leads to the selection of optimized analysis and planning for power generation and management. Renewable sources like wind energy play a vital role to support the modern smart grid system. However, it requires a proper commitment for scheduling of generating units, which needs proper load frequency control and unit commitment problem. In this research area, a novel methodology has been suggested, named Harris hawks optimizer (HHO), to solve the frequency constraint issues. The suggested algorithm was tested and examined for several regular benchmark functions like unimodal, multi-modal, and fixed dimension to solve the numerical optimization problem. The comparison was carried out for various existing models and simulation results demonstrate that the projected algorithm illustrates better results towards load frequency control problem of smart grid arrangement as compared with existing optimization models.",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
Linear Regression on Internet Banking Adoption Dataset Using WEKA,"Data mining or knowledge discovery in the database (KDD) is an excellent process to find out valuable information from a large collection of data. Data mining has successfully been used in different fields such as medical, marketing, banking, business, weather forecasting, etc. For the banking industry, data mining, its importance, and its techniques are vital because it helps to extract useful information from a large amount of historical data which enable to make useful decisions. Data mining is very useful for banking sector for better acquiring and targeting new customers and helps to analyze customers and their transaction behaviors. In the recent era, a new technology that has achieved considerable attention, especially among banks, is internet banking. Its large scope of applications, its advantages brings an immoderate change in a common human's life. Linear regression is one of the most commonly used and applied data mining techniques. Linear regression is really a very fast and simple regression algorithm and can give the best performance if the output variable of your data is a linear grouping of your inputs. In this paper, the linear regression is applied on internet banking adoption dataset in order to compute the weights or coefficients of linear expression and provides the predicted class value. The analysis here is done with the help of WEKA tool for data mining.",IS,BI_ANALYTICS,85,Clear IS paper with focus on Business Intelligence & Analytics
MODELING OF SEA SURFACE TEMPERATURE BASED ON PARTIAL LEAST SQUARE - STRUCTURAL EQUATION,"Variability of Sea Surface Temperature (SST) is one of the climatic features that influence global and regional climate dynamics. Missing data (gaps) in the SST dataset are worth investigating since they may statistically alter the value of the SST change. The partial least square-structural equation modeling (PLS-SEM) approach is used in this work to estimate the causality relationships between exogenous and endogenous latent variables. The findings of this study, which are significant indicators that have a loading factor value &gt; 0.7 are as follows: i) sea surface temperature (oC) as a measure of the latent variable changes in SST, ii) wind speed (m/s) and relative humidity (%) as a measure of the latent variable of weather, and iii) air temperature (oC), long-wave solar radiation (w/m2) as a measure of climate latent variables. The size of the Rsquare value is influenced by the number of gaps. The results of the boostrapping show that the latent variables of weather and climate have a significant effect on changes in SST which are indicated by the value of tstatistics &gt; ttabel. The structural model obtained Changes in SST (η) = -0.330 weather + 0.793 climate + ζ. The model shows that the weather has a negative coefficient, which means that the better the weather conditions, the lower the SST changes. Climate has a positive coefficient, which means that the better the climate, the SST changes will also increase. Rising sea surface temperatures caused by an increase in climate can lead to global warming, impacting El-Nino and La-Nina events.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Examining Students’ Readiness for MOOCs: Applying a Structural Equation Modeling Approach,"This study investigates students’ readiness to adopt Massive Open Online Courses (MOOCs) at the University of Ha’il. It applied Student Online Learning Readiness (SOLR) model to examine the constructs that might influence students’ readiness toward using MOOCs. A questionnaire was sent to students that measured the model’s latent constructs: technical competency (TC), social competency (SC), communication competency (CC), and student readiness. A total of 111 responses were received, and the model was analyzed relied on structural equation modeling (SEM). The findings showed that TC and CC had a significant positive effect on the readiness of students to use MOOCs. Surprisingly, SC had an insignificant effect on students’ readiness. The findings of this study provide educational decision-makers and designers with essential input for delivering effective MOOCs.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
"Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence","In this meta-ethnography, we explore three different angles of ethical artificial intelligence (AI) design implementation including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. Our qualitative research includes a literature review that highlights the cross-referencing of these angles by discussing the value and drawbacks of contrastive top-down, bottom-up, and hybrid approaches previously published. The novel contribution to this framework is the political angle, which constitutes ethics in AI either being determined by corporations and governments and imposed through policies or law (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technicalities of how AI is developed within a moral construct and in consideration of its users, with expected and unexpected consequences and long-term impact in the world. There is a focus on reinforcement learning as an example of a bottom-up applied technical approach and AI ethics principles as a practical top-down approach. This investigation includes real-world case studies to impart a global perspective, as well as philosophical debate on the ethics of AI and theoretical future thought experimentation based on historical facts, current world circumstances, and possible ensuing realities.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Regression Analysis: A Theoretical Approach,"Abstract The main objective of this document is to provide a comprehensive understanding in the area of simple regression, especially for undergraduate students majoring in economics, finance and statistics. JEL classification numbers: C01, C013, C51, C52. Keywords: Econometrics, single equation model, Estimation.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Precision Farming Adoption by Florida Citrus Producers: Probit Model Analysis,"Production practices in agriculture are constantly changing and being modified. The introduction of site-specific crop management (SSCM), also known as precision farming, can be considered the newest advance in production agriculture and mechanization. The use of multiple technologies and common production practices have opened a new era of “high-tech” farming. The use of soil sampling; yield monitoring; remote sensing; and variable-rate applications of herbicide, pesticide, and fertilizer, as well as the global positioning system (GPS) and a geographic information system (GIS) can be considered precision agriculture. This document is Circular 1461, one of a series rom the Department of Agricultural and Biological Engineering, Florida Cooperative Extension Service, Institute of Food and Agricultural Sciences, University of Florida. First published: February 2005.&#x0D; CIR1461/AE283: Precision Farming Adoption by Florida Citrus Producers: Probit Model Analysis (ufl.edu)",IS,DSS,85,Clear IS paper with focus on Decision Support Systems
Predicting determinants of Internet banking adoption,"Purpose  – The purpose of this paper is to explore the main determinants of Internet banking users on the basis of literature of technology acceptance model (TAM). Understanding and predicting main determinants of Internet banking is an important issue for banking industry and users.    Design/methodology/approach  – Service quality and trust were incorporated in the TAM together with demographic variables. The data were collected using Google Docs from 110 Omani Internet banking users. A two-staged regression-neural network model was applied to understand and predict Internet banking adoption.    Findings  – The results obtained from multiple linear regression model were compared with the results from neural network model to predict Internet banking adoption and the performance of latter model was found to superior. The neural network model was able to capture relative importance of all independent variables, service quality, trust, perceived usefulness, perceived ease of use, attitude and demographic variables, whereas perceived ease of use and demographic variables were not significant predictors of Internet banking adoption as per the regression model.    Practical implications  – This study provides useful insights with regard to development of Internet banking systems to banking professionals and information systems researchers in Oman and similar emerging economies.    Originality/value  – This study is probably the first attempt to model Internet banking adoption in Gulf Cooperation Council using a predictive rather than explanatory focus. The majority of studies in Internet banking adoption in Oman and elsewhere usually utilize modeling methods suited for explanatory purposes.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
The Descriptive Data Analysis for the Adoption of Community Cloud in Saudi HEI‐Based Factor Adoption,"Due to its increased reliability, adaptability, scalability, availability, and processing capacity, cloud computing is rapidly becoming a popular trend around the world. One of the major issues with cloud computing is making informed decision about adoption of community cloud (CC) computing (ACCC). To date, there are various technology acceptance theories and models to validate perspective of ACCC at both organizational and individual levels. However, no experimental studies have been carried out to provide a comprehensive assessment of the factors of ACCC, specifically in the area of the Saudi Higher Education (HEI) Institution. Thus, this research was aimed at exploring the factors of ACCC and the relationship to the experiences of the employees. The analysis of the employee context was driven by the success factors of technological, organizational, environmental, human, security, and advantage contexts on community cloud computing adoption in HEI. The data collection was a questionnaire‐based survey based on 106 responses. We present findings based on descriptive analysis in identifying the significant component that contributed to the effective implementation of ACCC. Security concerns are a significant influencing element in the adoption of community cloud technology.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Empirical Analysis of E-Procurement adoption in SMEs,"The significance of e-commerce in the global economy is expected to grow substantially in the 21st century. Business-to-business (B2B) e-commerce is said to be the most successful type of e-commerce because of the widespread implementation of consistent procedures for documents interchange (like Electronic Data Interchange (EDI)), delivery, trailing, distribution, and compensation amongst stock chain partners (B2C, B2E, and B2G). E-procurement refers to the act of purchasing properties and facilities electronically, typically through the use of the Internet and associated web-based tools. E-procurement is a part of B2B interchanges, and it is typically used to buy materials and components, as well as the more common MRO supplies. E-procurement has been shown to increase productivity and cut costs by some studies. There are many accounts in the literature of the rapid expansion of E-procurement to new republics and businesses. The learning goal of this research is to take stock of where E-procurement stands among small and middle-sized enterprises (SMEs) in Southcoast Massachusetts at current time. The primary purpose of this paper is to acquire more about the existing state of E-procurement in SMEs and features that impact the acceptance of E-procurement. A questionnaire study was used to amass the information. Information procurement via the internet (E-procurement) is described, along with the Southcoast of Massachusetts's conceptual model for its effective implementation.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
ACR benchmark testing of a novel high‐speed ring‐gantry linac kV‐CBCT system,"AbstractA new generation cone‐beam computed tomography (CBCT) system with new hardware design and advanced image reconstruction algorithms is available for radiation treatment simulation or adaptive radiotherapy (HyperSight CBCT imaging solution, Varian Medical Systems‐a Siemens Healthineers company). This study assesses the CBCT image quality metrics using the criteria routinely used for diagnostic CT scanner accreditation as a first step towards the future use of HyperSight CBCT images for treatment planning and target/organ delineations. Image performance was evaluated using American College of Radiology (ACR) Program accreditation phantom tests for diagnostic computed tomography systems (CTs) and compared HyperSight images with a standard treatment planning diagnostic CT scanner (Siemens SOMATOM Edge) and with existing CBCT systems (Varian TrueBeam version 2.7 and Varian Halcyon version 2.0). Image quality performance for all Varian HyperSight CBCT vendor‐provided imaging protocols were assessed using ACR head and body ring CT phantoms, then compared to existing imaging modalities. Image quality analysis metrics included contrast‐to‐noise (CNR), spatial resolution, Hounsfield number (HU) accuracy, image scaling, and uniformity. All image quality assessments were made following the recommendations and passing criteria provided by the ACR. The Varian HyperSight CBCT imaging system demonstrated excellent image quality, with the majority of vendor‐provided imaging protocols capable of passing all ACR CT accreditation standards. Nearly all (8/11) vendor‐provided protocols passed ACR criteria using the ACR head phantom, with the Abdomen Large, Pelvis Large, and H&amp;N vendor‐provided protocols produced HU uniformity values slightly exceeding passing criteria but remained within the allowable minor deviation levels (5–7 HU maximum differences). Compared to other existing CT and CBCT imaging modalities, both HyperSight Head and Pelvis imaging protocols matched the performance of the SOMATOM CT scanner, and both the HyperSight and SOMATOM CT substantially surpassed the performance of the Halcyon 2.0 and TrueBeam version 2.7 systems. Varian HyperSight CBCT imaging system could pass almost all tests for all vendor‐provided protocols using ACR accreditation criteria, with image quality similar to those produced by diagnostic CT scanners and significantly better than existing linac‐based CBCT imaging systems.",CS,CG_VIS,85,Clear CS paper with focus on Computer Graphics & Visualization
The Application of Structural Equation Modeling in the Applied Linguistics Research,"Structural equation model is the only statistical methods that can simultaneously do comprehensive examination about the relationships between the multi-dimensional variables. It has broad application prospects in the field of applied linguistics. In this paper, combined with the applied linguistics article that choose structural equation modeling as the technique on academic journals over the past decades, then discussing the differences in domestic and foreign researchers from both content and form, after comparison and analysis, putting forward the suggestions for the application of SEM in our domestic applied linguistics.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Structural Equation Modeling of Mental Toughness Among University Learners,"BACKGROUND: Mental toughness is recognized as an important component towards academic success thus making its psychological qualities determine how challenges are effectively addressed during pressurized situations. Challenges facing undergraduate learners in the context of mental toughness had been broadly investigated mostly in developed countries. Most of the studies centered on sports and descriptive findings lack critical analysis.&#x0D; &#x0D; OBJECTIVE: The main objective of the current study was to investigate the level of mental toughness of university learners and the impact on the learners&amp;#39; academic performance. The current study also investigated whether university learners who were reported with greater mental toughness are more likely to be academically successful than those lower in mental toughness.&#x0D; &#x0D; METHODS: This quantitative study employed SmartPls 3 software to predict the significant level of motivation, self-reliance, concentration and coping with pressure on academic performance among university learners. Two universities were considered for assessing the structural equation modeling of mental toughness. Additionally, sources of data included reviews of different books on the related topics, research studies, articles, journals, newspapers, and magazines. Substantial information has been gathered from these sources thus allowing for appropriate analysis, compilation, interpretation, and structuring of the entire study. Thus, in an attempt to isolate and categorize potential attributes of mental toughness and its impact on academic performance, the available literature reviewed. This quantitative study considered adoptable in handling bias findings. A sample size of 417 considered appropriate for a variance based structural equation modeling. A total number of 417 responses gathered from Angeles University Foundation (AUF) and Baliuag University (BU), Philippines considered for this mental toughness study.&#x0D; &#x0D; RESULTS: A total of a 75 percent from the questionnaires (477) returned from a sum 600 questionnaires distributed to specified respondents. Demographic details report that female responded with round-off 60%, this implied that female strive more in education than male. Ages 17-20 occupied 55% nursing/medicine marked around of 34% to top among the six colleges investigated in this study, next was college of business and administration marked around of 20% to take second place. This study suggested that students considered more to be medical doctors, professional nurses and business practitioners in the future rather than being professional teachers or system engineers. Reliability and validity of this study reported according to the Smart-Pls algorithm factor matrix, Cronbach&amp;#39;s alpha, rho_A, and composite reliability all above 0.7 thresholds. Also, the average variance extracted from 0.5 achieved. The discriminant validity of this study based on Fornell-Lackner criterion, factor loading at 0.6 above and Heterotrait Monotrait Ratio quality achieved. Conclusively, all supported path coefficients significant at the p-values &amp;lt; 0.01. In a nutshell, partial least squares algorithm reported about a 58% variance explained from the entire structured model.&#x0D; &#x0D; CONCLUSION: The adopted factors for this structural equation modeling of mental toughness for university learners achieved fifty-eight percent variance explained in the study. Future studies can be directed towards replicating the use of this model in other locations and different analytical techniques.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
“Regression-then-Fusion” or “Fusion-then-Regression”? A Theoretical Analysis for Generating High Spatiotemporal Resolution Land Surface Temperatures,"The trade-off between spatial and temporal resolutions in satellite sensors has inspired the development of numerous thermal sharpening methods. Specifically, regression and spatiotemporal fusion are the two main strategies used to generate high-resolution land surface temperatures (LSTs). The regression method statically downscales coarse-resolution LSTs, whereas the spatiotemporal fusion method can dynamically downscale LSTs; however, the resolution of downscaled LSTs is limited by the availability of the fine-resolution LSTs. Few studies have combined these two methods to generate high spatiotemporal resolution LSTs. This study proposes two strategies for combining regression and fusion methods to generate high spatiotemporal resolution LSTs, namely, the “regression-then-fusion” (R-F) and “fusion-then-regression” (F-R) methods, and discusses the criteria used to determine which strategy is better. The R-F and F-R have several advantages: (1) they fully exploit the information in the available data on the visible and near infrared (VNIR) and thermal infrared (TIR) bands; (2) they downscale the LST time series to a finer resolution corresponding to that of VNIR data; and (3) they inherit high spatial reconstructions from the regression method and dynamic temporal reconveyance from the fusion method. The R-F and F-R were tested with different start times and target times using Landsat 8 and Advanced Spaceborne Thermal Emission and Reflection Radiometer data. The results showed that the R-F performed better than the F-R when the regression error at the start time was smaller than that at the target time, and vice versa.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Exploring Urban Traffic Dynamics: Introducing a Benchmark Map for Comprehensive Testing and Evaluation,"Abstract Traffic signal control plays a crucial role in managing traffic flow and alleviating congestion on urban roads. This study proposes a deep reinforcement learning (DRL) approach to optimize traffic signal control and reduce traffic congestion in urban environments. Leveraging Perceiver transformers and a deep neural network, the approach uses traffic flow data — including speed, vehicle arrivals, and other relevant metrics — to enhance signal regulation. The DRL framework is based on the Q-learning algorithm and operates without relying on specific traffic models or rules. To evaluate different traffic control strategies, we introduce a benchmark map of Žilina city as a testing ground. Our method establishes a robust framework for optimising urban traffic, achieving significant improvements in traffic flow efficiency and congestion reduction, as demonstrated through simulations on this map.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
FOAM: A General Frequency-Optimized Anti-Overlapping Framework for Overlapping Object Perception,"Overlapping object perception aims to decouple the randomly overlapping foreground-background features, extracting foreground features while suppressing background features, which holds significant application value in fields such as security screening and medical auxiliary diagnosis. Despite some research efforts to tackle the challenge of overlapping object perception, most solutions are confined to the spatial domain. Through frequency domain analysis, we observe that the degradation of contours and textures due to the overlapping phenomenon can be intuitively reflected in the magnitude spectrum. Based on this observation, we propose a general Frequency-Optimized Anti-Overlapping Framework (FOAM) to assist the model in extracting more texture and contour information, thereby enhancing the ability for anti-overlapping object perception. Specifically, we design the Frequency Spatial Transformer Block (FSTB), which can simultaneously extract features from both the frequency and spatial domains, helping the network capture more texture features from the foreground. In addition, we introduce the Hierarchical De-Corrupting (HDC) mechanism, which aligns adjacent features in the separately constructed base branch and corruption branch using a specially designed consistent loss during the training phase. This mechanism suppresses the response to irrelevant background features of FSTBs, thereby improving the perception of foreground contour. We conduct extensive experiments to validate the effectiveness and generalization of the proposed FOAM, which further improves the accuracy of state-of-the-art models on four datasets, specifically for the three overlapping object perception tasks: Prohibited Item Detection, Prohibited Item Segmentation, and Pneumonia Detection. The code will be open source once the paper is accepted.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Анализ импульсных характеристик Watermark Benchmark для моделирования подводных акустических каналов связи,В статье проводится анализ импульсных характеристик из набора данных Watermark Benchmark применительно к моделированию подводных акустических каналов связи. Рассматриваются особенности структуры импульсного отклика и проводится оценка нестационарности представленных импульсных характеристик. Результаты анализа могут быть использованы для оптимизации алгоритмов обработки сигналов и разработки методов повышения помехоустойчивости и надёжности передачи данных в подводных акустических системах. The article analyzes the Watermark Benchmark impulse responses data set to model underwater acoustic communication channels. The impulse response structure is being analyzed and its nonstationarity is being assessed. The results of the analysis can be used to optimize signal processing algorithms and to develop methods for increasing the reliability of data transmission in underwater acoustic systems.,CS,CN_DS,85,Clear CS paper with focus on Computer Networks & Distributed Systems
"A Multifaceted Approach to Amazon's Financial Performance: Time Series, Difference in Difference, and Regression Discontinuity Analysis of R&amp;D, Marketing, and Mobile Adoption","In the contemporary business landscape, Amazon has emerged as a preeminent force, asserting its dominance across global e-commerce and cloud computing sectors. This ascendancy is underpinned by a steadfast commitment to innovation, manifested through substantial investments in research and development (R&amp;D) initiatives and strategic marketing endeavors. This study seeks to discern the influence of key variables, such as R&amp;D expenditure, marketing expenses, and historical financial data, on Amazon's net income (NI) and revenue generation. Additionally, the pervasive adoption of Amazon's mobile application among over half of US consumers underscores its integral role in modern online shopping experiences, a phenomenon catalyzed by the transformative introduction of the first iPhone in 2007, which significantly augmented Amazon's revenue trajectory. Through rigorous analysis, this research aims to elucidate the intricate interplay of factors shaping Amazon's financial performance, contributing to a deeper understanding of contemporary business dynamics.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Adoption of K-means clustering algorithm in smart city security analysis and mythical experience analysis of urban image,"Objective An information security evaluation model based on the K-Means Clustering (KMC) + Decision Tree (DT) algorithm is constructed, aiming to assess its value in evaluating smart city (SC) security. Additionally, the impact of SCs on individuals’ mythical experiences is investigated.   Methods An information security analysis model based on the combination of KMC and DT algorithms is established. A total of 38 SCs are selected as the research objects for practical analysis. The practical feasibility of the model is assessed using the receiver operating characteristic (ROC) curve, and its performance is compared with that of the Naive Bayes (NB), Logistic Regression (LR), Random Forest (RF), Support Vector Machine (SVM), and Gradient Boosting Machine (GBM) classification methods. Lastly, a questionnaire survey is conducted to obtain and analyze individuals’ mythical experiences in SCs.   Results (1) The area under the ROC curve is significantly higher than 0.9 (0.921 vs. 0.9). (2) Compared to the NB and LR algorithms, the security analysis model based on the combination of KMC and DT algorithms demonstrated higher true positive rate (TPR), accuracy, recall, F-Score, AUC-ROC, and AUC-PR. Additionally, the performance metrics of RF, SVM, and GBM are similar to those of the KMC+DT model. (3) When the attributes are the same, the difference in smart risk levels is small, while when the attributes are different, the difference in risk levels is significant. (4) The support rates for various types of new folk activities are as follows: offline shopping festivals (17.6%), New Year’s Eve celebrations (16.7%), Tibet tourism (15.6%), spiritual practices (16.2%), green leisure (16.0%), and suburban/rural tourism (15.8%). (5) High-risk cities (Grade A) showed stronger support for modern activities such as offline shopping festivals and green leisure, while low-risk cities (Grades C and D) tended to favor traditional cultural activities.   Conclusion The algorithm model constructed in this work is capable of effectively evaluating the information security risks of SCs and has practical value. A good city image and mythological experience are driving the development of cities.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
A Quantitative Analysis of Information Systems Management in the Educational Industry,"1. Purpose: One of the consequences of the COVID-19 pandemic period was the migration of educational centers from face-to-face learning to e-learning. Most centers adapted their educational services and technological resources so that the students could attend the courses online and the teachers (and the rest of the staff) could telework. So, technology departments have become critical in educational services and need to adapt their processes. The ITIL (Information Technology Infrastructure Library) standard guides companies for this transformation. If educational centers are involved in digital transformation, the question to solve is the following: How far are the processes used in the technology departments of educational centers from the ITIL standard adopted in the information technology industry? The purpose of this research was to investigate whether technology departments have implemented the necessary processes. 2. Methods. The research was conducted by means of an online form sent to educational organizations to gather information about their technological processes. The responses collected from the web forms were statistically analyzed. 3. Results and conclusion. The main finding in this paper was that technology departments in educational centers have yet to adopt the processes required for an intensive online service, demonstrating a weakness in educational institutions.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Benchmark f¨ur ein personalisiertes Empfehlungssystem mit zeitlicher Segmentierung basierend auf Assoziationsregeln,"In der heutigen Zeit ist es ¨ublich geworden, im Online-Marketing Empfehlungssysteme einzusetzen. Im Folgenden wird ein Empfehlungsalgorithmus vorgestellt, der auf Basis von Kau�?nformationen mit Hilfe von Assoziationsregeln und unter Ber¨ucksichtigung des Zeitpunkts des Kaufs personalisierte Produktempfehlungen generiert. Des Weiteren wird ein Benchmark mit dem vorgestellten, einem auf Assoziationsregeln basierenden und einem ItemKNN-Algorithmus unter Verwendung realer Kaufdaten erstellt. Die beiden zuletzt genannten Algorithmen wurden der Open-Source Bibliothek LibRec entnommen.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Quantitative assessment of information technology support in higher education management systems for student employability enhancement,"Abstract This paper synthesises the definition of information technology, the definition of students’ employability, and the principle of constructing the evaluation index system, and initially formulate the evaluation system of information technology-driven employability improvement. To ensure the accuracy of the evaluation index system, we used the Delphi method to conduct two rounds of screening. Finally, the evaluation index system for this paper was determined. Then, the hierarchical analysis algorithm is used to calculate the weight value of each index, which is substituted into the fuzzy comprehensive evaluation model as the input value to complete the quantitative assessment of the enhancement of students’ employment ability under the support of information technology. The results show that the affiliation matrix obeys the principle of maximum affiliation, and the quantitative assessment result of information technology-driven students’ employability enhancement in a school is deduced to be excellent, with the specific data expressed as 4.53601, which indicates that information technology fuels students’ employability enhancement and enables them to adapt to the society and the workplace more quickly.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Enhancing Explainability in AI Models: A Quantitative Comparison of XAI Techniques for Large Language Models and Healthcare Applications,"The growing need for Artificial intelligence (AI) in healthcare requires both accurate and explainable models. Elevating Transparency, Trust, and Decision-Making with Explainable AI in Medical Applications of LLMs. In this study, we conduct a comparative quantitative evaluation of the most important XAI methods (SHAP, LIME, and Attention-based mechanisms) for LLMs in healthcare. We evaluate these techniques in terms of interpretability, computational efficiency, fidelity, and clinical relevance. The findings underline trade-offs that matter, with SHAP offering very fine-tuned interpretation of model decisions at high computational costs, LIME giving additional insights by momentarily opening up the black-box model at moderate computational costs, and Attention-based methods providing clear alignment with predictions but no reasoning behind those predictions. This research contributes to the ethical and reliable deployment of AI in healthcare by revealing effective XAI strategies for improving clinical decisions and fostering trust among medical professionals and patients.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Optimizing Smart City Strategies: A Data-Driven Analysis Using Random Forest and Regression Analysis,"This study investigates the critical factors influencing smart city program success through a comprehensive data-driven analysis of 140 urban centers. Advanced machine learning techniques, specifically random forest algorithms, in conjunction with regression analysis, were employed to examine the correlations between 45 distinct attributes and respective smart city rankings. The findings reveal that the human development index (HDI) is a key predictor of smart city performance. Furthermore, the regression analysis revealed that elements such as education, healthcare, infrastructure, and digital services significantly enhance achieving higher HDI scores. Similarly, factors like education, sanitation, healthcare, and government transparency are closely associated with successfully implementing sharing platforms. These findings highlight the importance of investing in human capital, developing digital infrastructure, and promoting community engagement to create sustainable and resilient smart cities. Policymakers can utilize these findings to prioritize investments and devise effective strategies to improve their city’s ranking.",IS,BI_ANALYTICS,85,Clear IS paper with focus on Business Intelligence & Analytics
Quantitative of Atomic Spectra by Laser-Induced Teaching Integrating Multitarget Tracking Algorithm,"Laser-induced breakdown spectroscopy refers to the radiation spectrum formed by the vaporization and ionization of samples under high-temperature conditions. It is a new technology for detecting and analyzing the composition and content of materials. However, this technology is not yet very mature; in this regard, this paper proposes a quantitative study of atomic spectroscopy using laser-induced technology. On this basis, a method using laser-induced breakdown spectroscopy is proposed, and its quantitative analysis and application are carried out in detail. The laser-induced breakdown technology uses a high-power pulsed laser to form a plasma on the surface of the test piece and analyzes its radiation spectrum to obtain the composition and content of its elements. It also used the laser-induced breakdown technique to quantitatively analyze the Ti-doped Al2O3 ceramic system. Through the quantitative analysis of constant laser and constant plasma temperature, the linear correlation coefficient R2 of the calibration curve calculated with constant plasma temperature is obtained as 0.985, and the relative error is 5.3%. This paper uses a fixed laser pulse amplification voltage, the linear coefficient of the proportional relationship curve is only 0.92, and the error rate is 8.9%. By comparing the two curves, the Ti proportional curve obtained by a fixed plasma temperature is obtained in this paper. Its linearity is better than the Ti scaling curve obtained by constant laser voltage.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Comparative Analysis of Quantitative Predictive Models Utilizing Machine Learning for Business Decision Making,"Multivariate analysis is a critical decision-making tool as it allows business managers to understand and predict complex behavior. This is an outline of how various quantitative approaches under the Machine Learning paradigm contribute towards optimizing business decisions. The research sought to evaluate, with measurement of accuracy, the results of the predictive model developed by eight quantitative techniques applied with Machine Learning algorithms executed in Python with the aim of finding the best model to adequately estimate the output variable: price. In data processing, they were loaded, cleaned and then divided into two categories: 70% for training and 30% for testing. The models used in the comparative study were: multiple regression, Ridge regression, Lasso regression, Decision Tree, Gradient Boosting, Random Forest, Support Vector Regression and Neural Networks. Benchmark performance, according to measures of accuracy such as mean square error, mean absolute error, and R-squared, showed that the best fits were for the model developed using Gradient Boosting with an R² of 0.8175, followed by the Random Forest model with an R² of 0.7889. The two models mentioned above were the most stunning, as they provided the most ideal key indicators for business decision making.",IS,BI_ANALYTICS,85,Clear IS paper with focus on Business Intelligence & Analytics
A Quantitative Method for Selection of Enterprise Cloud Computing Models,"AbstractTargeting at methodological limitations in the decision-making of enterprise cloud computing adoption, this paper analyzes the main influence factors affecting enterprise cloud computing, builds up their hierarchical structures, and works out 8 key influence factors affecting cloud computing models, based on the structural model theory in system engineering. A quantitative method is provided for the selection of optimal or recommended enterprise cloud computing models, in terms of the relationship valuations among key influence factors, cloud computing models, and the factor weights for a specific enterprise.",IS,DSS,85,Clear IS paper with focus on Decision Support Systems
Model of the quantitative criterion calculation for security assessment of the information and telecommunications systems in the critical infrastructure of the state,"The subject of the article is methods and models for assessing the criticality of industry information and telecommunications systems (ITS). The purpose of this article is to analyze the existing methods and models of criticality assessment and use its results to propose a functional model for calculating the quantitative criterion for assessing the security of ITS. Results. Based on the known method of hierarchy analysis, a functional model for calculating the quantitative criterion for assessing ITS security is proposed, which, through the processing of expert assessments, allows to obtain a quantitative indicator of ITS security. This makes it possible to simplify the procedure for selecting experts, to avoid the specifics of processing expert data, as well as to assess ITS in a limited amount of statistics. Conclusions. The study showed that the developed model for calculating the quantitative criterion for assessing the security of ITS, using pairwise comparisons, allows experts to focus on the problem. In addition, the proposed model has a built-in quality criterion of the expert and allows to move from a qualitative assessment in the form of an ordered series of alphanumeric combinations, to a quantitative assessment in the form of the ratio of the basic security profile to the security profile defined by the expert.",IS,IS_SEC,85,Clear IS paper with focus on Information Systems Security
Enhancing Management Information Systems through Signal Processing: A Quantitative and Simulation-Based Evaluation,"This research focuses on signal processing techniques in Management Information Systems to increase data accuracy and processing efficiency. The performance assessment for MIS integrated with signal processing in the study utilized simulation-based research, case studies, and quantitative analysis. To be precise, a prototype of the system was designed to be test-run with synthetic data to simulate real-life cases, leading to impressive noise reduction and enhancement of clarity in data. Additionally, these techniques are discussed through a case study-related approach, demonstrating various practical issues and advantages in enhancing organizations' capabilities for decision-making. Based on the DSR methodology, it was possible to develop and test a novel artifact including signal processing, in which an increase in the quality of data was identified, together with gains in the efficiency of its processing. Quantitative analysis via t-tests evidenced that the improvement was statistically significant by all key indicators, including MSE reductions and enhancements in processing time. These results show that embedding signal processing algorithms into MIS frameworks indeed ensures scalability and practicality for real-time applications along with enhancing system performance. The given research contributes to the growing field of MIS through its presentation of a robust framework for integrating techniques of advanced data processing effectively, thereby yielding better decision support systems and operational efficiency.",IS,DSS,85,Clear IS paper with focus on Decision Support Systems
Gender Wage Differentials in Information Systems,"This paper investigates trends and changes in the gender earnings gap for individuals employed in clerical and professional level information systems positions in the U.S. labor market for the period of 1991 through 2008. It examines changes in the earnings gap for IS workers, specifically considering changes relative to the so-called “Internet bubble” observed primarily during the late 1990s. Quantitative analysis of changes in the wage gap, adjusted for key determinants, is based on data from the Current Population Survey (CPS). Examination of these data suggests that the gender earnings gap is persistent despite frequent claims to the contrary from industry surveys and that the gap is narrower for professional level positions. Furthermore, the data suggest that female IS workers, particularly in professional level occupations, may have experienced a beneficial effect from the internet bubble, but it is unclear whether or not that beneficial effect may be fading in the post-bubble internet bust of the early 21st century.",IS,IT_ETHICS,85,Clear IS paper with focus on IT Ethics & Social Responsibility
Enterprise Competencies for Effective Information Systems and Information Management,"A study of small and medium sized enterprises (SMEs) in Wales was undertaken to evaluate the effectiveness and competence of users of information technology (IT) and information systems (IS). A questionnaire survey, followed by selected follow-up interviews, was conducted: to determine levels of information technology (IT) and information systems (IS) use by small and medium enterprises (SMEs) in Wales; to identify key factors influencing SMEs’ ability to exploit opportunities that technology offers; to investigate whether IT/IS is managed in an active and strategic fashion; to assess the use and management of data within these enterprises; and to consider whether SMEs have opportunities to create strategic advantage from IT/IS. The results indicated that the utilization of IT is increasing but the majority of SMEs are under-exploiting the potential benefits due to a lack of management direction. Nevertheless, although few SMEs are being seen to achieve competitive advantage, those enterprises that are doing so tend to be focusing on acquiring knowledge, using their flexibility and ability to react speedily and generating innovative solutions. Th study identified areas where further research was needed to support the findings, particularly in terms of the regularly review of the level and exploitation of IT/ IS within the SME sector within Wales and the rest of the UK via quantitative surveys providing a base for a longitudinal comparison of data; the investigation of the factors that deter IT/IS development within SMEs; and the review of IS/IT within individual SME industries and sectors, utilizing case study methodology and allowing for the qualitative analysis of key factors underpinning usage. (Part 1 of this article was published in the previous issue of Business Information Review).",IS,IT_GOV_STRAT,85,Clear IS paper with focus on IT Governance & Strategy
"Optimizing Management and Service Systems in Higher Education: A Quantitative Examination of Data Imaging, Interaction Systems, and Decision Support for Informed Decision-Making and Performance Enhancement","Making informed decisions and improving organizational performance are crucial in the modern, data-driven environment. These processes are significantly shaped by a number of variables, including Data Imaging, Interaction Systems, Decision Support Systems, IT Infrastructure, and Technology Readiness. Interaction Systems enable communication and teamwork, Data Imaging translates complex data into visual insights, and Decision Support Systems offer cutting-edge analytics. The IT infrastructure serves as the foundation of technology, and technology readiness measures how ready people and universities are to adopt new technologies.&amp;nbsp;This research aims to explore the interplay between these variables within the context of organizational change theory and their impact on organizational performance and decision-making. Additionally, it examines the moderating effect of Technology Readiness and the mediating role of IT Infrastructure in the organizational change process. Structural Equation Modeling (SEM) in AMOS is used to do this study quantitatively. A total of 450 professionals from various fields are surveyed using reliable questionnaires to compile this data. Within the context of organizational change theory, this study provides insights into the complex interactions between these factors and their combined impact on organizational performance and decision-making. It offers insightful information about how university management can use technology and human resources to improve decision-making procedures and overall performance results. This study adds to both practical and theoretical knowledge, providing concrete recommendations for firms trying to thrive in a technologically driven society. It also increases theoretical understanding by offering a comprehensive framework and putting light on the roles of IT Infrastructure, and Technology Readiness in the decision-making and performance improvement of universities.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Investigating Antecedents to Older Adults’ Uptake of Health Information Systems: A Quantitative Case Study of Electronic Personal Health Records,"Introduction and Aim: older adults are among the primary beneficiaries of health information systems such as electronic Personal Health Records (PHRs), yet they are notably underrepresented in existing research. This study addresses this gap by focusing on this vulnerable group, aiming to enhance their engagement with PHR systems to improve health management. This research seeks to identify the key determinants that influence the intention of older adults to utilize PHR systems, thereby supporting the development of more user-friendly health information technologies for this demographic. Methods: to better understand the functionality of PHR systems and improve user competence, hands-on workshops were conducted for 135 older adults. The workshops aimed to enhance their capability to manage chronic conditions using these systems. Post-training, participants’ intentions to use PHRs were assessed via a survey, applying the Health Belief Model (HBM) as the analytical framework. Data analysis was performed using Partial Least Squares (PLS) path modeling to underscore the robustness of the methodological approach. Findings: the data analysis of the participants’ responses uncovered that seniors’ usage intention toward PHRs is a function of the PHR’s perceived usefulness (p &lt; 0.01, OR = 2.5, 95% CI 1.5–4.1), perceived barriers (p &lt; 0.05, OR = 1.3, 95% CI 0.8–2.1), perceived confidence (p &lt; 0.05, OR = 1.6, 95% CI 1.0–2.5), and cues to action (p &lt; 0.05, OR = 1.8, 95% CI 1.1–2.9). Of these, perceived usefulness was found to be the strongest predictor. The results also indicate that perceived susceptibility and perceived severity did not significantly impact seniors’ intention to use PHRs. Conclusions: according to the findings and based on the HBM literature, if individuals perceive more benefits and fewer barriers to the use of PHRs, greater self-efficacy, and better cues to action, they are more likely to adopt the system.",IS,HIS,75,Matched IS paper to Healthcare Information Systems
Benchmark Fonksiyonları için Altın Kartal Optimizasyon Algoritmasının Parametrelerini Optimize Etme Optimizing The Parameters of The Golden Eagle Optimizer Algorithm for Benchmark Functions,"Bu çalışmada, Altın Kartal Optimizasyon (AKO) algoritmasının performansını iyileştirmek için AKO algoritmasının parametreleri optimize edilmiştir. Bu sayede algoritmanın parametresinin en iyi değerinin elde edileceği ve elde edilen parametre değerleri için algoritmanın daha kararlı bir işlem gerçekleştireceği öngörülmektedir. Algoritmanın parametre optimizasyonu birçok çalışmada kullanılmaktadır. AKO algoritmasının iki farklı parametre değeri vardır. Bu parametreler sırasıyla saldırı ve seyirdir. Seyir parametre değeri [0.5-1], saldırı parametresi değeri [0.5-2] arasındadır. Algoritmanın her bir parametre değeri için 23 farklı kıyaslama fonksiyonu üzerinde deneysel çalışmalar yapılmıştır. Deneysel çalışma sonuçlarında en iyi parametrelerin değerleri belirlenmeye çalışılmıştır. Unimodal benchmark test fonksiyonlarında seyir parametresi 0.75 değeri ile iyi sonuçlar elde etmiştir. Saldırı parametresi ise fonksiyonlara bağlı olarak 1.5'e yaklaştığında optimum sonuca doğru yakınsadığı tablo ve grafiklerde verilmiştir. Benzer şekilde, multimodal kıyaslama testi sonuçlarında, seyir parametresi 0.75 değeri ile benzer şekilde iyi sonuçlar hesaplamıştır. Fonksiyonların özelliklerine bağlı olarak, değer 1.5'e yaklaştıkça saldırı parametresinin değerinin daha iyi bir çözüm bulduğu tablo ve grafiklerde gösterilmiştir.",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
Geomorphological quantitative analysis of Sperchios River Basin area (Central Greece) utilizing geographical information systems.,"In the present study the analysis of the morphometric quantitative parameters of Sperchios river basin, and specifically of the 8 main sub-basins of the northern and southern part, have been made. The integrated use of a Geographic Information System (GIS) allows a thorough spatial analysis of the data derived from digital terrain spatial models that reveal the geomorphological characteristics of an area. The thorough analysis of the results shows the significant difference of the morphological characteristics of the northern and southern part of the catchment area, due to the impact of the neotectonic activity of the area, which creates this asymmetrical topography.",NON_COMPUTING,UNKNOWN,90,"The paper primarily focuses on the geomorphological analysis of a river basin using GIS, which is a tool often used in geography and earth sciences rather than computing. The emphasis is on the application of GIS for spatial analysis of terrain models to understand geomorphological characteristics, which aligns more closely with environmental science or geography. While GIS involves computational tools, the core focus here is not on developing or analyzing computing methodologies but rather on applying existing GIS technology to a specific scientific problem. If a computing-related classification were necessary, it could be loosely associated with Data Science & Analytics due to the spatial data analysis component, but this is secondary to the primary focus on geom"
A meta-analysis of semantic classification of citations,"Abstract The aim of this literature review is to examine the current state of the art in the area of citation classification. In particular, we investigate the approaches for characterizing citations based on their semantic type. We conduct this literature review as a meta-analysis covering 60 scholarly articles in this domain. Although we included some of the manual pioneering works in this review, more emphasis is placed on the later automated methods, which use Machine Learning and Natural Language Processing (NLP) for analyzing the fine-grained linguistic features in the surrounding text of citations. The sections are organized based on the steps involved in the pipeline for citation classification. Specifically, we explore the existing classification schemes, data sets, preprocessing methods, extraction of contextual and noncontextual features, and the different types of classifiers and evaluation approaches. The review highlights the importance of identifying the citation types for research evaluation, the challenges faced by the researchers in the process, and the existing research gaps in this field.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
The Effect of Online Communication and Use of Information Systems on Audit Quality,"During the Covid-19 pandemic it became a challenge for the Public Accounting Firm to carry out audit procedures. During the Covid-19 pandemic, physical meetings or communication between auditors or public accountants and a company's clients were limited, so these meetings, which were generally held face-to-face, became online meetings. The main obstacle for auditors or public accountants during the Covid-19 pandemic is that the audit procedures that must be carried out for the 2020 financial statements will be different from the previous year. In general, audit engagements or communication with clients to obtain audit evidence is carried out face-to-face, but to reduce the spread of the Covid-19 virus, such communication is carried out online. Of course, these communication issues must still be guided by Auditing Standard 260 regarding communication with those charged with governance. This study aims to determine the effect of online audit communication and the use of information systems on audit quality. This research is research with the aim of testing the hypothesis. Studies that engage in hypothesis testing usually explain the nature of a particular relationship or determine the independence of two or more factors in a situation. This study uses an explanatory method where the researcher wants to find the cause of one or more problems. This study aims to determine the effect of online audit communication and the use of information systems on audit quality. This research is research with the aim of testing the hypothesis. Studies that engage in hypothesis testing usually explain the nature of a particular relationship or determine the independence of two or more factors in a situation. This study uses an explanatory method where the researcher wants to find the cause of one or more problems. This study aims to determine the effect of online audit communication and the use of information systems on audit quality. This research is research with the aim of testing the hypothesis. Studies that engage in hypothesis testing usually explain the nature of a particular relationship or determine the independence of two or more factors in a situation. This study uses an explanatory method where the researcher wants to find the cause of one or more problems.",IS,SOC_COMP,85,Clear IS paper with focus on Social Computing & Collaboration
"Assessment of GTO: Performance evaluation via constrained benchmark function, and Optimized of Three Bar Truss Design Problem","The aim of this paper is to show that the artificial gorilla troops optimization (GTO) algorithm, as an optimizer, can cope with test functions such as CEC2019, and also to best optimize the three bar truss design problem as a constrained optimization problem. As a method, two statistical measures such as the best values provided by the algorithms and the standard deviation showing the distance between the values were studied. At the same time, the convergence rate of the algorithms compared by the convergence curves were examined. For this purpose, it has been competed against two other swarm-based algorithms, sine-cosine algorithm (SCA) and golden eagle optimization (GEO). The optimization of the three bar truss design problem, which is another side of the study, has been made. The GTO algorithm reached the best values in the optimization of the parameters of the problem. In addition to the convergence curve, statistical results have examined, and the advantages of GTO are revealed through box-plot figures that evaluate the relationship between median and quartiles and the distribution among all results.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Demonstration of Structural Topic Modeling on Charter School Closures in 2019,"While evidence often exists in local newspapers, Facebook pages, and on other platforms, a lack of centralization means that researchers looking to determine the causes of school closure suffer from the unenviable task of manually hunting for data. Worse still, once they collect the texts, researchers need to sift through them to determine the underlining causes for closure. This sifting leads to a variety of issues related to human error. This paper demonstrates the efficacy of using a Structural Topic Model (STM) to automate this last step, reduce human bias, and save time. Topic Modeling is a machine learning technique that builds on a base of artificial intelligence research that seeks to automate complex meta-cognitive tasks. This method is new to the education space, but the paper aims at demonstrating the potential uses by leveraging closure data for the 2018-19 school year. After testing this method on the 2018-19 school year, the researcher determined that the top two reasons for charter school closure at the end of 2019 were financial fraud and low academic performance.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Exploring Risks Transferred from Cloud-Based Information Systems: A Quantitative and Longitudinal Model,"With the growing popularity of Internet of Things (IoT) and Cyber-Physical Systems (CPS), cloud- based systems have assumed a greater important role. However, there lacks formal approaches to modeling the risks transferred through information systems implemented in a cloud-based environment. This paper explores formal methods to quantify the risks associated with an information system and evaluate its variation throughout its implementation. Specifically, we study the risk variation through a quantitative and longitudinal model spanning from the launch of a cloud-based information systems project to its completion. In addition, we propose to redefine the risk estimation method to differentiate a mitigated risk from an unmitigated risk. This research makes valuable contributions by helping practitioners understand whether cloud computing presents a competitive advantage or a threat to the sustainability of a company.",IS,IS_SEC,85,Clear IS paper with focus on Information Systems Security
Analysis of benchmark characteristics and benchmark performance prediction,"Standard benchmarking provides to run-times for given programs on given machines, but fails to provide insight as to why those results were obtained (either in terms of machine or program characteristics) and fails to provide run-times for that program on some other machine, or some other programs on that machine. We have developed a machine-imdependent model of program execution to characterize both machine performance and program execution. By merging these machine and program characterizations, we can estimate execution time for arbitrary machine/program combinations. Our technique allows us to identify those operations, either on the machine or in the programs, which dominate the benchmark results. This information helps designers in improving the performance of future machines and users in tuning their applications to better utilize the performance of existing machines. Here we apply our methodology to characterize benchmarks and predict their execution times. We present extensive run-time statistics for a large set of benchmarks including the SPEC and Perfect Club suites. We show how these statistics can be used to identify important shortcoming in the programs. In addition, we give execution time estimates for a large sample of programs and machines and compare these against benchmark results. Finally, we develop a metric for program similarity that makes it possible to classify benchmarks with respect to a large set of characteristics.",CS,CA_HW,85,Clear CS paper with focus on Computer Architecture & Hardware
Evaluating the scientific impact of research infrastructures: The role of current research information systems,"Abstract Research infrastructures (RIs) offer researchers a multitude of research opportunities and services and play a key role in the performance, innovative strength, and international competitiveness of science. As an important part of the generation and use of new knowledge and technologies, they are essential for research policies. Because of their strategic importance and their need for significant funding, there is a growing demand for the assessment of their scientific output and impact. Current research information systems (CRIS) have contributed for many years now to the evaluation of universities and research organizations. Based on studies on the application of CRIS to infrastructures and on a recent French report on the scientometric assessment of RI, this paper analyzes the potential of CRIS and their data models and standards (in particular the international CERIF format and the German RDC model) for the monitoring and evaluation of RIs. The interaction between functional specificities of RI and standards for their assessment is outlined, with reference to their own potential to stimulate and share innovation in the networks located inside and outside RIs. This societal challenge, more than an academic issue, is on the way to further harmonization and consolidation of shared and common RI metrics.",IS,IS_RM,85,Clear IS paper with focus on Information Systems Research Methods
Sustainable Development Using SPSS an Emerging Software Tool: A Quantitative Analysis,"Incorporating economic, social, and environmental considerations into policymaking and corporate practices is essential for achieving sustainable development, which is multidimensional. This study looks into how statistical software tool like SPSS can be used to test and model the processes of long-term growth in a number of different areas. The study looks at big datasets from a lot of different industries and regions to find out how they measure important sustainability indicators like carbon emissions, resource efficiency, and socioeconomic equity. This is done by using complex quantitative methods to look at the data. Integrating classic and modern statistical software is made novel in this study; which makes easier to simulate complicated sustainability patterns and gives policymakers and companies information they can put into practice. Using data mining, AI-enhanced features, and machine learning algorithms in these tools, the research finds links between sustainability factors and the success of different sustainability initiatives that were not known before. Even though these software tools have a lot of potential, the study points out some limitations like the data quality, complexity to judge the long-term benefits of sustainability and to make sure that different measurement frameworks used in different regions and industries. The findings demonstrate that businesses may enhance their long-term profitability and environmental impact by incorporating sustainable development objectives into their operations. Additionally, the research highlights the importance of using data to inform decision-making in the creation of sustainable policies. Using SPSS has several benefits, such as improving predictive capacities, processing massive information accurately, and providing a thorough study of sustainable development initiatives. This study adds to theoretical and practical knowledge by providing a data-driven strategy for achieving global sustainability objectives.",IS,BI_ANALYTICS,85,Clear IS paper with focus on Business Intelligence & Analytics
Sensitivity Analysis of Spatial Autocorrelation Using Distinct Geometrical Settings,"Inferences based on spatial analysis of areal data depend greatly on the method used to quantify the degree of proximity between spatial units - regions. These proximity measures are normally organized in the form of weights matrices, which are used to obtain statistics that take into account neighbourhood relations between agents. In any scientific field where the focus is on human behaviour, areal datasets are greatly relevant since this is the most common form of data collection (normally as count data). The method or schema used to divide a continuous spatial surface into sets of discrete units influences inferences about geographical and social phenomena, mainly because these units are neither homogeneous nor regular. This article tests the effect of different geometrical data aggregation schemas - administrative regions and hexagonal surface tessellation - on global spatial autocorrelation statistics. Two geographical variables are taken into account: scale (resolution) and form (regularity). This is achieved through the use of different aggregation levels and geometrical schemas. Five different datasets are used, all representing the distribution of resident population aggregated for two study areas, with the objective of consistently test the effect of different spatial aggregation schemas.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Analysis of Fuzzy Clustering for the Adoption in Data Mining,"Data mining is the general methodology for retrieving useful information from big data. Clustering analysis is a mathematical method of classification for unsupervised machine learning. It can be adopted for data classification in Data mining. This paper combines the clustering process by fuzzy way and then deduces a special clustering algorithm with fast fuzzy c-means (FFCM) method. In summary, the paper illustrates the adoption of a series of fuzzy clustering methods in Data Mining. These methods have improved the computational efficiency with learning as the convergence speed is fast. The methodology of this paper presents significantly meaningful for information retrieval of big data.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Problems of Quantitative Models in Large Management Information Systems: A Comment,"In a recent article, Rosenman [Rosenman, B. B. 1980. Problems of quantitative models in large management information systems. Interfaces 10 (2, April) 102–105.] describes and arborescent inventory system where each stage or level obtains its parts from a uniques immediate predecessor and distributes these parts to a unique set of immediate successors. Various approaches have been proposed for determining optimal and near-optimal single cycle policies for deterministic arborescent systems for a single product. Because of complexity, the use of these approaches for large scale problems proved to be inappropriate. Some alternative approaches are also considered.",IS,SCMS,85,Clear IS paper with focus on Supply Chain Management Systems
ConVision Benchmark: A Contemporary Framework to Benchmark CNN and ViT Models,"Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have shown remarkable performance in computer vision tasks, including object detection and image recognition. These models have evolved significantly in architecture, efficiency, and versatility. Concurrently, deep-learning frameworks have diversified, with versions that often complicate reproducibility and unified benchmarking. We propose ConVision Benchmark, a comprehensive framework in PyTorch, to standardize the implementation and evaluation of state-of-the-art CNN and ViT models. This framework addresses common challenges such as version mismatches and inconsistent validation metrics. As a proof of concept, we performed an extensive benchmark analysis on a COVID-19 dataset, encompassing nearly 200 CNN and ViT models in which DenseNet-161 and MaxViT-Tiny achieved exceptional accuracy with a peak performance of around 95%. Although we primarily used the COVID-19 dataset for image classification, the framework is adaptable to a variety of datasets, enhancing its applicability across different domains. Our methodology includes rigorous performance evaluations, highlighting metrics such as accuracy, precision, recall, F1 score, and computational efficiency (FLOPs, MACs, CPU, and GPU latency). The ConVision Benchmark facilitates a comprehensive understanding of model efficacy, aiding researchers in deploying high-performance models for diverse applications.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Computational Analysis of Quantitative Characteristics of some Residual Properties of Solvable Baumslag-Solitar Groups,"Let $G_{k}$ be defined as $G_{k} = \langle a, b;\ a^{-1}ba = b^{k} \rangle$, where $k \ne 0$. It is known that, if $p$ is some prime number, then $G_{k}$ is residually a finite $p$-group if and only if $p \mid k - 1$. It is also known that, if $p$ and $q$ are primes not dividing $k - 1$, $p &lt; q$, and $\pi = \{p,\,q\}$, then $G_{k}$ is residually a finite $\pi$-group if and only if $(k, q) = 1$, $p \mid q - 1$, and the order of $k$ in the multiplicative group of the field $\mathbb{Z}_{q}$ is a $p$\-number. This paper examines the question of the number of two-element sets of prime numbers that satisfy the conditions of the last criterion. More precisely, let $f_{k}(x)$ be the number of sets $\{p,\,q\}$ such that $p &lt; q$, $p \nmid k - 1$, $q \nmid k - 1$, $(k, q) = 1$, $p \mid q - 1$, the order of $k$ modulo $q$ is a $p$\-number, and $p$, $q$ are chosen among the first $x$ primes. We state that, if $2 \leq |k| \leq 10000$ and $1 \leq x \leq 50000$, then, for almost all considered $k$, the function $f_{k}(x)$ can be approximated quite accurately by the function $\alpha_{k}x^{0.85}$, where the coefficient $\alpha_{k}$ is different for each $k$ and $\{\alpha_{k} \mid 2 \leq |k| \leq 10000\} \subseteq (0.28;\,0.31]$. We also investigate the dependence of the value $f_{k}(50000)$ on $k$ and propose an effective algorithm for checking a two-element set of prime numbers for compliance with the conditions of the last criterion. The results obtained may have applications in the theory of computational complexity and algebraic cryptography.",CS,CT_COMPLEX,85,Clear CS paper with focus on Computational Theory & Complexity
Performance Analysis of SMDO Method with Benchmark Functions with Matlab Toolbox,"SMDO method is a set and trial based optimization algorithm that is developed for online fine-tuning of controller parameters. SMDO method is implemented for several controller tuning applications. It can search parameter space with random backward and forward steps of each parameter. This property reduces risk of testing unstable control system configurations in controller design and thus makes the SMDO method more suitable for online parameter tuning of experimental systems. However, performance of SMDO has not been evaluated previously for benchmark functions in comparison with other well known heuristic optimization methods. This study aims to compare performances of Artificial Bee Colony (ABC), Cuckoo Search Optimization (CK), Particle Swarm Optimization (PSO) and Stochastic Multi-parameters Divergence Optimization (SMDO) methods for benchmark functions. Therefore, a benchmark tests program that is a user-friendly MATLAB GUI is introduced for user. This program can be downloaded from https://www.mathworks.com/matlabcentral/fileexchange/75043-smdo-method-with-benchmark-functions",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
A heuristic approach to the estimation of an efficient benchmark in the Croatian stock market Heuristički pristup procjeni efikasnog benchmark indeksa na hrvatskom dioničkom tržištu,"In this paper a heuristic approach, which solely relies on risk parameter estimation, is pursued to estimate the efficient benchmark in the Croatian stock market. Optimisation method focused on risk parity is employed allowing investors to diversify risk by relying on equal risk contribu tion to achieve optimal portfolio diversification. Six different benchmarks related to risk parity method variations and covariance matrix estimations are examined in order to compare their performance with the capitalization-weighted counterpart. This allows insight regarding the po tential sources of differences in their risk-reward characteristics. Results in this study are based on 28 out-of-sample estimations in the period from April 2005 to March 2019. The findings do not show evidence of risk parity method being able to provide exposure to rewarded risk factors in the Croatian stock market. Moreover, regarding the diversification of unrewarded risks even the benchmark portfolio with the lowest reported volatility is more volatile than the CROBEX bench mark. However, if only expansion sub-period is analysed all examined benchmarks outperform the CROBEX benchmark with the factor risk parity portfolio based on two or more components reporting the lowest volatility. Overall the results show that risk parity portfolios do outperform the equally-weighted benchmark and that assuming equal correlations of portfolio constituents or applying statistical shrinkage method for their estimation yields better results than relying on the principal components analysis.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
INTERNET OF THINGS PLATFORM BENCHMARK: AN ARTIFICIAL INTELLIGENCE ASSESSMENT BENCHMARK DE LA PLATEFORME DE L'INTERNET DES OBJETS : UNE ÉVALUATION DE L'INTELLIGENCE ARTIFICIELLE,The Internet of Things (IoT) represents a transformative technological concept that seamlessly becomes a part of the Internet across all industries. Artificial intelligence (AI) provides IoT with new capabilities used to analyze data in real-time and make informed decisions. There is a wide array of IoT devices with different computational capabilities and AI accelerators that need to be compared. The current paper proposes a comparison between two single-board computers using an existing AI benchmark.,CS,IOT,85,Clear CS paper with focus on Internet of Things (IoT)
Quantitative Model for Economic Analyses of Information Security Investment in an Enterprise Information System,"AbstractThe paper presents a mathematical model for the optimal security-technology investment evaluation and decision-making processes based on the quantitative analysis of security risks and digital asset assessments in an enterprise. The model makes use of the quantitative analysis of different security measures that counteract individual risks by identifying the information system processes in an enterprise and the potential threats. The model comprises the target security levels for all identified business processes and the probability of a security accident together with the possible loss the enterprise may suffer. The selection of security technology is based on the efficiency of selected security measures. Economic metrics are applied for the efficiency assessment and comparative analysis of different protection technologies. Unlike the existing models for evaluation of the security investment, the proposed model allows direct comparison and quantitative assessment of different security measures. The model allows deep analyses and computations providing quantitative assessments of different options for investments, which translate into recommendations facilitating the selection of the best solution and the decision-making thereof. The model was tested using empirical examples with data from real business environment.",IS,IS_SEC,85,Clear IS paper with focus on Information Systems Security
LZ based Compression Benchmark on PE Files,"The key element in runtime compression is the compression algorithm itself, that is used during processing. It has to be small in enough in decompression bytecode size to fit in the final executable, yet have to provide the best possible compression ratio. In our work we benchmark the top LZ based compression methods on Windows PE (both exe and dll) files, and present the results including the decompression overhead and the compression rates.",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
Method for quantitative criterion based transformation of the video information alphabet,"Subject of study: technologies implemented in modern video coding algorithms to ensure the appropriate level of reliability in the conditions of their compact presentation. The goal is to develop a technology for transforming the alphabet of a video information based on a quantitative criterion while ensuring the required quality in networks. Objectives: to formulate requirements to video images in dynamic video surveillance systems; to analyze the existing factors leading to an imbalance between the compression and quality characteristics of existing video coding algorithms; to develop a technology for transforming the alphabet of a video information based on a quantitative criterion (attribute) for the best presentation of the encoded data; to develop a mathematical model for the formation of a quantitativeindicator for the transformation of the video images; to analyze the effectiveness of using the developed mathematical model for the formation of a quantitative indicator to provide the required trustworthiness of data for the video information resource; to assess the effectiveness of the developed technology for transforming the original message in terms of a quantitative indicator to ensure the best presentation of the encoded data; to investigate the dynamics of the probabilistic and statistical characteristics of the original message as a result of transformation according to the quantitative criterion of the significance of the elements. The research methods: compression coding methods implemented on the basis of the JPEG algorithms. The research results: a new approach has been proposed based on the transformation of the encoded alphabet of data by use of a quantitative criterion. A mathematical model has been developed for the formation of a quantitative attributethat determines the significance of the elements of the original message. Conclusions. A technology has been developed for transforming the alphabet of the original message, which allows creating conditions for a more profitable presentation of the encoded data due to a significant increase in the dynamic range of probabilistic and statistical characteristics for the transformed message while ensuring the required level of video image quality.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
An Integrative Model of Information Systems Spending Growth,"This paper develops a model of the growth of information systems expenditures in the United States. The model incorporates two major factors that influence the rate and pattern of spending growth—the diffusion of technological innovation and the effect of price on the demand for computing. Traditional studies have focused on the role of innovation while ignoring the effects of price on the growth process. We show that while information systems expenses initially grew following an S-curve, more recent growth has converged to an exponential pattern. These patterns are consistent with our integrative price-adjusted S-curve growth model.",IS,DIGITAL_STRAT,85,Clear IS paper with focus on Digital Strategy & Business Models
The Effect Celebrity Endorser and Brand Equity to Purchase Decision in E- Commerce Tokopedia,"This study aims to determine the effect of Celebrity Endorser on elements of Brand Equity and its impact on purchasing decisions in one e-Commerce in Indonesia, Tokopedia. The number of samples in this study are 111 respondents who have used Tokopedia. The data collection method uses a questionnaire. In this study, Brand Equity itself divided into 4 dimensions hence the researchers look at the correlation between Celebrity Endorser and each dimension of Brand Equity, and purchase decision.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
The quantitative crunch,"PurposeSmall and specialist inter‐disciplinary conferences, particularly those relating to technology enhanced learning such as International Conference on Information and Communications Technology in Education, provide valuable opportunities for academics and academic‐related/professional staff to report upon their research and development activities, including their insights into teaching practice. However, the existence of such conferences is now under threat due to a global shift towards quantitative research assessment exercises, which favour bibliometrics, such as citation counts and impact factors, over peer review. The purpose of this paper is to contextualise the discussion by describing the nascent qualitative research assessment in Australia and its implications for small conferences. It also aims to present heuristic strategies to ensure that publications are recognised by quantitative research assessment exercises.Design/methodology/approachThe authors draw on a wide literature base as well as their experience as academics, conference organizers, professional developers, and researchers to describe the changes to the culture of research assessment and research management and their observed implications for small and specialist inter‐disciplinary conferences.FindingsConference organizers and scientific committees should consider several strategies to maximise bibliometric impact of conference papers. These strategies include: transparency in reviewing processes; building alliances with peer‐reviewed journals; considering boutique “by invitation” conference formats; and publishing papers which are indexed and standards based. The authors also point out that small and specialist conferences should leverage their communities of practice to facilitate publication and research opportunities and thereby increase the tangible benefits of participation.Originality/valueThis paper is valuable to conference organizers and participants who are adjusting to a culture of bibliometrics. This paper highlights key issues as well as suggests strategies to improve impact values.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Color as a High-Value Quantitative Tool for PET/CT Imaging,"The successful application of artificial intelligence (AI) techniques for the quantitative analysis of hybrid medical imaging data such as PET/CT is challenged by the differences in the type of information and image quality between the two modalities. The purpose of this work was to develop color-based, pre-processing methodologies for PET/CT data that could yield a better starting point for subsequent diagnosis and image processing and analysis. Two methods are proposed that are based on the encoding of Hounsfield Units (HU) and Standardized Uptake Values (SUVs) in separate transformed .png files as reversible color information in combination with .png basic information metadata based on DICOM attributes. Linux Ubuntu using Python was used for the implementation and pilot testing of the proposed methodologies on brain 18F-FDG PET/CT scans acquired with different PET/CT systems. The range of HUs and SUVs was mapped using novel weighted color distribution functions that allowed for a balanced representation of the data and an improved visualization of anatomic and metabolic differences. The pilot application of the proposed mapping codes yielded CT and PET images where it was easier to pinpoint variations in anatomy and metabolic activity and offered a potentially better starting point for the subsequent fully automated quantitative analysis of specific regions of interest or observer evaluation. It should be noted that the output .png files contained all the raw values and may be treated as raw DICOM input data.",CS,CG_VIS,85,Clear CS paper with focus on Computer Graphics & Visualization
Applying the Deep Learning Algorithm in Order to Provide a Model for Predicting the Financial Risk Management of Companies with a Quantitative Approach,"Companies face various risks during their business and financial cycle. These risks can be divided into different categories so that they can be identified and evaluated more easily. Therefore, it is important to monitor and accept changes caused by structural failures in the risk management process. The purpose of this research is to use deep learning algorithm in order to provide a model for predicting the financial risk of companies. Therefore, it is practical in terms of purpose. In terms of information gathering method, the research was library based and based on literature and theoretical background. It was also a quantitative research approach. Therefore, there is a need to provide a community and local model of risk forecasting that fits the financial and economic structure of companies active in Iran's capital market. In a small part of the statistical community, there were companies active in the capital market of Iran.The statistical sample was based on the method of systematic targeting of 199 active companies in the stock market between 2013 and 2014. The results of this research can lead to the expansion of the theoretical foundations of past researches related to predicting the financial risk of companies admitted to the stock exchange in Iran. In addition, the evidence of the research will show that this issue as a scientific achievement can provide useful information to the compilers of financial standards as well as the supervisory institutions of the country. Also, the results of the research can suggest new ideas for conducting new researches in the field of financial engineering, financial management and economics. The results showed that the error values of the training models in the deep learning approach in all cases of Lasso regression, ridge regression, artificial neural network training. And the random forest regression was less than 0.05.And the best method for machine learning is to use the combined method of ridge regression and artificial neural network.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Quantitative Guidelines for Telephone Information Systems,"The results of four sequential experiments were combined into integrated empirical models using data bridging. The resulting regression models can be viewed as quantitative design guidelines for telephone-based information systems. A total of ten independent variables involving environmental, hardware, dialogue, and user factors were considered across the four sequential studies that evaluated a telephone-based interface. Three dependent variables including total search time, user added keypresses, and message transcription accuracy were evaluated in each separate study. Polynomial regression was used to generate an integrated second-order empirical model for each of the three dependent variables. The major contributors to total search time were the time delay between the presentation of each menu item (input timeout) and the structure of the menus. Age of the user and menu structure were the primary contributors to user added keypresses required to recover from errors. Overall, the accuracy of message transcription was influenced primarily by background music, presentation rate of the synthesized speech, and the age of the user. Total search time in this sequential type of information delivery system is primarily dialogue driven. Critical dialogue parameters for this system were input timeout and the number of alternatives in menus. Due to the small number of errors in searching for information, the need to minimize user added keypresses for error correction was not of primary design importance. Accuracy of speech message transcriptions was primarily dependent upon the acoustical environment of the listener. Overall, the use of integrated empirical models offers several advantages including a basis for generalization across several studies and the ability to conduct specific interface design tradeoffs.",IS,IS_RM,85,Clear IS paper with focus on Information Systems Research Methods
The Production of Information Services: A Firm-Level Analysis of Information Systems Budgets,"Previous research has demonstrated that the production of information services can be characterized at the aggregate economy-wide level by the Cobb-Douglas production function. However, the underlying production process at the firm level has not yet been ascertained. The objective of this paper is to determine the form of the production process for information systems services at the firm level by conducting an empirical analysis of IS budget data. The production of information services is modeled using a production function with two inputs, hardware and personnel. We estimate various econometric specifications to determine several characteristics of the provision of information services, including the allocation of the information systems budget to its two largest components—hardware and personnel—and its implications for the form of the production function. After controlling for industry sector, we find that the ratio of personnel to hardware is independent of scale, which indicates a homothetic production function. We also find that the ratio of factor shares is constant with time, consistent with the Cobb-Douglas production function.We conclude that the underlying form of the production function is the same at the level of both the firm and the economy. Our analysis demonstrates how the application of production theory to the production of information services can yield useful insights from both a theoretical and managerial perspective.",IS,ISM,85,Clear IS paper with focus on Information Systems Management
Common Nearest Neighbor Clustering—A Benchmark,"Cluster analyses are often conducted with the goal to characterize an underlying probability density, for which the data-point density serves as an estimate for this probability density. We here test and benchmark the common nearest neighbor (CNN) cluster algorithm. This algorithm assigns a spherical neighborhood R to each data point and estimates the data-point density between two data points as the number of data points N in the overlapping region of their neighborhoods (step 1). The main principle in the CNN cluster algorithm is cluster growing. This grows the clusters by sequentially adding data points and thereby effectively positions the border of the clusters along an iso-surface of the underlying probability density. This yields a strict partitioning with outliers, for which the cluster represents peaks in the underlying probability density—termed core sets (step 2). The removal of the outliers on the basis of a threshold criterion is optional (step 3). The benchmark datasets address a series of typical challenges, including datasets with a very high dimensional state space and datasets in which the cluster centroids are aligned along an underlying structure (Birch sets). The performance of the CNN algorithm is evaluated with respect to these challenges. The results indicate that the CNN cluster algorithm can be useful in a wide range of settings. Cluster algorithms are particularly important for the analysis of molecular dynamics (MD) simulations. We demonstrate how the CNN cluster results can be used as a discretization of the molecular state space for the construction of a core-set model of the MD improving the accuracy compared to conventional full-partitioning models. The software for the CNN clustering is available on GitHub.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Learning failure in information systems development,"Abstract. Abstract. Information systems development is a high‐risk undertaking, and failures remain common despite advances in development tools and technologies. In this paper, we argue that one reason for this is the collapse of organizational intelligence required to deal with the complexities of systems development. Organizations fail to learn from their experience in systems development because of limits of organizational intelligence, disincentives for learning, organizational designs and educational barriers. Not only have many organizations failed to learn, but they have also learned to fail. Over time they accept and expect poor performance while creating organizational myths that perpetuate short‐term optimization. This paper illustrates learning failure in systems development and recommends tactics for overcoming it.",IS,KM,85,Clear IS paper with focus on Knowledge Management
Heart Disease Prediction: A Machine Learning Approach with Higher Accuracy,"Heart disease, a prevalent cardiovascular condition, poses significant health risks and affects millions worldwide. The alarming rise in heart disease cases in recent years demands proactive measures, making early prediction of these conditions crucial and concerning. By employing machine learning techniques, this study aims to identify patients who are more susceptible to heart disease based on diverse medical attributes. The Heart Disease Dataset from Kaggle, consisting of 1025 samples and 14 features, was incorporated into this investigation. And after preprocessing the dataset by removing duplicate and null values and implementing statistical imputation and several data graphs, like a scatter plot, box plot, histogram, etc., we split it into training and testing datasets and apply SMOTE technique on the training one. Various machinelearning approaches were used in this study, out of which the optimized decision tree gave the best accuracy of 98.96%.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
A Multi-Objective Framework for Balancing Fairness and Accuracy in Debiasing Machine Learning Models,"Machine learning algorithms significantly impact decision-making in high-stakes domains, necessitating a balance between fairness and accuracy. This study introduces an in-processing, multi-objective framework that leverages the Reject Option Classification (ROC) algorithm to simultaneously optimize fairness and accuracy while safeguarding protected attributes such as age and gender. Our approach seeks a multi-objective optimization solution that balances accuracy, group fairness loss, and individual fairness loss. The framework integrates fairness objectives without relying on a weighted summation method, instead focusing on directly optimizing the trade-offs. Empirical evaluations on publicly available datasets, including German Credit, Adult Income, and COMPAS, reveal several significant findings: the ROC-based approach demonstrates superior performance, achieving an accuracy of 94.29%, an individual fairness loss of 0.04, and a group fairness loss of 0.06 on the German Credit dataset. These results underscore the effectiveness of our framework, particularly the ROC component, in enhancing both the fairness and performance of machine learning models.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
IMPROVING DIGITAL SIGNATURE VERIFICATION ACCURACY THROUGH SUPPORT VECTOR MACHINE LEARNING: A COMPARATIVE STUDY,"Digital signatures are widely used in electronic documents, and their verification is crucial to ensure document authenticity and security. However, digital signature verification can be challenging, especially when dealing with large amounts of data. In this paper, we present a comparative study of three Support Vector Machine (SVM) based methods for improving digital signature verification accuracy. We used a dataset of 10,000 digital signatures and compared the performance of linear SVM, polynomial SVM, and radial basis function (RBF) SVM. Our results showed that all three SVM-based methods improved the accuracy of digital signature verification compared to traditional methods. The RBF SVM method was found to be the most effective method for improving accuracy, with an accuracy of 98%.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
An Exploratory Landscape Analysis-Based Benchmark Suite,"The choice of which objective functions, or benchmark problems, should be used to test an optimization algorithm is a crucial part of the algorithm selection framework. Benchmark suites that are often used in the literature have been shown to exhibit poor coverage of the problem space. Exploratory landscape analysis can be used to quantify characteristics of objective functions. However, exploratory landscape analysis measures are based on samples of the objective function, and there is a lack of work on the appropriate choice of sample size needed to produce reliable measures. This study presents an approach to determine the minimum sample size needed to obtain robust exploratory landscape analysis measures. Based on reliable exploratory landscape analysis measures, a self-organizing feature map is used to cluster a comprehensive set of benchmark functions. From this, a benchmark suite that has better coverage of the single-objective, boundary-constrained problem space is proposed.",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
A Comparative Analysis Using Different Machine Learning: An Efficient Approach for Measuring Accuracy of Face Recognition,"Feature extracting and training module can be done by using face recognition neural learning techniques. Moreover, these techniques are widely employed to extract features from human images. Some detection systems are capable to scan the full body, iris detection, and finger print detection systems. These systems have deployed for safety and security intension. In this research work, we compare different machine learning algorithms for face recognition. Four supervised face recognition machine-learning classifiers such as Principal Component Analysis (PCA), 1-nearest neighbor (1-NN), Linear Discriminant Analysis (LDA), and Support Vector Machine (SVM) are considered. The efficiency of multiple classification systems is also demonstrated and tested in terms of their ability to identify a face correctly. Face Recognition is a technique to identify faces of people whose images are stored in some databases and available in the form of datasets. Extensive experiments conducted on these datasets. The comparative analysis clearly shows that which machine-learning algorithm is the best in terms of accuracy of image detection. Despite the fact, other identification methods are also very effective; face recognition has remained a major focus of research due to its non-meddling nature and being the easy method of personal identification for people. The findings of this work would be useful identification of a suitable machine-learning algorithm in order to achieve better face recognition accuracy.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
QUANTITATIVE ASSESSMENT OF OPERATING CHARACTERISTICS OF PERSONAL DATA PROTECTION SYSTEMS IN MEDICAL INFORMATION SYSTEMS,"На основе анализа практических аспектов защиты персональных данных при автоматизированной обработке в организациях здравоохранения определен круг проблем, касающихся потребительского качества систем защиты информации. Одной из главных проблем защиты персональных данных в медицинских информационных системах является обеспечение своевременной настройки систем защиты информации администратором в соответствии с установленной политикой в организации. При этом ключевой проблемой является формирование условий работы администратора обеспечивающих стопроцентную гарантию реакции администратора на поступление заявок по настройке систем защиты информации, управлению пользователями, правами доступа, парированию угроз различной природы. В условиях отсутствия в настоящее время методических подходов к оценке временных (вероятностных) параметров деятельности администратора безопасности медицинских информационных систем, известных как операционные характеристики систем защиты информации, обеспечить стопроцентное соответствие настроек систем защиты информации текущей политике проблематично. В статье предложен вероятностный показатель для оценки операционных характеристик систем защиты информации. Разработана методика его оценки на основе эксперимента по фиксации движения курсора мыши при выполнении основных действий администратором и распределения его внимания (тепловой карты) по элементам интерфейса системы защиты информации. Представлены результаты оценок операционных характеристик системы защиты информации «Страж NT 3.0», выполненные с использованием предложенной экспериментальной методики Based on the analysis of the practical aspects of personal data protection (PD) during automated processing in healthcare organizations, a range of problems related to the consumer quality of information protection systems (ISS) has been identified. One of the main problems of PD protection in medical information systems (MIS) is to ensure the timely configuration of the information security system by the administrator in accordance with the established policy in the organization. At the same time, the key problem is the formation of the administrator's working conditions that provide one hundred percent guarantee of the administrator's reaction to the receipt of requests for setting up the information security system, managing users, access rights, and countering threats of various nature. In the absence of methodological approaches to assessing the temporal (probabilistic) parameters of the MIS security administrator's activities, known as the operational characteristics of the ISS, it is problematic to ensure that the ISS settings are 100% consistent with the current policy. The article proposes a probabilistic indicator for assessing the operational characteristics of the information security system. A methodology for its assessment was developed on the basis of an experiment on fixing the movement of the mouse cursor when performing basic actions by the administrator and distributing his attention (heat map) among the elements of the information security interface. The results of evaluations of the operational characteristics of the SZI ""Ctrazh NT 3.0"" carried out using the proposed experimental method are presented",IS,IS_SEC,85,Clear IS paper with focus on Information Systems Security
On the quantitative estimation of abstraction level increase in metaprograms,"Higher-level programming such as metaprogramming introduces a layer of abstraction above the domain language programs. Metaprogramming allows describing generic components and managing variability in a domain. It is especially useful for developing program generators for domains, where a great deal of commonalties exists. It allows increasing the level of abstraction and hiding details that are unnecessary to the designer. Information abstraction and hiding reduces the amount of 'user-visible' information. In this paper, we estimate the increase of abstraction by evaluating the information content at the lower (domain) and higher (meta) layers of abstraction. The estimation method is based on the Kolmogorov complexity and uses a common compression algorithm. The method is evaluated experimentally on families of DSP components.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Search Engine Optimization (SEO) in Promoting E-Commerce Start Aja. Com,"StartAja.com is an e-commerce designed to help connecting business people who have similar interests in certain business area. Currently, StartAja.com hasn’t have digital marketing strategy yet to simplify keyword searches in promoting StartAja.com. This research’s objective is to formulate a Search Engine Optimization (SEO) strategy. By using SEO, StartAja.com is expected to be the top ranked e-commerce in search engine websites. Research data are obtained through observing the interface and functionalities of StartAja.com, and also its promotion requirements. This research has discovered the right combination of keywords to be used in SEO, to help optimize the promotion of StartAja.com.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Pancomputationalism: Theory or metaphor?,"The theory that all processes in the universe are computational is attractive in its promise to provide an understandable theory of everything. I want to suggest here that this pancomputationalism is not sufficiently clear on which problem it is trying to solve, and how. I propose two interpretations of pancomputationalism as a theory: I) the world is a computer and II) the world can be described as a computer. The first implies a thesis of supervenience of the physical over computation and is thus reduced ad absurdum. The second is underdetermined by the world, and thus equally unsuccessful as theory. Finally, I suggest that pancomputationalism as metaphor can be useful.",CS,CT_COMPLEX,85,Clear CS paper with focus on Computational Theory & Complexity
Method for synthesis of information-learning systems (simulator) troubleshooting in radio-electronic objects,"Complication and fault tolerance of the equipment of complex radio-electronic objects (REO), incomplete coverage by means of internal diagnostic control of possible failures, the absence or presentation by the developers of diagnostic charts of troubleshooting in a tabular form, as well as the considerable remoteness of stationary points for carrying out control and diagnostic operations on them determined the prerequisites for the development of information and training systems (IOS) or simulators. The purpose of their development is to increase the efficiency of troubleshooting the equipment of electronic equipment in the places of their operation and the direct training of maintenance specialists. The IOS development method represents a system of approaches, principles, mathematical methods, models and techniques united by the unity of purpose for determining the laws of transformation of input information and justification based on the results obtained in terms of the maximum likelihood of trainees fulfilling the assigned tasks of troubleshooting in the electronic equipment for its optimal appearance (composition, technical characteristics and algorithms of functioning). In this case, the appearance of an IOS is structurally presented as a hierarchical system of functional elements for modeling parts (in the form of subsystems, complexes), components (means), functional blocks, boards and nodes of a specific REO, which are software equivalents of its control systems, information support and execution. The basis of the IOS synthesis is formed by the results of the analysis of a priori information on faults in the structure of complex electronic equipment. Using the methods of hierarchical decomposition and invariant immersion, a fault tree is formed in the structure of the REO at the levels of parts, components and functional blocks, boards and nodes. It is carried out on the basis of the sign of recognition of inhomogeneity (faults differ in time), eccentricity (the occurrence of several faults at the same time) and nonstationarity (various changes in the fault flow density are possible) of faults, their classification into elementary, group and multiple, which allows the method of systematic cover to form options for the structural and functional appearance of the IOS. The search for a solution is based on the use of graph theory, dynamic programming methods, branches and boundaries to substantiate the optimal (shortest) troubleshooting path by representing the elements of the REO structure in the IOS in the form of graph vertices and links (in the form of arcs) between them, identifying inherent in this level one or another malfunction. Information about faults in the IOS is formed on the basis of analysis, justification and optimization of troubleshooting functions using builtin diagnostic control points formed by a set of diagnostic tools built into the hardware and additional diagnostic control points (dialogue), decisions about faults in which are made in semi-automatic mode or manually trainees using general purpose test equipment. The search for the optimal solution is carried out on the basis of the formation of the region of feasible solutions formed on the Euler-Venn diagram in the form of the intersection of the troubleshooting functions and justification by the maximum element method of the optimal solution to eliminate the identified faults. The implementation of the method makes it possible to solve a complex multi-parameter optimization problem of troubleshooting in complex REO of a given subject area and to justify the requirements for the appearance of an IOS when developing a technical task for its development.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Heterogeneity Management Using OAEI Benchmark Dataset,"The evolution of ontologies and itsapplications are in various fields like artificial intelligence, reasoning, philosophy, biological science, and medical field. The components of ontologiesare concepts, instance, relationships, constraints, axioms and inference mechanism. Ontology is a main source for enabling interoperability in the semantic web. In this paper heterogeneities are identified between information systems and the possible rectification are carried out using OAEI benchmark datasets. Proposed method is compared with S-Match algorithm. The evaluation results shows that proposed method is performed better and structure changes of input ontologies not affect the results.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Transformation of Information Systems in the Tasikmalaya City Communication and Information Department,"Statistical data and information regarding government and regional development is a right of the community. But unfortunately, this has not been fully fulfilled in the community. This research then tries to look at the implementation of factors in managing information systems at the Tasikmalaya City Communication and Information Service. This research will be carried out using qualitative methods through case studies. The data used in this study came from observations and interviews. The results of this study then found that the management of information systems at the Tasikmalaya Office was still ineffective. Unprofessional executors and inadequate infrastructure have resulted in this. Several factors that can be useful in supporting information transformation at the Tasikmalaya City Service are data collection, data processing, data analysis and assessment, data presentation and dissemination, and documentation. Some of the inhibiting factors for this transformation are professional staff, infrastructure, and technology.",IS,ISM,85,Clear IS paper with focus on Information Systems Management
A Classification of Information Systems: Analysis and Interpretation,"Seventeen major types of information systems are identified and defined by vectors of their attributes and functions. These systems are then classified by numerical methods. The quantitative analysis is interpreted in terms of the development history of information system types. Two major findings are that the numerical classification autonomously follows the chronological appearance of system types and that, along the time line, systems have followed two major paths of development; these have been termed the applied artificial intelligence path and the human interface path. The development of new types of systems is considered within the framework of a theory of technological evolution. It is shown that newer types of systems result from gradual accretion of new technologies on one hand, and loss of older ones on the other. Conclusions are drawn concerning the value of taxonomy in studying information systems, in suggesting possible research directions, and the desirability of rationalizing research efforts within the IS discipline.",IS,IS_RM,85,Clear IS paper with focus on Information Systems Research Methods
The Accuracy of Supervised Learning Algorithm on Machine Learning Implementation: a Literature Review,"Machine Learning has become an integral element in technological development, having a significant impact on various sectors of life. This study explores the contribution of Machine Learning in big data processing, automated decision making, and predictive system development. The advantages of Machine Learning, especially in supervised learning, are emphasized by discussing algorithms such as regression, Support Vector Machines (SVM), and Neural Networks. Literature research includes five journals related to supervised learning applications, highlighting findings such as the effectiveness of the Random Forest algorithm in diagnosing pregnancy, the contribution of the SVM model in predicting student study periods, and the level of accuracy with the hybrid LSH and k-NN methods for weather prediction. The practical implementation of fruit detection using cameras shows real application in facilitating price checks and fruit recognition. In conclusion, the literature review confirms the potential and relevance of Machine Learning techniques, especially supervised learning, in providing solutions to various challenges in various sectors. It is recommended that further research explore different industrial sectors or specific case studies to gain a more comprehensive and relevant perspective on current trends in the development of Machine Learning techniques",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Prediction of Heart Disease using Decision Tree over Logistic Regression using Machine Learning with Improved Accuracy,"Aim: Predicting heart disease using the Decision Tree and comparing its feature extraction precision with the Logistic Regression algorithm for improving the accuracy of the prediction. Methods and Materials: In the proposed work, predicting heart disease was carried out using machine learning algorithms such as Logistic Regression (n=10) and Decision tree (n=10). Here the pretest power analysis was carried out with 80% and the sample size for the two groups are 20. Results: From the implemented experiment, the Decision Tree accuracy significantly better than the Logistic Regression 80.10%. There is a measurable 2-tailed huge distinction in accuracy for two algorithms is 0.001 (p&lt;0.05) Conclusion: The Decision Tree algorithm got better accuracy than Logistic Regression for Predicting heart disease.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Using Machine Learning for Quantum Annealing Accuracy Prediction,"Quantum annealers, such as the device built by D-Wave Systems, Inc., offer a way to compute solutions of NP-hard problems that can be expressed in Ising or quadratic unconstrained binary optimization (QUBO) form. Although such solutions are typically of very high quality, problem instances are usually not solved to optimality due to imperfections of the current generations quantum annealers. In this contribution, we aim to understand some of the factors contributing to the hardness of a problem instance, and to use machine learning models to predict the accuracy of the D-Wave 2000Q annealer for solving specific problems. We focus on the maximum clique problem, a classic NP-hard problem with important applications in network analysis, bioinformatics, and computational chemistry. By training a machine learning classification model on basic problem characteristics such as the number of edges in the graph, or annealing parameters, such as the D-Wave’s chain strength, we are able to rank certain features in the order of their contribution to the solution hardness, and present a simple decision tree which allows to predict whether a problem will be solvable to optimality with the D-Wave 2000Q. We extend these results by training a machine learning regression model that predicts the clique size found by D-Wave.",CS,QUANTUM,85,Clear CS paper with focus on Quantum Computing
"Sustainability, Accuracy, Fairness, and Explainability (SAFE) Machine Learning in Quantitative Trading","The paper investigates the application of advanced machine learning (ML) methodologies, with a particular emphasis on state-of-the-art deep learning models, to predict financial market dynamics and maximize profitability through algorithmic trading strategies. The study compares the predictive capabilities and behavioral characteristics of traditional machine learning approaches, such as logistic regression and support vector machines, with those of highly sophisticated deep learning architectures, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs). The findings underscore the fundamental distinctions between these methodologies, with deeply trained models exhibiting markedly different predictive behaviors and performance, particularly in capturing complex temporal patterns within financial data. A cornerstone of the paper is the introduction and rigorous analysis of a framework to evaluate models, by means of the SAFE framework (Sustainability, Accuracy, Fairness, and Explainability). The framework is designed to address the opacity of black-box ML models by systematically evaluating their behavior across a set of critical dimensions. It also demonstrates how models’ predictive outputs align with the observed data, thereby reinforcing their reliability and robustness. The paper leverages historical stock price data from International Business Machines Corporation (IBM). The dataset is partitioned into a training phase during which the models are calibrated, and a validation phase, used to evaluate the predictive performance of the generated trading signals. The study addresses two primary machine learning tasks: regression and classification. Classical models are utilized for classification tasks, with their outputs directly interpreted as trading signals, while advanced deep learning models are employed for regression, with predictions of future stock prices further processed into actionable trading strategies. To evaluate the effectiveness of each strategy, rigorous backtesting is conducted, incorporating visual representations such as equity curves to assess profitability and key risk metrics like maximum drawdown for risk management. Supplementary performance indicators, including hit rates and the incidence of false positions, are analyzed alongside the equity curves to provide a holistic assessment of each model’s performance. This comprehensive evaluation not only highlights the superiority of cutting-edge deep learning models in predicting financial market trends but also demonstrates the pivotal role of the SAFE framework in ensuring that machine learning models remain trustworthy, interpretable, and aligned with ethical considerations.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparison of the prediction accuracy of machine learning algorithms in crosslinguistic vowel classification,"AbstractMachine learning algorithms can be used for the prediction of nonnative sound classification based on crosslinguistic acoustic similarity. To date, very few linguistic studies have compared the classification accuracy of different algorithms. This study aims to assess how well machines align with human speech perception by assessing the ability of three machine learning algorithms, namely, linear discriminant analysis (LDA), decision tree (C5.0), and neural network (NNET), to predict the classification of second language (L2) sounds in terms of first language (L1) categories. The models were trained using the first three formants and duration of L1 vowels and fed with the same acoustic features of L2 vowels. To validate their accuracy, adult L2 speakers completed a perceptual classification task. The results indicated that NNET predicted with success the classification of all L2 vowels with the highest proportion in terms of L1 categories, while LDA and C5.0 missed only one vowel each. Furthermore, NNET exhibited superior accuracy in predicting the full range of above chance responses, followed closely by LDA. C5.0 did not meet the anticipated performance levels. The findings can hold significant implications for advancing both the theoretical and practical frameworks of speech acquisition.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing Salary Prediction Accuracy with Advanced Machine Learning Models,"Abstract. Accurate salary prediction is crucial for navigating the complexities of the job market and ensuring fair compensation practices. This research focuses on evaluating advanced machine learning models to improve salary prediction accuracy. The study integrates demographic, educational, and professional experience data to offer a comprehensive analysis of earning potential, aiming to foster equitable job markets and enhance strategic human resource planning. The methodology involves employing various models, including Linear Regression, Decision Tree, and Random Forest (RF). Key steps include preprocessing the dataset to address missing values, categorize data, and remove irrelevant features. The study finds that the RF model excels in predicting salaries, surpassing other models in performance metrics. This superior efficacy is attributed to RFs ability to handle complex, high-dimensional data and mitigate overfitting. The results of this study have significant implications for establishing fairer compensation practices and improving job market efficiency. By offering a reliable tool for understanding earning potential, this research contributes to better career decisions and strategic planning for both individuals and organizations. Future research will explore further refinements and applications of these models in real-world scenarios.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Optimizing data augmentation to improve machine learning accuracy on endemic frog calls,"The mountain chain of the Western Ghats on the Indian peninsula, a UNESCO World Heritage site, is home to about 200 frog species, 89 of which are endemic. Distinctive to each frog species, their vocalizations can be used for species recognition. Manually surveying frogs at night during the rain in elephant and big cat forests is difficult, so being able to autonomously record ambient soundscapes and identify species is essential. An effective machine learning (ML) species classifier requires substantial training data from this area. The goal of this study was to assess data augmentation techniques on a dataset of frog vocalizations from this region, which has a minimal number of audio recordings per species. Consequently, enhancing an ML model’s performance with limited data is necessary. We analyzed the effects of four data augmentation techniques (Time Shifting, Noise Injection, Spectral Augmentation, and Test-Time Augmentation) individually and their combined effect on the frog vocalization data and the public environmental sounds dataset (ESC-50). The effect of combined data augmentation techniques improved the model's relative accuracy as the size of the dataset decreased. The combination of all four techniques improved the ML model’s classification accuracy on the frog calls dataset by 94%. This study established a data augmentation approach to maximize the classification accuracy with sparse data of frog call recordings, thereby creating a possibility to build a real-world automated field frog species identifier system. Such a system can significantly help in the conservation of frog species in this vital biodiversity hotspot.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Effect of Feature Selection on the Accuracy of Machine Learning Model,"In real life data science problems, it’s almost rare that all the features in the dataset are useful for building a model. In machine learning, feature selection is the process of selecting a subset of relevant features or attributes for constructing a model. Removing irrelevant and redundant features and, selecting relevant features will improve the accuracy of a machine learning model. Furthermore, adding unnecessary variables to a model increases the overall complexity of the model. Our experiment indicates that the accuracy of a classification model is highly affected by the process of feature selection. We train three algorithms (K-Nearest Neighbors, Decision Tree, Multi-layer Perceptron) by selecting all the features and we got accuracies 49%, 84% and 71% accordingly. After doing some feature selection without any logical changes in models code the accuracy scores jumped to 82%, 86% and 78% accordingly which is quite impressive.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Prediction of Heart Disease using Forest Algorithm over Decision Tree using Machine Learning with Improved Accuracy,"Aim: To predict the heart disease using Forest Algorithm and comparing it with Decision Tree algorithm for improving the accuracy in predicting heart disease. Methods and Materials: Anticipating coronary illness expectation was completed utilising machine learning calculations, for example, Forest Algorithm and Decision tree. Here the pretest power analysis was carried out with 80% and the sample size for the two groups are 20. Results: Forest Algorithm accuracy is 90.00% while the Decision Tree algorithm has shown an accuracy of 85.00%. There is a measurable 2-tailed significant distinction in exactness for two calculations is 0.001 (p&lt;0.05) by performing independent samples T-tests. Conclusion: The Forest Algorithm accuracy is more significant and more accurate than the Decision Tree for predicting heart disease.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving Weather Forecasting Accuracy Using Machine Learning,"Weather forecasting has several applications in our daily lives, ranging from agriculture to event planning. Previous weather forecasting models relied on a complex combination of mathematical instruments, which was insufficient to achieve a higher categorization rate. We offer fresh revolutionary approaches for estimating monthly rainfall using machine learning algorithms in this study. Weather forecasts are created by gathering quantitative information about the current state of the atmosphere. Machine learning algorithms may learn complicated mappings from inputs to outputs using only samples and with little effort. The dynamic nature of the atmosphere makes accurate weather prediction challenging. The fluctuation in weather conditions in previous years must be used to anticipate future weather conditions. It is extremely likely that it will match within the next two weeks of the preceding year. We proposed using linear regressions with the Random forest algorithm to forecast weather using characteristics such as temperature, humidity and wind. It will forecast weather based on prior records thus, this prediction will be accurate.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine Learning Improves Accuracy of Virtual Flowmetering and Back-Allocation,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 192819, “Improving the Accuracy of Virtual Flowmetering and Back-Allocation Through Machine Learning,” by Pejman Shoeibi Omrani, SPE, Iulian Dobrovolschi, and Stefan Belfroid, SPE, TNO, and Peter Kronberger and Esteban Munoz, Wintershall Noordzee, prepared for the 2018 Abu Dhabi International Petroleum Exhibition and Conference, Abu Dhabi, 12–15 November. The paper has not been peer reviewed. In this study, the authors investigated a fully data-driven approach using artificial neural networks (ANNs) for real-time virtual flowmetering and back-allocation in production wells. The main goal was to develop computationally efficient data-driven models to determine multiphase production rates of individual phases (gas and liquid) in wells using existing measured data in fields. The results showed that ANNs were capable of estimating multiphase flow rates accurately in both simulated and field data. Introduction Virtual-flowmetering (VFM) methods are categorized in two types generally: physics-based models (hydrodynamical approach) and data-driven models. In the physics-based approach, multiphase flow rates are estimated by simplified hydrodynamical models using sensor data (e.g., pressure and temperature) as input parameters. The second approach is solely dependent on the available data in the field, performing statistical analysis on this data and deriving relations between input features and quantities of interest (in this case, multiphase flow rates). Such an approach requires a sufficient data set to train the models. In the context of data-driven VFM, such a data set could be obtained from periodic test-separator data or well tests. In this study, two types of ANNs were assessed: feed-forward (multilayer-perceptron) and recurrent [long short-term memory (LSTM)] to capture temporal dependencies. ANNs were used for real-time gas flowmetering at an individual well level using total production rates measured downstream of the combined production separators. Methodology The feed-forward ANN is a suitable method for modeling the relations between a set of features and output parameters by functional mapping of the features in the input layer to the outputs. Fig. 1 shows a schematic of a feed-forward ANN. The relation of the input to the output parameters is found by calibrating the weights and biases in the hidden (middle) layers. Depending on the complexity of the input/output relations, the number of hidden layers, and neurons therein, could be varied. The disadvantage of feed-forward networks is their inability to identify temporal trends in the data set.  Recurrent neural networks (RNNs) are suggested for prediction of systems in which states are changing continuously before reaching an equilibrium, or when the time-dependency of states is crucial for predicting these states. A common disadvantage of some RNNs is their inability to capture long-term dependencies in the data. Use of an LSTM network is suggested for predictions involving systems in which both short- and long-term dependencies are present. An LSTM network is composed of sever-al units of RNN, with each unit composed of a cell with an input, output, and forget gate.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine Learning Predicts Truck Breakdowns in Indonesia with 83% Accuracy,"PT. Varia Usaha Beton, a cement product company, faces frequent breakdowns of mixer trucks, reducing reliability from the target 90% to 60%. This study aims to predict truck breakdowns using a machine learning model based on the K-NN algorithm within the CRISP-DM framework. Data from the company's maintenance records were cleaned and split into training and testing sets. With k=20, the model achieved 90% accuracy on training data and 83% on testing data. These results can help improve maintenance scheduling and resource planning, enhancing truck reliability. Future research should compare other algorithms and consider different programming environments. Highlights: High Accuracy: K-NN model achieved 90% training and 83% testing accuracy. Maintenance Aid: Improves scheduling and resource planning for truck maintenance. Future Research: Compare algorithms and explore different programming environments. Keywords: Predictive Maintenance, Mixer Trucks, K-NN Algorithm, CRISP-DM, Machine Learning",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing Email Spam Filter's Accuracy Using Machine Learning,"In today's world, practically everyone uses emails on a regular basis. In our proposed research, we offer a machine learning-based technique for improving the accuracy of email spam filters. Traditional rule-based filters have become less effective as the number of spam emails has increased tremendously. Machine learning methods, particularly supervised learning, are often used to train models to determine if an email is spam or not. To achieve more accurate results when categorizing email spam, we need to build a simple and uncomplicated machine learning model. We chose the Naive Bayes strategy for our model since it is faster and more accurate than the rest of the algorithms. The recommended solution may be integrated into existing email systems to improve spam filtering capability. This review paper presents an outline of the machine learning model that we have proposed.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing Heart Disease Prediction Accuracy through Machine Learning Techniques and Optimization,"In the medical domain, early identification of cardiovascular issues poses a significant challenge. This study enhances heart disease prediction accuracy using machine learning techniques. Six algorithms (random forest, K-nearest neighbor, logistic regression, Naïve Bayes, gradient boosting, and AdaBoost classifier) are utilized, with datasets from the Cleveland and IEEE Dataport. Optimizing model accuracy, GridsearchCV, and five-fold cross-validation are employed. In the Cleveland dataset, logistic regression surpassed others with 90.16% accuracy, while AdaBoost excelled in the IEEE Dataport dataset, achieving 90% accuracy. A soft voting ensemble classifier combining all six algorithms further enhanced accuracy, resulting in a 93.44% accuracy for the Cleveland dataset and 95% for the IEEE Dataport dataset. This surpassed the performance of the logistic regression and AdaBoost classifiers on both datasets. This study’s novelty lies in the use of GridSearchCV with five-fold cross-validation for hyperparameter optimization, determining the best parameters for the model, and assessing performance using accuracy and negative log loss metrics. This study also examined accuracy loss for each fold to evaluate the model’s performance on both benchmark datasets. The soft voting ensemble classifier approach improved accuracies on both datasets and, when compared to existing heart disease prediction studies, this method notably exceeded their results.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
"Assessment of the Accuracy of Various Machine Learning Algorithms for Classifying Urban Areas through Google Earth Engine: A Case Study of Kabul City, Afghanistan","Accurate identification of urban land use and land cover (LULC) is important for successful urban planning and management. Although previous studies have explored the capabilities of machine learning (ML) algorithms for mapping urban LULC, identifying the best algorithm for extracting specific LULC classes in different time periods and locations remains a challenge. In this research, three machine learning algorithms were employed on a cloud-based system to categorize urban land use of Kabul city through satellite images from Landsat-8 and Sentinel-2 taken in 2023. The most advanced method of generating accurate and informative LULC maps from various satellite data and presenting accurate outcomes is the machine learning algorithm in Google Earth Engine (GEE). The objective of the research was to assess the precision and efficiency of various machine learning techniques, such as random forest (RF), support vector machine (SVM), and classification and regression tree (CART), in producing dependable LULC maps for urban regions by analyzing optical satellite images of sentinel and Landsat taken in 2023. The urban area was divided into five classes: built-up area, vegetation, bare-land, soil, and water bodies. The accuracy and validation of all three algorithms were evaluated. The RF classifier showed the highest overall accuracy of 93.99% and 94.42% for Landsat-8 and Sentinel-2, respectively, while SVM and CART had lower overall accuracies of 87.02%, 81.12%, and 91.52%, 87.77%, with Landsat-8 and Sentinel-2, respectively. The results of the present study revealed that in this classification and comparison, RF performed better than SVM and CART for classifying urban territory for Landsat-8 and Sentinel-2 using GEE. Furthermore, the study highlights the importance of comparing the performance of different algorithms before selecting one and suggests that using multiple methods simultaneously can lead to the most precise map.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Gradient domain machine learning with composite kernels: improving the accuracy of PES and force fields for large molecules,"Abstract The generalization accuracy of machine learning models of potential energy surfaces (PES) and force fields (FF) for large polyatomic molecules can be improved either by increasing the number of training points or by improving the models. In order to build accurate models based on expensive ab initio calculations, much of recent work has focused on the latter. In particular, it has been shown that gradient domain machine learning (GDML) models produce accurate results for high-dimensional molecular systems with a small number of ab initio calculations. The present work extends GDML to models with composite kernels built to maximize inference from a small number of molecular geometries. We illustrate that GDML models can be improved by increasing the complexity of underlying kernels through a greedy search algorithm using Bayesian information criterion as the model selection metric. We show that this requires including anisotropy into kernel functions and produces models with significantly smaller generalization errors. The results are presented for ethanol, uracil, malonaldehyde and aspirin. For aspirin, the model with composite kernels trained by forces at 1000 randomly sampled molecular geometries produces a global 57-dimensional PES with the mean absolute accuracy 0.177 kcal mol−1 (61.9 cm−1) and FFs with the mean absolute error 0.457 kcal mol−1 Å−1.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
EXPLORING THE ACCURACY AND RELIABILITY OF MACHINE LEARNING APPROACHES FOR STUDENT PERFORMANCE,"The purpose of this study is to examine the suitability of machine learning (ML) techniques for predicting students’ performance. By analyzing various ML algorithms, the authors assess the accuracy and reliability of these approaches, considering factors such as data quality, feature selection, and model complexity. The findings indicate that certain ML methods are more effective for student performance forecasting, emphasizing the need for a deliberate evaluation of these factors. This study provides significant contributions to the field of education and reinforces the growing use of ML in decision-making and student performance prediction.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Assessment of Machine Learning Techniques Through Accuracy Estimation,"ABSTRACT In this paper, we have worked on comparing various data mining algorithms using R tool and various comparison models. After comparison has been done, we have applied the best algorithm as per the result to make the prediction. In this paper, we worked on how to check algorithms on a dataset using R and find the most accurate algorithm model for our dataset. It cannot be easy to tell what algorithm to use on the dataset to get the best results. We don’t know the best parameters to use for particular algorithms. Here we worked on the strategy of trial and error to choose the accurate algorithm. Data set is used to train models and a test option is used to evaluate the model. Test metrics are used for comparison. Worked on the various models for which model to choose, to configure them, and pre-process them using data. Applied various techniques for comparing the accuracy of constructed models. We have worked on some algorithms compared them and after choosing one algorithm we can improve the result of algorithms by tuning various algorithms parameters by combining or changing parameters. Once we find the best algorithm applied on the dataset for prediction and tried to improve it by changing various criteria. Here in this paper, we have worked on the decision tree and tried to find out the best-resulting model for prediction. This learning opportunities can be further used in affordable energy, agriculture, and environmentally sound technologies etc. Keywords: SVM, Support Vector Machine; RF, Random Forest; LR, Linear Regression.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Accuracy Assessment of Machine Learning Algorithms Used to Predict Breast Cancer,"Machine learning (ML) was used to develop classification models to predict individual tumor patients’ outcomes. Binary classification defined whether the tumor was malignant or benign. This paper presents a comparative analysis of machine learning algorithms used for breast cancer prediction. This study used a dataset obtained from the National Cancer Institute (NIH), USA, which contains 1.7 million data records. Classical and deep learning methods were included in the accuracy assessment. Classical decision tree (DT), linear discriminant (LD), logistic regression (LR), support vector machine (SVM), and ensemble techniques (ET) algorithms were used. Probabilistic neural network (PNN), deep neural network (DNN), and recurrent neural network (RNN) methods were used for comparison. Feature selection and its effect on accuracy were also investigated. The results showed that decision trees and ensemble techniques outperformed the other techniques, as they both achieved a 98.7% accuracy.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Evaluating machine learning techniques for fluid mechanics: Comparative analysis of accuracy and computational efficiency,"This study focuses on applying machine learning (ML) techniques to fluid mechanics problems. Various ML techniques were used to create a series of case studies, where their accuracy and computational costs were compared, and behavior patterns in different problem types were analyzed. The goal is to evaluate the effectiveness and efficiency of ML techniques in fluid mechanics and to contribute to the field by comparing them with traditional methods. Case studies were also conducted using Computational Fluid Dynamics (CFD), and the results were compared with those from ML techniques in terms of accuracy and computational cost. For Case 1, after optimizing relevant parameters, the Artificial Neural Network (ANN), Random Forest (RF), and Support Vector Machine (SVM) models all achieved an R² value above 0.9. However, in Case 2, only the ANN method surpassed this threshold, likely due to the limited data available. In Case 3, all models except for Linear Regression (LR) demonstrated predictive abilities above the 0.9 threshold after parameter optimization. The LR method was found to have low applicability to fluid mechanics problems, while SVM and ANN methods proved to be particularly effective tools after grid search optimization.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Prediction of Heart Disease using Forest Algorithm over Linear Regression Algorithm using Machine Learning With Improved Accuracy,"Aim: To perform Predicting heart disease using the Forest algorithm and comparing its feature extraction precision with the Linear Regression Algorithm for improving the accuracy of the prediction. Methods and Materials: In the proposed work, Predicting heart disease was carried out using machine learning algorithms such as Linear Regression (n=10)and Forest algorithm(n=10). Here the pretest power analysis was carried out with 80% and the sample size for the two groups are 20. Results: From The implemented experiment, the Forest algorithm accuracy is 90.32% and the Linear Regression Algorithm 77.21%. There is a statistical 2-tailed significant difference in accuracy for two algorithms is 0.001 (p&lt;0.05) Conclusion: This study concludes that the Forest algorithm on patients healthcare analysis is significantly better than the Linear Regression Algorithm.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improved Accuracy for Heart Disease Diagnosis Using Machine Learning Techniques,"This work primarily focuses on diagnosis of heart disease before explicit visit to the expert doctor. Machine learning based systems have been found useful in medical diagnosis applications because of their ability to learn human like expertise and to utilize acquired knowledge for diagnosis. This work is performs classification of heart disease utilizing subject’s vital parameters. Pathological laboratory results available after testing are not understood by common people and patients have to wait till they visit expert doctors for inference. In this paper, traditional methods like linear regression to various machine learning based systems including back propagation neural network, support vector machine(SVM) and k-nearest neighbor are developed for heart diseases classification. The proposed system transforms sensor inputs to stroke stage classification. With a view to ascertain the efficacy of proposed system, performances of all methods are compared on standard Cleveland database and with similar work. Simulation results show 100 percent correct diagnosis and henceforth robustness of SVM based approaches for test data given.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Forecasting Accuracy through Machine Learning in Supply Chain Management,"Purpose: The use of machine learning (ML) techniques in economic and financial forecasting has gained significant attention due to their potential to improve the accuracy and robustness of predictions. This paper explores the application of various ML algorithms such as support vector machines, random forests, and deep learning models in forecasting economic variables, financial market trends, and macroeconomic indicators. Methodology: We assess the forecasting accuracy of these models relative to traditional econometric approaches, including ARIMA and VAR models. Findings: The analysis reveals that ML techniques, particularly deep learning, outperform classical methods in terms of predictive accuracy, especially in complex, nonlinear environments. We also discuss challenges associated with model interpretability, overfitting, and data quality, providing insights into how these limitations can be addressed. Unique Contribution to Theory, Practice and Policy: The findings contribute to a deeper understanding of how advanced machine learning can enhance forecasting methodologies, with implications for both theoretical modeling and practical applications in economic policy, risk management, and financial decision-making.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Analysis of Accuracy Metric of Machine Learning Algorithms in Predicting Heart Disease,"Introduction: Heart disease is, for the most part, alluding to conditions that include limited or blocked veins that can prompt a heart attack, chest torment or stroke. Earlier identification of heart disease may reduce the death rate. The cost of medical diagnosis makes it perverse to cure it for the large amount of people early. Using machine learning models performed on dataset. This article aims to find the most efficient and accurate machine learning models for disease prediction.Material and Methods: Several supervised machine learning algorithms were utilized to diagnosis and prediction of heart disease such as logistic regression, decision tree, random forest and KNN. The algorithms are applied to a dataset taken from the Kaggle site including 70000 samples. In algorithms, methods such as the importance of features, hold out validation, 10-fold cross-validation, stratified 10-fold cross-validation, leave one out cross-validation are the result of effective performance and increase accuracy. In addition, feature importance scores was estimated for each feature in some algorithms. These features were ranked based on feature importance score. All the work is done in the Anaconda environment based on python programming language and Scikit-learn library.Results: The algorithms performance is compared to each other so that performance based on ROC curve and some criteria such as accuracy, precision, sensitivity and F1 score were evaluated for each model. As a result of evaluation, random forest algorithm with F1 score 92%, accuracy 92% and AUC ROC 95%, has better performance than other algorithms.Conclusion: The area under the ROC curve and evaluating criteria related to a number of classifying algorithms of machine learning to evaluate heart disease and indeed, the diagnosis and prediction of heart disease is compared to determine the most appropriate classifier.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
High-Accuracy Prediction of Mechanical Properties of Ni-Cr-Fe Alloys Using Machine Learning,"Artificial intelligence-driven prediction models have emerged as powerful tools for estimating material properties with high accuracy, yet the preparation of training datasets often demands labor-intensive and time-consuming experimental procedures. Leveraging the Computational Materials Science (CMS) approach, this study utilizes phase transformation calculations and thermodynamic data to simulate the mechanical properties of Ni-Cr-Fe alloys. Using JMatPro software, mechanical properties (0.2% Proof Stress, Fracture Stress, and Young's Modulus) of 50 Ni-Cr-Fe alloy compositions were simulated across a temperature range of 540–920°C, generating a dataset of 1000 rows. This dataset was used to train an Artificial Neural Network (ANN) model, with 80% allocated for training and 20% for validation and testing. The trained AI model demonstrated robust predictive capabilities, achieving a 96,61% accuracy rate in forecasting material compositions with the desired thermo-physical properties at specific temperatures. To validate the model's reliability, predicted alloy compositions were re-simulated under identical conditions in JMatPro, confirming the high fidelity of the model's predictions. The results underscore the efficacy of Computational Materials Science (CMS)-generated datasets as a scalable and reliable source for training AI models in materials science. This study highlights the potential of integrating Computational Materials Science (CMS) and Machine Learning approaches to accelerate material design and development processes, delivering significant improvements in prediction speed and accuracy.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving the accuracy of medical diagnosis with causal machine learning,"AbstractMachine learning promises to revolutionize clinical decision making and diagnosis. In medical diagnosis a doctor aims to explain a patient’s symptoms by determining the diseases causing them. However, existing machine learning approaches to diagnosis are purely associative, identifying diseases that are strongly correlated with a patients symptoms. We show that this inability to disentangle correlation from causation can result in sub-optimal or dangerous diagnoses. To overcome this, we reformulate diagnosis as a counterfactual inference task and derive counterfactual diagnostic algorithms. We compare our counterfactual algorithms to the standard associative algorithm and 44 doctors using a test set of clinical vignettes. While the associative algorithm achieves an accuracy placing in the top 48% of doctors in our cohort, our counterfactual algorithm places in the top 25% of doctors, achieving expert clinical accuracy. Our results show that causal reasoning is a vital missing ingredient for applying machine learning to medical diagnosis.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Federated Learning in Privacy-Preserving Machine Learning: Balancing Model Accuracy and Data Security,"This research article provides information regarding the concerns of Privacy-Preserving Machine Learning (PPML) Techniques that includes Differential Privacy, Federated Learning, and Secure Multi-Party Computation. It is also noticed that Privacy Models are useful to achieve significant privacy and minimal loss in model accuracy. This further demonstrates that these kinds of strategies in the main application areas such as Healthcare, Banking, Internet-Of-Things (IOT), and Manufacturing, provides near-perfect privacy that can be useful while it meets minimal compromise in model accuracy. Some of the findings are enhanced data security, high accuracy of the chosen model, and ideas for future development.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Feature Selection Methods Simultaneously Improve the Detection Accuracy and Model Building Time of Machine Learning Classifiers,"The detection accuracy and model building time of machine learning (ML) classifiers are vital aspects for an intrusion detection system (IDS) to predict attacks in real life. Recently, researchers have introduced feature selection methods to increase the detection accuracy and minimize the model building time of a limited number of ML classifiers. Therefore, identifying more ML classifiers with very high detection accuracy and the lowest possible model building time is necessary. In this study, the authors tested six supervised classifiers on a full NSL-KDD training dataset (a benchmark record for Internet traffic) using 10-fold cross-validation in the Weka tool with and without feature selection/reduction methods. The authors aimed to identify more options to outperform and secure classifiers with the highest detection accuracy and lowest model building time. The results show that the feature selection/reduction methods, including the wrapper method in combination with the discretize filter, the filter method in combination with the discretize filter, and the discretize filter, can significantly decrease model building time without compromising detection accuracy. The suggested ML algorithms and feature selection/reduction methods are automated pattern recognition approaches to detect network attacks, which are within the scope of the Symmetry journal.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
Enhancing Diagnostic Accuracy of Co-occurring Diabetic and Thyroid Diseases using Machine Learning Techniques,"Efficient classification methods are crucial for accurately predicting co-occurring diabetic and thyroid diseases, addressing substantial global health challenges. These conditions affect individuals across diverse demographics, including males, females, infants, adolescents, and the elderly. This study employs ML algorithms to forecast co-occurring diabetic and thyroid diseases (DTD). Utilizing a dataset sourced from the UCI Machine Learning Repository, feature selection techniques were applied to identify relevant attributes and optimize predictive accuracy. Seven distinct machine learning algorithms, including ID3, J48, Zero R, Random Forest, Multilayer Perceptron, and Naive Bayes, were employed to classify subjects based on their disease status. Our analysis of various machine learning algorithms for predicting co-occurring diabetic and thyroid diseases (DTD) demonstrates notable differences in their performance. The Random Forest (RF) algorithm outperformed others with a remarkable accuracy rate of 95.123%, showcasing its potential for accurate disease classification. Following closely, the Naïve Bayes algorithm achieved an accuracy of 93.8596%, indicating its effectiveness in this context. Additionally, the ID3 algorithm demonstrated respectable performance with an accuracy of 87.7193%. These findings underscore the significance of employing machine learning methodologies, particularly Random Forest and Naïve Bayes, to enhance diagnostic accuracy and inform treatment strategies for individuals affected by thyroid or diabetic disorders.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparison of the red wine quality prediction accuracy using 5 Machine Learning Model,"Wine quality is rather important for alcohol industry, especially for some unique product (e.g., Vinho Verde). Since the quality evaluation of wine is often part of the certification process and is helpful for setting prices, it is necessary to find a model with the highest model accuracy to predict the quality. The paper collected the red wine data from a certain database and changed it to a classification problem with quality as 0 to bad and 1 to good. This dataset is like a normal distribution with the quality of 5 and 6 having the highest ratio. In order to find the approach with the best performance, five machine learning approaches are adopted and compared in terms of various metrics and judgments. According to the analysis, the Support-vector machine (SVM) model predicts preforms the best, i.e., the model has the highest accuracy. This article is a guide for the wine market for the model selection and shed light on guiding further exploration regarding to wine quality prediction.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Experimental Indication to Improve the NN Learning Accuracy by Integrity Constraints From the NN Training Data,"Various approaches to improve the classification rate of neural networks (NN) exist. Nevertheless, the application of integrity constraints to mitigate this rate is novel. This paper investigates the effectiveness of integrity constraints (ICs or short: constraints) to improve the performance of neural networks (NNs). This applies in particular to data reduction in training data. The study starts with the application of ICs to the initial NN classification, focusing on the development of data set-specific constraints. These constraints are created by machine learning algorithms, such as multiple linear regression. The method consists of applying these constraints to the misclassified data sets from different tests with the aim of reducing the misclassification rate. The effectiveness of this approach is quantified by comparing the original misclassification rates with those after applying the IC, where a significant reduction was observed in three different test cases. For example, one test case can be described as significant: In Test 1, the misclassification rate decreased from 0.78% to 0.19%, which corresponds to a reduction of 75.6%. Similar improvements were observed in the subsequent tests, underlining the potential of ICs to improve classification accuracy. Thus, this study provides convincing evidence that the NNIC approach is a valuable tool for mitigating misclassification problems in neural network applications. The combination of training a NN and subsequently apply ICs from the training data (NNIC approach) is new and the experimental results indicate evidence for improving the classification rate.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Exploring the Accuracy of Machine Learning and Deep Learning in Engine Knock Detection,"This study explores the use of machine learning for real-time detection of engine knocking, aiming to enhance early vehicle fault recognition. We extracted frequency modulation amplitude demodulation (FMAD) features from engine sound data and evaluated various machine-learning algorithms using MATLAB. The coarse decision tree algorithm emerged as the most effective, achieving a classification accuracy of 66.01%. Subsequently, by using deep learning models, we significantly improved the accuracy: a convolutional neural network (CNN) achieved 45.16%. accuracy, a deep learning recurrent neural network (RNN) model in LSTM achieved 90% accuracy, and further refinements pushed the accuracy to 93.55%. Additionally, we introduced a knock index to quantify noise levels during each engine cycle. This index, calculated from the integral of the absolute value of the first derivative of a band-pass-filtered vibration signal, provides a visual representation of knock strength. This approach shows promise for early detection of engine knocking, although further refinement of feature extraction methods and algorithm optimization is necessary for practical application. The study highlights the potential of integrating machine learning into real-time vehicle fault detection systems to improve their reliability and effectiveness.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving Extrapolation Accuracy of Voltage Prediction Model for Lithium-Ion Battery Using Machine Learning,"As battery electric vehicles (BEVs) become more widespread, collecting large amounts of time-series data on lithium-ion batteries in real-world settings has become possible. In recent years, there has been growing interest in methods that utilize collected battery data and machine learning (ML) models to predict battery performance accurately [1-4]. However, there is a challenge in that prediction accuracy declines when the distribution of training data is uneven, and it has been a barrier to practical application. This challenge is due to real-world data often being unevenly distributed, and ML models generally have low extrapolation accuracy [5-6]. A specific challenge for BEVs is that the data in the low state of charge (SOC) region is insufficient because batteries are infrequently discharged to lower SOC due to users' anxieties about running out of electricity, resulting in less prediction accuracy. Here, we studied several representative ML models and their characteristics to achieve high extrapolation accuracy of voltage prediction. We artificially generated data for training and testing using an electrochemical simulation model. To evaluate the extrapolation accuracy of each model, we trained the ML models using data that excluded the low SOC region. We quantified voltage prediction accuracy for the entire SOC region using root mean square error (RMSE). Fig. 1 shows results for four ML models (“Random Forest,” “Gaussian Process,” “LSTM,"" and “MLP-ReLU""). The results confirmed that accuracy highly depends on the model and that MLP-ReLU (multilayer perceptron with ReLU activation function) has the highest accuracy. For further study, we set up several SOC ranges and comprehensively evaluated the RMSEs of eight representative ML models. Our results confirmed that the MLP-ReLU showed the best accuracy [7]. Furthermore, we examined four representative MLP activation functions (“ReLU,” “identity,” “tanh,” and “logistic”) and confirmed that the ReLU performed the best. This study shows that MLP-ReLU is suitable for building a model with high extrapolation accuracy even when training data is unevenly distributed. From these results, we concluded that we are close to realizing practical applications for predicting battery performance with high accuracy by utilizing real-world battery data and ML models. References [1] M. K. Tran, S. Panchal, V. Chauhan, N. Brahmbhatt, A. Mevawalla, R. Fraser, and M. Fowler, “Python-based scikit-learn machine learning models for thermal and electrical performance prediction of high-capacity lithium-ion battery,” International Journal of Energy Research, vol 64, issue 2 (2022): p786-794. [2] F. Heinrich, P. Klapper, and M. Pruckner, “A comprehensive study on battery electric modeling approaches based on machine learning,” Energy Inform vol 4 (suppl 3), no 17 (2021). [3] A. Herle, J. Channegowda, and D. Prabhu, “Overcoming limited battery data challenges: A coupled neural network approach,” International Journal of Energy Research, vol 45, issue 14 (2021): p20474-20482. [4] S. Li, J. Li, H. He, and H. Wang, “Lithium-ion battery modeling based on Big Data,” Energy Procedia, vol 159 (2019): p168-173. [5] G. Pareschi, L. Küng, G. Georges, and K. Boulouchos, “Are travel surveys a good basis for EV models? Validation of simulated charging profiles against empirical data,” Applied Energy, vol 275 (2020): p115318. [6] M. Sugiyama, M. Krauledat, K. R. Müller, “Covariate Shift Adaptation by Importance Weighted Cross Validation,” Journal of Machine Learning Research, vol 8, no 35 (2007): p985-1005. [7] T. Kawahara, K. Sato, Y. Sato, “Battery Voltage Prediction Technology Using Machine Learning Model with High Extrapolation Accuracy,” International Journal of Energy Research, vol. 2023, Article ID 5513446, 17 pages, 2023.     Figure 1",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine Learning Applied to LoRaWAN Network for Improving Fingerprint Localization Accuracy in Dense Urban Areas,"In the area of low-power wireless networks, one technology that many researchers are focusing on relates to positioning methods such as fingerprinting in densely populated urban areas. This work presents an experimental study aimed at quantifying mean location estimation error in populated areas. Using a dataset provided by the University of Antwerp, a neural network was implemented with the aim of providing end-device location. In this way, we were able to measure the mean localization error in areas of high urban density. The results obtained show a deviation of less than 150 m in locating the end device. This offset can be decreased up to a few meters, provided that there is a greater density of nodes per square meter. This result could enable Internet of Things (IoT) applications to use fingerprinting in place of energy-consuming alternatives.",CS,IOT,85,Clear CS paper with focus on Internet of Things (IoT)
Enhancing Accuracy and Performance in Music Mood Classification through Fine-Tuned Machine Learning Methods,"Putting emotional labels on music, or ""music mood classification,"" is important for use in recommendation systems and music therapy. Using fine-tuned machine learning methods, this study aims to improve the accuracy and performance of classification. We used a large dataset with names for different types of music and moods to make sure that the model training was strong. Advanced feature extraction methods picked up both the traits of the audio stream and the lyrics. For audio features, color features, spectral contrast, and mel-frequency cepstral coefficients (MFCCs) were recovered. For poetry analysis, TF-IDF and word embeddings were used, along with natural language processing (NLP) methods. Logistic Regression, SGD Classifier, Gaussian Naive Bayes, Decision Tree, Random Forest, XGB Classifier, SVM Linear, and K-Nearest Neighbors (KNN) were some of the machine learning classification methods we used. Random Forest, XGB Classifier, and SVM Linear all did better than the others. We used grid search and random search to fine-tune the hyperparameters of these top-performing models in order to make them even better. Cross-validation made sure that the models were stable and could be used in other situations. Our results show that the highly tuned Random Forest, XGB, and SVM models greatly improved the accuracy of classification, with the XGB Classifier performing the best. This study adds to music information retrieval by creating a useful method for mood classification that can be used in real-life situations to improve user experiences and create more personalized music services.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Prediction of Heart Disease using Forest Algorithm over K-nearest neighbors using Machine Learning with Improved Accuracy,"Aim: To perform Predicting heart disease using the Forest algorithm and comparing its feature extraction precision with the K-nearest neighbors algorithm for working on the precision of the forecast. Materials and Methods: In the proposed work, Predicting heart disease was carried out using machine learning algorithms such as K-nearest neighbors algorithm (n=10) and Forest Algorithm (n=10). Here the pretest power examination was done with gpower 80% and the sample size for the two gatherings was 20. Results: From The implemented experiment, the Forest algorithm accuracy is significantly better and it is 90.0% than the K-nearest neighbors algorithm 83.00%. There is a measurable 2-tailed significant distinction in exactness for two calculations is 0.001 (p&lt;0.05) by performing Independent samples T-tests. Conclusion: The Forest algorithm got better Accuracy and classification of digits better than K-nearest neighbors algorithm for Predicting heart disease.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving monthly precipitation prediction accuracy using machine learning models: a multi-view stacking learning technique,"This research paper explores the implementation of machine learning (ML) techniques in weather and climate forecasting, with a specific focus on predicting monthly precipitation. The study analyzes the efficacy of six multivariate machine learning models: Decision Tree, Random Forest, K-Nearest Neighbors (KNN), AdaBoost, XGBoost, and Long Short-Term Memory (LSTM). Multivariate time series models incorporating lagged meteorological variables were employed to capture the dynamics of monthly rainfall in Rabat, Morocco, from 1993 to 2018. The models were evaluated based on various metrics, including root mean square error (RMSE), mean absolute error (MAE), and coefficient of determination (R2). XGBoost showed the highest performance among the six individual models, with an RMSE of 40.8 (mm). In contrast, Decision Tree, AdaBoost, Random Forest, LSTM, and KNN showed relatively lower performances, with specific RMSEs ranging from 47.5 (mm) to 51 (mm). A novel multi-view stacking learning approach is introduced, offering a new perspective on various ML strategies. This integrated algorithm is designed to leverage the strengths of each individual model, aiming to substantially improve the precision of precipitation forecasts. The best results were achieved by combining Decision Tree, KNN, and LSTM to build the meta-base while using XGBoost as the second-level learner. This approach yielded a RMSE of 17.5 millimeters. The results show the potential of the proposed multi-view stacking learning algorithm to refine predictive results and improve the accuracy of monthly precipitation forecasts, setting a benchmark for future research in this field.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving underwater localization accuracy with machine learning,"Machine learning classification and regression algorithms were applied to calibrate the localization errors of a time-difference-of-arrival (TDOA)-based acoustic sensor array used for tracking salmon passage through a hydroelectric dam on the Snake River, Washington, USA. The locations of stationary and mobile acoustic tags were first tracked using the approximate maximum likelihood algorithm. Next, ensembles of classification trees successfully identified and filtered data points with large localization errors. This prefiltering step allowed the creation of a machine-learned regression model function, which decreased the median distance error by 50% for the stationary tracks and by 34% for the mobile tracks. It also extended the previous range of sub-meter localization accuracy from 100 m to 250 m horizontal distance from the dam face (the receivers). Median distance errors in the depth direction were especially decreased, falling from 0.49 m to 0.04 m in the stationary tracks and from 0.38 m to 0.07 m in the mobile tracks. These methods would have application to the calibration of error in any TDOA-based sensor network with a steady environment and array configuration.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
EARLY DETECTION OF ORAL DISEASES USING MACHINE LEARNING: A COMPARATIVE STUDY OF PREDICTIVE MODELS AND DIAGNOSTIC ACCURACY,"Early detection of oral diseases is crucial for effective treatment and improved patient outcomes. This study develops and evaluates machine learning models for the detection of early-stage oral diseases using a comprehensive and diverse dataset comprising clinical records, demographic information, and intraoral images. The methodology involves systematic data preprocessing, feature selection, model training, and evaluation. Several machine learning algorithms, including Gradient Boosting, Random Forest, Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and Naïve Bayes, were employed and compared to identify the most effective model. Gradient Boosting achieved the highest performance with an accuracy of 95.2% and an AUC-ROC score of 0.98, demonstrating superior ability to classify early-stage oral diseases. Random Forest followed closely with an accuracy of 94.5% and an AUC-ROC score of 0.96. In contrast, SVM and KNN showed moderate performance with accuracies of 89.7% and 87.3%, respectively, while Naïve Bayes exhibited the lowest accuracy at 82.1%. The results highlight the importance of advanced ensemble methods in achieving higher accuracy and better classification for early detection. The study underscores the potential of machine learning to revolutionize oral healthcare by enabling timely disease detection, reducing diagnostic errors, and improving treatment outcomes. These findings contribute to the growing body of literature on artificial intelligence in healthcare and provide a foundation for developing scalable diagnostic solutions in clinical practice.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Statistical Analysis and Accuracy Assessment of Improved Machine Learning Based Opinion Mining Framework,"Sentiment analysis, also known as opinion mining, plays a crucial role in understanding and extracting valuable insights from textual data in various domains, including social media, customer feedback, and product reviews. This research presents an in-depth examination of an improved machine learning-based sentiment analysis framework, focusing on its statistical analysis and accuracy assessment. The research begins by introducing the framework's architecture, which incorporates advanced machine learning algorithms and natural language processing techniques. These enhancements aim to provide a more nuanced and context-aware sentiment analysis, addressing the limitations of traditional approaches. To evaluate the performance of the proposed framework, a comprehensive statistical analysis is conducted. Various statistical metrics, such as precision, recall, F1-score, and accuracy, are employed to assess its effectiveness in classifying text sentiments accurately. Additionally, the study explores the impact of different feature engineering and pre-processing techniques on model performance. The results of this study demonstrate the significant improvements achieved by the enhanced sentiment analysis framework in terms of accuracy and reliability. The statistical analysis confirms its superior performance in capturing subtle sentiment nuances, making it a valuable tool for applications requiring precise sentiment understanding. In conclusion, this research contributes to the field of sentiment analysis by presenting an improved machine learning-based framework and conducting a rigorous statistical assessment of its accuracy. The findings provide valuable insights for researchers and practitioners seeking to enhance sentiment analysis techniques and apply them effectively in various domains..",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Comparison of the ease of use and accuracy of two machine learning algorithms – forestry case study,"With the availability of massive amounts of data and cheap computing, machine learning has become increasingly viable to create extensive multivariate mathematical models of natural phenomena to help predict accurate future trends that would have been impossible for humans to accomplish by themselves. There is a wide variety of different machine learning algorithms available, and it is not always known which one will perform best for a given dataset. This can be determined after training and evaluating the different models and comparing them. In this case study, logistic regression and random forest models were compared in terms of accuracy and ease of use. We hypothesized that logistic regression would yield a higher accuracy and be easier to set up in a comparable scenario for a given dataset compared to random forest. Both algorithms used the same forestry dataset to see which one would outperform the other. Initially, logistic regression looked like the better choice, however, after a variety of comparisons, random forest yielded higher performance in both accuracy measurements (accuracy=0.9722, Fbeta=0.9722 for random forest vs. accuracy=0.7141, Fbeta=0.6990 for logistic regression) and did not require as much detailed tuning as logistic regression did.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparative Analysis of Machine Learning Models for Enhanced Software Fault Detection: Performance and Accuracy Evaluation,"Software defect prediction expectation is a critical portion of program designing that tries to discover and anticipate bugs or imperfections in program frameworks some time recently they happen. The objective is to come up with strategies and models that offer assistance program advancement groups choose which testing assignments are most imperative and how to best utilize their assets. A few machine learning (ML) strategies are utilized to do this. To create forecasts, these frameworks utilize a parcel of distinctive information, like code measures, past imperfection information, and data approximately the coder. The reason of this study is to use different machine learning strategies to make a determining demonstrate for finding computer program bugs. The proposed demonstrate is utilized in tests on the KC2 dataset from NASA's Guarantee library as portion of the study. It was attempted and compared how well diverse machine learning strategies worked. This consider utilized the KC2 dataset and found that the Decision Tree strategy got 73.28% accuracy, Naïve Bayes got 85.96%, K-Nearest Neighbour (KNN) got 82.57%, Support Vector Machine (SVM) got 86.74%, and Random Forest got 84.20%. The study about appeared that these different models can precisely anticipate program bugs when the Guarantee dataset KC2 is utilized. These comes about appear that machine learning models can offer assistance anticipate bugs superior, which can offer assistance groups speed up their tests and make way better utilize of their assets. The study appears that employing a run of machine learning models can enormously move forward the precision of foreseeing computer program abandons. This gives us valuable data almost regions of program frameworks that are more likely to have bugs, which helps keep software quality high.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Accuracy Comparison of Machine Learning Algorithms on World Happiness Index Data,"This study aims to compare the accuracy performances of different machine learning algorithms (Logistic Regression, Decision Tree, Support Vector Machines (SVMs), Random Forest, Artificial Neural Network, and XGBoost) using World Happiness Index data. The study is based on the 2024 World Happiness Report data and employs indicators such as Ladder Score, GDP Per Capita, Social Support, Healthy Life Expectancy, Freedom to Determine Life Choices, Generosity, and Perception of Corruption. Initially, the K-Means clustering algorithm is applied to group countries into four main clusters representing distinct happiness levels based on their socioeconomic profiles. Subsequently, classification algorithms are used to predict the cluster membership and the accuracy scores obtained serve as an indirect measure of the clustering quality. As a result of the analysis, Logistic Regression, Decision Tree, SVM, and Neural Network achieve high accuracy rates of 86.2%, whereas XGBoost exhibits the lowest performance at 79.3%. Furthermore, the practical implications of these findings are significant, as they provide policymakers with actionable insights to develop targeted strategies for enhancing national happiness and improving socioeconomic well-being. In conclusion, this study offers valuable information for more effective classification and analysis of World Happiness Index data by comparing the performance of various machine learning algorithms.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Exploring the Potential of Machine Learning in Healthcare Accuracy Improvement,"Machine learning techniques have shown great potential in the medical industry, particularly in the field of neuroimaging and the identification of neurological illnesses such as Autism Spectrum Disorder (ASD). By utilizing machine learning algorithms, researchers aim to predict the type of disability and analyze the predicted variations using different types of predictive models. These predictive models can be trained on neuroimaging data to identify patterns and markers that are indicative of ASD. By analyzing these patterns, machine learning algorithms can help in accurately predicting the presence and type of ASD in individuals. This can be immensely valuable in early diagnosis and intervention, leading to better outcomes for individuals with ASD. Furthermore, the applications of machine learning in the healthcare industry extend beyond just prediction. Machine learning algorithms can also be used to analyze large amounts of medical data, identify trends, and assist in decision-making processes. This can help healthcare professionals in providing more accurate diagnoses, personalized treatment plans, and improved patient care. It is important to note that the success and accuracy of machine learning models in the healthcare industry depend on various factors, including the quality and quantity of data available, the choice of algorithms, and the expertise of the researchers. Ongoing research and advancements in machine learning techniques hold great promise for improving the accuracy and effectiveness of medical diagnoses and treatments.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing Diabetes Prediction Accuracy through Hybrid Machine Learning Models: A Comparative Study,"This study investigates the effectiveness of various machine learning (ML) models in predicting the onset of diabetes, emphasizing the superior performance of hybrid models over single learner models. Employing a dataset comprising 10,000 individuals with features like Glucose level, BMI, Insulin, and more, we meticulously processed and engineered the data to optimize it for ML applications. We developed several models, including Decision Trees, Random Forest, KNN, and XGBoost, and then advanced to hybrid models using ensemble techniques like stacking and soft voting classifiers. Our findings indicate that hybrid models significantly outperform single learner models. These models achieved remarkable accuracy (98.11%), precision (97.31%), and ROC AUC (99.82%), highlighting their potential in clinical settings. The study underscores the value of hybrid ML models in enhancing predictive accuracy and reliability in diabetes diagnostics.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
A Multiple-Layer Machine Learning Architecture for Improved Accuracy in Sentiment Analysis,"Abstract Twitter is an online micro-blogging platform through which one can explore the hidden valuable and delightful information about the current context at any point of time, which also serves as a data source to carry out sentiment analysis. In this paper, the sentiments of large amount of tweets generated from Twitter in the form of big data have been analyzed using machine learning algorithms. A multi-tier architecture for sentiment classification is proposed in this paper, which includes modules such as tokenization, data cleaning, preprocessing, stemming, updated lexicon, stopwords and emoticon dictionaries, feature selection and machine learning classifier. Unigram and bigrams have been used as feature extractors together with χ2 (Chi-squared) and Singular Value Decomposition for dimensionality reduction together with two model types (Binary and Reg), with four types of scaling methods (No scaling, Standard, Signed and Unsigned) and represented them in three different vector formats (TF-IDF, Binary and Int). Accuracy is considered as the evaluation standard for random forest and bagged trees classification methods. Sentiments were analyzed through tokenization and having several stages of pre-processing and several combinations of feature vectors and classification methods. Through which it was possible to achieve an accuracy of 84.14%. Obtained results conclude that, the proposed scheme gives a better accuracy when compared with existing schemes in the literature.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
A Machine Learning Approach to Improve Ranging Accuracy with AoA and RSSI,"Ranging accuracy is a critical parameter in time-based indoor positioning systems. Indoor environments often have complex structures, which make centimeter-level-accurate ranging a challenging task. This study proposes a new distance measurement method to decrease the ranging error in multipath environment. Our method uses an artificial neural network that utilizes the received signal strength indicator along with a signal’s angle of arrival to calculate the line-of-sight distance. This combination results in a significant reduction of the error caused by multipath effects that common RSSI-based methods suffer from. It outperforms traditional ranging methods while the implementation complexity is kept low.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparison and analysis of multiple machine learning algorithms on prediction accuracy in Parkinson's patients,"This paper describes an experiment on Parkinsons disease classification using multiple classification algorithms for comparison. Parkinsons disease is a common neurological disorder, and early diagnosis and classification are important for the assessment of treatment and prognosis. Therefore, the research implications of this paper are clear. The classification algorithms used in the experiment include adaboost classification model, XGBoost classification model, logistic regression regression model, random forest plain Bayesian classification model, bp neural network and support vector machine. The experimental results show that adaboost classification model performs well when dealing with small sample data, XGBoost classification model performs well when dealing with large-scale datasets, and logistic regression regression model and random forest plain Bayesian classification model also have good performance. The bp neural network and support vector machine, on the other hand, perform poorly in terms of classification results and require a much larger dataset for support. These experimental results have important reference value for the classification and diagnosis of Parkinsons disease. Different classification algorithms are suitable for different dataset sizes and characteristics, so in practical applications, we can choose different classification algorithms according to the size and characteristics of the dataset to achieve the optimal classification effect. In conclusion, the results of this paper provide a reference for the classification and diagnosis of Parkinsons disease, as well as a guide for choosing appropriate classification algorithms. In the future, we can further expand the dataset size and use more classification algorithms for comparison to improve the accuracy and robustness of Parkinsons disease classification.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Towards improving machine learning algorithms accuracy by benefiting from similarities between cases,"Data preprocessing is a necessary core in data mining. Preprocessing involves handling missing values, outlier and noise removal, data normalization, etc. The problem with existing methods which handle missing values is that they deal with the whole data ignoring the characteristics of the data (e.g., similarities and differences between cases). This paper focuses on handling the missing values using machine learning methods taking into account the characteristics of the data. The proposed preprocessing method clusters the data, then imputes the missing values in each cluster depending on the data belong to this cluster rather than the whole data. The author performed a comparative study of the proposed method and ten popular imputation methods namely mean, median, mode, KNN, IterativeImputer, IterativeSVD, Softimpute, Mice, Forimp, and Missforest. The experiments were done on four datasets with different number of clusters, sizes, and shapes. The empirical study showed better effectiveness from the point of view of imputation time, Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and coefficient of determination (R2 score) (i.e., the similarity of the original removed value to the imputed one).",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving the Accuracy of Continuous Blood Glucose Measurement Using Personalized Calibration and Machine Learning,"Despite tremendous developments in continuous blood glucose measurement (CBGM) sensors, they are still not accurate for all patients with diabetes. As glucose concentration in the blood is &lt;1% of the total blood volume, it is challenging to accurately measure glucose levels in the interstitial fluid using CBGM sensors due to within-patient and between-patient variations. To address this issue, we developed a novel data-driven approach to accurately predict CBGM values using personalized calibration and machine learning. First, we scientifically divided measured blood glucose into smaller groups, namely, hypoglycemia (&lt;80 mg/dL), nondiabetic (81–115 mg/dL), prediabetes (116–150 mg/dL), diabetes (151–181 mg/dL), severe diabetes (181–250 mg/dL), and critical diabetes (&gt;250 mg/dL). Second, we separately trained each group using different machine learning models based on patients’ personalized parameters, such as physical activity, posture, heart rate, breath rate, skin temperature, and food intake. Lastly, we used multilayer perceptron (MLP) for the D1NAMO dataset (training to test ratio: 70:30) and grid search for hyperparameter optimization to predict accurate blood glucose concentrations. We successfully applied our proposed approach in nine patients with type 1 diabetes and observed that the mean absolute relative difference (MARD) decreased from 17.8% to 8.3%.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Optimized Cardiac and Circulatory Disorders Prediction: A High-Accuracy Model Selection Approach that utilizes Machine Learning and Deep Learning.,"Cardiac and Circulatory Disorders are among the primary causes of death globally, making early detection crucial for successful treatment and prevention, This research presents an advanced predictive cardiac and circulatory Disorders risk assessment model using machine learning and deep learning techniques. Three different datasets were analyzed, and the most accurate dataset was selected to train the models. Various algorithms, including Decision Tree, XGBoost(Extreme Gradient Boosting), Ada-Boost, Multi-Layer Perceptron (MLP), and Tabnet were implemented and compared. The MLP model attained the highest accuracy of 98% among all evaluated approaches, surpassing traditional machine learning models. The proposed system is designed to support healthcare professionals in identifying at-risk patients early, enabling timely intervention and improved healthcare results.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Detection of Parkinson Diseases with More Accuracy using Machine Learning Technique,"Parkinson's malady is the most common neurodegenerative confusion influencing more than 10 million individuals around the world. There is no single test which can be regulated for diagnosing Parkinson's illness. In light of these challenges, to explore a machine learning way to deal with precisely analyze Parkinson's, utilizing a given dataset. To keep this issue in medicinal parts need to anticipate the malady influenced or not by discovering exactness figuring utilizing AI strategies. The point is to examine AI based methods for Parkinson sickness by expectation results in best precision with finding arrangement reportIn the beginning times of Parkinson ailment, your face may appear practically zero articulation. Your arms may not swing when you walk.. At times, your specialist may recommend medical procedure to manage certain locales of your cerebrum and improve your indications.To propose, an AI based strategy to precisely foresee the illness by discourse and tremor manifestations by expectation results as best exactness from looking at administer grouping AI calculations. Also, to look at furthermore, talk about the execution of different AI calculations from the given transport traffic division dataset with assessment arrangement report, distinguish the outcome demonstrates that the viability of the proposed AI calculation procedure can be thought about with best exactness with accuracy, Recall and F1 Score.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Research on improving accuracy and efficiency of animal data collection and classification using machine learning,"With the continuous expansion of machine learning algorithms in various application domains, the application value of new algorithms, such as Support Vector Machines and Convolutional Neural Networks in data classification, has garnered increasing attention. This paper takes machine learning algorithms as the research entry point, explores the concept of machine learning, and delves into its application value in data classification. This paper, starting with an overview of machine learning algorithms, analyzes the supervised and unsupervised learning problems in machine learning, focusing on the applications of Convolutional Neural Networks, Support Vector Machine models, and logistic regression algorithms in data classification. This study emphasizes designing and implementing a machine learning-based image classification system. Through an in-depth exploration of the application of machine learning algorithms in data classification, a fully functional system is constructed, encompassing multiple modules, including machine vision and software development. This system accurately classifies and recognizes images, providing practical tools and technical support for image processing and analysis. In this study, the goal of achieving good image classification is realized through research and the application of machine learning algorithms. By designing and implementing a machine learning-based image classification system, the accuracy and efficiency of classification in handling massive data are improved. This system also demonstrates wide-ranging prospects in software development and machine vision, among other fields.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Enhancing Heart Attack Prediction Accuracy through Optimized Machine Learning and Deep Learning: A Survey,"Heart attacks are a leading cause of mortality worldwide. Early and accurate prediction can significantly improve patient outcomes. This paper explores the potential of optimized machine learning and deep learning techniques to enhance heart attack prediction accuracy. We discuss the challenges associated with traditional methods and propose a framework that leverages advancements in machine learning and deep learning. The framework may involve techniques like data pre-processing, feature selection, hyperparameter tuning, and ensemble methods to optimize the performance of machine learning algorithms like random forests and support vector machines. Additionally, it may explore deep learning architectures like convolutional neural networks or recurrent neural networks for feature extraction and pattern recognition from complex medical data. The goal is to achieve a more robust and accurate prediction model for heart attacks. This can empower healthcare professionals to identify high-risk individuals, allowing for preventative measures and early intervention, ultimately saving lives. The paper will delve into the specific methods employed for optimization, evaluate their effectiveness, and discuss the potential impact on improving heart attack prediction accuracy",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Accuracy analysis of Japanese machine translation based on machine learning and image feature retrieval,"At present, there are still many deficiencies in Chinese-Japanese machine translation methods, the processing of corpus information is not deep enough, and the translation process lacks rich language knowledge support. In particular, the recognition accuracy of Japanese characters is not high. Based on machine learning technology, this study combines image feature retrieval technology to construct a Japanese character recognition model and uses Japanese character features as the algorithm recognition object. Moreover, this study expands image features by generating a brightness enhancement function using a bilateral grid. In order to exclude the influence of the edge and contour of the image scene on the analysis of the image source, the brightness value of the HDR image is used instead of the pixel value of the image as the image data. In addition, this research designs experiments to study the translation effects of this research model. The research results show that the model proposed in this paper has certain effects and can provide theoretical references for subsequent related research.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
"Accuracy of deep learning, a machine learning technology, using ultra-wide-field fundus ophthalmoscopy for detecting idiopathic macular holes","We aimed to investigate the detection of idiopathic macular holes (MHs) using ultra-wide-field fundus images (Optos) with deep learning, which is a machine learning technology. The study included 910 Optos color images (715 normal images, 195 MH images). Of these 910 images, 637 were learning images (501 normal images, 136 MH images) and 273 were test images (214 normal images and 59 MH images). We conducted training with a deep convolutional neural network (CNN) using the images and constructed a deep-learning model. The CNN exhibited high sensitivity of 100% (95% confidence interval CI [93.5–100%]) and high specificity of 99.5% (95% CI [97.1–99.9%]). The area under the curve was 0.9993 (95% CI [0.9993–0.9994]). Our findings suggest that MHs could be diagnosed using an approach involving wide angle camera images and deep learning.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine Learning in Bank Customer Churn Prediction: Improving Accuracy through Feature Engineering,"Customer churn is a critical challenge in the banking sector, where retaining existing customers is more cost-effective than acquiring new ones. This study explores the role of feature engineering and model optimization in enhancing machine learning-based churn prediction. Using a dataset of bank customers, various preprocessing techniques of categorical encoding, feature scaling, and feature selection were applied. Model optimization was performed using hyperparameter tuning technique of Randomized Search CV to refine key parameters. Further, the models were trained with only the most influential features of customer complaints, number of products, age, account activity, and credit card ownership, resulting in improved computational efficiency and interpretability while maintaining strong predictive performance. Multiple machine learning models were evaluated, with XGBoost emerging as the most effective due to its ability to handle complex patterns and structured data efficiently. The findings provide actionable insights for banks to implement targeted retention strategies, ultimately reducing churn rates and improving customer engagement.",IS,CRM,85,Clear IS paper with focus on Customer Relationship Management (CRM)
Using machine learning algorithms with improved accuracy to analyze and predict employee attrition,"Human migration is based on pull factors that individuals evaluate when it comes to moving to a different territory. Likewise, employee attrition is a phenomenon that represents the tendency to a reduction in employees within an organization. This research paper aims to develop and evaluate machine learning algorithms, namely Decision Tree, Random Forest, and Binary Logistic Regression, to predict employee attrition using the IBM dataset available on Kaggle. The objective is to provide organizations with a proactive approach to employee retention and human resource management by creating accurate predictive models. Employee attrition has significant implications for an organization's reputation, profitability, and overall structure. By accurately predicting employee attrition, organizations can identify the factors contributing to it and implement data-driven human resources management practices. This study contributes to improving decision-making processes, including hiring and firing decisions, and ultimately enhances an organization's capital. The IBM dataset used in this study consists of anonymized employee records and their employment outcomes. It provides a comprehensive HR data representation for analysis and prediction. Three machine learning algorithms, Decision Tree, Random Forest, and Binary Logistic Regression, were utilized in this research. These algorithms were selected for their potential to improve accuracy in predicting employee attrition. The Logistic Regression model yielded the highest accuracy of 87.44% among the tested algorithms. By leveraging this study's findings, organizations can develop predictive models to identify factors contributing to employee attrition. These insights can inform strategic decisions and optimize human resource management practices.",IS,BI_ANALYTICS,85,Clear IS paper with focus on Business Intelligence & Analytics
Assessing Meteorological Drought Patterns and Forecasting Accuracy with SPI and SPEI Using Machine Learning Models,"The rising frequency and severity of droughts requires accurate monitoring and forecasting to reduce the impact on water resources and communities. This study aims to investigate drought monitoring and categorization, while enhancing drought forecasting by using three machine learning models—Artificial Neural Network (ANN), Support Vector Machine (SVM), and Random Forest (RF). The models were trained on the study region’s historic precipitation and temperature data (minimum and maximum) from 1960 to 2021. The Standardized Precipitation Index (SPI) and Standardized Precipitation Evapotranspiration Index (SPEI) were computed for a time scale of 3, 6 and 12 months. The monthly precipitation data were used for creating lag scenarios and were used as input features for the models to improve the models’ performance and reduce overfitting. Statistical parameters like the coefficient of determination (R2), Mean Absolute Error (MAE), Root mean square error (RMSE) and Nash–Sutcliffe Efficiency (NSE) were determined to evaluate the model accuracy. For forecasting, the SPEI3, ANN and SVM models show better performance (R2 &gt; 0.9) than the RF models when the 3-month lag data were used as input features. For SPEI6 and SPEI12, the 6-month lag and 12-month lag data, respectively, were needed to increase the models’ accuracy. The models exhibited RMSE values of 0.27 for ANN, 0.28 for SVM, and 0.37 for RF for the SPEI3, indicating the superior performance of the former two. The models’ accuracy increases as the lag period increases for SPI forecasting. Overall, the ANN and SVM models outperformed the RF model for forecasting long-term drought.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
GUI Based Prediction of Diabetes Stages by Finding the Accuracy using Machine Learning Approach,"In 2050, the world's diabetic patients will arrive at 642 million, which implies that one of the ten grown-ups later on is experiencing diabetes. Diabetes mellitus (DM) is characterized as a gathering of metabolic issues applying critical tension on human wellbeing around the world. DM is a persistent sickness portrayed by hyperglycemia and it might cause numerous inconveniences. To forestall this issue, to break down the given medical clinic dataset by directed AI technique(SMLT) with catch a few data resembles, variable ID, uni-variate examination, bi-variate and multi-variate investigation, missing worth therapies and dissect the information approval, information cleaning/getting ready and information perception will be done on the whole given dataset. Our analysis provides a comprehensive guide to sensitivity analysis of model parameters with regard to performance in prediction of diabetic patients by given attributes of dataset with evaluation of GUI based user interface diabetes attribute prediction. Additionally, it observes to lead an increase the highest accuracy in diabetic prediction of attributes by a significantly better classification report, identify the confusion matrix and to categorizing data from priority and the result shows that the effectiveness of the proposed machine learning algorithm technique can be compared with best accuracy with precision, Recall and F1 Score.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Accuracy assessment of extreme learning machine in predicting soil compression coefficient,"The compression coefficient (Cc) is an important soil mechanical parameter that represents soil compressibility in the process of consolidation. In this study, a machine learning derived model, namely extreme learning algorithm (ELM), was used to predict the Cc of soil. A total of 189 experimental results were used and randomly divided to construct the training and testing parts for the development and validation of ELM. Monte Carlo approach was applied to take into account the random sampling of samples constituting the training dataset. A number of 13 input parameters reflecting the experiment were used as the input variables to predict the output Cc. Several statistical criteria, such as mean absolute error (MAE), root mean square error (RMSE), correlation coefficient (R) and the Monte Carlo convergence estimator were used to assess the performance of ELM in predicting the Cc of soil. The results showed that ELM had a strong capacity to predict the Cc of soil, with the R value &gt; 0.95. The convergence of results, as well as the capability of ELM were fully investigated to understand the advantage of using ELM as a predictor.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing writing scores predictive accuracy using advanced machine learning methods,"Effective manuscript evaluation plays an important role both in publishing and professional communications, including education. Proper and effective means of evaluation can reduce time-consuming processes, guarantee quality outputs, and lead to well-informed decisions. The study applies the structured approach as a means of providing input on how to improve accuracy and efficiency in assessment. For this, QDA supported by ZO and MRFO algorithms was used. Such advanced algorithms as ZO and MRFO were combined with the QDA model to enhance its performance regarding the evaluation of writing scores. This was achieved by integrating ZO and MRFO algorithms into the QDA model, which resulted in a high level of improvement in accuracy and efficiency. After elaborate analysis, the QDZO emerged as the best-performing model with an accuracy value of 0.938. The QDMR came closest to QDZO with an accuracy value of 0.911, while the QDA model, being a baseline model without the incorporation of any of the optimization algorithms, was the worst, represented by an accuracy value of 0.876. The lower accuracy value here indicates that the baseline model does not perform well, which means an advanced optimization algorithm should be integrated to enhance its performance.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing Cyber Threat Detection Accuracy: An AI-Powered Approach with Feature Selection and Machine Learning with Ensemble Learning for Cyber Threat Detection,"The rapid evolution of cyber threats necessitates advanced detection mechanisms to ensure robust network security. This study presents an AI-driven ensemble-based cyber threat detection system leveraging the CICIDS2017 dataset. Our multi-stage methodology integrates data preprocessing, attack data filtering, feature selection, and machine learning model evaluation. Data preprocessing involves cleaning, normalization, and handling missing values to enhance data quality. Attack data filtering isolates malicious and benign traffic for effective model training. Feature selection employs the Random Forest Regressor to identify key predictive attributes. The proposed system evaluates multiple machine learning algorithms, including Naive Bayes, Quadratic Discriminant Analysis (QDA), and Multi-Layer Perceptron (MLP), considering accuracy, precision, and computational efficiency. Furthermore, an ensemble model aggregates predictions from multiple classifiers to enhance detection reliability. A web-based Streamlit application facilitates real-time attack classification, presenting ensemble-based probabilistic predictions for eight attack types, including DDoS, DoS variants, and infiltration attempts. The results highlight the potential of integrating ensemble learning with feature selection and preprocessing techniques to reduce false positives, improve detection accuracy, and enable real-time threat mitigation in large-scale networks.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
Precision Geolocation of Medicinal Plants: Assessing Machine Learning Algorithms for Accuracy and Efficiency,"This study investigates the precision geolocation of medicinal plants, a critical endeavor bridging ecology, conservation, and pharmaceutical research. By employing machine learning algorithms—gradient boosting machine (GBM), random forest (RF), and support vector machine (SVM)—within the cross-industry standard process for data mining (CRISP-DM) framework, both the accuracy and efficiency of medicinal plant geolocation are enhanced. The assessment employs precision, recall, accuracy, and F1 score performance metrics. Results reveal that SVM and GBM algorithms exhibit superior performance, achieving an accuracy of 97.29%, with SVM showing remarkable computational efficiency. Meanwhile, despite inferior performance, RF remains competitive especially when model interpretability is required. These outcomes highlight the efficacy of SVM and GBM in medicinal plant geolocation and accentuate their potential to advance environmental research, conservation strategies, and pharmaceutical explorations. The study underscores the interdisciplinary significance of accurately geolocating medicinal plants, supporting their conservation for future pharmaceutical innovation and ecological sustainability.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
"Forecasting Accuracy of Traditional Regression, Machine Learning, and Deep Learning: A Study of Environmental Emissions in Saudi Arabia","Currently, the world is facing the problem of climate change and other environmental issues due to higher emissions of greenhouse gases. Saudi Arabia is not an exception due to the dependence of the Saudi economy on fossil fuels, which adds to the problem. However, due to the nonlinear pattern of pollution-creating gases, including nitrogen and sulfur dioxide, it is not effortless to rely on forecasting accuracy. Nevertheless, it is essential to denoise the data to extract the reliable outcomes used by different econometric approaches. Hence, the current paper introduces a hybrid model combining compressed sensor denoising (CSD) with traditional regression, machine learning, and deep learning techniques. Comparing different hybrid models and various denoising techniques revealed that CSD-GAN is the best model for accurately predicting NO2 and SO2, as compared with ARIMA, RLS, and SVR. Also, when the comparison is made between predicted and actual NO2 and SO2 levels, these are aligned, proving that CSD-GAN is superior in its level and direction of prediction. It can be concluded that the GAN model is the best hybrid model for predicting NO2 and SO2 emissions in Saudi Arabia. Hence, this model is recommended to policymakers for predicting environmental externalities and framing policies accordingly.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine learning models applied in analyzing breast cancer classification accuracy,"There have been many attempts made to classify breast cancer data, since this classification is critical in a wide variety of applications related to the detection of anomalies, failures, and risks. In this study machine learning (ML) models are reviewed and compared. This paper presents the classification of breast cancer data using various ML models. The effectiveness of models comparatively evaluated through result using benchmark of accuracy which was not done earlier. The models considered for the study are k-nearest neighbor (kNN), decision tree classifier, support vector machine (SVM), random forest (RF), SVM kernels, logistic regression, Naïve Bayes. These classifiers were tested, analyzed and compared with each other. The classifier, decision tree, gets the highest accuracy i.e. 97.08% among all these models is termed as the best ML algorithm for the breast cancer data set.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Multi Class Data Classification to Improve Accuracy in Sentiment Analysis using Machine Learning,"Sentiment analysis means classifying a text into different emotional classes. These days most of the sentiment analysis techniques divide the text into either binary or ternary classification in this paper we are classifying the movie reviews into 5 classes. Multi class sentiment analysis is a technique which can be used to know the exact sentiment of a review not just polarity of a given textual statement from positive to negative. So that one can know the precise sentiment of a review . Multi class sentiment analysis has always been a challenging task as natural languages are difficult to represent mathematically. The number of features are also generally large which requires huge computational power so to reduce the number of features we will use parts-of-speech tagging using textblob to extract the important features. Sentiment analysis is done using machine learning, where it requires training data and testing data to train a model. Various kinds of models are trained and tested at last one model is selected based on its accuracy and confusion matrix. It is important to analyze the reviews in textual form because large amount of reviews is present all over the web. Analyzing textual reviews can help the firms that are trying to find out the response of their products in the market. In this paper sentiment analysis is demonstrated by analyzing the movie reviews, reviews are taken from IMDB website.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
A Study on Enhancing Crop Yield Prediction with Accuracy through Machine and Deep Learning Models,"Accurate crop yield prediction is essential for optimizing agricultural resource allocation and supporting farmers in making informed crop management decisions. This study investigates the effectiveness of various machine learning and deep learning models for predicting crop yields based on key agricultural parameters, including Nitrogen (N), Phosphorous (P), Potassium (K), temperature, humidity, pH, and rainfall. We apply multiple algorithms—Logistic Regression, Multilayer Perceptron, Sequential Minimal Optimization (SMO), J48, Random Forest, REP Tree, and a proposed deep learning model—to evaluate their predictive accuracy in classifying agricultural items. Each model’s performance is assessed using metrics such as Kappa statistic, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Relative Absolute Error (RAE), Root Relative Squared Error (RRSE), and the time taken to build the model. Findings reveal that while traditional machine learning models like Random Forest and REP Tree show robust performance, the proposed deep learning model demonstrates enhanced accuracy and efficiency, making it highly promising for crop yield prediction applications. This study underscores the potential of advanced learning algorithms to improve agricultural productivity by providing actionable insights for crop planning and resource management.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Accuracy Comparison between Five Machine Learning Algorithms for Financial Risk Evaluation,"An accurate prediction of loan default is crucial in credit risk evaluation. A slight deviation from true accuracy can often cause financial losses to lending institutes. This study describes the non-parametric approach that compares five different machine learning classifiers combined with a focus on sufficiently large datasets. It presents the findings on various standard performance measures such as accuracy, precision, recall and F1 scores in addition to Receiver Operating Curve-Area Under Curve (ROC-AUC). In this study, various data pre-processing techniques including normalization and standardization, imputation of missing values and the handling of imbalanced data using SMOTE will be discussed and implemented. Also, the study examines the use of hyper-parameters in various classifiers. During the model construction phase, various pipelines feed data to the five machine learning classifiers, and the performance results obtained from the five machine learning classifiers are based on sampling with SMOTE or hyper-parameters versus without SMOTE and hyper-parameters. Each classifier is compared to another in terms of accuracy during training and prediction phase based on out-of-sample data. The 2 data sets used for this experiment contain 1000 and 30,000 observations, respectively, of which the training/testing ratio is 80:20. The comparative results show that random forest outperforms the other four classifiers both in training and actual prediction.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Group Bias and the Complexity/Accuracy Tradeoff in Machine Learning-Based Trauma Triage Models,"Trauma triage occurs in suboptimal environments for making consequential decisions. Published triage studies demonstrate the extremes of the complexity/accuracy tradeoff, either studying simple models with poor accuracy or very complex models with accuracies nearing published goals. Using a Level I Trauma Center’s registry cases (n=50,644), this study describes, uses, and derives observations from a methodology to more thoroughly examine this tradeoff. This or similar methods can provide the insight needed for practitioners to balance understandability with accuracy. Additionally, this study incorporates an evaluation of group-based fairness into this tradeoff analysis to provide an additional dimension of insight into model selection. The experiments allow us to draw several conclusions regarding the machine learning models in the domain of trauma triage and demonstrate the value of our tradeoff analysis to provide insight into choices regarding model complexity, model accuracy, and model fairness.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving Menstrual Cycle Prediction Accuracy using Advanced Machine Learning Model Methods,"A menstrual cycle prediction system was developed in this paper. To train the dataset obtained from Kaggle, twenty-two machine-learning algorithms were used. Twelve features were used to obtain the predicting variable. The model accuracy and performance were obtained using six standard evaluation metrics as shown in the result section. 0.9864 was obtained as the R2 score for the Decision Tree algorithm, which was the best amongst other models. This indicates that with the Decision Tree, the ovulation day of women can be predicted with higher accuracy. After saving the model with the highest accuracy, it was deployed on a web application using Python, Django, HTML, CSS. Also, VS Code and Jupyter Noteboook were the IDEs that were used in developing this system.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparative analysis of Indonesian news validity detection accuracy using machine learning,"Hoax news prediction is required to anticipate the growth of hoax news in social media. This study aimed to determine the best model for predicting whether the news is a hoax or valid based on the dataset taken from Kaggle.com. This study used several data prediction methods: Support Vector Machine (SVM), Random Forest, Logistic Regression, and Naïve Bayes. After the research processes and data testing, the results showed that the best model for predicting hoax news was SVM, which had the highest accuracy, precision, and recall score of the others.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
The Effect of the Ransomware Dataset Age on the Detection Accuracy of Machine Learning Models,"Several supervised machine learning models have been proposed and used to detect Android ransomware. These models were trained using different datasets from different sources. However, the age of the ransomware datasets was not considered when training and testing these models. Therefore, the detection accuracy for those models is inaccurate since they learned using features from specific ransomware, old or new ransomware, and they did not learn using diverse ransomware features from different ages. This paper sheds light on the importance of considering the age of ransomware datasets and its effects on the detection accuracy of supervised machine learning models. This proves that supervised machine learning models trained using new ransomware dataset are inefficient in detecting old types of ransomware and vice versa. Moreover, this paper collected a large and diverse dataset of ransomware applications that comprises new and old ransomware developed during the period 2008–2020. Furthermore, the paper proposes a supervised machine learning model that is trained and tested using the diverse dataset. The experiments show that the proposed model is efficient in detecting Android ransomware regardless of its age by achieving an accuracy of approximately 97.48%. Moreover, the results shows that the proposed model outperforms the state-of-the-art approaches considered in this work.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
Machine Learning on Encrypted Data: Analyzing Efficiency and Accuracy Trade-Offs,"In this modern era, phishing has become a great problem. Because of this, it can be observed that the personal information of people is leaked from emails &amp; websites. Hence, it is needed that these instances of phishing are to be reduced. In doing one of the best tools that can be used is Machine Learning. This is a process of using historical data for making prediction of future scenarios. In this project, the details of the approached that can be used for the detection of phishing are analyzed. Moreover, the algorithms that are used by ML for this purpose are also envisaged here. The description of the process of collection of data is presented here. In addition to this, the results that shows the effectiveness of ML in the detection of phishing is also discussed here.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
How Machine Learning Classification Accuracy Changes in a Happiness Dataset with Different Demographic Groups,"This study aims to explore how machine learning classification accuracy changes with different demographic groups. The HappyDB is a dataset that contains over 100,000 happy statements, incorporating demographic information that includes marital status, gender, age, and parenthood status. Using the happiness category field, we test different types of machine learning classifiers to predict what category of happiness the statements belong to, for example, whether they indicate happiness relating to achievement or affection. The tests were initially conducted with three distinct classifiers and the best performing model was the convolutional neural network (CNN) model, which is a deep learning algorithm, achieving an F1 score of 0.897 when used with the complete dataset. This model was then used as the main classifier to further analyze the results and to establish any variety in performance when tested on different demographic groups. We analyzed the results to see if classification accuracy was improved for different demographic groups, and found that the accuracy of prediction within this dataset declined with age, with the exception of the single parent subgroup. The results also showed improved performance for the married and parent subgroups, and lower performances for the non-parent and un-married subgroups, even when investigating a balanced sample.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
High-accuracy prediction of colorectal cancer chemotherapy efficacy using machine learning applied to gene expression data,"Introduction: FOLFOX and FOLFIRI chemotherapy are considered standard first-line treatment options for colorectal cancer (CRC). However, the criteria for selecting the appropriate treatments have not been thoroughly analyzed.Methods: A newly developed machine learning model was applied on several gene expression data from the public repository GEO database to identify molecular signatures predictive of efficacy of 5-FU based combination chemotherapy (FOLFOX and FOLFIRI) in patients with CRC. The model was trained using 5-fold cross validation and multiple feature selection methods including LASSO and VarSelRF methods. Random Forest and support vector machine classifiers were applied to evaluate the performance of the models.Results and Discussion: For the CRC GEO dataset samples from patients who received either FOLFOX or FOLFIRI, validation and test sets were &amp;gt;90% correctly classified (accuracy), with specificity and sensitivity ranging between 85%-95%. In the datasets used from the GEO database, 28.6% of patients who failed the treatment therapy they received are predicted to benefit from the alternative treatment. Analysis of the gene signature suggests the mechanistic difference between colorectal cancers that respond and those that do not respond to FOLFOX and FOLFIRI. Application of this machine learning approach could lead to improvements in treatment outcomes for patients with CRC and other cancers after additional appropriate clinical validation.",CS,BIOINFO,85,Clear CS paper with focus on Bioinformatics & Computational Biology
DRMF: Optimizing machine learning accuracy in IoT crop recommendation with domain rules and MissForest imputation,"In the realm of IoT-driven precision agriculture, addressing missing data is crucial for reliable crop recommendation systems. This paper proposes the Domain Rules and MissForest (DRMF) algorithm to handle the above mentioned challenge. The proposed DRMF algorithm was thoroughly tested on an IoT agriculture dataset with the introduction of a missingness mechanism in the form of MAR with 10 % of missing values. A comparison analysis with the usual imputation techniques such as Mean Imputation, kNN Imputation, Linear Regression, EM Algorithm, Multiple Imputation, and the standard MissForest was performed and the proposed method was found to perform better. The DRMF algorithm attained an unmatched Root Mean Squared Error (RMSE) value of 0.025 and a Mean Absolute Error (MAE) value of 0.012, displaying a significant superiority over its competitors. It is important to note that the algorithm also achieved a Mean Absolute Percentage Error (MAPE) of 5.0% and an R-squared value of 0.970, with the overall accuracy rate being 99.0%. The quantitative findings serve to emphasize the effectiveness of the DRMF algorithm in improving the prediction accuracy of crop recommendation models. The novelty of this research is in the combined approach that merges the computational power of the MissForest algorithm, and the insight offered by domain-specific agricultural rules.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
ACS NSQIP Risk Calculator Accuracy Using a Machine Learning Algorithm Compared to Regression,"Background: The ACS NSQIP risk calculator (RC) uses regression to make predictions for 14, 30-day surgical outcomes. While this approach provides accurate (discrimination and calibration) risk estimates, they might be improved by machine learning (ML). To investigate this possibility, accuracy for regression-based risk estimates were compared to estimates from an extreme gradient boosting (XGB) ML algorithm.   Methods: A cohort of 5,020,713 million NSQIP patient records was randomly divided into 80% for model construction and 20% for validation. Risk predictions using regression and XGB-ML were made for 13 RC binary 30-day surgical complications and 1 continuous outcome (length of stay, LOS). For the binary outcomes, discrimination was evaluated using AUROC (area under the receiver operating characteristic curve) and AUPRC (area under the precision recall curve), and calibration was evaluated using Hosmer-Lemeshow (H-L) statistics. Mean squared error (MSE) and a calibration curve analog were evaluated for the continuous LOS outcome.   Results: For every binary outcome, discrimination (AUROC and AUPRC) was slightly greater for XGB-ML than for regression (mean [across the outcomes] AUROC was 0.8299 versus 0.8251, and mean AUPRC was 0.1558 versus 0.1476, for XGB-ML and regression, respectively). For each outcome miscalibration was greater (larger H-L values) with regression; there was statistically significant miscalibration for all regression-based estimates but only for 4 of 13 when XGB-ML was used. For LOS, MSE was lower for XGB-ML.   Conclusions: XGB-ML provided more accurate risk estimates than regression in terms of discrimination and calibration. Differences in calibration between regression and XGB-ML were of substantial magnitude and support transitioning the RC to XGB-ML.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine Learning Demonstrates High Accuracy for Disease Diagnosis and Prognosis in Plastic Surgery,"Introduction: Machine learning (ML) is a set of models and methods that can detect patterns in vast amounts of data and use this information to perform various kinds of decision-making under uncertain conditions. This review explores the current role of this technology in plastic surgery by outlining the applications in clinical practice, diagnostic and prognostic accuracies, and proposed future direction for clinical applications and research.   Methods: EMBASE, MEDLINE, CENTRAL and ClinicalTrials.gov were searched from 1990 to 2020. Any clinical studies (including case reports) which present the diagnostic and prognostic accuracies of machine learning models in the clinical setting of plastic surgery were included. Data collected were clinical indication, model utilised, reported accuracies, and comparison with clinical evaluation.   Results: The database identified 1181 articles, of which 51 articles were included in this review. The clinical utility of these algorithms was to assist clinicians in diagnosis prediction (n=22), outcome prediction (n=21) and pre-operative planning (n=8). The mean accuracy is 88.80%, 86.11% and 80.28% respectively. The most commonly used models were neural networks (n=31), support vector machines (n=13), decision trees/random forests (n=10) and logistic regression (n=9).   Conclusions: ML has demonstrated high accuracies in diagnosis and prognostication of burn patients, congenital or acquired facial deformities, and in cosmetic surgery. There are no studies comparing ML to clinician's performance. Future research can be enhanced using larger datasets or utilising data augmentation, employing novel deep learning models, and applying these to other subspecialties of plastic surgery.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Human Being Detection from UWB NLOS Signals: Accuracy and Generality of Advanced Machine Learning Models,"This paper studies the problem of detecting human beings in non-line-of-sight (NLOS) conditions using an ultra-wideband radar. We perform an extensive measurement campaign in realistic environments, considering different body orientations, the obstacles’ materials, and radar–obstacle distances. We examine two main scenarios according to the radar position: (i) placed on top of a mobile cart; (ii) handheld at different heights. We empirically analyze and compare several input representations and machine learning (ML) methods—supervised and unsupervised, symbolic and non-symbolic—according to both their accuracy in detecting NLOS human beings and their adaptability to unseen cases. Our study proves the effectiveness and flexibility of modern ML techniques, avoiding environment-specific configurations and benefiting from knowledge transference. Unlike traditional TLC approaches, ML allows for generalization, overcoming limits due to unknown or only partially known observation models and insufficient labeled data, which usually occur in emergencies or in the presence of time/cost constraints.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparison and analysis of prediction accuracy between traditional machine learning algorithms and XGBoost algorithm in music emotion classification,"Music emotion classification refers to determining the emotional types of music, such as happiness, sadness, anger, and passion, based on aspects like rhythm, melody, and tone. The development of artificial intelligence can be applied to music recommendation by employing machine learning algorithms to ascertain emotional attributes of music, enabling precise music suggestions for users to enhance their musical experience. This paper compares and analyzes the classification performance of traditional Random Forest machine learning algorithms and the XGBoost model on a Turkish music emotion dataset. We selected a comprehensive dataset from Kaggle, containing a vast array of music samples and their corresponding emotional labels, making it an extensive music emotion classification dataset. From the experimental results, it's evident that the traditional Random Forest machine learning model outperforms the XGBoost model in terms of accuracy, precision, and recall. The accuracy of the traditional Random Forest machine learning model stands at 80.8%, whereas the XGBoost model's accuracy is 75%. The recall rate for the traditional Random Forest machine learning model is 80.8%, while for XGBoost, it's 77.2%. The F1 score for the traditional Random Forest machine learning model is 80.5%, whereas for XGBoost, it's 75.3%. These parameters indicate that the traditional Random Forest machine learning model exhibits superior predictive performance in music emotion classification. However, the XGBoost model possesses its own advantages, such as faster learning and prediction speeds, high accuracy, and strong generalization capabilities. In summary, the traditional Random Forest machine learning model demonstrates better robustness and interpretability, effectively handling samples with noise and missing data, thus finding widespread practical application. On the other hand, the XGBoost model excels in rapid training and prediction, coupled with higher accuracy and versatility, making it advantageous in dealing with large datasets. The research outcomes of this paper hold significant importance for the study and application of music emotion classification. The experimental results presented herein offer valuable insights for researchers and practitioners, aiding them in selecting appropriate machine learning models, optimizing, and adjusting them to achieve the best classification results.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine Learning Predicts Accuracy in Eyewitnesses’ Voices,"Abstract An important task in criminal justice is to evaluate the accuracy of eyewitness testimony. In this study, we examined if machine learning could be used to detect accuracy. Specifically, we examined if support vector machines (SVMs) could accurately classify testimony statements as correct or incorrect based purely on the nonverbal aspects of the voice. We analyzed 3,337 statements (76.61% accurate) from 51 eyewitness testimonies along 94 acoustic variables. We also examined the relative importance of each of the acoustic variables, using Lasso regression. Results showed that the machine learning algorithms were able to predict accuracy between 20 and 40% above chance level (AUC = 0.50). The most important predictors included acoustic variables related to the amplitude (loudness) of speech and the duration of pauses, with higher amplitude predicting correct recall and longer pauses predicting incorrect recall. Taken together, we find that machine learning methods are capable of predicting whether eyewitness testimonies are correct or incorrect with above-chance accuracy and comparable to human performance, but without detrimental human biases. This offers a proof-of-concept for machine learning in evaluations of eyewitness accuracy, and opens up new avenues of research that we hope might improve social justice.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine learning for accuracy in density functional approximations,"AbstractMachine learning techniques have found their way into computational chemistry as indispensable tools to accelerate atomistic simulations and materials design. In addition, machine learning approaches hold the potential to boost the predictive power of computationally efficient electronic structure methods, such as density functional theory, to chemical accuracy and to correct for fundamental errors in density functional approaches. Here, recent progress in applying machine learning to improve the accuracy of density functional and related approximations is reviewed. Promises and challenges in devising machine learning models transferable between different chemistries and materials classes are discussed with the help of examples applying promising models to systems far outside their training sets.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing Underwater Object Recognition: Integrating Transfer Learning with Hybrid Optimization Techniques for Improved Detection Accuracy,"Underwater object recognition presents unique challenges due to varying water conditions, low visibility, and the presence of noise. This research proposes an advanced methodology that combines transfer learning and hybrid optimization techniques to enhance recognition accuracy in underwater environments. Specifically, a pre-trained EfficientNet model is employed for feature extraction, leveraging its capacity to capture diverse features in underwater images. The model is then optimized using a hybrid Particle Swarm Optimization and Genetic Algorithm (PSOGA) to fine-tune hyperparameters such as learning rate, number of layers, and activation functions. This hybrid approach balances exploration and exploitation in the search space, allowing the model to converge on an optimal solution that maximizes accuracy. The model is evaluated against nine existing deep learning models, including ResNet-50, VGG-16, EfficientNet-B0, and MobileNetV2. The proposed PSOGA model achieves a superior accuracy of 98.32%, surpassing the best-performing models like EfficientNet-B0, which reached 95.89%. Furthermore, the model outperforms traditional optimizers like Adam, RMSprop, and AdaGrad, which attained lower accuracies. Precision, recall, and F1-score for the PSOGA model also demonstrate remarkable improvements, highlighting the model's effectiveness in underwater object recognition. The combination of transfer learning and hybrid optimization enables the model to generalize well across diverse underwater environments while maintaining computational efficiency.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Enhancing the Accuracy of Question-Answering Systems Using Machine Learning: A Case Study in the Tourism Domain,"With the rapid development of smart tourism, the accuracy of question-answering systems (QAS) has become crucial in enhancing user experience, especially in contexts where human-computer interaction is central. In tourism, QAS typically handle queries related to locations, attractions, and transportation, where the information varies widely and the phrasing can change frequently. This makes the semantic understanding and response generation a challenging task for QAS. This paper explores how machine learning (ML) has been employed to improve QAS accuracy, focusing on intent recognition, semantic matching, context modeling, and the use of pre-trained language models (PLMs). By analyzing the various techniques and methods used in tourism, this study provides a comparative overview of their effectiveness in addressing domain-specific needs. The findings suggest that incorporating domain knowledge, fine-tuning models, and leveraging multimodal semantics and multilingual capabilities are key strategies for improving the accuracy of tourism-specific QAS.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Thermal Comfort Prediction Accuracy with Machine Learning between Regression Analysis and Naïve Bayes Classifier,"Various data analysis methods can make thermal comfort prediction models. One method that is often used is multiple linear regression statistical analysis. Regression analysis needs to be checked for accuracy with other analytical methods. This study compares the making of a thermal comfort prediction model with regression analysis and naïve Bayes analysis. The research method used quantitative methods for data collection regarding thermal comfort. The thermal comfort variable, consisting of eight independent variables and one dependent variable, was measured at Wonosobo High School, Indonesia. The analysis to make the prediction model was carried out with two different analyses: multiple linear regression analysis and naïve Bayes analysis. The results show that naïve Bayes is more accurate than multiple linear regression analysis.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparative Analysis of Machine Learning Algorithms with SMOTE and Boosting Techniques in Accuracy Improvement,"This research explores and enhances accuracy in sentiment classification related to Indonesia's Capital City relocation by combining Naive Bayes (NB), Random Forest (RF), SMOTE, and XGBoost. The study addresses challenges of unbalanced data and complexity in social media sentiment analysis. The combination of RF with SMOTE achieved the highest accuracy at 91.25%, demonstrating SMOTE's effectiveness in balancing the dataset and improving minority class detection. While adding XGBoost slightly reduced accuracy (90.92%), it increased the NB model's accuracy from 77.45% to 85.97% when combined with SMOTE. RF alone reached 87.46% and improved to 88.78% with XGBoost. The study underscores the importance of selecting and combining techniques to maximize sentiment prediction accuracy. Future research could explore deep learning or transformer models for even better results, offering deeper insights into public sentiment and aiding effective policy strategy development.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Evaluating the Accuracy of Classification Algorithms for Detecting Heart Disease Risk,"The healthcare industry generates enormous amounts of complex clinical data that make the prediction of disease detection a complicated process. In medical informatics, making effective and efficient decisions is very important. Data Mining (DM) techniques are mainly used to identify and extract hidden patterns and interesting knowledge to diagnose and predict diseases in medical datasets. Nowadays, heart disease is considered one of the most important problems in the healthcare field. Therefore, early diagnosis leads to a reduction in deaths. DM techniques have proven highly effective for predicting and diagnosing heart diseases. This work utilizes the classification algorithms with a medical dataset of heart disease; namely, J48, Random Forest, and Naïve Bayes to discover the accuracy of their performance. We also examine the impact of the feature selection method. A comparative and analysis study was performed to determine the best technique using Waikato Environment for Knowledge Analysis (Weka) software, version 3.8.6. The performance of the utilized algorithms was evaluated using standard metrics such as accuracy, sensitivity and specificity. The importance of using classification techniques for heart disease diagnosis has been highlighted. We also reduced the number of attributes in the dataset, which showed a significant improvement in prediction accuracy. The results indicate that the best algorithm for predicting heart disease was Random Forest with an accuracy of 99.24%.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Optimizing Machine Learning-based Sentiment Analysis Accuracy in Bilingual Sentences via Preprocessing Techniques,"With the recent advances in Natural Language Processing (NLP) technologies, the ability to process, analyze, and understand sentiments expressed in user-generated reviews regarding the products and services they use is becoming more achievable. Despite the latest improvements in this field, little attention has been given to multilingual sentiment analysis. In this article, a framework is presented for sentiment analysis in Arabic and English using two datasets (ASTD, AJGT) along with their translations. Preprocessing techniques, including n-gram tokenization, Arabic-specific stop words removal, punctuation removal, removing repeating characters, parts of speech tagging, stemming, and lemmatization, are applied. Four machine learning classifiers, namely Logistic Regression (LR), Random Forest (RF), Naive Bayes (NB), and Support Vector Machine (SVM), are employed. We highlight existing specialized research in sentiment analysis for Arabic and English, as well as the employed techniques in each. Furthermore, the impact of preprocessing on accuracy results for both Arabic and English languages is investigated through separate experiments for each step. Experimental results on the ASTD dataset demonstrate close performance across classifiers, with the SVM classifier achieving the highest accuracy of 70%. However, the accuracy varied when using the AJGT dataset, with the NB classifier yielding the best accuracy at approximately 87%. The experiments on the translated datasets from Arabic to English did not exhibit significant differences, although some features performed slightly better using the Arabic datasets.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Evaluating Material Failure with Synthetic Data and Machine Learning for Enhanced Predictive Accuracy,"This study investigates the use of Random Forest and Gradient Boosting models to predict material failure based on stress and strain data. Both models achieved high accuracy, with Gradient Boosting slightly outperforming Random Forest in identifying failure cases, as demonstrated by a higher recall for the failure class. A decision boundary visualization reveals that the Random Forest model effectively separates failure and non-failure regions, though some misclassification persists. This comparative analysis, supported by confusion matrices and classification reports, highlights the strengths and limitations of each model in capturing failure patterns. Future work includes exploring additional features, advanced algorithms, interpretability methods, and real-time monitoring systems to improve prediction accuracy and industrial applicability.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing Machine Learning Accuracy in Detecting Preventable Diseases using Backward Elimination Method,"In the current landscape of abundant high-dimensional datasets, addressing classification challenges is pivotal. While prior studies have effectively utilized Backward Elimination (BE) for disease detection, there is a notable absence of research demonstrating the method's significance through comprehensive comparisons across diverse databases. The study aims to extend its contribution by applying BE across multiple machine learning algorithms (MLAs)Nave Bayes (NB), k-Nearest Neighbors (KNN), and Support Vector Machine (SVM)on datasets associated with preventable diseases (i.e. heart failure (HF), breast cancer (BC), and diabetes). This study aims to elucidate and recommend significant differences observed in the application of BE across diverse datasets and machine learning (ML) methods. This study conducted testing on four distinct datasetsraisin, HF, BC, and early-stage diabetes risk prediction datasets. Each dataset underwent evaluation with three MLAs: NB, KNN, and SVM. The application of BE successfully eliminated non-significant attributes, retaining only influential ones in the model. In addition, t-test results revealed a significant impact on accuracy across all datasets (p-value &lt; 0.05). In specific algorithmic evaluations, SVM exhibited the highest accuracy for the raisin dataset at 87.22%. Additionally, KNN attained the utmost accuracy in the heart failure dataset with an accuracy of 86.31%. In the breast cancer dataset, KNN again excelled, achieving an accuracy of 83.56%. For the diabetes dataset, KNN proved the most accurate, reaching 96.15%. These results underscore the efficacy of BE in enhancing the execution of MLAs for disease detection.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving the Accuracy of Neurons Spike Sorting by Using Supervised Machine Learning,"The brain is important for both the functioning and reasoning ability of the body. It plays a fundamental role in the coordination of body functioning as well as reasoning and thinking. To understand how the brain is working, we need to know how neurons communicate with each other by firing (Action potential) which is known as spike. To record these activities neurologists used the multi-electrode which record thousands of spikes at the same time. Therefore, neurologists used the Spike Sorting Algorithm (SSA) to know which spike belongs to which neuron. The accuracy of the spike sorting is the most important point. Accordingly, machine learning is used to improve the accuracy of the spike sorting. In this paper, the Principal Component Analysis (PCA) is implemented to extract features and for clustering step, the Supervised Machine Learning is applied by using the Support Vector Machine (SVM) and K-Nearest Neighbors (KNN) to compare the accuracy of the supervised clustering with the Template Matching method. A comparison between the results of Applied Machine Learning is achieved at different levels of noise to check the accuracy of each algorithm. The results showed that when the noise level was low, KNN accuracy reached 100% while SVM reached 95% and template 100 %. However, when the noise level increased to 0.5, the accuracy of KNN became 94 % and template 85.6 % and SVM 90 %.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
A Review Article On Enhancing Email Spam Filter’s Accuracy Using Machine Learning,"In today’s era, almost everyone is using emails on their daily basis. In our proposed research, we suggest a machine learning-based strategy for enhancing email spam filters' accuracy. Traditional rule-based filters have grown less effective as spam emails have multiplied exponentially. Models can be trained to identify emails as spam or not using machine learning algorithms, particularly supervised learning. We need to create a simple and straightforward machine learning model in order to reach more accurate results while categorizing email spam. We picked the Naive Bayes technique for our model since it is quicker and more accurate than other algorithms. The suggested method can have incorporated into current email systems to enhance spam filtering functionality. This review paper provides an overview of the machine learning model we have suggested.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Accuracy of Machine Learning Models to Predict In-hospital Cardiac Arrest,"Purpose/Aims Despite advances in healthcare, the incidence of in-hospital cardiac arrest (IHCA) has continued to rise for the past decade. Identifying those patients at risk has proven challenging. Our objective was to conduct a systematic review of the literature to compare the IHCA predictive performance of machine learning (ML) models with the Modified Early Warning Score (MEWS).   Design The systematic review was conducted following the Preferred Reporting Items of Systematic Review and Meta-Analysis guidelines and registered on PROSPERO CRD42020182357.   Method Data extraction was completed using the Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modeling Studies checklist. The risk of bias and applicability were evaluated using the Prediction model Risk of Bias Assessment Tool.   Results Nine articles were included in this review that developed or validated IHCA prediction models and compared them with the MEWS. The studies by Jang et al and Kim et al showed that their ML models outperformed MEWS to predict IHCA with good to excellent predictive performance.   Conclusions The ML models presented in this systematic review demonstrate a novel approach to predicting IHCA. All included studies suggest that ML models had similar or better predictive performance compared with MEWS. However, there is substantial variability in performance measures and concerns for risk of bias.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Boosting Reservoir Prediction Accuracy: A Hybrid Methodology Combining Traditional Reservoir Simulation and Modern Machine Learning Approaches,"This study presents a comprehensive investigation into the application of reservoir simulation and machine learning techniques to improve the understanding and prediction of reservoir behavior, focusing on the Sarir C-Main field. The research uses various data sources to develop robust reservoir static and dynamic models, including seismic cubes, well logs, base maps, check shot data, and production history. The methodology involves data quality control, log interpretation, seismic interpretation, horizon and surface interpretation, fault interpretation, gridding, domain conversion, property and petrophysical modeling, well completion, fluid model definition, and rock physics functions. History matching and prediction are performed using simulation cases, and machine learning techniques such as data gathering, cleaning, dynamic time warping (DTW), long short-term memory (LSTM), and transfer learning are applied. The results obtained through the Petrel simulation demonstrate the effectiveness of depletion strategy, history matching, and completion in capturing reservoir behavior. Furthermore, the machine learning techniques, specifically DTW and LSTM, exhibit promising results in predicting oil production. The study concludes that machine learning approaches, such as the LSTM model, offer distinct advantages. They require significantly less time and can yield reliable predictions. By leveraging the power of transfer learning, accurate predictions can be achieved efficiently when limited data are available, offering a more streamlined and practical alternative to traditional reservoir simulation methods.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
High Accuracy of Epileptic Seizure Detection Using Tiny Machine Learning Technology for Implantable Closed-Loop Neurostimulation Systems,"Background: Epilepsy is one of the most common and devastating neurological disorders, manifesting with seizures and affecting approximately 1–2% of the world’s population. The criticality of seizure occurrence and associated risks, combined with the overwhelming need for more precise and innovative treatment methods, has led to the development of invasive neurostimulation devices programmed to detect and apply electrical stimulation therapy to suppress seizures and reduce the seizure burden. Tiny Machine Learning (TinyML) is a rapidly growing branch of machine learning. One of its key characteristics is the ability to run machine learning algorithms without the need for high computational complexity and powerful hardware resources. The featured work utilizes TinyML technology to implement an algorithm that can be integrated into the microprocessor of an implantable closed-loop brain neurostimulation system to accurately detect seizures in real-time by analyzing intracranial EEG (iEEG) signals. Methods: A dataset containing iEEG signal values from both non-epileptic and epileptic individuals was utilized for the implementation of the proposed algorithm. Appropriate data preprocessing was performed, and two training datasets with 1000 records of non-epileptic and epileptic iEEG signals were created. A test dataset with an independent dataset of 500 records was also created. The web-based platform Edge Impulse was used for model generation and visualization, and different model architectures were explored and tested. Finally, metrics of accuracy, confusion matrices, and ROC curves were used to evaluate the performance of the model. Results: Our model demonstrated high performance, achieving 98% and 99% accuracy on the validation and test EEG datasets, respectively. Our results support the use of TinyML technology in closed-loop neurostimulation devices for epilepsy, as it contributes significantly to the speed and accuracy of seizure detection. Conclusions: The proposed TinyML model demonstrated reliable seizure detection in real-time by analyzing EEG signals and distinguishing epileptic activity from normal brain electrical activity. These findings highlight the potential of TinyML in closed-loop neurostimulation systems for epilepsy, enhancing both speed and accuracy in seizure detection.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Temporal shift and accuracy of machine learning in heart transplant outcomes,"Abstract  Background Accurate prediction of outcomes following a heart transplant is critical to explaining risks and benefits to patients and decision-making when considering potential organ offers. Given the large number of potential variables to be considered, this task may be most efficiently performed using machine learning (ML).   Purpose We trained and tested different ML algorithms to accurately predict outcomes following a cardiac transplant using the United Network of Organ Sharing (UNOS) database.   Methods We included 67 939 adult and pediatric patients enrolled in the UNOS database between January 1994 and December 2016 who underwent cardiac transplantation (median age 53 [IQR 38 – 60], 72.7% males). In our models, as an input, we included 114 features that have been collected from recipients and donors prior to transplant. The primary outcome was all-cause mortality at one-year post-transplant. We evaluated three different ML methods: XGBoost, Random Forest (RF) and L2 regularized logistic regression. Algorithms were trained and tested using shuffled 10-fold cross-validation (CV) as well as rolling CV. In the rolling CV, to mimic prospective procedure, ML models were trained by incrementally adding patients according to transplant year and testing models on the data in the following year. The hyperparameters, controlling the learning process, were tuned using Bayesian optimization. Prognostic accuracy for one-year all-cause mortality was characterized using the area under the receiver-operating characteristic curve (AUC).   Results In total, 8,394 patients died within 1 year of transplant. We observed a substantial difference in prognostic accuracy between the shuffled 10-fold CV and the rolling CV. In the 10-fold CV, XGBoost and RF achieved high predictive performance with AUC of 0.848 (95% CI: 0.842–0.854) and 0.891 (95% CI: 0.886–0.896), respectively. In the rolling CV, which is a more realistic setting, AUC dropped to 0.673 (95% CI: 0.661–0.684) for XGBoost and 0.670 (0.657–0.683) for RF. Predictive performance of L2 regularized logistic regression remained stable across the two CV procedures, achieving AUC 0.669 (95% CI: 0.662–0.676) in the 10-fold shuffled CV and 0.665 (95% CI: 0.649–0.680) in the rolling CV procedure (Figure).   Conclusions Our study suggests that ML models could be used to predict mortality in the first year post-transplant. We also show that the choice of CV procedure is crucial for evaluating ML models, particularly in data collected over a long period of time. The difference between the shuffled and rolling CV in the predictive performance of the tree-based ML models might indicate temporal dataset shift. In the rolling CV, all three methods achieved similar predictive performance.   Funding Acknowledgement Type of funding sources: Public grant(s) – National budget only. Main funding source(s): Research Foundation Flanders (FWO)",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Ensemble machine learning prediction accuracy: local vs. global precision and recall for multiclass grade performance of engineering students,"This study examines the prediction accuracy of ensemble machine learning models by comparing local and global precision, recall, and accuracy for multiclass grading of engineering students. It also investigates the performance of various machine learning models in predicting the multiclass grading outcomes for these students. The primary goal is to address challenges in multiclass data preparation and evaluate the best machine learning models using both micro and macro accuracy metrics derived from baseline comparisons. The results highlight a significant comparative analysis of prediction accuracy across different algorithms, emphasizing the importance of employing multiple receiver operating characteristic curves, areas under the curves, and a one-vs-rest classification approach when target features are represented as letter grades. The algorithms examined include decision trees, K-nearest neighbors, random forests, support vector machines, XGBoost, gradient boosting, and bagging. Gradient boosting achieves the highest global accuracy for macro predictions at 67%. It is followed by random forests at 64%, bagging at 65%, K-nearest neighbors at 60%, XGBoost at 60%, decision trees at 55%, and support vector machines at 59%. When considering micro prediction accuracy at the individual student level, support vector machines, random forests, and XGBoost closely align with true student grades, with accuracies of 19, 22, and 33%, respectively, at baseline. Notably, these models accurately predict the C grade with 97% precision, whereas predicting the A grade proves more challenging, with an accuracy of only 66%. These findings are further corroborated by precision-recall error plots. The grid search for random forest algorithms achieved a score of 79% when optimally tuned; however, the training accuracy was 99%. The results have implications for both students and educational institutions, helping identify areas for improvement and recognizing high achievers, which ultimately contributes to enhanced academic outcomes for engineering students.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
A principled machine learning framework improves accuracy of stage II colorectal cancer prognosis,"AbstractAccurate prognosis is fundamental in planning an appropriate therapy for cancer patients. Consequent to the heterogeneity of the disease, intra- and inter-pathologist variability, and the inherent limitations of current pathological reporting systems, patient outcome varies considerably within similarly staged patient cohorts. This is particularly true when classifying stage II colorectal cancer patients using the current TNM guidelines. The aim of the present work is to address this problem through the use of machine learning. In particular, we introduce a data driven framework which makes use of a large number of diverse types of features, readily collected from immunofluorescence imagery. Its outstanding performance in predicting mortality in stage II patients (AUROC = 0:94), exceeds that of current clinical guidelines such as pT stage (AUROC = 0:65), and is demonstrated on a cohort of 173 colorectal cancer patients.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine learning interatomic potential with DFT accuracy for general grain boundaries in α-Fe,"AbstractTo advance the development of high-strength polycrystalline metallic materials towards achieving carbon neutrality, it is essential to design materials in which the atomic level control of general grain boundaries (GGBs), which govern the material properties, is achieved. However, owing to the complex and diverse structures of GGBs, there have been no reports on interatomic potentials capable of reproducing them. This accuracy is essential for conducting molecular dynamics analyses to derive material design guidelines. In this study, we constructed a machine learning interatomic potential (MLIP) with density functional theory (DFT) accuracy to model the energy, atomic structure, and dynamics of arbitrary grain boundaries (GBs), including GGBs, in α-Fe. Specifically, we employed a training dataset comprising diverse atomic structures generated based on crystal space groups. The GGB accuracy was evaluated by directly comparing with DFT calculations performed on cells cut near GBs from nano-polycrystals, and extrapolation grades of the local atomic environment based on active learning methods for the entire nano-polycrystal. Furthermore, we analyzed the GB energy and atomic structure in α-Fe polycrystals through large-scale molecular dynamics analysis using the constructed MLIP. The average GB energy of α-Fe polycrystals calculated by the constructed MLIP is 1.57 J/m2, exhibiting good agreement with experimental predictions. Our findings demonstrate the methodology for constructing an MLIP capable of representing GGBs with high accuracy, thereby paving the way for materials design based on computational materials science for polycrystalline materials.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparison and Analysis of the Accuracy of Various Machine Learning Algorithms in Bitcoin Price Prediction,"Based on the dataset of Bitcoin Price dataset, this paper studied Bitcoin price prediction by using support vector machine model, random forest model, neural network model, XGBoost model and LightGBM model. The models were evaluated by MSE, RMSE, MAE, MAPE and R. First, we divided the Bitcoin Price dataset into a training set and a test set according to a ratio of 7:3, with 70 as the training set and 30 as the test set. We take the stock price change (return) as the target variable, the other variables as the input variables, and use the training set to train the model and the test set to test the model. After model comparison, we found that XGBoost's MSE, RMSE, MAE, MAPE and R are all optimal, and its prediction effect is also the best. The performance of the other four models ranges from good to different, including LightGBM, random Forest, support vector machine and neural network. Among them, the MSE of the neural network is dozens of times that of the other four models, so it performs the worst. The XGBoost model performs well in dealing with high-dimensional sparse data and nonlinear relationships, while LightGBM and random Forest are suitable for dealing with large-scale data. Support vector machines are suitable for dealing with high-dimensional data and nonlinear relationships, while neural networks require more tuning and optimization to take advantage of their advantages. In summary, the research results of this paper can provide value for the prediction of Bitcoin price in the future, and also provide a certain reference for selecting a suitable machine learning model.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Evaluation of Machine Learning Model’s Accuracy for Heart Attack Prediction,"Machine learning is a type of artificial intelligence that allows applications to become more accurate at predicting results without being explicitly programmed to do so. Machine learning techniques such as Decision tree, K-Nearest Neighbors (KNN), Naïve Bayes, Support Vector Machine (SVM), Logistic Regression, Multi layer perceptron etc, are used to predict the heart attack. These studies offers an analysis of the existing machine learning algorithm and provides a comprehensive overview of the previous research and evaluate the accuracy of the machine learning modals. Both low and high-risk patients for a heat attack were evaluated for the study. The results indicate that methods Logistic Regression and Support Vector Machine algorithms outperform other traditional classifiers in terms of prediction accuracy and generalization.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Implementation of Machine Learning to Increase the Accuracy of Forecasting the Operating Modes of Deep-Sea Pumping Stations,"This study endeavors at implementing ML algorithms that are capable of refining the forecast of operating modes of deep-water pumping stations which offshore processes draw their energy from. The classic forecasting methods often do not take into account the complexity of the underwater environment, and so they tend to show suboptimal efficiency, higher maintenance costs and of course wastage of resources. For this study, different ML algorithms including neural networks, support vector machines, random forests, gradient boosting, and linear regression are employed to evaluate how they can imagine operating circumstances under conditions of changes. The rental housing datasets, which contain historical operational data, environmental factors as well as system parameters, are applied to training and validation processes. Data illustrates enhanced capabilities of AI systems with leading candidates being neural network, random forests, and gradient boosting in demonstrating the exact relationships in the sample. The models deliver better performance than the traditional techniques, thereby allowing to assess in-depth the interaction scheme between environmental variables and working modes. These pivotal variables, depth, temperature and pump characteristics are among those that got scrutinized; therefore, insights as to what ought to be embraced for an efficient prediction. Comparative analyses bring forth the tradeoff between the model complexity and interoperability, which state that the algorithm chosen toward application must be thought out very wisely. Ensemble models, which contain a spectrum of different models with each one strong with its own abilities, are seen to be among the balanced way of making precise and useful forecasts. The deep sea water pumping stations developed model based on ML(ML) represents an example in practice that sets the framework for increased operational efficiency, reduced maintenance costs, and optimized resource utilization. The findings of this research uncover crucial aspect for engineers, researchers, as well as industry, experts who are prospects of deep-water resource extraction sector. This itself implies a transformation approach toward addressing the problems encountered in dynamic deep-sea environments. With developments in the area of ML, there is a lot of scope for future research ventures to explore new algorithms and real-time techniques which will help to further improve the forecasting capabilities and will certainly result in viable offshore operations. Thus, it can be said that the future of sustainable and resilient offshore operations can to some extent be credited to ML.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Random Forest and Extreme Learning Machine Algorithms for High Accuracy Credit Card Fraud Detection,"Abstract: The banking sector is facing a huge issue with credit card fraud, and research has shown that machine learning algorithms are a useful tool for identifying fraudulent actions of this kind. In this investigation, we offer a method for detecting fraudulent use of credit cards that makes use of a hybrid of two machine learning algorithms known as Random Forest (RF) and Extreme Learning Machine (ELM). We compiled a dataset using information obtained from a wide variety of sources, and then we preprocessed it to eliminate any inconsistencies and errors. Following this, the RF and ELM algorithms were put into action and trained on the dataset in order to provide forecasts on the occurrence of fraudulent acts. Measures of performance such as determining how accurate the algorithms are are examples. According to the findings of our research, the ELM algorithm is more accurate than the RF algorithm when it comes to the detection of fraudulent credit card activity.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
"Enhancing Predictive Accuracy in Machine Learning: Techniques for Model Optimization and Feature Selection""s","Artificial intelligence (AI) is very relevant in areas like healthcare, finance and e-commerce where a close estimate is crucial in decision making. There are still problems with performance even with the recent developments of Machine Learning models, which stem from insufficient preprocessing, unsuitable feature selection, and poor hyperparameter optimization, which restrict their applicability to multiple domains. This research aims at developing a systematic, step-by-step, and stage-wise improvement of the accuracy of the ML models through data preprocessing and feature selection, and model selection. This study evaluates techniques such as normalization, Recursive Feature Elimination (RFE), and Bayesian hyperparameter tuning by applying this approach to datasets in the healthcare, finance, and e-commerce domains. The findings show that preprocessing increases the accuracy by 5-8%, while RFE maintains 95% of the accuracy with a feature reduction of 30-50%, Bayesian optimization also increases the accuracy by 10-15%, making the overall accuracy of the models to be 96%. This work underscores the importance of the proposed integrated approach for constructing reliable, explainable, and scalable ML models for various fields. The results of the study provide a clear and easily replicable approach useful for future studies in the field and for industries that require high levels of accuracy and model parsimony.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
The Accuracy Analysis of Different Machine Learning Classifiers for Detecting Suicidal Ideation and Content,"Suicide is the matter of purposely causing one’s death and suicidal ideation refers to thoughts or preoccupations with ending one’s own life. Studies have explored verbal and written communications related to suicide, including analyzing suicide notes, online discussions, and social media posts to identify linguistic and content markers that may help in early detection and intervention. The primary purpose of this study is to detect signs of risk of suicide/self-harm in social media users by investigating several frequency-based featuring and prediction-based featuring methods along with different baseline machine learning classifiers. The algorithms applied for analysis are Decision Tree, K-Nearest Neighbors, Random Forest, Multinomial Naïve Bayes, and SVM. Our experimental results showed that the best performance is obtained by the FastText embedding with SVM model having the highest accuracy of 93.76% which outperforms other baselines. The aim of this work is to learn the significance of analysis and do a comparative study of algorithms to find the best suited algorithm.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine Learning Models for Predicting Hypothyroidism: Utilizing Synthetic Data for Improved Accuracy,"This study presents a novel approach to early hypothyroidism detection by integrating synthetic data generation with machine learning (ML) techniques. Facing the challenge of limited and imbalanced healthcare datasets, we employ synthetic data to augment training sets, ensuring a richer and more diverse data pool for ML application. Key indicators of early hypothyroidism are distilled through feature selection, optimizing ML model inputs. We test various ML classifiers, including Support Vector Machines (SVM), Random Forests (RF), and Gradient Boosting Machines (GBM), demonstrating enhanced diagnostic accuracy with our approach. Initial outcomes suggest that combining synthetic data with ML significantly boosts early detection capabilities, offering a promising direction for overcoming traditional data scarcity in medical diagnostics.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Accuracy assessment of applied supervised machine learning models on usual data probability distributions,"Abstract In this paper, an application analysis of supervised classification techniques on several probability distributions is carried out. Accuracy as well as usual standard metrics have been highlighted to rate the performance of generated learning models. Using data that fit different distributions, we investigated whether the application of a classification method had an optimizing impact on the accurateness of its correlated learning model.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving Accuracy in Word Class Tagging through the Combination of Machine Learning Systems,"We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system. We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora. Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second-stage classifiers. All combination taggers outperform their best component. The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Comparision of Different Machine Learning Algorithms to Predict the Diagnostic Accuracy Parameters of Celiac Serological Tests,"Celiac disease; is an autoimmune digestive system disease characterized by chronic intestinal inflammation and villus antrophy and triggered by dietary gluten genetically susceptible individuals. Diagnosis is based on serological tests and small bowel biopsy. Because of the diversity in the clinical features of the disease, various patient profile and the non-standardized serological tests, it is difficult to diagnose the celiac disease. Sensitivity, specificity, positive and negative predictive values are important parameters for the accuracy of the tests and they are missing in some clinicial studies. It is difficult do standardize the tests with these missing values for clinicians. The aim of this study is to train different machine learning algorithms and to test their performance in prediction of the diagnostic accurary parameters of celiac serological tests. Decision trees are effective machine learning algorithms for predicting potential covariates with %88,7 accuracy.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Improving Real Estate Investment Trusts (REITs) time-series prediction accuracy using machine learning and technical analysis indicators,"Abstract The primary goal of investors who include Real Estate Investment Trusts (REITs) in their portfolios is to achieve better returns while reducing the overall risk of their investments. REITs are entities responsible for owning and managing real estate properties. To achieve greater returns while reducing risk, it is essential to accurately predict future REIT prices. This study explores the predictive capability of five different machine learning algorithms used to predict REIT prices. These algorithms include Ordinary Least Squares Linear Regression, Support Vector Regression, k-Nearest Neighbours Regression, Extreme Gradient Boosting, and Long/Short-Term Memory Neural Networks. Additionally, historical REIT prices are supplemented with Technical Analysis indicators (TAIs) to aid in price predictions. While TA indicators are commonly used in stock market forecasting, their application in the context of REITs has remained relatively unexplored. The study applied these algorithms to predict future prices for 30 REITs from the United States, United Kingdom, and Australia, along with 30 stocks and 30 bonds. After obtaining our price predictions, we employ a Genetic Algorithm (GA) to optimise weights of a diversified portfolio. Our results reveal several key findings: (i) all machine learning algorithms demonstrated low average and standard deviation values in the error rate distributions, outperforming commonly used statistical benchmarks such as Holt’s Linear Trend Method (HLTM), Trigonometric Box-Cox Autoregressive Time Series (TBATS), and Autoregressive Integrated Moving Average (ARIMA); (ii) incorporating Technical Analysis indicators in the ML algorithms resulted in a significant reduction in prediction errors, up to 60% in some cases; and (iii) a multi-asset portfolio constructed using predictions that incorporated Technical Analysis indicators outperformed a portfolio based solely on predictions derived from past prices. Furthermore, this study employed Shapley Value-based techniques, specifically SHAP and SAGE, to analyse the importance of the features used in the analysis. These techniques provided additional evidence of the value added by Technical Analysis indicators in this context.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Evaluation of the Accuracy of Machine Learning Predictions of the Czech Republic’s Exports to the China,"The objective of this contribution is to predict the development of the Czech Republic’s (CR) exports to the PRC (People’s Republic of China) using ANN (artificial neural networks). To meet the objective, two research questions are formulated. The questions focus on whether growth in the CR’s exports to the PRC can be expected and whether MLP (Multi-Layer Perceptron) networks are applicable for predicting the future development of the CR’s exports to the PRC. On the basis of previously obtained historical data, ANN with the best explanatory power are generated. For the purpose specified, three experiments are carried out, the results of which are described in detail. For the first, second and third experiments, ANN for predicting the development of exports are generated on the basis of a time series with a 1-month, 5-month and 10-month time delay, respectively. The generated ANN are the MLP and regression time series neural networks. The MLP turn out to be the most efficient in predicting the future development of the CR’s exports to the PRC. They are also able to predict possible extremes. It is also determined that the USA–China trade war has significantly affected the CR’s exports to the PRC.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Prediction of Accuracy in Emergency Health Records using Hybrid Machine Learning Model,"The quantity of digital information contained in electronic health records(EHR) has increased dramatically during the last ten years. Numerous researchers have discovered that these records may be used for a variety of other purposes as well, including applications in clinical informatics. Additionally, within the same time period, significant advancements in the area of deep learning have been made by the machine learning community. Using EHR data, we examine the existing research on applying deep learning to clinical activities. In this article we will discuss various deep learning techniques used for the classification of electronic health records along with proposing of Hybrid model for finding classification accuracy of various models.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Enhancing diagnostic accuracy for unidentified primary tumors with remora-optimized novel machine learning approach,"Unidentified originating Tumors (UPT) are a kind of cancer where the originating location of the disease is still unclear, despite cancer cells throughout the body. In other words, even when malignant cells are present, medical professionals are unable to pinpoint the precise organ or tissue where the disease first started. The medical industry offers a big opportunity for Machine Learning (ML) to contribute significantly to illness prediction. One of the primary health challenges that each country faces is the tumor or cancer. In this study, we create a Remora Optimized Gated Recurrent Neural Network (RO-GRNN) to predict UPTs. The proposed approach consists of three stages: pre-processing, classification, and optimization. At first, we classified the UPT using the datasets given in the UCI ML library. The acquired Magnetic Resonance Imaging (MRI)/ Computed Tomography (CT) images will therefore have noise, lowering the efficiency with which classification may be accomplished. Pre-processing techniques like filtering and contrast augmentation could be used to eliminate unwanted noise from the offered images. The Gated Recurrent Neural Network (GRNN) and the Remora Optimization Algorithm (ROA) were both used in the development of the suggested classifier. Multiple optimization procedures, including basic and parametric optimization, take advantage of the ROA inside the GRNN. The proposed approach is employed in Origin Pro, and F1-Measure, recall, precision, and accuracy are some of the performance matrices that are used to evaluate effectiveness. The suggested approach is contrasted with the current ANN, SVM, and LSTM approaches. This research revealed that the proposed method has a 98% accuracy rate for UPT prediction.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Machine Learning Approaches for Effective Credit Card Fraud Detection: Addressing Imbalance and Enhancing Accuracy,"Credit card fraud has emerged as a significant threat to the financial sector, driven by the rapid growth in online transactions and the evolving sophistication of fraudulent activities. This research aims to design and implement a machine learning-based solution capable of detecting fraudulent credit card transactions effectively. By addressing challenges such as dataset imbalance and false positives, the research employs preprocessing techniques including Synthetic Minority Oversampling Technique (SMOTE), along with advanced machine learning algorithms like Logistic Regression, XGBoost, and Isolation Forest. It highlights the potential of these models to enhance fraud detection accuracy and scalability, providing a practical and deployable tool for real-world applications.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Weighted <i>k</i>‐nearest neighbor based data complexity metrics for imbalanced datasets,"AbstractEmpirical behavior of a classifier depends strongly on the characteristics of the underlying imbalanced dataset; therefore, an analysis of intrinsic data complexity would appear to be vital in order to choose classifiers suitable for particular problems. Data complexity metrics (CMs), a fairly recent proposal, identify dataset features which imply some difficulty for the classification task and identify relationships with classifier accuracy. In this paper, we introduce two CMs for imbalanced datasets, which help in explaining the factors responsible for the deterioration in classifier performance. These metrics are based on the weighted k‐nearest neighbors approach. The experiments are performed in MATLAB software using 48 simulated datasets and 22 real‐world datasets for different choices of neighborhood size k considered as 3, 5, 7, 9, 11. The results help to illustrate the usefulness of the proposed metrics.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
"Review of statistical network analysis: models, algorithms, and software","AbstractThe analysis of network data is an area that is rapidly growing, both within and outside of the discipline of statistics.This review provides a concise summary of methods and models used in the statistical analysis of network data, including the Erdős–Renyi model, the exponential family class of network models, and recently developed latent variable models. Many of the methods and models are illustrated by application to the well‐known Zachary karate dataset. Software routines available for implementing methods are emphasized throughout.The aim of this paper is to provide a review with enough detail about many common classes of network models to whet the appetite and to point the way to further reading. © 2012 Wiley Periodicals, Inc. Statistical Analysis and Data Mining, 2012",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
A Statistical Analysis of Summarization Evaluation Metrics Using Resampling Methods,"Abstract The quality of a summarization evaluation metric is quantified by calculating the correlation between its scores and human annotations across a large number of summaries. Currently, it is unclear how precise these correlation estimates are, nor whether differences between two metrics’ correlations reflect a true difference or if it is due to mere chance. In this work, we address these two problems by proposing methods for calculating confidence intervals and running hypothesis tests for correlations using two resampling methods, bootstrapping and permutation. After evaluating which of the proposed methods is most appropriate for summarization through two simulation experiments, we analyze the results of applying these methods to several different automatic evaluation metrics across three sets of human annotations. We find that the confidence intervals are rather wide, demonstrating high uncertainty in the reliability of automatic metrics. Further, although many metrics fail to show statistical improvements over ROUGE, two recent works, QAEval and BERTScore, do so in some evaluation settings.1",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Advancing Cyber Resilience Analysis with Performance-Based Metrics from Infrastructure Assessments,"Cyber resilience is becoming increasingly recognized as a critical component of comprehensive cybersecurity practices. Current cyber resilience assessment approaches are primarily qualitative methods, making validation of their resilience analyses and enhancement recommendations difficult, if not impossible. The evolution of infrastructure resilience assessment methods has paralleled that of their cyber counterparts. However, the development of performance-based assessment methods has shown promise for overcoming the validation challenge for infrastructure systems. This paper describes a hybrid infrastructure resilience assessment approach that combines both qualitative analysis techniques with performance-based metrics. The qualitative component enables identification of system features that limit resilience, and the quantitative metrics can be used to evaluate and confirm the effectiveness of proposed mitigation options. The authors propose adaptation of this methodology for cyber resilience analysis. A case study is presented to demonstrate how the approach could be applied to a hypothetical system.",IT,IT_SEC_RISK,85,Clear IT paper with focus on IT Security & Risk Management
Empirical Analysis of Pair Programming Using Bloom's Taxonomy and Programmer Rankers Algorithm to Improve the Software Metrics in Agile Development,"Collaborative programming is a co-operative effort of 2 teams to n-teams to share knowledge, synergize and produce better code. Pairing, Swarming and Mobbing are the standard of agile technology which are adapted by many organizations. In this paper, software development is carried out using Pair Programming(PP) in a medium sized organization developing mobile applications using android is presented. The first method uses the Programmers Competency Matrix (PCM) based on Blooms Taxonomy to assess the skill of the programmers. Since, the pairs are chosen randomly in the PCM method, a novel algorithm is proposed to pair the programmers by Programmer Ranking Algorithm (PRA). The two proposed methods are evaluated in an organization and the results are validated. The results prove that PP definitely improves the software development process than when it is developed by individual programmers. The PRA methodology outperforms the PCM because the PRA chooses the pair wisely using the programming skills of the programmer.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
High-Accuracy Machine Learning Model for Predicting Diabetes Mellitus Progression,"Diabetes mellitus, a chronic metabolic disorder marked by persistent hyperglycemia, presents a major global health challenge, affecting over 463 million adults worldwide. Timely and accurate prediction of disease progression is crucial for mitigating complications and improving patient outcomes. This research paper details the development and validation of an advanced machine learning model designed to predict diabetes progression. The proposed model integrates various machine learning algorithms, such as regression analysis, decision trees, and neural networks, to enhance predictive accuracy. Feature selection techniques are employed to identify the most relevant predictors, ensuring a comprehensive yet streamlined approach. The model achieves a high accuracy of 94.4%, with a mean absolute error (MAE) of 0.404 and a root mean square error (RMSE) of 0.206, highlighting its effectiveness in predicting diabetes progression. The primary objectives of this study are to construct a machine learning model capable of accurately predicting diabetes progression, validate the model with real-world patient data, and compare its performance against existing predictive models. The findings of this research hold significant implications for diabetes care, facilitating personalized treatment plans and proactive interventions, ultimately enhancing patient quality of life. Additionally, the insights gained from this study may guide future research and development in the field of predictive analytics for chronic disease management.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Comparing Measured Agile Software Development Metrics Using an Agile Model-Based Software Engineering Approach versus Scrum Only,"This study compares the reliability of estimation, productivity, and defect rate metrics for sprints driven by a specific instance of the agile approach (i.e., scrum) and an agile model-Bbased software engineering (MBSE) approach called the integrated Scrum Model-Based System Architecture Process (sMBSAP) when developing a software system. The quasi-experimental study conducted ten sprints using each approach. The approaches were then evaluated based on their effectiveness in helping the product development team estimate the backlog items that they could build during a time-boxed sprint and deliver more product backlog items (PBI) with fewer defects. The commitment reliability (CR) was calculated to compare the reliability of estimation with a measured average scrum-driven value of 0.81 versus a statistically different average sMBSAP-driven value of 0.94. Similarly, the average sprint velocity (SV) for the scrum-driven sprints was 26.8 versus 31.8 for the MBSAP-driven sprints. The average defect density (DD) for the scrum-driven sprints was 0.91, while that of the sMBSAP-driven sprints was 0.63. The average defect leakage (DL) for the scrum-driven sprints was 0.20, while that of the sMBSAP-driven sprints was 0.15. The t-test analysis concluded that the sMBSAP-driven sprints were associated with a statistically significant larger mean CR, SV, DD, and DL than that of the scrum-driven sprints. The overall results demonstrate formal quantitative benefits of an agile MBSE approach compared to an agile alone, thereby strengthening the case for considering agile MBSE methods within the software development community. Future work might include comparing agile and agile MBSE methods using alternative research designs and further software development objectives, techniques, and metrics.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Analysis of Developers’ Network and Change Burst Metrics as Predictors of Software Faults,"Introduction: Many software quality metrics that can be used as proxies of measuring software quality by predicting software faults have previously been proposed. However determining a superior predictor of software faults given a set of metrics is difficult since prediction performances of the proposed metrics have been evaluated in non–uniform experimental contexts. There is need for software metrics that can guarantee consistent superior fault prediction performances across different contexts. Such software metrics would enable software developers and users to establish software quality. Objectives: This research sought to determine a predictor for software faults that requires least effort to detect software faults and has least cost of misclassifying software components as faulty or not given developers’ network metrics and change burst metrics. Methods: Experimental data for this study was derived from Jmeter, Gedit, POI and Gimp open source software projects. Logistic regression was used to predict faultiness of a file while linear regression was used to predict number of faults per file. Results: Change burst metrics model exhibited the highest fault detection probabilities with least cost of mis-classification of components as compared to the developers’ network model. Conclusion: The study found that change burst metrics could effectively predict software faults.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Software Fault Severity Prediction Using Git History Metrics and Commits,"In this paper, we propose new software agnostic metrics extracted from Git history. We compared the proposed metrics to many traditional code-based metrics in terms of fault severity prediction. We used three Machine Learning Algorithms (Random Forest, SVM and Multilayer Perceptron) to build the prediction models. We used data (source code, source code metrics, fault severity information) collected from three different data sources. Results show that the proposed software agnostic metrics perform better in terms of fault severity prediction compared to traditional code-based metrics. They were able to achieve 84% of accuracy in fault severity prediction. We also introduced some terms extracted from commits and showed their effectiveness for fault severity classification.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Improving Motor Imagery EEG Signals Classification Accuracy with CSP by Available Machine Learning Approach,"In brain computer interface (BCI) systems, the electroencephalography (EEG) signals give a pathway to a motor disabled person to communicate outside using the brain signal and a computer. EEG signals of different motor imagery (MI) movements can be differentiated using an effective classification technique to aid a motor disabled patient. The purpose of this paper is to classify two different types of MI movement tasks, movement of the left hand and movement of the right foot EEG signals accurately. For this purpose we have used a publicly available dataset. Since the feature extraction for classification is an important task, so we have used popular common spatial pattern (CSP) method for spatial feature extraction. Two different machine learning classifiers named support vector machine (SVM) and K-nearest neighbor (KNN) have been used to verify the proposed method. We got the highest average results 95.55%, 98.73% and 92.38% in case of SVM and 93.5%, 98.73% and 90.15% in case of KNN for classification accuracy, sensitivity, and specificity, respectively when a Butterworth band-pass filter passed through [10–30] Hz. On the other hand accuracy came to 89.4% in [10-30] Hz when applying CSP for feature extraction and fisher linear discriminant analysis (FLDA) for classification on this dataset earlier.&#x0D; Journal of Engineering Science 12(2), 2021, 67-77",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Statistical dataset on software metrics in object-oriented systems,"This paper presents a set of statistical data on the software metrics of object-oriented systems. The data were generated from the Qualitas.class Corpus, which gathered a large amount of metrics data from the 111 systems included in Qualitas Corpus. We used the R project for Statistical Computing to generate 6 statistical graphs, 4 summarization/aggregation tables and an R script, for each of the 21 metrics evaluated in each of the 111 systems. This amounted to 13,986 graphs, 8,800 tables and 2,200 R scripts. We also utilized EasyFit to fit a large number of distributions to each dataset, which in turn provided the best fitting statistical distribution, as well as the fit ranking. We also provide a MySQL database dump that normalizes the metric measures and facilitates data manipulation tasks such as filtering and aggregation. By making this set available, we intend to help researchers in their work on software metrics.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
"A machine and deep learning analysis among SonarQube rules, product, and process metrics for fault prediction","Abstract Background Developers spend more time fixing bugs refactoring the code to increase the maintainability than developing new features. Researchers investigated the code quality impact on fault-proneness, focusing on code smells and code metrics.  Objective We aim at advancing fault-inducing commit prediction using different variables, such as SonarQube rules, product, process metrics, and adopting different techniques.  Method We designed and conducted an empirical study among 29 Java projects analyzed with SonarQube and SZZ algorithm to identify fault-inducing and fault-fixing commits, computing different product and process metrics. Moreover, we investigated fault-proneness using different Machine and Deep Learning models.  Results We analyzed 58,125 commits containing 33,865 faults and infected by more than 174 SonarQube rules violated 1.8M times, on which 48 software product and process metrics were calculated. Results clearly identified a set of features that provided a highly accurate fault prediction (more than 95% AUC). Regarding the performance of the classifiers, Deep Learning provided a higher accuracy compared with Machine Learning models.  Conclusion Future works might investigate whether other static analysis tools, such as FindBugs or Checkstyle, can provide similar or different results. Moreover, researchers might consider the adoption of time series analysis and anomaly detection techniques.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Proposed Metrics for Process Capability Analysis in Improving Software Quality: An Empirical Study,"A software project faces its top expense on defect removal; thereby delaying the schedules. There has been increasing demand for high quality software. Here, high quality software means, delivering defect free software and meeting the predictable results within time and cost constraints. Software defect prediction strives to improve software quality and testing efficiency. The research work presented here is an empirical study and analyzes importance of different metrics used in the organization. The paper examines the impact of LSL and USL, known as organization baselines, on various projects and proposes four metrics for capability analysis metrics. These can prove beneficial for categorizing the process of software development. These metrics aim to improve the ongoing software development process and are helpful in determining the quality of these processes in terms of their specification limits. Also, the paper attempts to justify the probability of the values related to the data provided by normal distribution or Gaussian distribution.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Combining metrics for software evolution assessment by means of Data Envelopment Analysis,"SUMMARYResearch and practice in software engineering have led to an extensive set of metrics for the evaluation of almost every aspect of software development. One of the major challenges for any quality model is the combination of metrics, which are complementary to each other. In this paper, we propose the use of Data Envelopment Analysis (DEA), a non‐parametric technique employed in economics, as a means of providing a unified view of selected design metrics. The proposed application of DEA aims at assessing the overall trend of quality during the evolution of software systems, by considering releases of a project as units to be ranked. An important benefit derived from the use of DEA is the ability to “normalize” the evaluation over the size characteristics of the examined systems, which is vital when comparing projects of different scale. Results are presented for successive versions of two open‐source, one industrial and one research project, whereas validation, whereas validation is performed by comparing the findings with the results obtained by Analytic Hierarchy Process, which is an acknowledged multi‐criteria decision analysis approach. According to the results, DEA enables the perception of global trends in qualitative characteristics, which would be otherwise difficult to recognize and interpret. Copyright © 2012 John Wiley &amp; Sons, Ltd.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Comparative Analysis of Deep Learning Architectures for Predicting Software Quality Metrics in Behavior-Driven and Test-Driven Development Approaches,"The impact of software development methodologies on quality metrics is a crucial area of study in empirical software engineering. This research evaluates the performance of three deep learning architectures: Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Long Short-Term Memory (LSTM), in predicting key software quality indicators, including maintainability index, test coverage, and code complexity, for projects developed using Behavior-Driven Development (BDD) and Test-Driven Development (TDD) approaches. Using a static tabular dataset containing software quality metrics, the models are evaluated based on Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and the R^2 coefficient. The MLP achieves the best performance, with the lowest RMSE (6.41) and MAE (6.34) and the highest R^2 value (−4.21), demonstrating its suitability for tabular data. The CNN performs moderately, while the LSTM underperforms due to its reliance on temporal dependencies absent from the dataset. These results emphasize the need for careful architectural alignment with dataset characteristics. The findings contribute to understanding the predictive power of deep learning models in software quality analysis and highlight the potential of MLP as a robust tool for such predictions. Future work can explore hybrid models and domain-specific feature engineering to enhance prediction accuracy.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Revealing accuracy in climate dynamics: enhancing evapotranspiration estimation using advanced quantile regression and machine learning models,"AbstractThis study examines the effectiveness of various quantile regression (QR) and machine learning (ML) methodologies developed for analyzing the relationship between meteorological parameters and daily reference evapotranspiration (ETref) across diverse climates in Iran spanning from 1987 to 2022. The analyzed models include D-vine copula-based quantile regression (DVQR), multivariate linear quantile regression (MLQR), Bayesian model averaging quantile regression (BMAQR), as well as machine learning algorithms such as extreme learning machine (ELM), random forest (RF), M5 model Tree (M5Tree), least squares support vector regression algorithm (LSSVR), and extreme gradient boosting (XGBoost). Additionally, empirical equations (EEs) such as Baier and Robertson (BARO), Jensen and Haise (JEHA), and Penman (PENM) models were considered. While the EEs demonstrated acceptable performance, the QR and ML models exhibited superior accuracy. Among these, the MLQR model displayed the highest accuracy compared to DVQR and BMAQR models. Moreover, LSSVR, XGBoost, and M5Tree models outperformed ELM and RF models. Notably, LSSVR, XGBoost, and MLQR models exhibited comparable performance (R2 and NSE &gt; 0.92, MBE and RMSE &lt; 0.5, and SI &gt; 0.05) to M5Tree and BMAQR models across all climates. Importantly, these models significantly outperformed EEs, DVQR, ELM, and RF models in all climates. In conclusion, high-dimensional QR and ML models are recommended as promising alternatives for accurately estimating daily ETref in diverse global climate conditions.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
An empirical analysis of source code metrics and smart contract resource consumption,"AbstractA smart contract (SC) is a programme stored in the Ethereum blockchain by a contract‐creation transaction. SC developers deploy an instance of the SC and attempt to execute it in exchange for a fee, paid in Ethereum coins (Ether). If the computation needed for their execution turns out to be larger than the effort proposed by the developer (i.e., the gasLimit), their client instantiation will not be completed successfully.In this paper, we examine SCs from 11 Ethereum blockchain‐oriented software projects hosted on GitHub.com, and we evaluate the resources needed for their deployment (i.e., the gasUsed). For each of these contracts, we also extract a suite of object‐oriented metrics, to evaluate their structural characteristics.Our results show a statistically significant correlation between some of the object‐oriented (OO) metrics and the resources consumed on the Ethereum blockchain network when deploying SCs. This result has a direct impact on how Ethereum developers engage with a SC: evaluating its structural characteristics, they will be able to produce a better estimate of the resources needed to deploy it. Other results show specific source code metrics to be prioritised based on application domains when the projects are clustered based on common themes.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Networked software’ performance metrics and analysis with ibm svc config advisor,"IBM SVC is a virtualization product by IBM which deals with storage virtualization. In this IBM SVC, numbers of hetrogenous hosts are connected by means of high speed communication network. To manage configuration of these networked component is very difficult task. To automate this configuration checking is need of the world. Hence IBM SVC Config Advisor tool is developed by IBM. This tool deals with remote configuration check for storage stand including IBM SVC and Storwize products. This paper introduces the IBM SVC Config Advisor tool along with performance statistics. This paper mainly deals with the networked tool’s performance statistics collection and analysis. Here IBM SVC Config Advisor is used as networked Tool for analysis. This paper can be useful to analyze software which are highly network dependent in nature.",IT,IT_PERF_MON,85,Clear IT paper with focus on IT Performance Monitoring
Meta-analytics: tools for understanding the statistical properties of sports metrics,"AbstractIn sports, there is a constant effort to improve metrics that assess player ability, but there has been almost no effort to quantify and compare existing metrics. Any individual making a management, coaching, or gambling decision is quickly overwhelmed with hundreds of statistics. We address this problem by proposing a set of “meta-metrics”, which can be used to identify the metrics that provide the most unique and reliable information for decision-makers. Specifically, we develop methods to evaluate metrics based on three criteria: (1) stability: does the metric measure the same thing over time (2) discrimination: does the metric differentiate between players and (3) independence: does the metric provide new information? Our methods are easy to implement and widely applicable so they should be of interest to the broader sports community. We demonstrate our methods in analyses of both NBA and NHL metrics. Our results indicate the most reliable metrics and highlight how they should be used by sports analysts. The meta-metrics also provide useful insights about how to best construct new metrics that provide independent and reliable information about athletes.",IS,BI_ANALYTICS,85,Clear IS paper with focus on Business Intelligence & Analytics
SBFSelector,"Tracking changes in code using revision history shared by collaborative teams during software evolution improves traceability. Existing techniques provides incomplete and inaccurate revision history due to lack in detection of renaming and shifting at file, class, and method granularities simultaneously. This research analyzes and prioritizes the metrics responsible for detecting such changes and update the revision history. This improves the traceability by tracking complete and accurate revision history that further improves the processes related to mining software repositories. It proposes SBFSelector algorithm that uses Jaccard Similarity and cosine similarity based on the prioritized metrics to identify these changes. Result shows that 73% metrics belongs to size and complexity that holds more significance over remaining categories. Random forest is best classifier for tracking changes with 0.99 true positive rate and 0.01 false positive rate. It improves traceability by increasing the Kappa statistic and true positive rate as compared to Understand tool.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Extension of Object-Oriented Metrics Suite for Software Maintenance,"Software developers require information to understand the characteristics of systems, such as complexity and maintainability. In order to further understand and determine characteristics of object-oriented (OO) systems, this paper describes research that identifies attributes that are valuable in determining the difficulty in implementing changes during maintenance, as well as the possible effects that such changes may produce. A set of metrics are proposed to quantify and measure these attributes. The proposed complexity metrics are used to determine the difficulty in implementing changes through the measurement of method complexity, method diversity, and complexity density. The paper establishes impact metrics to determine the potential effects of making changes to a class and dependence metrics that are used to measure the potential effects on a given class resulting from changes in other classes. The case study shows that the proposed metrics provide additional information not sufficiently provided by the related existing OO metrics. The metrics are also found to be useful in the investigation of large systems, correlating with project outcomes.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
The evaluation of software systems' structure using quantitative software metrics,"AbstractThe design and analysis of the structure of software systems has typically been based on purely qualitative grounds. In this paper we report on our positive experience with a set of quantitative measures of software structure. These metrics, based on the number of possible paths of information flow through a given component, were used to evaluate the design and implementation of a software system (the UNIX operating system kernel) which exhibits the interconnectivity of components typical of large‐scale software systems. Several examples are presented which show the power of this technique in locating a variety of both design and implementation defects. Suggested repairs, which agree with the commonly accepted principles of structured design and programming, are presented. The effect of these alterations on the structure of the system and the quantitative measurements of that structure lead to a convincing validation of the utility of information flow metrics.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Vovel metrics—novel coupling metrics for improved software fault prediction,"Software is a complex entity, and its development needs careful planning and a high amount of time and cost. To assess quality of program, software measures are very helpful. Amongst the existing measures, coupling is an important design measure, which computes the degree of interdependence among the entities of a software system. Higher coupling leads to cognitive complexity and thus a higher probability occurrence of faults. Well in time prediction of fault-prone modules assists in saving time and cost of testing. This paper aims to capture important aspects of coupling and then assess the effectiveness of these aspects in determining fault-prone entities in the software system. We propose two coupling metrics, i.e., Vovel-in and Vovel-out, that capture the level of coupling and the volume of information flow. We empirically evaluate the effectiveness of the Vovel metrics in determining the fault-prone classes using five projects, i.e., Eclipse JDT, Equinox framework, Apache Lucene, Mylyn, and Eclipse PDE UI. Model building is done using univariate logistic regression and later Spearman correlation coefficient is computed with the existing coupling metrics to assess the coverage of unique information. Finally, the least correlated metrics are used for building multivariate logistic regression with and without the use of Vovel metrics, to assess the effectiveness of Vovel metrics. The results show the proposed metrics significantly improve the predicting of fault prone classes. Moreover, the proposed metrics cover a significant amount of unique information which is not covered by the existing well-known coupling metrics, i.e., CBO, RFC, Fan-in, and Fan-out. This paper, empirically evaluates the impact of coupling metrics, and more specifically the importance of level and volume of coupling in software fault prediction. The results advocate the prudent addition of proposed metrics due to their unique information coverage and significant predictive ability.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
EVALUATION OF SOFTWARE METRICS FOR SOFTWARE PROJECTS,"Software metrics are developed and used by the many software organizations for the evaluation and confirmation of good code, working and maintenance of the software product. Software metrics measure and identify various types of software complexities such as size metrics, control flow metrics and data flow metrics. One of the significant objective of software metrics is that it is applicable to both a process and product metrics. Ndepend is the most advanced as well as flexible tool available in the market. We have ensured the Quality of the project by using Ndepend metrics. So we have concluded that software metrics are easy to understand and applicable on the software, so favourable among software professionals.It is most prevalent and important testing metrics used in organizations. Metrics are used to improve software productivity and quality. This thesis introduces the most commonly used software metrics proposed and reviews their use in constructing models of the software development process.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Derivation of local software quality metrics (software quality circles),"AbstractSoftware is a product in serious need of quality control technology. Major effort notwithstanding, software engineering has produced few metrics for aspects of software quality that have the potential of being universally applicable. The present paper suggests that, although universal metrics are elusive, metrics that are applicable and useful in a fully defined setting are readily available. A theory is presented that a well‐defined software work group can articulate their operational concept of quality and derive useful metrics for that concept and their environment.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Object Metrics for Green Software,"Today, the energy consumption of computers represents a significant part of the overall consump-tion. The purpose of this article is to apply object and architectural metrics to observe the impact on applica-tion consumption. This article focuses on the most common object applications to date, and their architec-tures that are already useful to optimize the reusability, composability or dynamicity of these applications. To do this, consumption must be evaluated and compared according to the variations of object and architec-tural metrics. These observations help to determine how effective these metrics could be.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Software metrics and measurement principles,"Software measurement is widely advocated as a fundamental constituent of an engineering approach to planning and controlling software development. Unfortunately, there is a dichotomy between the quantity of developed metrics and those used. This paper provides a tutorial review of software engineering measurement indicating the depth and breadth of the field. Individual metrics are not described due to the interest of this paper being on the measurement process and not the products of that process. Generic problems have been identified within existing measurement processes, these provide learning points for the expression of measurement principles. These principles are classified and described according to their position within the formulation, analysis and application stages of measurement. Conclusions are elaborated that suggest that existing measurement frameworks for applying measurement - often called measurement methods - do not provide sufficient support for the principles and their continued use will only serve to replicate the problems. In order to improve the products i.e. metrics, the measurement process requires improvement through inclusion of these principles in a new method.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Statistical analysis software,"Purpose. Analysis of existing software to perform statistical analysis for further use as part of the selection of the necessary software for data processing. Methodology. To conduct a software review, an analysis of scientific articles and open sources on statistical analysis software was conducted. Findings. Choosing the right statistical software is a key decision in the field of data analysis, with numerous options to meet a variety of needs. This article provides a comprehensive overview of five leading statistical software tools: IBM SPSS Statistics, RStudio, Stata, Minitab, and Python. This paper reveals key insights into the capabilities, functions, and suitability of each tool for various analytical tasks. This review concludes that the choice of statistical software should be consistent with specific project requirements, data complexity, and user experience. Researchers and analysts should consider their analytical goals and preferences when choosing the most appropriate tool. In addition, to make informed decisions in this dynamic field, it is important to stay abreast of new trends in data analysis and machine learning. Originality. The conducted analysis revealed the possibilities and application of the most popular software for solving problems of statistical analysis. The work provides a comprehensive overview of current trends and innovations in the field of software for statistical analysis, offering readers a deeper understanding of existing tools. Practical value. The conducted analysis will allow to choose software for solving a specific task of statistical analysis based on its characteristics and existing requirements. This work helps to identify the practical benefits of statistical analysis software and promotes the implementation of these tools in various fields of activity, providing improvements in analysis and decision-making processes.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Lessons Learnt from Gauging Software Metrics of Cabin Software in a Commercial Airliner,"In order to achieve high safety standards, avionic software needs to be developed with very high requirements regarding safety, reliability, and determinism as well as real-time constraints, which are often in conflict with the development of maintainable software systems. Nevertheless, the ability to change the software of an airplane is of increasing importance, since it consists of a multitude of partially hardware-specific subsystems which may need replacement during a plane’s lifespan of typically numerous decades. Thus, as a first step towards optimizing maintainability of avionic software we have benchmarked the cabin software of a commercial airliner with common software metrics. Such a benchmarking of avionic software contributes valuable insights into the current practice of developing critical software and the application of software metrics in this context. We conclude from the obtained results that it is important to pay more attention to long-term maintainability of aviation software. Additionally we have derived some initial recommendations for the development of future avionic software systems.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Analysis of the Different Statistical Metrics in Machine Learning,"The evaluation of machine learning models plays a pivotal role in ensuring their effectiveness across various domains. Metrics serve as vital tools for this purpose, quantifying model performance in tasks, e.g., classification, regression, and clustering. This study delves into the fundamental metrics used in machine learning, presenting their formulas and applications, including accuracy, precision, F1-Score, RMSE, and the Silhouette Score. The analysis underscores the importance of selecting metrics tailored to specific tasks, acknowledging the potential biases and interpretability challenges that may arise. While metrics provide invaluable insights, they also exhibit limitations, particularly in cases where trade-offs between metrics are inevitable. Looking to the future, this study envisions a landscape where multi-metric assessments, improved interpretability, domain-specific metrics, and explainable AI converge to address current limitations. These advancements promise more robust and transparent model evaluations, adapting to dynamic real-world applications. In summary, this exploration of metrics in machine learning highlights their crucial role in benchmarking model performance, fostering the development of reliable AI systems, and shaping transformative applications in diverse fields. Metrics not only aid in informed decision-making but also contribute to advancements in science, industry, and society.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Usability evaluation of component based software system using software metrics,"Component Based Software Engineering (CBSE) provides a way to create a new Component Based Software System (CBSS) by utilizing the existing components. The primary reason for that is to minimize the software development time, cost and effort. CBSS also increases the component reusability. Due to these advantages, software industries are working on CBSS and continuously trying to provide quality product. Usability is one of the major quality factors for CBSS. It should be measured before delivering the software product to the customer, so that if there are any usability flaws, it can be removed by software development team. In this paper, work has been done to evaluate the usability of CBSS based on major usability sub-factors (learnability, operability, understandability and configurability). For this purpose, firstly software metrics are identified for each usability sub-factor and the value of each sub-factor is evaluated for a component based software project. Secondly, overall usability of the software project is evaluated by using the calculated value of each usability sub-factor. Usability for the same project was also evaluated using Fuzzy approach in MATLAB to validate the experimental work of this research paper. It was identified that the value of usability obtained from software metrics and fuzzy model was very similar. This research work will be useful for the software developer to evaluate the usability of any CBSS and will also help them to compare different version of any CBSS in term of their usability.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
A New Approach to Locate Software Vulnerabilities Using Code Metrics,"Automatic vulnerabilities prediction assists developers and minimizes resources allocated to fix software security issues. These costs can be minimized even more if the exact location of vulnerability is correctly indicated. In this study, the authors propose a new approach to using code metrics in vulnerability detection. The strength part of the proposed approach lies in using code metrics not to simply quantify characteristics of software components at a coarse granularity (package, file, class, function) such as complexity, coupling, etc., which is the approach commonly used in previous studies, but to quantify extracted pieces of code that hint presence of vulnerabilities at a fine granularity (few lines of code). Obtained results show that code metrics can be used with a machine learning technique not only to indicate vulnerable components wish was the aim of previous approaches but also to detect and locate vulnerabilities with very good accuracy.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
Coupling Metrics for Aspect Oriented Software,"The Aspect Oriented Software (AOS) paradigm emerged as a response to the limitations of ObjectOriented Programming, specifically its inability to modularize cross-cutting concerns effectively. However, AOS have inherent complexity that keeps increasing as software is modified and most of the existing metrics have not been theoretically or empirically validated. This means we cannot rely on them for measurement of AOS complexity. This paper proposes four base metrics and two composite coupling metrics for analyzing the complexity of AOS. The metrics were derived using the Entity-Attribute-MetricTool (EAMT) model. The metrics were theoretically validated using Briand’s framework, and a tool was developed to automate the computation of these metrics. Theoretical results indicate that the proposed metrics are mathematically sound. A between-subjects experimental study was conducted to validate the proposed metrics and results indicate that the proposed metrics are strongly correlated with modularity, meaning they are important for modularity assessment in AOS-based software.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
An Empirical Study of Social Networks Metrics in Object-Oriented Software,"We study the application to object-oriented software of new metrics, derived from Social Network Analysis. Social Networks metrics, as for instance, the EGO metrics, allow to identify the role of each single node in the information flow through the network, being related to software modules and their dependencies. These metrics are compared with other traditional software metrics, like the Chidamber-Kemerer suite, and software graph metrics. We examine the empirical distributions of all the metrics, bugs included, across the software modules of several releases of two large Java systems, Eclipse and Netbeans. We provide analytical distribution functions suitable for describing and studying the observed distributions. We study also correlations among metrics and bugs. We found that the empirical distributions systematically show fat-tails for all the metrics. Moreover, the various metric distributions look very similar and consistent across all system releases and are also very similar in both the studied systems. These features appear to be typical properties of these software metrics.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
In-Depth Analysis and Prediction of Coupling Metrics of Open Source Software Projects,"This research was conducted to perform an in-depth analysis of the coupling metrics of 10 Open Source Software (OSS) projects obtained from the Comets dataset. More precisely, we analyze the dataset of object-oriented OSS projects (having 17 code related metrics such as coupling, complexity, and size metrics) to (1) examine the relationships among the coupling and other metrics (size, complexity), (2) analyze the pattern in the growth of software metrics, and (3) propose a model for prediction of coupling. To generalize the model of coupling prediction, we have applied different machine learning algorithms and validated their performance on similar datasets. The results indicated that the Random forests algorithm outperforms all other models. The relation analysis specifies the existence of strong positive relationships between the coupling, size, and complexity metrics while the pattern analysis pinpointed the increasing growth trend for coupling. The obtained outcomes will help the developers, project managers, and stakeholders in better understating the state of software health",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
A Measures and Metrics Framework for Software Safety,"A set of software-safety metrics is presented in this paper. The proposed metrics framework is intended to address software-safety risk assessment and can be used to evaluate the software-safety quality of a process. The framework attempts to assess safety risks from the beginning of the software development lifecycle and helps to implement efficient solutions. The complete framework is based on McCall's Factors, Criteria and Metrics approach and useful for assessing the degree of safety risk prevalence. The objective is to identify software elements that can lead to accidents if not properly addressed with the support of the safety engineering and software engineering disciplines. Various safety analysis methods including hazard analysis, standards, and guidelines are reviewed for their limitations and practical problems are considered. The framework considers measures that contribute to safety assessment through metrics in each of the Software Development Life Cycle phase. An experimental laboratory setup as 'Safety-Critical System' called Railroad Crossing Critical System (RCCS), which implements a safety-critical software, is used for demonstration and to validate the utility of the framework.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Comparing ϕ and the F-measure as performance metrics for software-related classifications,"Abstract Context The F-measure has been widely used as a performance metric when selecting binary classifiers for prediction, but it has also been widely criticized, especially given the availability of alternatives such as ϕ (also known as Matthews Correlation Coefficient).  Objectives Our goals are to (1) investigate possible issues related to the F-measure in depth and show how ϕ can address them, and (2) explore the relationships between the F-measure and ϕ.  Method Based on the definitions of ϕ and the F-measure, we derive a few mathematical properties of these two performance metrics and of the relationships between them. To demonstrate the practical effects of these mathematical properties, we illustrate the outcomes of an empirical study involving 70 Empirical Software Engineering datasets and 837 classifiers.  Results We show that ϕ can be defined as a function of Precision and Recall, which are the only two performance metrics used to define the F-measure, and the rate of actually positive software modules in a dataset. Also, ϕ can be expressed as a function of the F-measure and the rates of actual and estimated positive software modules. We derive the minimum and maximum value of ϕ for any given value of the F-measure, and the conditions under which both the F-measure and ϕ rank two classifiers in the same order.  Conclusions Our results show that ϕ is a sensible and useful metric for assessing the performance of binary classifiers. We also recommend that the F-measure should not be used by itself to assess the performance of a classifier, but that the rate of positives should always be specified as well, at least to assess if and to what extent a classifier performs better than random classification. The mathematical relationships described here can also be used to re-interpret the conclusions of previously published papers that relied mainly on the F-measure as a performance metric.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Metrics for measuring the quality of object-oriented software,"This paper presents metrics for measuring the quality of object-oriented software. Quality of software generally depends on five parameters namely efficiency, understandability, complexity, reusability and maintainability. All of these parameters are associated with certain metrics. Quality of software can be measured by using object-oriented metrics. Software tends to become more complex over a series of releases and maintaining them becomes a more difficult task. Thus quality of software also tends to decrease over time. In this paper, a study is performed using object-oriented metrics that are computed over different releases of the software application JFreeChart.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
"Search based Software Engineering of Software Metrics, Code Smells and Refactoring Techniques Using Optimization Algorithms","Software refactoring is a technique for reorganising and improving the efficiency of existing software code. By refining the non-functional aspects of software, numerous refactoring methods are currently being employed to build more intelligible and less composite codes. By applying multiple systems to the source code, refactoring can improve code maintainability even further, preserving the behaviour of the code. Refactoring allows for the eradication of bugs and the expansion of the program's capabilities. This paper provides a comprehensive assessment of source code with foul odours, the influence on software quality of using certain refactoring methodologies to eradicate the foul smell. Between 2008 and 2010, a total of 76 studies from 42 journals, 20 conferences, and 14 additional sources in the year 2008 and 2022 were available. This study was graded on the number of unpleasant odours identified, refactoring strategies applied, and their impact on software metrics. The foul smells of ""method of long,"" ""envious feature “and” class of data"" were discovered or corrected in the majority of inquiries. The odour of ""envious feature"" was detected in 39.66 per cent of nominated investigations. The majority of studies looked at the effects of restructuring on software intricacy and coupling measures. Surprisingly, instead of patented software, the majority of the investigations employed massive opensource datasets released in Java. Finally, this research makes suggestions for further into the refactoring research code.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
An Empirical Analysis of Approach‐Based Metrics Model for Architectural Erosion Detection,"ABSTRACTContextSoftware architecture plays a crucial role in the success or failure of software development and design. Over time, architectural degradation—known as architectural erosion—can compromise system quality and maintainability. Metrics‐based approaches have become a prevalent method for identifying and addressing this phenomenon.ObjectiveThis study aims to (1) investigate whether various metrics‐based approaches can effectively determine architectural erosion and support the development of a formal model, and (2) assess the construct reliability and validity of the proposed model.MethodsA formal model was constructed based on selected metrics approaches derived from the literature. A questionnaire‐based survey was conducted with 130 software engineering professionals experienced in architectural erosion and metrics. Structural Equation Modelling (SEM) was employed to evaluate the model's construct reliability, construct validity, and research hypotheses.ResultsThe analysis revealed significant relationships between most classifications of metrics‐based approaches and architectural erosion. However, no substantial relationships were found for the classifications of architectural complexity and architectural technical debt.ConclusionThe proposed model offers a valuable foundation for the formal definition and evaluation of metrics‐based approaches to architectural erosion. Its empirical validation provides meaningful insights for both researchers and practitioners aiming to assess or mitigate architectural erosion in software systems.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
A relationship between software coverage metrics and reliability,AbstractSoftware engineers have tacitly assumed that there exists a link between coverage and reliability. An argument is presented which leads to an inverse logarithmic relationship which depends upon the structure of the software and on a parameter called the strength of the coverage metric. The relationship is developed by considering reliability as a function of an idealized coverage metric and then interpreting the result for a range of actual coverage metrics.,CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Investigating the Effect of Software Complexity Metrics on Software Cost,"Nowadays, software is expected to have an extended lifespan, which makes the evaluation of its complexity at the early stages critical in upcoming maintenance. Indeed, complexity is proportional to the evolution of software. Software metrics were introduced as tools that allow us to obtain an objective measurement of the complexity of software. Hence, enabling software engineering to assess and manage software complexity. Reducing software costs is one of the major concerns of software engineering which creates an increasing need for new methodologies and techniques to control those costs. Software complexity metrics can help us to do so. In this paper, we would investigate how those metrics can be used to reduce software costs. We would first analyze the most popular complexity metrics and distinguish their properties. Then, we will show how each of those metrics fit within the software life cycle. Finally, we will provide a detailed approach to use the complexity metrics to reduce software costs.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Copula-based software metrics aggregation,"AbstractA quality model is a conceptual decomposition of an abstract notion of quality into relevant, possibly conflicting characteristics and further into measurable metrics. For quality assessment and decision making, metrics values are aggregated to characteristics and ultimately to quality scores. Aggregation has often been problematic as quality models do not provide the semantics of aggregation. This makes it hard to formally reason about metrics, characteristics, and quality. We argue that aggregation needs to be interpretable and mathematically well defined in order to assess, to compare, and to improve quality. To address this challenge, we propose a probabilistic approach to aggregation and define quality scores based on joint distributions of absolute metrics values. To evaluate the proposed approach and its implementation under realistic conditions, we conduct empirical studies on bug prediction of ca. 5000 software classes, maintainability of ca. 15000 open-source software systems, and on the information quality of ca. 100000 real-world technical documents. We found that our approach is feasible, accurate, and scalable in performance.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Weighted software metrics aggregation and its application to defect prediction,"AbstractIt is a well-known practice in software engineering to aggregate software metrics to assess software artifacts for various purposes, such as their maintainability or their proneness to contain bugs. For different purposes, different metrics might be relevant. However, weighting these software metrics according to their contribution to the respective purpose is a challenging task. Manual approaches based on experts do not scale with the number of metrics. Also, experts get confused if the metrics are not independent, which is rarely the case. Automated approaches based on supervised learning require reliable and generalizable training data, a ground truth, which is rarely available. We propose an automated approach to weighted metrics aggregation that is based on unsupervised learning. It sets metrics scores and their weights based on probability theory and aggregates them. To evaluate the effectiveness, we conducted two empirical studies on defect prediction, one on ca. 200 000 code changes, and another ca. 5 000 software classes. The results show that our approach can be used as an agnostic unsupervised predictor in the absence of a ground truth.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
"Non-Homogeneous Autoregressive Processes for Tracking (Software) Reliability Growth, and Their Bayesian Analysis","SUMMARY We motivate a non-homogeneous autoregressive process as a model for reliability growth and consider two formulations, both Bayesian, which alleviate a limitation that least squares estimators for such processes of order greater than 1 exist only when repeated measurements of the time series are available. In one formulation, the prior assumption involves exchangeability of coefficients, whereas in the other an autoregressive structure is imposed on them. Both formulations enable us to cast the resulting processes in state space form for which the Kalman filter algorithm can be used. The first formulation involves an adaptation of standard techniques whereas the second results in a new methodology for adaptive filtering which is facilitated by an approximation due to Lindley. The procedures are demonstrated via a consideration of some data on software failures.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
DG-metrics formulization for DGML-based software design,"DGML-based software-design representation uses specially designed XML tags to represent the design elements. This representation opens new possibilities for verifying DGML-based software design against existing design metrics and creation of new design metrics. This paper discusses newly created design metrics for the DGML-based design DG-Metrics. This helps in identifying error prone modules in early stages of software development process, tuning design modules for better performance and in managing the later phases of software development cycle with ease.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
AnaIisis Kepuasan Pelanggan Pada E-Commerce Shopee Dengan Metode UTAUT,"Kemajuan teknologi memberikan kemudahan untuk melakukan aktivitas. Salah satu contoh aktivitasnya yaitu berbelanja online. Situs e-commerce terpandang di lndonesia saIah satunya yaitu Shopee yang berasal Singapura. Suatu e-commerce harus dapat untuk meningkatkan kepuasan pengguna, tanpa menganggu penggunanya. Tujuan dari penelitian ini adalah untuk untuk meneliti bagaimana analisis kepuasan pelanggan pada e-commerce. Shopee dengan metode UTAUT. Model UTAUT memiliki berbagai kategori, seperti performance. expectancy, effort. expentancy, sociaI infIuence, dan faciIitating condition. UTAUT juga berguna untuk menjelaskan bagaimana cara pengguna memperhatikan penggunaan sistem informasi dan perilaku mereka selanjutnya. Hasil penelitian ini secara garis besar menggunakan SPSS yaitu akan dilakukan uji validitas dan realibel lalu melalui hipotesis yang akan diuji dengan uji statistik t. Analisis terhadap sistem teknologi informasi Shopee terhadap kepuasan pelanggan dengan metode UTAUT sangat penting untuk mengetahui permasalahan dari sistem Shopee yang merugikan pengguna bahkan sistem tersebut sendiri dan bagaimana bisa memuaskan pelanggan. Penelitian dalam paper ini menghasilkan bahwa setiap variabel independent, yaitu performance expectancy, effort expentancy, social infIuence, dan faciIitating condition berpengaruh signifikan positif pada variabeI dependent yaitu kepuasan peIanggan Shopee dalam penggunaan sistem Shopee.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Some observations on staff estimation metrics for object,"Object-Oriented software projects are becoming more popular than structured (functional) technology based projects in the present scenario. Object Technology (OT) offers support to deliver products to market more quickly and to provide high quality products with lower maintenance costs. As expertise in managing Object-Oriented (OO) project grows, such a body of OO metric knowledge will become increasingly usable across the Industry. There is need for good OO metrics for both process and product management. Estimation is an important field of software engineering. In this work we have proposed model for staffing estimation by using available metrics in OO and taking affecting factors into consideration.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
A Preliminary Analysis of Software Engineering Metrics-based Criteria for the Evaluation of Learning Objects Reusability,"Reusability of learning objects is evaluated on the basis of a priori software reusability analysis, which are related to cohesion and coupling aspects. A number of reusability metrics extracted from metadata records are defined and analyzed to provide an aggregate reusability evaluation for learning objects in a repository. The evaluation is validated and compared with an expert-based a posteriori evaluation method",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Using software metrics for predicting vulnerable classes and methods in Java projects: A machine learning approach,"Abstract[Context]A software vulnerability becomes harmful for software when an attacker successfully exploits the insecure code and reveals the vulnerability. A single vulnerability in code can put the entire software at risk. Therefore, maintaining software security throughout the software life cycle is an important and at the same time challenging task for development teams. This can also leave the door open for vulnerable code being evolved during successive releases. In recent years, researchers have used software metrics‐based vulnerability prediction approaches to detect vulnerable code early and ensure secure code releases. Software metrics have been employed to predict vulnerability specifically in C/C++ and Java‐based systems. However, the prediction performance of metrics at different granularity levels (class level or method level) has not been analyzed. In this paper, we focused on metrics that are specific to lower granularity levels (Java classes and methods). Based on statistical analysis, we first identified a set of class‐level metrics and a set of method‐level metrics and then employed them as features in machine learning techniques to predict vulnerable classes and methods, respectively. This paper describes a comparative study on how our selected metrics perform at different granularity levels. Such a comparative study can help the developers in choosing the appropriate metrics (at the desired level of granularity). [Objective] The goal of this research is to propose a set of metrics at two lower granularity levels and provide evidence for their usefulness during vulnerability prediction (which will help in maintaining secure code and ensure secure software evolution). [Method] For four Java‐based open source systems (including two releases of Apache Tomcat), we designed and conducted experiments based on statistical tests to propose a set of software metrics that can be used for predicting vulnerable code components (i.e., vulnerable classes and methods). Next, we used our identified metrics as features to train supervised machine learning algorithms to classify Java code as vulnerable or non‐vulnerable. [Result] Our study has successfully identified a set of class‐level metrics and a second set of method‐level metrics that can be useful from a vulnerability prediction standpoint. We achieved recall higher than 70% and precision higher than 75% in vulnerability prediction using our identified class‐level metrics as features of machine learning. Furthermore, method‐level metrics showed recall higher than 65% and precision higher than 80%.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
Performance Evaluation of Smart Grid Based on Optimal Fuzzy Algorithm Method,The global upsurge in research and construction of smart grid is creating new opportunities and challenges. This paper proposes a procedure and method for assessing the operation performance of the smart grid. The hierarchical analysis of factors reflects the operation situation in the overall goals. An incentive regulation model based on the optimal fuzzy algorithm and data envelopment analysis is presented.,CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Performance Evaluation of English Part of Speech Tagging Based on Multi-feature Knowledge Algorithm,"Abstract In order to solve the traditional English part-of-speech tagging methods, a variety of English part-speech tagging methods are introduced. This paper systematically studies the method of multi-feature knowledge algorithm to mark English part of speech, then demonstrates it theoretically, and gives some examples to illustrate the characteristics of this method, and then evaluates it objectively.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
"Project-process approach to organization management: model, implementation algorithm, performance evaluation parameters","Актуальность исследования обусловлена высоким уровнем конкуренции на рынке, необходимостью адаптации предприятий к быстро меняющимся условиям внешней среды, повышения финансовой и экономической устойчивости за счет реализации внутренних резервов и потенциала компании, оптимизации расходов, повышения качества продукции и услуг. Этим требованиям отвечает проектно-процессный подход к управлению организацией. В работе представлена модель гибридного проектно-процессного подхода к управлению организацией, алгоритм его внедрения, обоснованы параметры оценки эффективности. The relevance of the research is due to the high level of competition in the market, the need for enterprises to adapt to rapidly changing environmental conditions, increase financial and economic stability through the implementation of internal reserves and the potential of the company, optimize costs, improve the quality of products and services. The project-process approach to the management of the organization meets these requirements. The paper presents a model of a hybrid design-process approach to the management of an organization, an algorithm for its implementation, and the parameters of efficiency evaluation are substantiated.",IS,BPM,85,Clear IS paper with focus on Business Process Management
A Learning Ontology in Computer Programming Approach,"Advances in science and technology have made computer programming an inseparable part of our lives and have raised users' expectations from software. This situation has led to an increase in the complexity of computer programming and software development processes. To manage this complexity, models are increasingly adopted as the main structure of computer programming. On the other hand, developments in the field of linked data has spurred the use of ontologies—concepts not new to computer science—in various domains. In computer programming approaches that consider models as primary structures, it is important to formally represent requirements and ensure traceability between requirements and lower-level analysis and design models. Additionally, adapting or extending existing ontologies is one of the methods that can be employed to reduce the costs of computer programming activities. To achieve this, it is necessary to examine the differences in computer programming and the fundamentals of ontologies. These differences can be categorized under the headings of layered architecture, open-closed world approaches, and interoperability approaches. Taking into consideration the ease of incorporating ontologies in computer programming process and the difficulties reported in the scientific literature, this study proposed a model of knowledge discovery based on computer programming strategy with analogies and obtained a set of patterns for possible scenarios that can be used with a classification of the ontology in learning levels by the topics in computer programming paradigm. The aim of this research is to determine the impact of ontological learning paradigm in computer programming process by drawing a basic ontological learning map by computer programming features.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
"From Agile to DevOps, Holistic Approach for Faster and Efficient Software Product Release Management","Release management is one of the most important software processes and is a set of processes that includes the compilation, configuration, and management of software versions in different environments. In recent years, changes in processes, technologies, and tools and changes in practices and understanding have paved the way for more effective, efficient, sustainable, reusable models and methods in this field. The purpose of this study is to examine the DevOps idea to produce a flow, highlight their benefits, and investigate with a model how these philosophies, which are two of the most important processes and methods in software development today, can reveal an effective release management process. What has been learned from the research is how the agile and DevOps practices, which have become widespread in recent years, can be positioned in a general flow in the release management process, although there are different practices, flows, disciplines, and technology. Sharing a case study on these issues in future studies and an experience sharing research where the flow is applied as a case study will reveal positive feedback on the real-life application and results of the flow and the model. Further, a literature review studies in which deficiencies in the literature are identified will be useful in determining the gaps in the process.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Comparison between Saudi Arabia and USA: Prevention and Dealing with Cyber Security,"Cyber security practices mainly involve the prevention of external threats to software, hardware, server data, and other assets which are connected to the internet. Organizations follow a lot of cyber security practices to protect their systems and databases from malicious cyber actors. Cybercriminals use different techniques like spear-phishing, phishing, password attack, denial of service, ransomware, etc. to cause harm to people, organizations, and governments and steal important information from them. We analyzed the use of deep learning algorithms to deal with cyber-attacks. Deep neural networks or deep learning consist of machine learning procedures to support the network to fix complex issues and learn from unmanaged data. In addition, we also analyzed some of the cyber security laws and practices implemented in the US and Saudi Arabia to work collaboratively against cyber threats. It is observed that both countries are doing well against cyberthreats, but they need to work even more to provide training and support to professionals in the public sector who handle sensitive data about cyber security.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
Towards a framework and a benchmark for testing tools for multi‐threaded programs,"AbstractMulti‐threaded code is becoming very common, both on the server side, and very recently for personal computers as well. Consequently, looking for intermittent bugs is a problem that is receiving more and more attention. As there is no silver bullet, research focuses on a variety of partial solutions. We outline a road map for combining the research within the different disciplines of testing multi‐threaded programs and for evaluating the quality of this research. We have three main goals. First, to create a benchmark that can be used to evaluate different solutions. Second, to create a framework with open application programming interfaces that enables the combination of techniques in the multi‐threading domain. Third, to create a focus for the research in this area around which a community of people who try to solve similar problems with different techniques can congregate. We have started creating such a benchmark and describe the lessons learned in the process. The framework will enable technology developers, for example, developers of race detection algorithms, to concentrate on their components and use other ready made components (e.g. an instrumentor) to create a testing solution. Copyright © 2006 John Wiley &amp; Sons, Ltd.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Accuracy And Performance Enhancement In Machine Learning Approach For Cloud Application,"there is need to improve the performance, accuracy and scalability. proposed work is supposed to provide from hybrid approach that would make more efficient machine learning system for cloud application.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Better than Trees: Applying Semilattices to Balance the Accuracy and Complexity of Machine Learning Models,"Balancing the accuracy and the complexity of models is a well established and ongoing challenge. Models can be misleading if they are not accurate, but models may be incomprehensible if their accuracy depends upon their being complex. In this paper, semilattices are examined as an option for balancing the accuracy and the complexity of machine learning models. This is done with a type of machine learning that is based on semilattices: algebraic machine learning. Unlike trees, semilattices can include connections between elements that are in different hierarchies. Trees are a subclass of semilattices. Hence, semilattices have higher expressive potential than trees. The explanation provided here encompasses diagrammatic semilattices, algebraic semilattices, and interrelationships between them. Machine learning based on semilattices is explained with the practical example of urban food access landscapes, comprising food deserts, food oases, and food swamps. This explanation describes how to formulate an algebraic machine learning model. Overall, it is argued that semilattices are better for balancing the accuracy and complexity of models than trees, and it is explained how algebraic semilattices can be the basis for machine learning models.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Domain-specific implications of error-type metrics in risk-based software fault prediction,"Abstract In software development, Software Fault Prediction (SFP) is essential for optimising resource allocation and improving testing efficiency. Traditional SFP methods typically use binary-class models, which can provide a limited perspective on the varying risk levels associated with individual software modules. This study explores the impacts of Error-type Metrics on the fault-proneness of software modules in domain-specific software projects. Also, it aims to enhance SFP methods by introducing a risk-based approach using Error-type Metrics. This method categorises software modules into High, Medium, and Low-Risk categories, offering a more granular and informative fault prediction framework. This approach aims to refine the fault prediction process and contribute to more effective resource allocation and project management in software development. We explore the domain-specific impact of Error-type Metrics through Principal Component Analysis (PCA), aiming to fill a gap in the existing literature by offering insights into how these metrics affect machine learning models across different software domains. We employ three machine learning models - Support Vector Machine (SVM), Random Forest (RF), and Extreme Gradient Boosting (XGB) - to test our approach. The Synthetic Minority Over-sampling Technique (SMOTE) is used to address class imbalance. Our methodology is validated on fault data from four open-source software projects, aiming to confirm the robustness and generalisability of our approach. The PCA findings provide evidence of the varied impacts of Error-type Metrics in different software environments. Comparative analysis indicates a strong performance by the XGB model, achieving an accuracy of 97.4%, a Matthews Correlation Coefficient of 96.1%, and an F1-score of 97.4% across the datasets. These results suggest the potential of the proposed method to contribute to software testing and quality assurance practices. Our risk-based SFP approach introduces a new perspective to risk assessment in software development. The study’s findings contribute insights into the domain-specific applicability of Error-type Metrics, expanding their potential utility in SFP. Future research directions include refining our fault-counting methodology and exploring broader applications of Error-type Metrics and our proposed risk-based approach.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
The mathematical validity of software metrics,"Mathematical modelling, especially in software engineering metrics, requires more emphasis to be placed on the validity of the selected mathematical tools and techniques to be used. There are a plethora of occurrences in the recent literature of the misapplication of quantitative techniques together with errors in the underlying mathematics. Software engineering metrics needs to take more care in its use of mathematics if it is to gain credence in the scientific and engineering communities.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
SOFTWARE METRICS IN SOFTWARE ENGINEERING AND ARTIFICIAL INTELLIGENCE,This paper examines the utility of much of the software metrics research that has been carried out in software engineering to problems in artificial intelligence. The paper first reviews the work that has been carried out and then makes a number of suggestions about how it could be transferred to the artificial intelligence arena — applying it in particular to expert system development.,CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Assessment of usability metrics for object-oriented software system,The demand for efficient software is increasing day by day. For this reason software developers need appropriate metrics for the development of software applications. Usability is one of the most important fields in software engineering and a highly focused quality factor. It is a key factor in the development of successful software applications. Object-oriented design techniques have become one of the most powerful mechanisms to develop efficient software system. Object-oriented software can play important role in usability for software applications. It can not only help in reducing the cost but also in developing highly usable software systems. This paper focuses some important issues and analyzes the relationship between usability and object-oriented metrics,CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Quantifying software architecture quality report on the first international workshop on software architecture metrics,"Architects of complex software systems face the challenge of how best to assess the achievement of quality attributes and other key drivers, how to reveal issues and risks early, and how to make decisions about architecture improvement. Software architecture quality has a large impact on this effort, but it is usually not assessed with quantitative measures. As the pace of software delivery and technology churn increases, organizations need guidance on how to meet the business goals of their software. There is an increasing need to provide ongoing insight into the quality of the system being developed. Additionally, it is highly desirable to improve feedback between development and deployment through measurable means for intrinsic quality, value, and cost. There is increasing attention to fields such as software analytics and empirical software engineering and measurement that can provide the theory, tooling, or inspiration to develop measurement and analysis frameworks for software architecture. This paper reports on the results of the First International Workshop on Software Architecture Metrics, where participants discussed challenges and opportunities in quantifying software architecture quality.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Metrics-based design selection tool for aspect oriented software development,"Software metrics provide a means for software practitioners to assess the quality of their software. Ideally, this information should be available earlier in the software development lifecycle, since changes are much more expensive to incorporate in the later stages. Design level metrics offer an elegant way of capturing this information. Research in software design metrics has focused primarily on procedural and object oriented software. However, such metrics are currently not available for Aspect Oriented Software Development (AOSD), which is an emerging paradigm. Aspect Oriented Programming (AOP) is an approach that allows programmers to modularize crosscutting concerns that are scattered across multiple modules. Separation of concerns through aspects has the advantages of increased reliability, adaptability and better reuse. The objective of this paper is to propose suitable metrics for the Aspect Oriented Design (AOD) and to develop a tool that will automatically select a better design based on the proposed metrics. In this paper, class and sequence diagrams are used to represent an AOD. The proposed design level metrics are applied to two alternative designs of an illustrative case study. The tool selects the design that better suits stakeholder requirements, based on logical inferences obtained from these metrics regarding the quality of the Aspect Oriented software.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Towards a conceptual framework for object oriented software metrics,"The development of software metrics for object oriented (OO) languages is receiving increasing attention. We examine the reasons why this is a much more challenging problem than for conventional languages. It seems premature to develop and apply OO metrics while there remains uncertainty not only about the precise definitions of many fundamental quantities and their subsequent impact on derived metrics, but also a lack of qualitative understanding of the structure and behaviour of OO systems. We argue that establishing a standard terminology and data model will help provide a framework for both theoretical and empirical work and increase the chances of early success. One potential benefit is improvement of the ability to perform independent validation of models and metrics. We propose a data model and terminology and illustrate the importance of such definitions by examining the seemingly straightforward concept of the number of methods per class. We discuss the implications of ambiguities in definitions for a suite of metrics which has recently been proposed. Preliminary results from our analysis of industrial systems are presented.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Developing the UNPAD SAS (Universitas Padjajaran Statistical Analysis Series) Software,"Student in Faculty of Psycho­logy think that Statistics is very difficult for them, because Statistics is viewed as a hard science than Psychology which is viewed as a soft science. Various attempts have been made to improve student atti­tudes toward statistics, so that student ha­ve more positive attitudes. One of the ef­forts is to transform the curiculum of Sta­tistics in Faculty of Psychology UniversitasPadjadjaran by adding SPSS (Statistical Packages for Social Sciences) practicum courses since 2009. There are a variety of data analysis con­tained in SPSS can be used for data pro­cessing. However, there are still some sta­tistical data analysis used by students of the Faculty of Psychology that is not ava­ilable in SPSS. The aimed of this research is to develop software namely UniversitasPadjadjaran Statistical Analysis Series, which is statistical data analysis software that consist analysis that does not exist in SPSS or other data analysis software. In this preliminary research, modules are de­veloped only for Database Management and Descriptive Statistics.The software development will be carried out by (SDLC = Software Development Li­fe Cycle). SDLC is a series of step or phase that presents a model for development and lifecycle management software or applica­tions.The resulting software is tested on 144 stu­dents in Psychology Faculty in UniversitasPadjadjaran. The trial results showed that the software is most appropriate and ""user friendly"" software.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Deriving Compact Test Suites for Telecommunication Software Using Distance Metrics,"This paper proposes a string edit distance based test selection method to generate compact test sets for telecommunications software. Following the results of previous research, a trace in a test set is considered to be redundant if its edit distance from others is less than a given parameter. The algorithm first determines the minimum cardinality of the target test set inaccordance with the provided parameter, then it selects the test set with the highest sum of internal edit distances. The selection problem is reduced to an assignment problem in bipartite graphs.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Investigating diversity and impact of the popularity metrics for ranking software packages,"AbstractContextCommunity‐based collaborative approach in open source software paradigm promotes reuse of existing software packages. There are several repositories (e.g., npm) for packages and have their own set of metrics for ranking.ObjectiveThis study explores the diversity of different popularity metrics and also the relationship between popularity metrics and development activity of the packages. Another aim is to create a package popularity index by aggregating a set of noncollinear popularity metrics.MethodUsing 195 K packages from different repositories, we investigated the correlation between different popularity metrics. K‐medoids algorithm helped to identify packages with different levels of popularity. Random forests method is utilized to create the package popularity index. Lastly, we used scikit‐learn implementation for determining feature importance in the model.ResultsPopularity metrics of the Github platform are very strongly correlated (R ≥ 0.85) for highly popular packages. Popular packages have high‐development activity. However, the number of downloads of a package does not associate with development activity. Not all the metrics are important for determining popularity of a software package.ConclusionThis study provides practical guidelines to understand important metrics to determine the popularity of software packages. Researchers should focus on non‐collinear metrics, thereby avoiding similar metrics while aggregating for building models.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Examining the Predictive Capability of Advanced Software Fault Prediction Models – An Experimental Investigation Using Combination Metrics,"Background: Fault prediction is a key problem in software engineering domain. In recent years, an increasing interest in exploiting machine learning techniques to make informed decisions to improve software quality based on available data has been observed. Aim: The study aims to build and examine the predictive capability of advanced fault prediction models based on product and process metrics by using machine learning classifiers and ensemble design. Method: Authors developed a methodological framework, consisting of three phases i.e., (i) metrics identification (ii) experimentation using base ML classifiers and ensemble design (iii) evaluating performance and cost sensitiveness. The study has been conducted on 32 projects from the PROMISE, BUG, and JIRA repositories. Result: The results shows that advanced fault prediction models built using ensemble methods show an overall median of F-score ranging between 76.50% and 87.34% and the ROC(AUC) between 77.09% and 84.05% with better predictive capability and cost sensitiveness. Also, non-parametric tests have been applied to test the statistical significance of the classifiers. Conclusion: The proposed advanced models have performed impressively well for inter project fault prediction for projects from PROMISE, BUG, and JIRA repositories.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Why quality?,"The widespread use of information services will only be accepted by users if their quality is of acceptable level. It is therefore of high interest to be able to estimate, or even measure the quality of a system under construction. UML is now a de-factor standard for modelling systems to be build. This paper indicates how UML diagrams are related to software quality metrics as described in ISO/IEC 9126 and similar quality standards. The paper discusses relevant quality metrics, analyses sources of errors and relates them to the UML diagrams used in software engineering.The paper discusses the sub-attributes of the attribute 'Functionality' in more detail and relates the to the relevant UML diagrams.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Evaluating and comparing software metrics in the software engineering laboratory,"There has appeared in the literature a great number of metrics that attempt to measure the effort or complexity in developing and understanding software(1). There have also been several attempts to independently validate these measures on data from different organizations gathered by different people(2). These metrics have many purposes. They can be used to evaluate the software development process or the software product. They can be used to estimate the cost and quality of the product. They can also be used during development and evolution of the software to monitor the stability and quality of the product. Among the most popular metrics have been the software science metrics of Halstead, and the cyclomatic complexity metric of McCabe. One question is whether these metrics actually measure such things as effort and complexity. One measure of effort may be the time required to produce a product. One measure of complexity might be the number of errors made during the development of a product. A second question is how these metrics compare with standard size measures, such as the number of source lines or the number of executable statements, i.e., do they do a better job of predicting the effort or the number of errors? Lastly, how do these metrics relate to each other?",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
The relationship between modular metrics and fuzzy metrics revisited,"In a famous article published in 1975, Kramosil and Mich\'{a}lek introduced a notion of fuzzy metric that was the origin of numerous researches and publications in several frameworks and fields. In 2010, Chistyakov introduced and discussed in detail the concept of modular metric. Since then, some authors have investigated the problem of establishing connections between the notions of fuzzy metric and modular metric, obtaining positive partial solutions. In this paper, we are interested in determining the precise relationship between these two concepts. To achieve this goal, we examine a proof, based on the use of uniformities, of the important result that the topology induced by a fuzzy metric is metrizable. As a consequence of that analysis, we introduce the notion of a weak fuzzy metric and show that every weak fuzzy metric, with continuous t-norm the minimum t-norm, generates a modular metric and, conversely, we show that every modular metric generates a weak fuzzy metric, with continuous t-norm the product t-norm. It follows that every modular metric can be generated from a suitable weak fuzzy metric, and that several examples and properties of modular metrics can be directly deduced from those previously obtained in the field of fuzzy metrics.",CS,CT_COMPLEX,85,Clear CS paper with focus on Computational Theory & Complexity
Using Metrics for Risk Prediction in Object-Oriented Software: A Cross-Version Validation,"This work aims to investigate the potential, from different perspectives, of a risk model to support Cross-Version Fault and Severity Prediction (CVFSP) in object-oriented software. The risk of a class is addressed from the perspective of two particular factors: the number of faults it can contain and their severity. We used various object-oriented metrics to capture the two risk factors. The risk of a class is modeled using the concept of Euclidean distance. We used a dataset collected from five successive versions of an open-source Java software system (ANT). We investigated different variants of the considered risk model, based on various combinations of object-oriented metrics pairs. We used different machine learning algorithms for building the prediction models: Naive Bayes (NB), J48, Random Forest (RF), Support Vector Machines (SVM) and Multilayer Perceptron (ANN). We investigated the effectiveness of the prediction models for Cross-Version Fault and Severity Prediction (CVFSP), using data of prior versions of the considered system. We also investigated if the considered risk model can give as output the Empirical Risk (ER) of a class, a continuous value considering both the number of faults and their different levels of severity. We used different techniques for building the prediction models: Linear Regression (LR), Gaussian Process (GP), Random forest (RF) and M5P (two decision trees algorithms), SmoReg and Artificial Neural Network (ANN). The considered risk model achieves acceptable results for both cross-version binary fault prediction (a g-mean of 0.714, an AUC of 0.725) and cross-version multi-classification of levels of severity (a g-mean of 0.758, an AUC of 0.771). The model also achieves good results in the estimation of the empirical risk of a class by considering both the number of faults and their levels of severity (intra-version analysis with a correlation coefficient of 0.659, cross-version analysis with a correlation coefficient of 0.486).",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
An Empirical Study of the Effect of Power Law Distribution on the Interpretation of OO Metrics,"Context. Software metrics are surrogates of software quality. Software metrics can be used to find possible problems or chances for improvements in software quality. However, software metrics are numbers that are not easy to interpret. Previous analysis of software metrics has shown fat tails in the distribution. The skewness and fat tails of such data are properties of many statistical distributions and more importantly the phenomena of the power law. These statistical properties affect the interpretation of software quality metrics. Objectives. The objective of this research is to validate the effect of power laws on the interpretation of software metrics. Method. To investigate the effect of power law properties on software quality, we study five open-source systems to investigate the distribution and their effect on fault prediction models. Results. Study shows that power law behavior has an effect on the interpretation and usage of software metrics and in particular the CK metrics. Many metrics have shown a power law behavior. Threshold values are derived from the properties of the power law distribution when applied to open-source systems. Conclusion. The properties of a power law distribution can be effective in improving the fault-proneness models by setting reasonable threshold values.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
PEMERINGKATAN SOFTWARE APLIKASI BERDASARKAN PROPERTI KUALITAS DISAIN DAN METRICS FOR OBJECT ORIENTED SOFTWARE MENGGUNAKAN ANALYTIC HIERARCHY PROCESS,"Sebuah metode untuk mengukur kualitas desain berdasarkan hasil implementasinya dalam Java source codes diusulkan dalam penelitian ini. Metode yang diusulkan menggabungkan Metrics for Object-Oriented Software Engineering (MOOSE), properti kualitas desain software dan konsep Analytic Hierarchy Process (AHP). Sebagai studi kasus, metode ini diterapkan pada sejumlah aplikasi ERP yang bersifat open source yaitu Adempiere, OpenBravo, Plazma, FreedomERP, dan JAllInOne. Pengukuran MOOSE dilakukan dengan bantuan tool CKJM 1.8. Hasil ukur MOOSE dikelompokkan dalam properti kualitas yaitu efficiency, understandability, reusability, testability dan maintainability. Kombinasi MOOSE dan AHP yang dihasilkan dapat menjadi alat bantu dalam menentukan peringkat kualitas software dari aspek orientasi objek. A method for measuring the quality of the design is based on the results of its implementation in the Java source codes proposed in this study. The proposed method combines Metrics for Object-Oriented Software Engineering (MOOSE), property and the concept of software design quality Analytic Hierarchy Process (AHP). As a case study, this method is applied to a number of applications that are open source ERP is Adempiere, Openbravo, Plazma, FreedomERP, and JAllInOne. MOOSE measurements done with the aid tool CKJM 1.8. MOOSE measuring results are grouped in quality properties that are efficiency, understandability, reusability, testability and maintainability. MOOSE and AHP combination that have been produced can be a useful tool in determining the quality ratings of aspects of object oriented software.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
On effort-aware metrics for defect prediction,"AbstractContextAdvances in defect prediction models, aka classifiers, have been validated via accuracy metrics. Effort-aware metrics (EAMs) relate to benefits provided by a classifier in accurately ranking defective entities such as classes or methods. PofB is an EAM that relates to a user that follows a ranking of the probability that an entity is defective, provided by the classifier. Despite the importance of EAMs, there is no study investigating EAMs trends and validity.AimThe aim of this paper is twofold: 1) we reveal issues in EAMs usage, and 2) we propose and evaluate a normalization of PofBs (aka NPofBs), which is based on ranking defective entities by predicted defect density.MethodWe perform a systematic mapping study featuring 152 primary studies in major journals and an empirical study featuring 10 EAMs, 10 classifiers, two industrial, and 12 open-source projects.ResultsOur systematic mapping study reveals that most studies using EAMs use only a single EAM (e.g., PofB20) and that some studies mismatched EAMs names. The main result of our empirical study is that NPofBs are statistically and by orders of magnitude higher than PofBs.ConclusionsIn conclusion, the proposed normalization of PofBs: (i) increases the realism of results as it relates to a better use of classifiers, and (ii) promotes the practical adoption of prediction models in industry as it shows higher benefits. Finally, we provide a tool to compute EAMs to support researchers in avoiding past issues in using EAMs.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Statistical and regression analysis of impact of faults on object oriented software,"These days the Object-Oriented (OO) paradigm is used extensively in the development of software systems. The OO metrics can be employed to access the quality of these OO systems. Many metrics and metric suits have been developed by the scientists to access the quality of software. The properties of object-oriented designs can be measured using object-oriented metrics. With the help of metrics, estimations of project milestones can be attained more accurately. It is possible to develop software projects with nominal faults. A study shows that an estimated 42% on account of corrective maintenance can be saved by using object-oriented metrics. This paper assesses the capability of Object Oriented metrics to identify fault-proneness in Object Oriented software systems using statistical and regression analysis. Three projects from the NASA data set have been used to determine the applicability of object-oriented CK metrics. The CK metrics are used to predict the bugs in the class. The fault-proneness using OO metrics has been calculated. The information collected using this method will increase the quality and reliability of the OO software systems.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Новейший способ обмена метриками,"Сбор метрик программного обеспечения является фундаментальной деятельностью, которая необходима для проведния практически любого эмпирического исследования в области программной инженерии. Однако, даже при наличии широкого спектра инструментов, сбор таких фундаментальных данных по-прежнему занимает много времени. Более того, каждый исследователь собирает практически одни и те же данные (например, метрики CK, цикломатическая сложность МакКейба и т.д.) из практически одних и тех же проектов (например, из известных проектов с открытым исходным кодом). Объем такой дублирующей работы, выполняемой в сообществе, уменьшает усилия, которые исследователи могут потратить на наиболее ценную часть своих исследований, такую как разработка новых теорий и моделей и их эмпирическая оценка. В данной работе предлагается новый подход для сбора и обмена данными метрик программного обеспечения, позволяющий сотрудничать исследователям и сократить количество напрасных усилий в сообществе разработчиков программного обеспечения. Мы стремимся достичь этой цели, предлагая Формат обмена программными метриками (SMEF)и REST API для сбора, хранения и обмена данными метрик программного обеспечения. In almost every empirical software engineering study, software metrics collection is a fundamental activity. Although many tools exist to collect this data, it still takes a considerable amount of time. In addition, almost all researchers collect essentially the same data (e.g., CK metrics, McCabe Cyclomatic Complexity, etc.) from essentially the same sources (e.g., well-known open-source projects).Having so much duplication of work done within a community reduces the amount of time that researchers can spend developing new ideas and evaluating them empirically, which is the most valuable part of their research. In this paper, we propose a novel approach for getting and sharing software metrics data that will allow them to collaborate and reduce the amount of wasted effort. SMEF, a file format for exchanging software metrics information, and a REST API, targeted at this objective, are proposed in this paper.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Exploring the relationship between performance metrics and cost saving potential of defect prediction models,"AbstractContext:Performance metrics are a core component of the evaluation of any machine learning model and used to compare models and estimate their usefulness. Recent work started to question the validity of many performance metrics for this purpose in the context of software defect prediction.Objective:Within this study, we explore the relationship between performance metrics and the cost saving potential of defect prediction models. We study whether performance metrics are suitable proxies to evaluate the cost saving capabilities and derive a theory for the relationship between performance metrics and cost saving potential.Methods:We measure performance metrics and cost saving potential in defect prediction experiments. We use a multinomial logit model, decision, and random forest to model the relationship between the metrics and the cost savings.Results:We could not find a stable relationship between cost savings and performance metrics. We attribute the lack of the relationship to the inability of performance metrics to account for the property that a small proportion of very large software artifacts are the main driver of the costs.fact that performance metrics are incapable of accurately considering the costs associate with individual artifacts, which is required due to the exponential distribution of artifact sizes.Conclusion:Any defect prediction study interested in finding the best prediction model, must consider cost savings directly, because no reasonable claims regarding the economic benefits of defect prediction can be made otherwise.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Usability Estimation of Software System by using Object-Oriented Metrics,"Usability is one of the most important quality factors in the fields of software engineering and an important issue in the development of successful software applications. To develop efficient software systems, software developers need appropriate metrics. Object-oriented metrics can play important role in the development of successful and usable software applications because object-oriented design techniques have become one of the most powerful mechanisms to fulfill the demand of efficient software systems. Currently there are no comprehensive criteria for estimating usability by using object-oriented metrics. This paper focuses on usability and presents a comparative analysis of various usability models and metrics. It then analyzes the relationship between usability and object-oriented metrics.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
E-Commerce Adoption by Women Entrepreneurs in India: An Application of the UTAUT Model,"The wide use of ICT applications has opened enormous opportunities for large, medium and even small organizations. This study aims to investigate the extent of adoption of e-commerce applications by the women owned SMEs in India, with special focus on behavioral factors which influence them to do so. The Unified Theory of Acceptance and Use of Technology (UTAUT) model was utilized to determine the strength of the constructs in influencing e-commerce adoption amongst women entrepreneurs. Using a structured questionnaire, responses were solicited via a field survey amongst 144 women entrepreneurs in two districts of Kolkata and 24 Parganas (South) in the State of West Bengal, India. The results show that three constructs, namely, performance expectancy, effort expectancy and social influence significantly affect the behavioral intention of these women entrepreneurs to use e-commerce. Experience and voluntariness to use are the moderators significantly correlated with effort expectancy, facilitating conditions and social influence. Moreover, it has been revealed that facilitating conditions and the behavioral intention positively influence their usage behavior.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Peningkatan behavioral intentions dan use behavioral pada konsumen Inagreen Farm menggunakan Utaut Model (Studi kasus: Marketplace Tokopedia),"The increasing number of internet users has led to a rise in online shopping, which in turn has increased the number of e-commerce users and prompted companies in the agricultural sector to offer their products online. Understanding behavioral intention is important as a guide to improving service quality. The aim of this research is to analyze consumer characteristics, identify the factors that influence behavioral intention and use behavior, and determine the implications for UTAUT model. The UTAUT model is analyzed using structural equation modeling (SEM). Data collection is conducted through an online questionnaire. The results of the analysis show that the factors influencing behavioral intention are effort expectancy (EE) and social influence (SI). The factor influencing use behavior is behavioral intention (BI). Meningkatnya pengguna internet membuat masyarakat banyak melakukan berbelanja secara online, hal ini membuat pengguna e-commerce semakin meningkat dan perusahaan sektor pertanian melakukan produk secara online. Pentingnya mengetahui behavioral intention sebagai pedoman meningkatkan kualitas layanan. Tujuan penelitian ini adalah menganalisis karakteristik konsumen, faktor apa yang mempengaruhi behavioral intention dan use behaviour dan implikasi bagi perusahaan dengan menggunakan model UTAUT. Model UTAUT dianalisis menggunakan structural equation model (SEM). Pengambilan data dilakukan dengan menyebarkan kuesioner secara online. Hasil analisis faktor-faktor yang mempengaruhi behaviour intention yaitu effort expectancy (EE) dan social influence (SI). Faktor yang mempengaruhi use behaviour yaitu behavior intention (BI)..",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Effect of declarations on software metrics,"The attractiveness of software science [HAL77] is to some extent due to the simplicity of its instrumentation. Upon learning the detailed rules of counting operators and operands, the experiments and derivations using various algorithms and languages can be repeated. Proposed or actual applications of software science are quite varied (For example, see [SEN79]). The size and construction time of a program can be estimated from the problem specification and the choice of programming language. An estimate of the number of program bugs can be shown to depend on programming effort. Optimal choice of module sizes for multimodule implementations can be computed. Elements of software science have applications to the analysis of technical prose. The purpose of this experiment is three fold. First, we want to apply software science metrics to the language 'C'. The second purpose of the experiment is to study the effect of including declaration statements while counting operators and operands. Finally, we have set out to determine whether the area of application has any influence on software science metrics.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
ANALISIS PENERIMAAN PENGGUNA APLIKASI SIAKAD DESKTOP UNIVERSITAS BRAWIJAYA MENGGUNAKAN UTAUT,"Education services from tertiary institutions have applied a lot of the Academic Information System (SIAKAD). In the research on heuristic evaluation in the Academic Information System of Brawijaya University, there were several problems related to users, including problems in the ease of users to interact or use applications, SIAKAD Desktop which did not provide information to the user about the condition of a process within a certain time period or time. On the other hand, universities must guarantee the best service. Therefore, it is necessary to evaluate the acceptance of SIAKAD Desktop users to find out what factors can affect the use of Desktop SIAKAD. UTAUT as a theoretical model will be used to explore the acceptance and use of the technological environment, especially for SIAKAD users in the academic section of the Faculty of Computer Science, Universitas Brawijaya. The data that has been collected will then be analyzed using the Partial Least Square Sequence Equation Modeling (PLS SEM) method. The results of the analysis show that only the conditions of the facilities significantly influence the behavior of using SIAKAD Desktop. The UTAUT model can only explain 44.8% of the factor variants using SIAKAD Desktop which is included in the moderate category. Therefore, it is necessary to develop a research model construct construct so that the prediction of the variance accuracy of the research model increases. In addition, it is expected that further research can cover a wider population target so that data is more reliable.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Algorithmic Resistance and Online Privacy: Extending the Meta-UTAUT Model with Particular Privacy Concerns,"This study delved into the perceived benefits and privacy concerns individuals face when interacting with algorithms, and explored their relation to algorithmic resistance. Based on technology acceptance research and online privacy studies, an extended Meta-UTAUT model was proposed. A total of 434 valid samples were obtained in China. The results show that perceived benefits (including performance expectancy, effort expectancy, social influence, and facilitating conditions) are negatively related to algorithmic resistance attitude. Moreover, concerns for technology and financial privacy are positively related to algorithmic resistance intention.This result identifies the aspects of privacy highly esteemed in the interaction between individuals and algorithms. Finally, the contributions, practical and theoretical significance, and limitations of this study were discussed.",IS,DG_PRIVACY,85,Clear IS paper with focus on Data Governance & Privacy
UTAUT Model: Identifying the Driving Factors of the Intention to Use Paylater,"Information and Communication Technology (ICT) has developed rapidly that drives to the growth of digital transaction in recent years. E-commerce companies create features in order to deal with their customers, such as paylater in their digital payment. It offers a payment method that is almost similar to a credit card which allows customers to buy products or services right now with the delaying of payment. This phenomenon becomes regular in e-commerce business. Thus, this study aims to examine what factors influence individual behavioral intention in adopting paylater by using Unified Theory of Acceptance and Use of Technology (UTAUT) theory. A total of 167 respondents was involved in this study. Data were analyzed by using Structural Equation Modeling (SEM). The result showed that only performance expectancy and facilitating conditions have a significant influence on behavioral intention.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Analysis Of Epuskesmas Adoption In Tasikmalaya Regency Using Utaut Model,"The 4.0 Revolution has rendered information technology an indispensable necessity in human life. Technological progress, particularly in healthcare, offers significant conveniences. The healthcare sector's technological evolution is marked by the digital transformation of Community Health Center (Puskesmas) information systems, with ePuskesmas becoming one such system. ePuskesmas serves as an online and integrated Puskesmas information system following the standards set by the Ministry of Health of the Republic of Indonesia through the Puskesmas Information System (SIP). Despite not being universally implemented, particularly in Tasikmalaya District, this study uses the UTAUT model. Variables include Performance Expectancy, Effort Expectancy, Social Influence, Facilitating Conditions, Behavioral Intention, and Use Behavior. This study uses a quantitative method by distributing online questionnaires to ePuskesmas users in 40 Puskesmas in Tasikmalaya District. All collected data met validity and reliability criteria and were assessed using SmartPLS 4.0 software. Hypotheses were tested using Partial Least Square Structural Equation Modeling (PLS-SEM), revealing that Performance Expectancy and Effort Expectancy significantly and positively influence Behavioral Intention. Behavioral Intention and Facilitating Conditions significantly and positively impact the Use Behavior of ePuskesmas.",IS,HIS,85,Clear IS paper with focus on Healthcare Information Systems
Analisis Sistem Informasi Panda (SIP) Terhadap Penerimaan PenggunaMenggunakan Metode UTAUT,"PT. Puskomedia Indonesia Kreatif built a Village Information System named ""Panda"". PT. Puskomedia Indonesia Kreatif developed the Panda Information System with the hope that it would make it easier for village officials to get information with data owned by the village. The problem with the Panda Information System is that there has been no training on how to use the panda information system and there is no manual so that villages are less interested in using the application. The purpose of this study is to find out what factors influence the behavior of Panda Information System users. This study uses the UTAUT (Unified Theory of Acceptance and Use of Technology) method and uses the variables of Performance Expectancy, Effort Expectancy, Social Influence, Facilitating Condition, Behavioral Intention and Use Behavior. The software used is SPSS 25 and SmartPLS. The analytical method used is descriptive analysis and SEM analysis. Results Based on the research, it was found that the pValues of the Behavioral Intention variable on Use Behavior was 0.142 &gt; 0.05, variable Effort Expectancy on Behavioral Intention is 0.179 &gt; 0.05, variable Facilitating Conditions on Use Behavior is 0.346 &gt; 0.05, variable Performance Expectancy on Behavioral Intention is 0.498 &gt; 0.05, andvariable Social Influence on Behavioral Intention is 0.418 &gt; 0.05 so it is considered insignificant.",IS,IS_RM,85,Clear IS paper with focus on Information Systems Research Methods
Understanding Lookahead Dynamics Through Laplace Transform,"We introduce a frequency-domain framework for convergence analysis of hyperparameters in game optimization, leveraging High-Resolution Differential Equations (HRDEs) and Laplace transforms. Focusing on the Lookahead algorithm--characterized by gradient steps $k$ and averaging coefficient $\alpha$--we transform the discrete-time oscillatory dynamics of bilinear games into the frequency domain to derive precise convergence criteria. Our higher-precision $O(\gamma^2)$-HRDE models yield tighter criteria, while our first-order $O(\gamma)$-HRDE models offer practical guidance by prioritizing actionable hyperparameter tuning over complex closed-form solutions. Empirical validation in discrete-time settings demonstrates the effectiveness of our approach, which may further extend to locally linear operators, offering a scalable framework for selecting hyperparameters for learning in games.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images,"Reconstructing an animatable 3D human from casually captured images of an articulated subject without camera or human pose information is a practical yet challenging task due to view misalignment, occlusions, and the absence of structural priors. While optimization-based methods can produce high-fidelity results from monocular or multi-view videos, they require accurate pose estimation and slow iterative optimization, limiting scalability in unconstrained scenarios. Recent feed-forward approaches enable efficient single-image reconstruction but struggle to effectively leverage multiple input images to reduce ambiguity and improve reconstruction accuracy. To address these challenges, we propose PF-LHM, a large human reconstruction model that generates high-quality 3D avatars in seconds from one or multiple casually captured pose-free images. Our approach introduces an efficient Encoder-Decoder Point-Image Transformer architecture, which fuses hierarchical geometric point features and multi-view image features through multimodal attention. The fused features are decoded to recover detailed geometry and appearance, represented using 3D Gaussian splats. Extensive experiments on both real and synthetic datasets demonstrate that our method unifies single- and multi-image 3D human reconstruction, achieving high-fidelity and animatable 3D human avatars without requiring camera and human pose annotations. Code and models will be released to the public.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning,"Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities,particularly in environments with complex reward landscapes.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization,"Transformer models face scalability challenges in causal language modeling (CLM) due to inefficient memory allocation for growing key-value (KV) caches, which strains compute and storage resources. Existing methods like Grouped Query Attention (GQA) and token-level KV optimization improve efficiency but rely on rigid resource allocation, often discarding ""low-priority"" tokens or statically grouping them, failing to address the dynamic spectrum of token importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that dynamically optimizes token-wise computation and memory allocation. Unlike prior approaches, mixSGA retains all tokens while adaptively routing them to specialized experts with varying KV group sizes, balancing granularity and efficiency. Our key novelties include: (1) a token-wise expert-choice routing mechanism guided by learned importance scores, enabling proportional resource allocation without token discard; (2) weight-sharing across grouped attention projections to minimize parameter overhead; and (3) an auxiliary loss to ensure one-hot routing decisions for training-inference consistency in CLMs. Extensive evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show mixSGA's superiority over static baselines. On instruction-following and continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower perplexity under the same KV budgets.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Identification of Impulse Response Functions for Nonlinear Dynamic Models,"We explore the issues of identification for nonlinear Impulse Response Functions in nonlinear dynamic models and discuss the settings in which the problem can be mitigated. In particular, we introduce the nonlinear autoregressive representation with Gaussian innovations and characterize the identified set. This set arises from the multiplicity of nonlinear innovations and transformations which leave invariant the standard normal density. We then discuss possible identifying restrictions, such as non-Gaussianity of independent sources, or identifiable parameters by means of learning algorithms, and the possibility of identification in nonlinear dynamic factor models when the underlying latent factors have different dynamics. We also explain how these identification results depend ultimately on the set of series under consideration.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning,"Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited ""out-of-the-box"" capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
A Survey on Imitation Learning for Contact-Rich Tasks in Robotics,"This paper comprehensively surveys research trends in imitation learning for contact-rich robotic tasks. Contact-rich tasks, which require complex physical interactions with the environment, represent a central challenge in robotics due to their nonlinear dynamics and sensitivity to small positional deviations. The paper examines demonstration collection methodologies, including teaching methods and sensory modalities crucial for capturing subtle interaction dynamics. We then analyze imitation learning approaches, highlighting their applications to contact-rich manipulation. Recent advances in multimodal learning and foundation models have significantly enhanced performance in complex contact tasks across industrial, household, and healthcare domains. Through systematic organization of current research and identification of challenges, this survey provides a foundation for future advancements in contact-rich robotic manipulation.",CS,ROB_AUTO,85,Clear CS paper with focus on Robotics & Automation
Hierarchical Multi-Positive Contrastive Learning for Patent Image Retrieval,"Patent images are technical drawings that convey information about a patent's innovation. Patent image retrieval systems aim to search in vast collections and retrieve the most relevant images. Despite recent advances in information retrieval, patent images still pose significant challenges due to their technical intricacies and complex semantic information, requiring efficient fine-tuning for domain adaptation. Current methods neglect patents' hierarchical relationships, such as those defined by the Locarno International Classification (LIC) system, which groups broad categories (e.g., ""furnishing"") into subclasses (e.g., ""seats"" and ""beds"") and further into specific patent designs. In this work, we introduce a hierarchical multi-positive contrastive loss that leverages the LIC's taxonomy to induce such relations in the retrieval process. Our approach assigns multiple positive pairs to each patent image within a batch, with varying similarity scores based on the hierarchical taxonomy. Our experimental analysis with various vision and multimodal models on the DeepPatent2 dataset shows that the proposed method enhances the retrieval results. Notably, our method is effective with low-parameter models, which require fewer computational resources and can be deployed on environments with limited hardware.",CS,DL_IR,85,Clear CS paper with focus on Digital Libraries & Information Retrieval
Leveraging Vision-Language Pre-training for Human Activity Recognition in Still Images,"Recognising human activity in a single photo enables indexing, safety and assistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled as walking, running, sitting, and standing, scratch CNNs scored 41% accuracy. Fine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive vision-language pre-training decisively improves still-image action recognition in real-world deployments.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Deep Learning-Based Multi-Object Tracking: A Comprehensive Survey from Foundations to State-of-the-Art,"Multi-object tracking (MOT) is a core task in computer vision that involves detecting objects in video frames and associating them across time. The rise of deep learning has significantly advanced MOT, particularly within the tracking-by-detection paradigm, which remains the dominant approach. Advancements in modern deep learning-based methods accelerated in 2022 with the introduction of ByteTrack for tracking-by-detection and MOTR for end-to-end tracking. Our survey provides an in-depth analysis of deep learning-based MOT methods, systematically categorizing tracking-by-detection approaches into five groups: joint detection and embedding, heuristic-based, motion-based, affinity learning, and offline methods. In addition, we examine end-to-end tracking methods and compare them with existing alternative approaches. We evaluate the performance of recent trackers across multiple benchmarks and specifically assess their generality by comparing results across different domains. Our findings indicate that heuristic-based methods achieve state-of-the-art results on densely populated datasets with linear object motion, while deep learning-based association methods, in both tracking-by-detection and end-to-end approaches, excel in scenarios with complex motion patterns.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
JENGA: Object selection and pose estimation for robotic grasping from a stack,"Vision-based robotic object grasping is typically investigated in the context of isolated objects or unstructured object sets in bin picking scenarios. However, there are several settings, such as construction or warehouse automation, where a robot needs to interact with a structured object formation such as a stack. In this context, we define the problem of selecting suitable objects for grasping along with estimating an accurate 6DoF pose of these objects. To address this problem, we propose a camera-IMU based approach that prioritizes unobstructed objects on the higher layers of stacks and introduce a dataset for benchmarking and evaluation, along with a suitable evaluation metric that combines object selection with pose accuracy. Experimental results show that although our method can perform quite well, this is a challenging problem if a completely error-free solution is needed. Finally, we show results from the deployment of our method for a brick-picking application in a construction scenario.",CS,ROB_AUTO,85,Clear CS paper with focus on Robotics & Automation
Uncertainty-Aware Remaining Lifespan Prediction from Images,"Predicting mortality-related outcomes from images offers the prospect of accessible, noninvasive, and scalable health screening. We present a method that leverages pretrained vision transformer foundation models to estimate remaining lifespan from facial and whole-body images, alongside robust uncertainty quantification. We show that predictive uncertainty varies systematically with the true remaining lifespan, and that this uncertainty can be effectively modeled by learning a Gaussian distribution for each sample. Our approach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on an established Dataset, and further improves to 4.79 and 5.07 years MAE on two new, higher-quality datasets curated and published in this work. Importantly, our models provide well-calibrated uncertainty estimates, as demonstrated by a bucketed expected calibration error of 0.62 years. While not intended for clinical deployment, these results highlight the potential of extracting medically relevant signals from images. We make all code and datasets available to facilitate further research.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
BOW: Bottlenecked Next Word Exploration,"Large language models (LLMs) are typically trained via next-word prediction (NWP), which provides strong surface-level fluency but often lacks support for robust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel RL framework that rethinks NWP by introducing a reasoning bottleneck where a policy model first generates a reasoning path rather than predicting the next token directly, after which a frozen judge model predicts the next token distribution based solely on this reasoning path. We train the policy model using GRPO with rewards that quantify how effectively the reasoning path facilitates next-word recovery. Compared with other continual pretraining baselines, we show that BOW improves both the general and next-word reasoning capabilities of the base model, evaluated on various benchmarks. Our findings show that BOW can serve as an effective and scalable alternative to vanilla NWP.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
DicFace: Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration,"Video face restoration faces a critical challenge in maintaining temporal consistency while recovering fine facial details from degraded inputs. This paper presents a novel approach that extends Vector-Quantized Variational Autoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a video restoration framework through variational latent space modeling. Our key innovation lies in reformulating discrete codebook representations as Dirichlet-distributed continuous variables, enabling probabilistic transitions between facial features across frames. A spatio-temporal Transformer architecture jointly models inter-frame dependencies and predicts latent distributions, while a Laplacian-constrained reconstruction loss combined with perceptual (LPIPS) regularization enhances both pixel accuracy and visual quality. Comprehensive evaluations on blind face restoration, video inpainting, and facial colorization tasks demonstrate state-of-the-art performance. This work establishes an effective paradigm for adapting intensive image priors, pretrained on high-quality images, to video restoration while addressing the critical challenge of flicker artifacts. The source code has been open-sourced and is available at https://github.com/fudan-generative-vision/DicFace.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Estimating Intractable Posterior Distributions through Gaussian Process regression and Metropolis-adjusted Langevin procedure,"Numerical simulations are crucial for modeling complex systems, but calibrating them becomes challenging when data are noisy or incomplete and likelihood evaluations are computationally expensive. Bayesian calibration offers an interesting way to handle uncertainty, yet computing the posterior distribution remains a major challenge under such conditions. To address this, we propose a sequential surrogate-based approach that incrementally improves the approximation of the log-likelihood using Gaussian Process Regression. Starting from limited evaluations, the surrogate and its gradient are refined step by step. At each iteration, new evaluations of the expensive likelihood are added only at informative locations, that is to say where the surrogate is most uncertain and where the potential impact on the posterior is greatest. The surrogate is then coupled with the Metropolis-Adjusted Langevin Algorithm, which uses gradient information to efficiently explore the posterior. This approach accelerates convergence, handles relatively high-dimensional settings, and keeps computational costs low. We demonstrate its effectiveness on both a synthetic benchmark and an industrial application involving the calibration of high-speed train parameters from incomplete sensor data.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns,"Long-term memory (LTM) is essential for large language models (LLMs) to achieve autonomous intelligence in complex, evolving environments. Despite increasing efforts in memory-augmented and retrieval-based architectures, there remains a lack of standardized benchmarks to systematically evaluate LLMs' long-term memory abilities. Existing benchmarks still face challenges in evaluating knowledge retention and dynamic sequential reasoning, and in their own flexibility, all of which limit their effectiveness in assessing models' LTM capabilities. To address these gaps, we propose a novel benchmark framework based on interactive fiction games, featuring dynamically branching storylines with complex reasoning structures. These structures simulate real-world scenarios by requiring LLMs to navigate hierarchical decision trees, where each choice triggers cascading dependencies across multi-turn interactions. Our benchmark emphasizes two distinct settings to test reasoning complexity: one with immediate feedback upon incorrect decisions, and the other requiring models to independently trace back and revise earlier choices after failure. As part of this benchmark, we also construct a new dataset designed to test LLMs' LTM within narrative-driven environments. We further validate the effectiveness of our approach through detailed experiments. Experimental results demonstrate the benchmark's ability to robustly and reliably assess LTM in LLMs.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Action Dubber: Timing Audible Actions via Inflectional Flow,"We introduce the task of Audible Action Temporal Localization, which aims to identify the spatio-temporal coordinates of audible movements. Unlike conventional tasks such as action recognition and temporal action localization, which broadly analyze video content, our task focuses on the distinct kinematic dynamics of audible actions. It is based on the premise that key actions are driven by inflectional movements; for example, collisions that produce sound often involve abrupt changes in motion. To capture this, we propose $TA^{2}Net$, a novel architecture that estimates inflectional flow using the second derivative of motion to determine collision timings without relying on audio input. $TA^{2}Net$ also integrates a self-supervised spatial localization strategy during training, combining contrastive learning with spatial analysis. This dual design improves temporal localization accuracy and simultaneously identifies sound sources within video frames. To support this task, we introduce a new benchmark dataset, $Audible623$, derived from Kinetics and UCF101 by removing non-essential vocalization subsets. Extensive experiments confirm the effectiveness of our approach on $Audible623$ and show strong generalizability to other domains, such as repetitive counting and sound source localization. Code and dataset are available at https://github.com/WenlongWan/Audible623.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Advancing Image-Based Grapevine Variety Classification with a New Benchmark and Evaluation of Masked Autoencoders,"Grapevine varieties are essential for the economies of many wine-producing countries, influencing the production of wine, juice, and the consumption of fruits and leaves. Traditional identification methods, such as ampelography and molecular analysis, have limitations: ampelography depends on expert knowledge and is inherently subjective, while molecular methods are costly and time-intensive. To address these limitations, recent studies have applied deep learning (DL) models to classify grapevine varieties using image data. However, due to the small dataset sizes, these methods often depend on transfer learning from datasets from other domains, e.g., ImageNet1K (IN1K), which can lead to performance degradation due to domain shift and supervision collapse. In this context, self-supervised learning (SSL) methods can be a good tool to avoid this performance degradation, since they can learn directly from data, without external labels. This study presents an evaluation of Masked Autoencoders (MAEs) for identifying grapevine varieties based on field-acquired images. The main contributions of this study include two benchmarks comprising 43 grapevine varieties collected across different seasons, an analysis of MAE's application in the agricultural context, and a performance comparison of trained models across seasons. Our results show that a ViT-B/16 model pre-trained with MAE and the unlabeled dataset achieved an F1 score of 0.7956, outperforming all other models. Additionally, we observed that pre-trained models benefit from long pre-training, perform well under low-data training regime, and that simple data augmentation methods are more effective than complex ones. The study also found that the mask ratio in MAE impacts performance only marginally.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Active Multimodal Distillation for Few-shot Action Recognition,"Owing to its rapid progress and broad application prospects, few-shot action recognition has attracted considerable interest. However, current methods are predominantly based on limited single-modal data, which does not fully exploit the potential of multimodal information. This paper presents a novel framework that actively identifies reliable modalities for each sample using task-specific contextual cues, thus significantly improving recognition performance. Our framework integrates an Active Sample Inference (ASI) module, which utilizes active inference to predict reliable modalities based on posterior distributions and subsequently organizes them accordingly. Unlike reinforcement learning, active inference replaces rewards with evidence-based preferences, making more stable predictions. Additionally, we introduce an active mutual distillation module that enhances the representation learning of less reliable modalities by transferring knowledge from more reliable ones. Adaptive multimodal inference is employed during the meta-test to assign higher weights to reliable modalities. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing approaches.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning,"Autonomous vehicles that navigate in open-world environments may encounter previously unseen object classes. However, most existing LiDAR panoptic segmentation models rely on closed-set assumptions, failing to detect unknown object instances. In this work, we propose ULOPS, an uncertainty-guided open-set panoptic segmentation framework that leverages Dirichlet-based evidential learning to model predictive uncertainty. Our architecture incorporates separate decoders for semantic segmentation with uncertainty estimation, embedding with prototype association, and instance center prediction. During inference, we leverage uncertainty estimates to identify and segment unknown instances. To strengthen the model's ability to differentiate between known and unknown objects, we introduce three uncertainty-driven loss functions. Uniform Evidence Loss to encourage high uncertainty in unknown regions. Adaptive Uncertainty Separation Loss ensures a consistent difference in uncertainty estimates between known and unknown objects at a global scale. Contrastive Uncertainty Loss refines this separation at the fine-grained level. To evaluate open-set performance, we extend benchmark settings on KITTI-360 and introduce a new open-set evaluation for nuScenes. Extensive experiments demonstrate that ULOPS consistently outperforms existing open-set LiDAR panoptic segmentation methods.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law,"Scaling law builds the relationship between training computation and validation loss, enabling researchers to effectively predict the loss trending of models across different levels of computation. However, a gap still remains between validation loss and the model's downstream capabilities, making it untrivial to apply scaling law to direct performance prediction for downstream tasks. The loss typically represents a cumulative penalty for predicted tokens, which are implicitly considered to have equal importance. Nevertheless, our studies have shown evidence that when considering different training data distributions, we cannot directly model the relationship between downstream capability and computation or token loss. To bridge the gap between validation loss and downstream task capabilities, in this work, we introduce Capability Salience Vector, which decomposes the overall loss and assigns different importance weights to tokens to assess a specific meta-capability, aligning the validation loss with downstream task performance in terms of the model's capabilities. Experiments on various popular benchmarks demonstrate that our proposed Capability Salience Vector could significantly improve the predictability of language model performance on downstream tasks.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!,"We study online decision making problems under resource constraints, where both reward and cost functions are drawn from distributions that may change adversarially over time. We focus on two canonical settings: $(i)$ online resource allocation where rewards and costs are observed before action selection, and $(ii)$ online learning with resource constraints where they are observed after action selection, under full feedback or bandit feedback. It is well known that achieving sublinear regret in these settings is impossible when reward and cost distributions may change arbitrarily over time. To address this challenge, we analyze a framework in which the learner is guided by a spending plan--a sequence prescribing expected resource usage across rounds. We design general (primal-)dual methods that achieve sublinear regret with respect to baselines that follow the spending plan. Crucially, the performance of our algorithms improves when the spending plan ensures a well-balanced distribution of the budget across rounds. We additionally provide a robust variant of our methods to handle worst-case scenarios where the spending plan is highly imbalanced. To conclude, we study the regret of our algorithms when competing against benchmarks that deviate from the prescribed spending plan.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation,"Large Language Models (LLMs) have shown strong potential for recommendation by framing item prediction as a token-by-token language generation task. However, existing methods treat all item tokens equally, simply pursuing likelihood maximization during both optimization and decoding. This overlooks crucial token-level differences in decisiveness-many tokens contribute little to item discrimination yet can dominate optimization or decoding. To quantify token decisiveness, we propose a novel perspective that models item generation as a decision process, measuring token decisiveness by the Information Gain (IG) each token provides in reducing uncertainty about the generated item. Our empirical analysis reveals that most tokens have low IG but often correspond to high logits, disproportionately influencing training loss and decoding, which may impair model performance. Building on these insights, we introduce an Information Gain-based Decisiveness-aware Token handling (IGD) strategy that integrates token decisiveness into both tuning and decoding. Specifically, IGD downweights low-IG tokens during tuning and rebalances decoding to emphasize tokens with high IG. In this way, IGD moves beyond pure likelihood maximization, effectively prioritizing high-decisiveness tokens. Extensive experiments on four benchmark datasets with two LLM backbones demonstrate that IGD consistently improves recommendation accuracy, achieving significant gains on widely used ranking metrics compared to strong baselines.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
GeoRecon: Graph-Level Representation Learning for 3D Molecules via Reconstruction-Based Pretraining,"The pretraining-and-finetuning paradigm has driven significant advances across domains, such as natural language processing and computer vision, with representative pretraining paradigms such as masked language modeling and next-token prediction. However, in molecular representation learning, the task design remains largely limited to node-level denoising, which is effective at modeling local atomic environments, yet maybe insufficient for capturing the global molecular structure required by graph-level property prediction tasks, such as energy estimation and molecular regression. In this work, we present GeoRecon, a novel graph-level pretraining framework that shifts the focus from individual atoms to the molecule as an integrated whole. GeoRecon introduces a graph-level reconstruction task: during pretraining, the model is trained to generate an informative graph representation capable of accurately guiding reconstruction of the molecular geometry. This encourages the model to learn coherent, global structural features rather than isolated atomic details. Without relying on additional supervision or external data, GeoRecon outperforms node-centric baselines on multiple molecular benchmarks (e.g., QM9, MD17), demonstrating the benefit of incorporating graph-level reconstruction for learning more holistic and geometry-aware molecular embeddings.",CS,BIOINFO,85,Clear CS paper with focus on Bioinformatics & Computational Biology
MT-PCR: A Hybrid Mamba-Transformer with Spatial Serialization for Hierarchical Point Cloud Registration,"Point cloud registration (PCR) is a fundamental task in 3D computer vision and robotics. Most existing learning-based PCR methods rely on Transformers, which suffer from quadratic computational complexity. This limitation restricts the resolution of point clouds that can be processed, inevitably leading to information loss. In contrast, Mamba-a recently proposed model based on state space models (SSMs)-achieves linear computational complexity while maintaining strong long-range contextual modeling capabilities. However, directly applying Mamba to PCR tasks yields suboptimal performance due to the unordered and irregular nature of point cloud data. To address this challenge, we propose MT-PCR, the first point cloud registration framework that integrates both Mamba and Transformer modules. Specifically, we serialize point cloud features using Z-order space-filling curves to enforce spatial locality, enabling Mamba to better model the geometric structure of the input. Additionally, we remove the order indicator module commonly used in Mamba-based sequence modeling, leads to improved performance in our setting. The serialized features are then processed by an optimized Mamba encoder, followed by a Transformer refinement stage. Extensive experiments on multiple benchmarks demonstrate that MT-PCR outperforms Transformer-based and concurrent state-of-the-art methods in both accuracy and efficiency, significantly reducing while GPU memory usage and FLOPs.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
KEPLA: A Knowledge-Enhanced Deep Learning Framework for Accurate Protein-Ligand Binding Affinity Prediction,"Accurate prediction of protein-ligand binding affinity is critical for drug discovery. While recent deep learning approaches have demonstrated promising results, they often rely solely on structural features, overlooking valuable biochemical knowledge associated with binding affinity. To address this limitation, we propose KEPLA, a novel deep learning framework that explicitly integrates prior knowledge from Gene Ontology and ligand properties of proteins and ligands to enhance prediction performance. KEPLA takes protein sequences and ligand molecular graphs as input and optimizes two complementary objectives: (1) aligning global representations with knowledge graph relations to capture domain-specific biochemical insights, and (2) leveraging cross attention between local representations to construct fine-grained joint embeddings for prediction. Experiments on two benchmark datasets across both in-domain and cross-domain scenarios demonstrate that KEPLA consistently outperforms state-of-the-art baselines. Furthermore, interpretability analyses based on knowledge graph relations and cross attention maps provide valuable insights into the underlying predictive mechanisms.",CS,BIOINFO,85,Clear CS paper with focus on Bioinformatics & Computational Biology
Adapting LLMs for Minimal-edit Grammatical Error Correction,"Decoder-only large language models have shown superior performance in the fluency-edit English Grammatical Error Correction, but their adaptation for minimal-edit English GEC is still underexplored. To improve their effectiveness in the minimal-edit approach, we explore the error rate adaptation topic and propose a novel training schedule method. Our experiments set a new state-of-the-art result for a single-model system on the BEA-test set. We also detokenize the most common English GEC datasets to match the natural way of writing text. During the process, we find that there are errors in them. Our experiments analyze whether training on detokenized datasets impacts the results and measure the impact of the usage of the datasets with corrected erroneous examples. To facilitate reproducibility, we have released the source code used to train our models.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs,"Test-time scaling has recently emerged as a promising approach for enhancing the reasoning capabilities of large language models or vision-language models during inference. Although a variety of test-time scaling strategies have been proposed, and interest in their application to the medical domain is growing, many critical aspects remain underexplored, including their effectiveness for vision-language models and the identification of optimal strategies for different settings. In this paper, we conduct a comprehensive investigation of test-time scaling in the medical domain. We evaluate its impact on both large language models and vision-language models, considering factors such as model size, inherent model characteristics, and task complexity. Finally, we assess the robustness of these strategies under user-driven factors, such as misleading information embedded in prompts. Our findings offer practical guidelines for the effective use of test-time scaling in medical applications and provide insights into how these strategies can be further refined to meet the reliability and interpretability demands of the medical domain.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Pro-AD: Learning Comprehensive Prototypes with Prototype-based Constraint for Multi-class Unsupervised Anomaly Detection,"Prototype-based reconstruction methods for unsupervised anomaly detection utilize a limited set of learnable prototypes which only aggregates insufficient normal information, resulting in undesirable reconstruction. However, increasing the number of prototypes may lead to anomalies being well reconstructed through the attention mechanism, which we refer to as the ""Soft Identity Mapping"" problem. In this paper, we propose Pro-AD to address these issues and fully utilize the prototypes to boost the performance of anomaly detection. Specifically, we first introduce an expanded set of learnable prototypes to provide sufficient capacity for semantic information. Then we employ a Dynamic Bidirectional Decoder which integrates the process of the normal information aggregation and the target feature reconstruction via prototypes, with the aim of allowing the prototypes to aggregate more comprehensive normal semantic information from different levels of the image features and the target feature reconstruction to not only utilize its contextual information but also dynamically leverage the learned comprehensive prototypes. Additionally, to prevent the anomalies from being well reconstructed using sufficient semantic information through the attention mechanism, Pro-AD introduces a Prototype-based Constraint that applied within the target feature reconstruction process of the decoder, which further improves the performance of our approach. Extensive experiments on multiple challenging benchmarks demonstrate that our Pro-AD achieve state-of-the-art performance, highlighting its superior robustness and practical effectiveness for Multi-class Unsupervised Anomaly Detection task.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
A deep learning model for chemical shieldings in molecular organic solids including anisotropy,"Nuclear Magnetic Resonance (NMR) chemical shifts are powerful probes of local atomic and electronic structure that can be used to resolve the structures of powdered or amorphous molecular solids. Chemical shift driven structure elucidation depends critically on accurate and fast predictions of chemical shieldings, and machine learning (ML) models for shielding predictions are increasingly used as scalable and efficient surrogates for demanding ab initio calculations. However, the prediction accuracies of current ML models still lag behind those of the DFT reference methods they approximate, especially for nuclei such as $^{13}$C and $^{15}$N. Here, we introduce ShiftML3.0, a deep-learning model that improves the accuracy of predictions of isotropic chemical shieldings in molecular solids, and does so while also predicting the full shielding tensor. On experimental benchmark sets, we find root-mean-squared errors with respect to experiment for ShiftML3.0 that approach those of DFT reference calculations, with RMSEs of 0.53 ppm for $^{1}$H, 2.4 ppm for $^{13}$C, and 7.2 ppm for $^{15}$N, compared to DFT values of 0.49 ppm, 2.3 ppm, and 5.8 ppm, respectively.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
EmbodiedPlace: Learning Mixture-of-Features with Embodied Constraints for Visual Place Recognition,"Visual Place Recognition (VPR) is a scene-oriented image retrieval problem in computer vision in which re-ranking based on local features is commonly employed to improve performance. In robotics, VPR is also referred to as Loop Closure Detection, which emphasizes spatial-temporal verification within a sequence. However, designing local features specifically for VPR is impractical, and relying on motion sequences imposes limitations. Inspired by these observations, we propose a novel, simple re-ranking method that refines global features through a Mixture-of-Features (MoF) approach under embodied constraints. First, we analyze the practical feasibility of embodied constraints in VPR and categorize them according to existing datasets, which include GPS tags, sequential timestamps, local feature matching, and self-similarity matrices. We then propose a learning-based MoF weight-computation approach, utilizing a multi-metric loss function. Experiments demonstrate that our method improves the state-of-the-art (SOTA) performance on public datasets with minimal additional computational overhead. For instance, with only 25 KB of additional parameters and a processing time of 10 microseconds per frame, our method achieves a 0.9\% improvement over a DINOv2-based baseline performance on the Pitts-30k test set.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
A Memetic Walrus Algorithm with Expert-guided Strategy for Adaptive Curriculum Sequencing,"Adaptive Curriculum Sequencing (ACS) is essential for personalized online learning, yet current approaches struggle to balance complex educational constraints and maintain optimization stability. This paper proposes a Memetic Walrus Optimizer (MWO) that enhances optimization performance through three key innovations: (1) an expert-guided strategy with aging mechanism that improves escape from local optima; (2) an adaptive control signal framework that dynamically balances exploration and exploitation; and (3) a three-tier priority mechanism for generating educationally meaningful sequences. We formulate ACS as a multi-objective optimization problem considering concept coverage, time constraints, and learning style compatibility. Experiments on the OULAD dataset demonstrate MWO's superior performance, achieving 95.3% difficulty progression rate (compared to 87.2% in baseline methods) and significantly better convergence stability (standard deviation of 18.02 versus 28.29-696.97 in competing algorithms). Additional validation on benchmark functions confirms MWO's robust optimization capability across diverse scenarios. The results demonstrate MWO's effectiveness in generating personalized learning sequences while maintaining computational efficiency and solution quality.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs,"Moral competence is the ability to act in accordance with moral principles. As large language models (LLMs) are increasingly deployed in situations demanding moral competence, there is increasing interest in evaluating this ability empirically. We review existing literature and identify three significant shortcoming: (i) Over-reliance on prepackaged moral scenarios with explicitly highlighted moral features; (ii) Focus on verdict prediction rather than moral reasoning; and (iii) Inadequate testing of models' (in)ability to recognize when additional information is needed. Grounded in philosophical research on moral skill, we then introduce a novel method for assessing moral competence in LLMs. Our approach moves beyond simple verdict comparisons to evaluate five dimensions of moral competence: identifying morally relevant features, weighting their importance, assigning moral reasons to these features, synthesizing coherent moral judgments, and recognizing information gaps. We conduct two experiments comparing six leading LLMs against non-expert humans and professional philosophers. In our first experiment using ethical vignettes standard to existing work, LLMs generally outperformed non-expert humans across multiple dimensions of moral reasoning. However, our second experiment, featuring novel scenarios designed to test moral sensitivity by embedding relevant features among irrelevant details, revealed a striking reversal: several LLMs performed significantly worse than humans. Our findings suggest that current evaluations may substantially overestimate LLMs' moral reasoning capabilities by eliminating the task of discerning moral relevance from noisy information, which we take to be a prerequisite for genuine moral skill. This work provides a more nuanced framework for assessing AI moral competence and highlights important directions for improving moral competence in advanced AI systems.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
"SuperPoint-SLAM3: Augmenting ORB-SLAM3 with Deep Features, Adaptive NMS, and Learning-Based Loop Closure","Visual simultaneous localization and mapping (SLAM) must remain accurate under extreme viewpoint, scale and illumination variations. The widely adopted ORB-SLAM3 falters in these regimes because it relies on hand-crafted ORB keypoints. We introduce SuperPoint-SLAM3, a drop-in upgrade that (i) replaces ORB with the self-supervised SuperPoint detector--descriptor, (ii) enforces spatially uniform keypoints via adaptive non-maximal suppression (ANMS), and (iii) integrates a lightweight NetVLAD place-recognition head for learning-based loop closure. On the KITTI Odometry benchmark SuperPoint-SLAM3 reduces mean translational error from 4.15% to 0.34% and mean rotational error from 0.0027 deg/m to 0.0010 deg/m. On the EuRoC MAV dataset it roughly halves both errors across every sequence (e.g., V2\_03: 1.58% -> 0.79%). These gains confirm that fusing modern deep features with a learned loop-closure module markedly improves ORB-SLAM3 accuracy while preserving its real-time operation. Implementation, pretrained weights and reproducibility scripts are available at https://github.com/shahram95/SuperPointSLAM3.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
A fluctuating lattice Boltzmann method for viscoelastic fluid flows,"This study introduces a novel fluctuating lattice Boltzmann (LB) method for simulating viscoelastic fluid flows governed by the Oldroyd-B model. In contrast to conventional LB approaches that explicitly compute the divergence of the polymer stress tensor using finite-difference schemes, the proposed method incorporates the polymer stress implicitly by introducing a polymer stress fluctuation term directly into the evolution equation. This treatment avoids the need for stress-gradient computations, and preserves the physical characteristics of viscoelastic fluid flows. The proposed method is validated against four classical benchmark problems: the simplified four-roll mill, planar Poiseuille flow, unsteady Womersley flow, and the three-dimensional Taylor-Green vortex. The numerical results show excellent agreement with analytical solutions and previous numerical results, confirming the method's reliability in viscoelastic fluid dynamics. Moreover, performance evaluations demonstrate that the present model reduces the memory occupancy and enhances computational efficiency, highlighting its potential for large-scale simulations of complex viscoelastic flows systems.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
A High-Order Quadrature Method for Implicitly Defined Hypersurfaces and Regions,"This paper presents a high-order accurate numerical quadrature algorithm for evaluating integrals over curved surfaces and regions defined implicitly via a level set of a given function restricted to a hyperrectangle. The domain is divided into small tetrahedrons, and by employing the change of variables formula, the approach yields an algorithm requiring only one-dimensional root finding and standard Gaussian quadrature. The resulting quadrature scheme guarantees strictly positive weights and inherits the high-order accuracy of Gaussian quadrature. Numerical convergence tests confirm the method's high-order accuracy.",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
Video Individual Counting With Implicit One-to-Many Matching,"Video Individual Counting (VIC) is a recently introduced task that aims to estimate pedestrian flux from a video. It extends conventional Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that only learns to count repeated pedestrian patterns across frames, the key problem of VIC is how to identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, mainly follow a one-to-one (O2O) matching strategy where the same pedestrian must be exactly matched between frames, leading to sensitivity to appearance variations or missing detections. In this work, we show that the O2O matching could be relaxed to a one-to-many (O2M) matching problem, which better fits the problem nature of VIC and can leverage the social grouping behavior of walking pedestrians. We therefore introduce OMAN, a simple but effective VIC model with implicit One-to-Many mAtchiNg, featuring an implicit context generator and a one-to-many pairwise matcher. Experiments on the SenseCrowd and CroHD benchmarks show that OMAN achieves the state-of-the-art performance. Code is available at \href{https://github.com/tiny-smart/OMAN}{OMAN}.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Multipole Attention for Efficient Long Context Reasoning,"Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design,"Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning capabilities. However, financial applications face challenges due to the lack of high-quality multimodal reasoning datasets and the inefficiency of existing training paradigms for reasoning enhancement. To address these issues, we propose an integrated framework, FinLMM-R1, combining an automated and scalable pipeline for data construction with enhanced training strategies to improve the multimodal reasoning of LMM. The Automated and Scalable Pipeline (ASP) resolves textual-visual misalignment in financial reports through a separate paradigm of question-answer generation and image-question alignment, ensuring data integrity and extraction efficiency. Through ASP, we collect 89,378 aligned image-question pairs from 23,397 financial reports, covering tasks such as arithmetic reasoning, statistics reasoning, financial explanation, and financial knowledge. Moreover, we introduce the Thinking with Adversarial Reward in LMM (TAR-LMM), extending the prior two-stage training framework [1] with additional reward mechanisms. In the first stage, we focus on text-only tasks with format and accuracy rewards to guide the model in generating well-structured thinking contents. In the second stage, we construct multi-image contrastive samples with additional reward components including image selection, thinking content length, and adversarial reward to jointly optimize the LMM across visual perception, reasoning efficiency, and logical coherence. Extensive experiments on 7 benchmarks show ASP-derived dataset and training framework significantly improve answer accuracy and reasoning depth over existing reasoning LMMs in both general and financial multimodal contexts.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
MAGIC: Multi-Agent Argumentation and Grammar Integrated Critiquer,"Automated Essay Scoring (AES) and Automatic Essay Feedback (AEF) systems aim to reduce the workload of human raters in educational assessment. However, most existing systems prioritize numeric scoring accuracy over the quality of feedback. This paper presents Multi-Agent Argumentation and Grammar Integrated Critiquer (MAGIC), a framework that uses multiple specialized agents to evaluate distinct writing aspects to both predict holistic scores and produce detailed, rubric-aligned feedback. To support evaluation, we curated a novel dataset of past GRE practice test essays with expert-evaluated scores and feedback. MAGIC outperforms baseline models in both essay scoring , as measured by Quadratic Weighted Kappa (QWK). We find that despite the improvement in QWK, there are opportunities for future work in aligning LLM-generated feedback to human preferences.",CS,UNKNOWN,0,
A Comprehensive Survey on Continual Learning in Generative Models,"The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models remain fundamentally constrained by catastrophic forgetting - a persistent challenge where adapting to new tasks typically leads to significant degradation in performance on previously learned tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative models in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative models, including large language models, multimodal large language models, vision language action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, offering deeper insights into the field. The project page of this paper is available at https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Compensation of Measurement Overhead in Parallel Performance Profiling,"Performance profiling generates measurement overhead during parallel program execution. Measurement overhead, in turn, introduces intrusion in a program's runtime performance behavior. Intrusion can be mitigated by controlling instrumentation degree, allowing a tradeoff of accuracy for detail. Alternatively, the accuracy in profile results can be improved by reducing the intrusion error due to measurement overhead. Models for compensation of measurement overhead in parallel performance profiling are described. An approach based on rational reconstruction is used to understand properties of compensation solutions for different parallel scenarios. From this analysis, a general algorithm for on-the-fly overhead assessment and compensation is derived.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Performance measurement in a university computing facility,"Methods of software performance monitoring and data reduction, recently implemented at the University of Wisconsin, have been successfully used to provide a meaningful set of information for management decision making. Emphasis in performance measurement has been placed on I/O request data, resource utilization, user and system file usage summaries, and individual job accounting statistics.The major goals at Wiscon in have been to maximize system through put and to reduce turnaround time for small jobs, while keeping turnaround for larger jobs within a reasonable limit. Over a two year period throughput was increased by a factor of two and mean turnaround time was decreased by a factor of ten.Although implemented for a specific system, the UNIVAC 1108, the overall approach is applicable to a wide range of general purpose multiprogramming time sharing systems. The description of the data collection and analysis facility emphasizes this generality.",IT,IT_PERF_MON,85,Clear IT paper with focus on IT Performance Monitoring
ERP investment analysis using the strategic alignment model,PurposeTo develop a model to assist in the analysis of enterprise resource planning (ERP) systems based upon a validated approach to aligning information systems with enterprise strategies.Design/methodology/approachThis paper presents a proposed analytic network process (ANP) model built on the strategic alignment model (SAM) to support the alignment of ERP investment decisions to strategies. The model developed is generic enough to be applied to a wide range of enterprises and applications while being based on the well‐documented SAM. A small illustrative example is provided to show the use and applicability of the model.FindingsThe SAM is found to provide a useful framework for investment analysis. ANP provides a conceptually easy to use but decisionally robust model which can be used to operationalize the SAM for investment analysis.Practical implicationsThe model provides practitioners a flexible tool for making investment decisions not only for ERP systems but also for other IS investments. The paper provides academics and researchers with a methodology for the important area of IS alignment.Originality/valueThe paper represents a novel use of the multi‐attribute decision‐making method of ANP. It is also a new applied tool within the area of strategic IS evaluation.,IS,ES_ERP,85,Clear IS paper with focus on Enterprise Systems & ERP
Productivity in High Performance Computing,"HPC (high performance computing) has been a popular acronym for decades, and has been applied to many types of architectures, software and applications. The “P” has recently been overloaded to mean both performance and productivity. We present a survey of today’s performance and productivity situation relative to the constituent HPC hardware and software components, and provide some analysis of current controversies and open issues. Since “HPC” will continue to be applied to whatever is happening at the leading edge of computer architecture, system and development software, and algorithms and applications, there is no hope or need to define or clean up terminology (this paper uses HPC to denote hiperc and hiproc, with context determining meaning). Instead, it is important to clarify the whys and wherefores of the state of the art, in order to focus on new work that will maximize future benefits. This paper gives a broad discussion of HPC productivity in terms of effective architectures, run-time system software, and applications development tools. There are costs and trade-offs associated with each of these, and in fact multiple marketplaces consume these products. The range of demands placed on HPC, by owners and users of systems ranging from public research laboratories to private scientific and engineering companies, enrich the topic with many competing technologies and approaches. Rather than expecting to eliminate each other in the short run, these HPC competitors should be learning from one another in order to stay in the race. It seems clear that the dynamics between “commodity” and “custom” building blocks will remain at the center of HPC debates for some time, and indeed these competing forces form the engine of improvement for overall HPC cost/effectiveness.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
A Sentiment Analysis on Pesantren Entrepreneurship,"This study aims to look at the development of public sentiment towards pesantren entrepreneurship. This study uses the sentiment analysis method with the help of SentiStrength software. The type of data used is secondary data derived from indexed scientific publications database Dimensions. Found as many as 100 scientific publications related to research topics. Then the results show that positive sentiment prevailed with a percentage of 52%, followed by neutral sentiment of 31% and negative sentiment of 17%. 2021 is the year with the highest number of sentiments which are dominated by positive sentiment as many as 13 scientific publications. According to the findings, the majority of people agree with the existence of pesantren entrepreneurship. As a result, an increase in the number of scientific publications on pesantren entrepreneurship in indexed journals is necessary to maintain this favorable view.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
NCTU‐VT: a freeware for wireless VoIP performance measurement,"AbstractVoice over IP (VoIP) is a promising low‐cost voice communication over the wireless IP network. To provide satisfactory VoIP services, the Quality of Service (QoS) of the wireless network should be guaranteed. This paper proposes a VoIP performance measurement freeware called NCTU VoIP Testing Tool (NCTU‐VT). We compare NCTU‐VT with two commercial tools SmartVoIPQoS and IxChariot in terms of packet loss, latency, and Mean Opinion Score (MOS) of the VoIP sessions in Wi‐Fi network. Our study indicates that these three tools can accurately measure VoIP performance in Wi‐Fi environment. Copyright © 2010 John Wiley &amp; Sons, Ltd.",CS,CN_DS,85,Clear CS paper with focus on Computer Networks & Distributed Systems
Measuring TeraGrid: workload characterization for a high-performance computing federation,"TeraGrid has deployed a significant monitoring and accounting infrastructure in order to understand its operational success. In this paper, we present an analysis of the jobs reported by TeraGrid for 2008. We consider the workload from several perspectives: traditional high-performance computing (HPC) workload characteristics; grid-oriented workload characteristics; and finally user- and group-oriented characteristics. We use metrics reported in prior studies of HPC and grid systems in order to understand whether such metrics provide useful information for managing and studying resource federations. This study highlights the importance of distinguishing between analyses of job patterns and work patterns; that small sets of users dominate the workload both in terms of job and work patterns; and that aggregate analyses across even loosely coupled federations, with incomplete information for individual systems, reflect patterns seen in more tightly coupled grids and in single HPC systems.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
High Performance Computing Design by Code Migration for Distributed Desktop Computing Grids,"Large scale loosely coupled PCs can organize clusters and form desktop computing grids on sharing each processing power; power of PCs, transaction distributions, network scales, network delays, and code migration algorithms characterize the performance of the computing grids. This article describes the design methodologies of workload management in distributed desktop computing grids. Based on the code migration experiments, transfer policy for computation was determined and several simulations for location policies were examined, and the design methodologies for distributed desktop computing grids are derived from the simulation results. The language for distributed desktop computing is designed to accomplish the design methodologies.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
A performance model for the communication in fast multipole methods on high-performance computing platforms,"Exascale systems are predicted to have approximately 1 billion cores, assuming gigahertz cores. Limitations on affordable network topologies for distributed memory systems of such massive scale bring new challenges to the currently dominant parallel programing model. Currently, there are many efforts to evaluate the hardware and software bottlenecks of exascale designs. It is therefore of interest to model application performance and to understand what changes need to be made to ensure extrapolated scalability. The fast multipole method (FMM) was originally developed for accelerating N-body problems in astrophysics and molecular dynamics but has recently been extended to a wider range of problems. Its high arithmetic intensity combined with its linear complexity and asynchronous communication patterns make it a promising algorithm for exascale systems. In this paper, we discuss the challenges for FMM on current parallel computers and future exascale architectures, with a focus on internode communication. We focus on the communication part only; the efficiency of the computational kernels are beyond the scope of the present study. We develop a performance model that considers the communication patterns of the FMM and observe a good match between our model and the actual communication time on four high-performance computing (HPC) systems, when latency, bandwidth, network topology, and multicore penalties are all taken into account. To our knowledge, this is the first formal characterization of internode communication in FMM that validates the model against actual measurements of communication time. The ultimate communication model is predictive in an absolute sense; however, on complex systems, this objective is often out of reach or of a difficulty out of proportion to its benefit when there exists a simpler model that is inexpensive and sufficient to guide coding decisions leading to improved scaling. The current model provides such guidance.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
A checkpoint compression study for high-performance computing systems,"As high-performance computing systems continue to increase in size and complexity, higher failure rates and increased overheads for checkpoint/restart (CR) protocols have raised concerns about the practical viability of CR protocols for future systems. Previously, compression has proven to be a viable approach for reducing checkpoint data volumes and, thereby, reducing CR protocol overhead leading to improved application performance. In this article, we further explore compression-based CR optimization by exploring its baseline performance and scaling properties, evaluating whether improved compression algorithms might lead to even better application performance and comparing checkpoint compression against and alongside other software- and hardware-based optimizations. Our results highlights are that: (1) compression is a very viable CR optimization; (2) generic, text-based compression algorithms appear to perform near optimally for checkpoint data compression and faster compression algorithms will not lead to better application performance; (3) compression-based optimizations fare well against and alongside other software-based optimizations; and (4) while hardware-based optimizations outperform software-based ones, they are not as cost effective.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Developing a High-Performance Computing/Numerical Analysis Roadmap,"A roadmap activity in the UK has leveraged US and European efforts for identifying the challenges and barriers in the development of high-performance computing (HPC) algorithms and software. The activity has identified the Grand Challenge to provide:  1. Algorithms and software that application developers can reuse in the form of high-quality, high performance, sustained software components, libraries and modules  2. A community environment that allows the sharing of software, communication of interdisciplinary knowledge and the development of appropriate skills.  Through a series of workshops and discussions with UK HPC application groups and numerical analysts, five areas of challenge have emerged.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Reproducibility and variable precision computing,"Recently some scientific computing users have discovered that they can replace 64-bit with 32-bit operations for carefully selected portions of the computation, and still retain acceptable accuracy in the final results. In addition, developers of some emerging applications such as machine learning have discovered that they can achieve acceptable results with only 16-bit precision in certain portions of the code. At the other end of the precision spectrum, some users have explored using 128-bit arithmetic in some particularly demanding applications, while others have done computations using much higher precision—hundreds or even thousands of digits. Such work has underscored the need to develop new mathematical and software frameworks to support a dynamically variable level of precision, and, more generally, to rethink what “reproducibility” means in a variable precision environment. This article summarizes some of the work being done in this arena, and lists research problems that need to be solved.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
High-performance computing selection of models of DNA substitution for multicore clusters,"This paper presents the high-performance computing (HPC) support of jModelTest2, the most popular bioinformatic tool for the statistical selection of models of DNA substitution. As this can demand vast computational resources, especially in terms of processing power, jModelTest2 implements three parallel algorithms for model selection: (1) a multithreaded implementation for shared memory architectures; (2) a message-passing implementation for distributed memory architectures, such as clusters; and (3) a hybrid shared/distributed memory implementation for clusters of multicore nodes, combining the workload distribution across cluster nodes with a multithreaded model optimization within each node. The main limitation of the shared and distributed versions is the workload imbalance that generally appears when using more than 32 cores, a direct consequence of the heterogeneity in the computational cost of the evaluated models. The hybrid shared/distributed memory version overcomes this issue reducing the workload imbalance through a thread-based decomposition of the most costly model optimization tasks. The performance evaluation of this HPC application on a 40-core shared memory system and on a 528-core cluster has shown high scalability, with speedups of the multithreaded version of up to 32, and up to 257 for the hybrid shared/distributed memory implementation. This can represent a reduction in the execution time of some analyses from 4 days down to barely 20 minutes. The implementation of the three parallel execution strategies of jModelTest2 presented in this paper are available under a GPL license at http://code.google.com/jmodeltest2.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Compatibility enhancement and performance measurement for socket interface with PCIe interconnections,"Abstract Today the key technology of high-performance computing systems is the emergence of interconnect technology that makes multiple computers into one computer cluster. This technique is a general method in which each constituent node processes its own operation and communicates with different nodes. Therefore, a high-performance network has been required. InfiniBand and Gigabit Ethernet technologies are typical examples of high-performance network. As an alternative to those technologies the development of interconnection technology using PCI Express (Peripheral Component Interconnect Express), officially abbreviated as PCIe or PCI-e, is a high-speed serial computer expansion bus standard, which has characteristics of high speed, low power, and high protocol efficiency is actively under development. In a high-performance network, the TCP/IP protocol consumes CPU resources and memory bandwidth by nature, which is a bottleneck. In this paper, we implement the PCIe based interconnection network system with low latency, low power, RDMA, and other characteristics. We utilize Socket API which is mainly used in the user-level application program rather than the existing MPI and PGAS model interfaces. The implemented PCIe interconnection network system was measured with the metrics as the bandwidth using the Iperf Benchmark which uses the Socket API. The bandwidth at 4 Mbyte of transmission data size was measured to be 1084 Mbyte/s bandwidth based on PCIe, which is about 96 times higher than that of 11.2 Mbyte/s bandwidth based on Ethernet.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
The Application Perspective: Seeking Productivity and Performance,"In this note we propose two projects: (1) creating a hierarchical programming model from current models; and (2) extracting application primitives from the “13 dwarfs”. The first topic addresses the need for a unified and manageable framework for very large-scale concurrent execution. This is the productivity part: less complexity will drive better mapping of algorithms to architecture, which will also contribute to better performance. The second topic focuses mostly on the processor and the node with the aim of laying the groundwork for software and silicon optimized kernels. While it is understood that applications primitives are outside the scope of IESP, the motivation for introducing it here is that it is a companion issue and that increasing the efficiency of each processor provides high return for science, at all levels of system size.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Adaptive Parameter Identification of Battery Pack inElectric Vehicles with Real-Driving Signals,"This paper presents an adaptive identification method for battery parameters in automotive applications. A simple yet accurate electrical equivalent model (ECM) with varying parameters is used to represent the whole battery pack. The modeling process requires the current, voltage, and SOC signals of the battery. Detailed physical knowledge of the battery pack and inside cells are not necessary. The ECM parameter identification approach is developed by employing the NLMS (normalized least mean square) algorithm, which is an advanced adaptive algorithm having fast convergence rate and easier to be implemented. This approach is verified on a 51.2 V, 95AH LiFePO4 battery pack operated in three-wheeler electric bikes. Battery signals during vehicle daily real-world driving were collected over a period of time and used for the ECM parameter identification. The identified internal resistance R0, R1 and capacitance C1 changes obviously over the period of time and the battery degradation is well reflected through the identified parameters of the ECM.",CS,EMBEDDED_RT,85,Clear CS paper with focus on Embedded Systems & Real-time Computing
High-performance Physics Simulations Using Multi-core CPUs and GPGPUs in a Volunteer Computing Context,"This paper presents two conceptually simple methods for parallelizing a Parallel Tempering Monte Carlo simulation in a distributed volunteer computing context, where computers belonging to the general public are used. The first method uses conventional multi-threading. The second method uses CUDA, a graphics card computing system. Parallel Tempering is described, and challenges such as parallel random number generation and mapping of Monte Carlo chains to different threads are explained. While conventional multi-threading on central processing units is well-established, GPGPU programming techniques and technologies are still developing and present several challenges, such as the effective use of a relatively large number of threads. Having multiple chains in Parallel Tempering allows parallelization in a manner that is similar to the serial algorithm. Volunteer computing introduces important constraints to high performance computing, and we show that both versions of the application are able to adapt themselves to the varying and unpredictable computing resources of volunteers’ computers, while leaving the machines responsive enough to use. We present experiments to show the scalable performance of these two approaches, and indicate that the efficiency of the methods increases with bigger problem sizes.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Analysis of a Randomized Controlled Trial of Student Performance in Parallel Programming using a New Measurement Technique,"There are many paradigms available to address the unique and complex problems introduced with parallel programming. These complexities have implications for computer science education as ubiquitous multi-core computers drive the need for programmers to understand parallelism. One major obstacle to student learning of parallel programming is that there is very little human factors evidence comparing the different techniques to one another, so there is no clear direction on which techniques should be taught and how. We performed a randomized controlled trial using 88 university-level computer science student participants performing three identical tasks to examine the question of whether or not there are measurable differences in programming performance between two paradigms for concurrent programming: threads compared to process-oriented programming based on Communicating Sequential Processes. We measured both time on task and programming accuracy using an automated token accuracy map (TAM) technique. Our results showed trade-offs between the paradigms using both metrics and the TAMs provided further insight about specific areas of difficulty in comprehension.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Toward a modular precision ecosystem for high-performance computing,"With the memory bandwidth of current computer architectures being significantly slower than the (floating point) arithmetic performance, many scientific computations only leverage a fraction of the computational power in today’s high-performance architectures. At the same time, memory operations are the primary energy consumer of modern architectures, heavily impacting the resource cost of large-scale applications and the battery life of mobile devices. This article tackles this mismatch between floating point arithmetic throughput and memory bandwidth by advocating a disruptive paradigm change with respect to how data are stored and processed in scientific applications. Concretely, the goal is to radically decouple the data storage format from the processing format and, ultimately, design a “modular precision ecosystem” that allows for more flexibility in terms of customized data access. For memory-bounded scientific applications, dynamically adapting the memory precision to the numerical requirements allows for attractive resource savings. In this article, we demonstrate the potential of employing a modular precision ecosystem for the block-Jacobi preconditioner and the PageRank algorithm—two applications that are popular in the communities and at the same characteristic representatives for the field of numerical linear algebra and data analytics, respectively.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
High Performance FFT Algorithms for Cache-Coherent Multiprocessors,"Computing one-dimensional fast Fourier transforms (FFTs) on microprocessors requires different algorithms, depending on whether the problem fits in the data cache. This paper describes efficient algorithms for both cases. Some implementations of out-of-cache one-dimensional FFTs use a six-step approach to reduce the number of cache misses. The six-step approach may be altered into a seven-step approach that allows increased data cache reuse. A natural parallelism is also developed. Performance results using these techniques are given for the Hewlett-Packard HP 9000 V-Class V2250 server.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
A Pragmatic Analysis Of Scheduling Environments On New Computing Platforms,"Today, large scale parallel systems are available at relatively low cost. Many powerful such systems have been installed all over the world and the number of users is always increasing. The use of such clusters requires special administration tools, which have been developed recently and most frequently rely on intuitive heuristics to solve the underlying difficult scheduling problems. On the other hand, recent theoretical work has designed a number of models, such as the Parallel Task model, specifically aimed at cluster environments.  The objective of this work is to study and analyze the path from theoretical models and algorithms to their implementation on an actual environment on a real platform. We outline in detail the divergences between classical models and a real batch scheduling system, and propose solutions to adapt a theoretically well-founded algorithm to such a system. Experimental results show that this implementation performs about as well as FCFS with backfilling, and much better in the difficult instances. We hope that further usage of this algorithm in a live system will show that interaction between theory and practice can be fruitful.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Automatic Performance Tuning for Fast Fourier Transforms,"In this paper we discuss architecture-specific performance tuning for fast Fourier transforms (FFTs) implemented in the UHFFT library. The UHFFT library is an adaptive and portable software library for FFTs developed by the authors. We present the optimization methods used at different levels, starting with the algorithm selection used for the library code generation and ending with the actual implementation and specification of the appropriate compiler optimization options. We report on the performance results for several modern microprocessor architectures.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
On the performance and energy efficiency of sparse linear algebra on GPUs,"In this paper we unveil some performance and energy efficiency frontiers for sparse computations on GPU-based supercomputers. We compare the resource efficiency of different sparse matrix–vector products (SpMV) taken from libraries such as cuSPARSE and MAGMA for GPU and Intel’s MKL for multicore CPUs, and develop a GPU sparse matrix–matrix product (SpMM) implementation that handles the simultaneous multiplication of a sparse matrix with a set of vectors in block-wise fashion. While a typical sparse computation such as the SpMV reaches only a fraction of the peak of current GPUs, we show that the SpMM succeeds in exceeding the memory-bound limitations of the SpMV. We integrate this kernel into a GPU-accelerated Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) eigensolver. LOBPCG is chosen as a benchmark algorithm for this study as it combines an interesting mix of sparse and dense linear algebra operations that is typical for complex simulation applications, and allows for hardware-aware optimizations. In a detailed analysis we compare the performance and energy efficiency against a multi-threaded CPU counterpart. The reported performance and energy efficiency results are indicative of sparse computations on supercomputers.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Loop Splitting for High Performance Computers,This paper discusses program transformations and algorithm modifications that reduce execution time of iterative methods for solv ing partial differential equations on high performance computers. Tech niques typically associated with par allel computers turn out to be essen tial to obtain optimal performance on current superscalar uniprocessors.,CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Improving the performance scalability of the community atmosphere model,"The Community Atmosphere Model (CAM), which serves as the atmosphere component of the Community Climate System Model (CCSM), is the most computationally expensive CCSM component in typical configurations. On current and next-generation leadership class computing systems, the performance of CAM is tied to its parallel scalability. Improving performance scalability in CAM has been a challenge, due largely to algorithmic restrictions necessitated by the polar singularities in its latitude–longitude computational grid. Nevertheless, through a combination of exploiting additional parallelism, implementing improved communication protocols, and eliminating scalability bottlenecks, we have been able to more than double the maximum throughput rate of CAM on production platforms. We describe these improvements and present results on the Cray XT5 and IBM BG/P. The approaches taken are not specific to CAM and may inform similar scalability enhancement activities for other codes.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Data Partitioning with a Functional Performance Model of Heterogeneous Processors,"In this paper, we address the problem of optimal distribution of computational tasks on a network of heterogeneous computers when one or more tasks do not fit into the main memory of the processors and when relative speeds vary with the problem size. We propose a functional performance model of heterogeneous processors that integrates many essential features of a network of heterogeneous computers having a major impact on its performance such as the processor heterogeneity, the heterogeneity of memory structure, and the effects of paging. Under this model, the speed of each processor is represented by a continuous function of the size of the problem whereas traditional models use single numbers to represent the speeds of the processors. We formulate a problem of partitioning of an n-element set over p heterogeneous processors using this model and design an algorithm of the complexity O(p × log2n) solving the problem.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
A Survey of Graph Comparison Methods with Applications to Nondeterminism in High-Performance Computing,"The convergence of extremely high levels of hardware concurrency and the effective overlap of computation and communication in asynchronous executions has resulted in increasing nondeterminism in High-Performance Computing (HPC) applications. Nondeterminism can manifest at multiple levels: from low-level communication primitives to libraries to application-level functions. No matter its source, nondeterminism can drastically increase the cost of result reproducibility, debugging workflows, testing parallel programs, or ensuring fault-tolerance. Nondeterministic executions of HPC applications can be modeled as event graphs, and the applications’ nondeterministic behavior can be understood and, in some cases, mitigated using graph comparison algorithms. However, a connection between graph comparison algorithms and approaches to understanding nondeterminism in HPC still needs to be established. This survey article moves the first steps toward establishing a connection between graph comparison algorithms and nondeterminism in HPC with its three contributions: it provides a survey of different graph comparison algorithms and a timeline for each category’s significant works; it discusses how existing graph comparison methods do not fully support properties needed to understand nondeterministic patterns in HPC applications; and it presents the open challenges that should be addressed to leverage the power of graph comparisons for the study of nondeterminism in HPC applications.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Compositional Development of Performance Models in Poems,"Performance models are software systems in which the components implement abstractions of the behavior of a total system. This paper describes a capability for semiautomatic development of performance models of computer systems spanning applications, operating systems, and hardware by composition from a library of components. Compositional development of performance models is a domain-specific instance of the general problem of software component reuse or design reuse. The concepts enabling compositional development of performance models in POEMS are encapsulation of analysis-level objects with associative interfaces and hierarchical dynamic data flow graphs as a structuring model. Objects with associative interfaces will be called compositional objects. Compositional objects and hierarchical dynamic data flow graphs provide a framework for the development of performance models that incorporate multiple modes of evaluation, span multiple semantic domains, span multiple levels of abstraction, and parallel implementation. Algorithms for composition through associative interfaces with automatic generation of parallel executables for the performance models will be defined.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
The Dynamics of Performance Collapse in Large-Scale Networks and Computers,"Failures in communication networks have become all too familiar. The AT&amp;T phone system was brought to its knees in 1990 and their frame-relay network shut down west coast bank ATMs in 1998. These were hard network failures. Less obvious is the congestive failure of packet networks even without hard errors. This spontaneous collapse in performance is observed as orders-of-magnitude drop in packets/second delivered or increased packet delay. Such effects were seen on the Internet in 1986 and led to the implementation of the TCP/IP “slow-start” congestion avoidance algorithm. That same algorithm is now responsible for latency overhead in HTTP traffic on the World Wide Web. This paper outlines an approach developed by the author based on the surprising observation that the degree of instability in computer networks is logically equivalent to estimating the rate of decay in an unstable (radioactive) atom and suggests the application of the Feynman path integral from quantum mechanics. The advantage of this approach is threefold: (a) it makes the dynamics of large transients intuitively clear, (b) it furnishes an estimator for the mean time to collapse, and (c) it provides corrections to other estimators (e.g., Catastrophe Theory and the Theory of Large Deviations).",CS,CN_DS,85,Clear CS paper with focus on Computer Networks & Distributed Systems
Improved Performance for Nodal Spectral Element Operators,"The computational performance of the nodal spectral element method (SEM) for tetrahedral grids is evaluated in the context of a high-performance computing platform. The elemental SEM operator is accelerated by the symmetry-based factorization technique of Hesthaven and Teng (SIAM J. Sci. Comp. 21, p. 2352, 2000), which results in a reduced number of floating point operations and memory accesses. However, performance evaluation reveals that a naive implementation of the algorithm causes a severe degradation of computational efficiency. Two algorithmic modifications are proposed which regain (and partly exceed) the efficiency of the original, non-factorized operator and thus recover the asymptotic 9/5 speedup due to factorization.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
A Performance Model of the Parallel Ocean Program,"In this paper we describe a performance model of the Parallel Ocean Program (POP). In particular, the latest version of POP (v2.0) is considered, which has similarities and differences to the earlier version (v1.4.3) as commonly used in climate simulations. The performance model encapsulates an understanding of POP’s data decomposition, processing flow, and scaling characteristics. The model is parametrized in many of the main input parameters to POP as well as characteristics of a processing system such as network latency and bandwidth. The performance model has been validated to date on a medium-sized (128 processor) AlphaServer ES40 system with the QsNet-1 interconnection network, and also on a larger scale (2048 processor) Blue Gene/Light system. The accuracy of the performance model is high when using two standard benchmark configurations, one of which represents a realistic configuration similar to that used in Community Climate System Model coupled climate simulations. The performance model is also used to explore the performance of POP after possible optimizations to the code, and different task to processor assignment strategies, whose performance cannot be currently measured.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Accelerating many-nucleon basis generation for high performance computing enabled ab initio nuclear structure studies,We present the problem of generating a many-nucleon basis in [Formula: see text]-scheme for ab initio nuclear structure calculations in a symmetry-adapted no-core shell model framework. We first discuss and analyze the basis construction algorithm whose baseline implementation quickly becomes a significant bottleneck for large model spaces and heavier nuclei. The outcomes of this analysis are utilized to propose a new scalable version of the algorithm. Its performance is consequently studied empirically using the Blue Waters supercomputer. The measurements show significant acceleration achieved with over two orders of magnitude speedups realized for larger model spaces.,CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
"Resilient gossip-inspired all-reduce algorithms for high-performance computing: Potential, limitations, and open questions","We investigate the usefulness of gossip-based reduction algorithms in a high-performance computing (HPC) context. We compare them to state-of-the-art deterministic parallel reduction algorithms in terms of fault tolerance and resilience against silent data corruption (SDC) as well as in terms of performance and scalability. New gossip-based reduction algorithms are proposed, which significantly improve the state-of-the-art in terms of resilience against SDC. Moreover, a new gossip-inspired reduction algorithm is proposed, which promises a much more competitive runtime performance in an HPC context than classical gossip-based algorithms, in particular for low accuracy requirements.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Myths and legends in high-performance computing,"In this thought-provoking article, we discuss certain myths and legends that are folklore among members of the high-performance computing community. We gathered these myths from conversations at conferences and meetings, product advertisements, papers, and other communications such as tweets, blogs, and news articles within and beyond our community. We believe they represent the zeitgeist of the current era of massive change, driven by the end of many scaling laws such as Dennard scaling and Moore’s law. While some laws end, new directions are emerging, such as algorithmic scaling or novel architecture research. Nevertheless, these myths are rarely based on scientific facts, but rather on some evidence or argumentation. In fact, we believe that this is the very reason for the existence of many myths and why they cannot be answered clearly. While it feels like there should be clear answers for each, some may remain endless philosophical debates, such as whether Beethoven was better than Mozart. We would like to see our collection of myths as a discussion of possible new directions for research and industry investment.",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Performance of a Fully Parallel Sparse Solver,"The performance of a fully parallel direct solver for large sparse-symmetric positive definite systems of linear equations is demonstrated. The solver is designed for distributed-memory, message-passing parallel computer systems. All phases of the computation, including sym bolic processing as well as numeric factorization and triangular solution, are performed in parallel. A parallel Cartesian-nested dissection algorithm is used to compute a fill- reducing ordering for the matrix and an appropriate partitioning of the problem across the processors. The separator tree resulting from nested dissection is used to identify and exploit large-grain parallelism in the remaining steps of the computation. The parallel performance of the solver is reported for a series of test problems on the Thinking Machines CM-5 and the Intel Touchstone Delta. The parallel efficiency, scalability, and absolute perfor mance of the solver, as well as the relative importance of the various phases of the computation, are investigated empirically",CS,PARALLEL_HPC,85,Clear CS paper with focus on Parallel Computing & High Performance Computing
Distinct Computations Emerge From Compositional Curricula in In-Context Learning,"In-context learning (ICL) research often considers learning a function in-context through a uniform sample of input-output pairs. Here, we investigate how presenting a compositional subtask curriculum in context may alter the computations a transformer learns. We design a compositional algorithmic task based on the modular exponential-a double exponential task composed of two single exponential subtasks and train transformer models to learn the task in-context. We compare (a) models trained using an in-context curriculum consisting of single exponential subtasks and, (b) models trained directly on the double exponential task without such a curriculum. We show that models trained with a subtask curriculum can perform zero-shot inference on unseen compositional tasks and are more robust given the same context length. We study how the task and subtasks are represented across the two training regimes. We find that the models employ diverse strategies modulated by the specific curriculum design.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Inventory Information System Design Analysis (Case Study at CV. Mysneaker Retail Indo),"This research is a qualitative research that examines the design of inventory information systems at CV. Mysneaker Retail Indo. This study analyzes the information system on inventory at CV. Mysneaker Retail Indo. This study uses observation with interviews. The results of this observation are written on the observation sheet carried out by the observer. Unstructured interviews were conducted in order to obtain more comprehensive interview results with informants. Research data was collected through observation, unstructured interviews, and documentation. CV. Mysneaker Retail Indo has collaborated with a third service party, namely mokapos, to help carry out the inventory system for this business by the end of 2022. This application is available on desktop and mobile. This application makes inventory management in managing inventory fast and accurate so as to minimize errors in data. This business uses Use Case Diagrams. Use Case Inventory Diagram proposed, there are several involved in the system namely Warehouse Admin, Marketing and Owner.",IS,SCMS,85,Clear IS paper with focus on Supply Chain Management Systems
Using the Balanced Scorecard Approach to Appraise the Performance of Cloud Computing,"Cloud computing and the Internet of Things (IoT) are currently the two dominant Internet technologies, but cloud computing embodies the maturity of the computing field with its clear path to software generation, transmission, distribution and control. The growth and popularity of cloud-virtualized resources (infrastructure, platform and software), have been accompanied with an increase in cloud performance evaluation that could be used for decision-making. As reported in the literature, most of the evaluations and simulations tend to be one-dimensional that focus on easily measurable criteria such as load balancing and response time. This preliminary paper presents a more holistic approach using the technique of balanced scorecard that analyses cloud computing with respect to finance, customer, internal processes, and learning and growth perspectives. A critical analysis shows that cost reduction is not enough to cause a customer to embrace cloud computing. Other factors such as value proposition, internal and growth perspectives also play a role.",IS,IT_GOV_STRAT,85,Clear IS paper with focus on IT Governance & Strategy
Computer‐aided design system upgrade process: a case study,"Reports on the computer‐aided design (CAD) upgrade implementation process at PBR Automotive Pty Ltd, Melbourne, Australia. Views the implementation as successful since many of the desired outcomes have been achieved or surpassed. The key success factors were detailed planning, user involvement and vendor support. These combined to create an atmosphere of excitement in the project and success. Reports on the future plans that include the development of an integrated information system at PBR which will involve customers and suppliers in addition to internal personnel. The CAD system upgrade serves as a launching board for the development of such a system.",IS,IT_PM,85,Clear IS paper with focus on IT Project Management
Voltage Stability of Inverter-Based Systems: Impact of Parameters and Irrelevance of Line Dynamics,"This paper investigates voltage stability in inverter-based power systems concerning fold and saddle-node bifurcations. An analytical expression is derived for the sensitivity of the stability margin using the normal vector to the bifurcation hypersurface. Such information enables efficient identification of effective control parameters in mitigating voltage instability. Comprehensive analysis reveals that reactive loading setpoint and current controller's feedforward gain are the most influential parameters for enhancing voltage stability in a grid-following (GFL) inverter system, while the voltage controller's feedforward gain plays a dominant role in a grid-forming (GFM) inverter. Notably, both theoretical and numerical results demonstrate that transmission line dynamics have no impact on fold/saddle-node bifurcations in these systems. Results in this paper provide insights for efficient analysis and control in future inverter-dominated power systems through reductions in parameter space and model complexity.",CS,EMBEDDED_RT,75,Matched CS paper to Embedded Systems & Real-time Computing
The effect of providing learning analytics on student behaviour and performance in programming: a randomised controlled experiment,"AbstractWe use a randomised experiment to study the effect of offering half of 556 freshman students a learning analytics dashboard and a weekly email with a link to their dashboard, on student behaviour in the online environment and final exam performance. The dashboard shows their online progress in the learning management systems, their predicted chance of passing, their predicted grade and their online intermediate performance compared with the total cohort. The email with dashboard access, as well as dashboard use, has positive effects on student behaviour in the online environment, but no effects are found on student performance in the final exam of the programming course. However, we do find differential effects by specialisation and student characteristics.",IS,ED_TECH_ELEARN,75,Matched IS paper to Educational Technology & E-learning
Exploring Discrete Factor Analysis with the discFA Package in R,"Literature suggested that using the traditional factor analysis for the count data may be inappropriate. With that in mind, discrete factor analysis builds on fitting systems of dependent discrete random variables to data. The data should be in the form of non-negative counts. Data may also be truncated at some positive integer value. The discFA package in R allows for two distributions: Poisson and Negative Binomial, in combination with possible zero inflation and possible truncation, hence, eight different alternatives. A forward search algorithm is employed to find the model optimal factor model with the lowest AIC. Several different illustrative examples from psychology, agriculture, car industry, and a simulated data will be analyzed at the end.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine Approach,"Numerical consistency across tables in disclosure documents is critical for ensuring accuracy, maintaining credibility, and avoiding reputational and economic risks. Automated tabular numerical cross-checking presents two significant challenges: (C1) managing the combinatorial explosion of candidate instances at the document level and (C2) comprehending multi-faceted numerical semantics. Previous research typically depends on heuristic-based filtering or simplified context extraction, often struggling to balance performance and efficiency. Recently, large language models (LLMs) have demonstrated remarkable contextual understanding capabilities that helps address C2 at the instance level, yet they remain hampered by computational inefficiency (C1) and limited domain expertise. This paper introduces CoFiTCheck, a novel LLM-based coarse-to-fine framework that addresses these challenges through two sequential stages: embedding-based filtering and discriminative classification. The embedding-based filtering stage introduces an instructional parallel encoding method to efficiently represent all numerical mentions in a table with LLMs, as well as a decoupled InfoNCE objective to mitigate the isolated mention problem. The discriminative classification stage employs a specialized LLM for fine-grained analysis of the remaining candidate pairs. This stage is further enhanced by our crosstable numerical alignment pretraining paradigm, which leverages weak supervision from cross-table numerical equality relationships to enrich task-specific priors without requiring manual annotation. Comprehensive evaluation across three types of real-world disclosure documents demonstrates that CoFiTCheck significantly outperforms previous methods while maintaining practical efficiency.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Visualizing statistical models: Removing the blindfold,"AbstractVisualization can help in model building, diagnosis, and in developing an understanding about how a model summarizes data. This paper proposes three strategies for visualizing statistical models: (i) display the model in the data space, (ii) look at all members of a collection, and (iii) explore the process of model fitting, not just the end result. Each strategy is accompanied by examples, includingmanova, classification algorithms, hierarchical clustering, ensembles of linear models, projection pursuit, self‐organizing maps, and neural networks.",CS,CG_VIS,85,Clear CS paper with focus on Computer Graphics & Visualization
"Business Process Analysis and Implementation of Odoo Open Source ERP System in Inventory, Purchasing and Sales Activities","The development of information technology is the main thing for trading companies in developing their business. Resources within the company must be managed and utilized optimally, one of which is inventory, buying and selling of goods. ERP (enterprise resource planning) system is an integrated information system concept that can be utilized by companies and SMEs (micro, small and medium enterprises) to improve company performance. Captain Gadget is one of the medium-sized businesses engaged in retail sales of mobile phones, mobile accessories and spare parts for mobile phones, which is located in Malang City. Captain Gadget has three employees who run its core business. In the procurement of goods, it is carried out by the procurement admin, in the management of stock goods it is carried out by the warehouse admin and in sales it is carried out by the sales department. The company in processing its data using Microsoft Excel application, while the recording and bookkeeping of cash is done manually. This causes less effective and efficient performance. For this reason, it is necessary to configure and implement an ERP (enterprise resource planning) system to support business process activities on the Captain gadget. One of them is using Odoo ERP software based on Open Source which can be adapted to the system needs in business processes at Captain Gadget.",IS,ES_ERP,85,Clear IS paper with focus on Enterprise Systems & ERP
Development of Web-Based Experiment for Integer Programming Using Java Applet,"Integer (Mixed integer) programming is one of important mathematical programs for solving many practical problems, such as economic management, optimization control and supply chain. The integer linear programming problem and its solution algorithm are studied in this paper, and Web-based mathematical experiments of Branch-and-bound algorithm developed using Java applets. The system of the experiment can not only demonstrate the basic principle and iterating procedure of the algorithm by online example, but also solve any new integer programming inputted by user. The developed system provides users an efficient assisted learning platform with an interactive mode.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Enlightenment of Physical Education Teaching Experiment Based on Cloud Computing to the Current Physical Education Reform,"Our country has a large land area, and the development of physical education is not balanced. In daily teaching activities, teachers and students use computers and networks to teach, which generates massive amounts of data. Schools are limited by funds and cannot meet the growing demand for storage of teaching resources. It is also unable to realize the sharing of teaching resources. In order to solve the problems existing in the existing education and teaching platform of the school, especially in the teaching reform, to meet the requirements of all parties facing physical education, the concept of cloud computing was proposed, and the services and methods provided by the cloud computing-based teaching resource platform were discussed. Through the questionnaire survey method of college students and teachers, statistical methods and logical analysis methods were used to analyze the data collected in the questionnaire. Summary and analysis are as follows. The survey results show that more than 50% of the people are dissatisfied with the current physical education and believe that it has not played its due role, and more than 70% of the people agree with the reform of physical education. The experimental results also show that interesting and diverse physical education courses can attract students to participate and increase their interest. From the overall survey results, the problem of college physical education courses is more serious, and it is urgent to optimize teaching from the cloud computing level. On the one hand, it is necessary to improve the relevant cloud computing and other technical platform facilities; on the other hand, it is necessary to improve the teaching level of teachers and change the current educational concept to make it livelier and more interesting.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Solving parametric linear systems,"Algorithms in computer algebra are usually designed for a fixed set of domains. For example, algorithms over the domain of polynomials are not applicable to parameters because the inherent assumption that the indeterminate X bears no algebraic relation to other objects is violated.We propose to use a technique from model theory known as constraint programming to gain more flexibility, and we show how it can be applied to the Gaussian algorithm to be used for parametric systems. Our experiments suggest that in practice this leads to results comparable to the algorithm for parametric linear systems by Sit [9] --- at least if the parameters are sparse.",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
The analysis of association rules: Latent class analysis,"AbstractAssociation rules are used to extract information from transactional databases with a collection of items also called “tokens” or “words.” The aim of association rule analysis is to indicate what and how items go with what items in a set of transactions called “documents.” This approach is used in the analysis of text records, of blogs in social media and of shopping baskets. We present here an approach to analyze documents using latent class analysis (LCA) clustering of document term matrices. A document term matrix (DTM) consists of rows referring to documents and columns corresponding to items. In binary weights, “1” indicates the presence of a term in a document and “0” otherwise. The clustering of similar documents provides stratified data sets used to enhance the interpretability of measures of interest such as lift, odds ratios and relative linkage disequilibrium. The article demonstrates the approach with two case studies. A first example consists of comments recorded in a survey aimed at pet owners. A second, much larger example, is based on online reviews to crocs sandals. Association rules describe combinations of terms in the pet survey and crocs reviews. In Section 3, we compute, for these case studies, association rule measures of interest defined in Section 2. We first introduce the case studies to motivate the methods proposed here. In Section 4, we provide a new approach with an enhanced interpretations of measures such as lift by comparing them across clusters derived from an LCA of the DTM. A key result is the application of clustered data in analyzing observational data. This enhances generalizability and interpretability of findings from text analytics. The article concludes with a discussion in Section 5.",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
Statistical analysis on Alzheimer's disease,"Alzheimer's disease is a progressive neurodegenerative disease that occurs mostly in the elderly and has memory impairment as the main clinical symptom. There is no ideal treatment for Alzheimer's disease, so early prevention is important. In this paper, we use brain structural information to diagnose Alzheimer's disease features and cognitive-behavioral characteristics, which is important for early and accurate diagnosis of mild cognitive impairment. To investigate the factors influencing Alzheimer's disease, a correlation analysis model was developed after preprocessing the missing values of the data. First, the data features were viewed, the missing values of the data were analyzed, and the useless features were removed and the missing values of the remaining features were filled with the average value. To verify the accuracy of the subsequent intelligent diagnosis model and clustering model, this paper divides the training set and test set according to PTID. Finally, the top ten important features are selected and the Spearman coefficients are chosen according to the distribution of the features for correlation analysis. Machine learning methods were utilized to build an Alzheimer's classification model to solve the problem of intelligent diagnosis of Alzheimer's disease. The pre-processed dataset in the above paper was trained with the model, and five methods of logistic regression, support vector machine, KNN classification, decision tree classification and XGB were utilized to build the classification model, and the accuracy, recall and F1 value of each model were visualized and compared, among which the accuracy of XGB model reached 83%, which is reasonable for the intelligent diagnosis of the disease. A K-Means-based clustering model for disease types was established using the K-Means clustering algorithm, clustering CN, MCI and AD into three major classes, and then refining MCI into three subclasses. The optimal K-values and random seeds were firstly found using the elbow principle, then the cluster analysis was performed using the feature values and data sets selected after preprocessing, and finally the MCI in MCI was extracted and sub-clustered into three subclasses SMC, EMCI and LMCI. In order to investigate the evolution pattern of different categories of diseases over time, patients with 3 categories of diseases are screened separately for analysis in this paper. Firstly, by combining the results above and reviewing the data, the features irrelevant to this task and columns containing a large number of missing values were removed, the remaining features were selected and probability density plots were drawn, and all discrete features and all features that were essentially zero were continued to be screened out. After that, the 15 features of CN, MCI and AD diseases were plotted separately over time to reveal their evolution patterns over time. We reviewed the relevant literature, sorted out and summarized the existing studies at home and abroad, and summarized the criteria for determining the five stages of Alzheimer's disease and the early intervention of the disease.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Business Network Modelling,"Business networks have appeared as a reaction to changes taking place in the world economy and logistic networks can be considered as examples of such networks. The approach proposed in the paper is based on the idea to represent the business network members with services provided by them, and to achieve interoperability via application of the SOA standards. The approach is based on usage of such technologies as Web services, ontology, and context management. Web services enable interoperability at the technological level. Ontologies are used for description of knowledge domains and enable interoperability at the level of semantics. The purpose of the context is to represent only relevant information from the large amount of the information and the application of the approach is demonstrated on the case study from the area of dynamic logistics. The considered problem takes into account a continuously changing problem environment and requires nearly real-time solving.",IS,SCMS,85,Clear IS paper with focus on Supply Chain Management Systems
Information System Design Web-Based Cash Bookkeeping Case Study of Juice Shop 1 Putra,"The development of increasingly advanced technology makes business actors have to follow the developments that occur in the entrepreneurial environment. In this study, he designed a website-based cash bookkeeping at Toko Jus 1 Putra. In-store cash bookkeeping is still done manually, so the creation of reports is quite time-consuming and what is produced cannot be optimally. The purpose of this study was to produce easy and accurate cash bookkeeping in recording reports during sales transactions, the purchase of Jus 1 Putra. The research method used is Waterfall and uses the Laravel framework. The testing method used is Black Box Testing. In data collection carried out through observation, interviews, and literature studies. The results of this research that have been built by the infrastructure system are successful, based on the design of business needs provide with features that help to carry out business processes.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Capturing Process Knowledge for Multi-Channel Information Systems,"In this paper, a case study is used to evaluate the business process characterizing modeling (BPCM) language. The BPCM-framework is meant to guide both business stakeholders and model developers during model-based development. The focus of the approach is the use of BPCM as a starting point for capturing process knowledge when planning and developing information system support. Based on information within the BPCM models, goal models and process models can be developed and used for further development of the BPCM model. The approach in this paper is evaluated using a case study related to the arrangement of a conference series. Through the case study, the authors have confirmed the potential usability and usefulness of BPCM for early stage knowledge capture, getting input for further improvement of the approach.",IS,BPM,85,Clear IS paper with focus on Business Process Management
Statistical analysis of antifragility in Hungarian ice hockey games,"Professional ice hockey provides a great environment for studying the antifragile behaviour of teams because of publicly available results and statistics. This study examines three-goal events in which a team gave up a goal but responded by scoring two goals. Thirty-four ice hockey games are studied from the last ten seasons of the Hungarian first league to identify the events’ characteristics and to determine whether antifragile behaviour emerged in these events. The results indicate that if the opponent scores first and has a one- or two-goal lead, the team that responds with two goals after strengthening the line exhibits a convex and, therefore, an antifragile behaviour. Antifragility has been found in 22 cases, lending support to the assumption that antifragile behaviour emerges from high-level cooperation.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Taguchi Design of Experiment versus Dynamic Programming Approach in the Optimization of Turning Process,"The paper examines the influence of cutting parameters, namely cutting speed and feed rate on the tool life in machining process of cylindrical billets made from a Duplex Stainless Steel (DSS). Two optimization methods is presents, one based on the Taguchi design of the experiment with orthogonal array L9 and signal-to-noise ratio (S/N) and the second based on the dynamic programming approach with modified Dijkstra's algorithm have been used to find optimal levels of the control parameters. ANOVA was performed to determine the significance of the input variables. A predictive mathematical model has been developed through a regression analysis to study the response. The results at optimum cutting conditions are predicted using estimated values. Finally, the features, the merits and the limitations of the presented optimization approaches were discussed.",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
Evaluating a Taxonomy for Mobility Requirements by a Controlled Experiment,"Requirements taxonomies have been found useful in software requirements elicitation and specification, both for educational purposes and in practical usage, for instance, as checklists to ensure that important categories of requirements are not forgotten, and for guidance on how to write various types of requirements. While mobile information systems are becoming increasingly important, traditional requirements taxonomies do not have any category for mobility requirements. This paper reports on a controlled experiment where two groups of students both got the same excerpts of the well-known Volere requirements taxonomy, but for one treatment group the tutorial material was also extended with additional material on mobility requirements as a requirements category in its own right. Using the provided taxonomy material for guidance, the students were asked to write requirements for a system presented in a natural language case description; afterwards their output was analyzed to score the number and quality of requirements found by each student. The main finding was that the students using the extended taxonomy also found more requirements, but there was no significant difference in the quality of requirements between the two groups.",IS,MBA,75,Matched IS paper to Mobile Business Applications
Symbolic missing data imputation in principal component analysis,"AbstractThe concept of symbolic data has been developed with the aim of representing variables whose measurement is affected by some internal variation. This idea has been mainly concerned with the need of aggregating individuals in order to summarize large datasets into smaller matrices of manageable size, retaining as much of the original knowledge as possible. Nevertheless it is often applied also with variables structured from their outset as symbolic variables, although measured on single individuals. This paper deals with the latter framework, and aims at showing that symbolic data analysis techniques can be applied to the field of missing values treatment. The algorithm for a symbolic imputation technique in principal component analysis is presented as a generalization of the basic strategy called interval imputation. An illustrative example and a real data case study show how the proposed technique works. © 2011 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 4: 171–183, 2011",CS,DS_ANALYTICS,85,Clear CS paper with focus on Data Science & Analytics
A replicated experiment of pair-programming in a 2nd-year software development and design computer science course,"This paper presents the results of a replicated pair programming experiment conducted at the University of Auckland (NZ) during the first semester of 2005. It involved 190 second year Computer Science students attending a software design and construction course. We replicated the experiment described in [18], investigating similar issues to those reported in [32] and employing a subset of the questionnaires used in [32]. Our results confirm the use of pair programming as an effective programming/design learning technique.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Controlled Experimental Design for Statistical Comparison of Integer Programming Algorithms,"Testing and comparison of integer programming algorithms is an integral part of the algorithm development process. When test problems are randomly generated, the techniques of statistical experimental design can provide a basis around which to structure computational experiments. This paper formulates the problem of constructing and analyzing controlled integer programming tests in the experimental design context and develops approaches to dealing with a number of issues that arise. Both analytic results and empirical evidence from a large experiment are employed in deriving the suggested techniques.",CS,ALGO_DS,85,Clear CS paper with focus on Algorithms & Data Structures
Programming Away Human Rights and Responsibilities? “The Moral Machine Experiment” and the Need for a More “Humane” AV Future,"Dilemma situations involving the choice of which human life to save in the case of unavoidable accidents are expected to arise only rarely in the context of autonomous vehicles (AVs). Nonetheless, the scientific community has devoted significant attention to finding appropriate and (socially) acceptable automated decisions in the event that AVs or drivers of AVs were indeed to face such situations. Awad and colleagues, in their now famous paper “The Moral Machine Experiment”, used a “multilingual online ‘serious game’ for collecting large-scale data on how citizens would want AVs to solve moral dilemmas in the context of unavoidable accidents.” Awad and colleagues undoubtedly collected an impressive and philosophically useful data set of armchair intuitions. However, we argue that applying their findings to the development of “global, socially acceptable principles for machine learning” would violate basic tenets of human rights law and fundamental principles of human dignity. To make its arguments, our paper cites principles of tort law, relevant case law, provisions from the Universal Declaration of Human Rights, and rules from the German Ethics Code for Autonomous and Connected Driving.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Green,"Energy-efficient computing is important in several systems ranging from embedded devices to large scale data centers. Several application domains offer the opportunity to tradeoff quality of service/solution (QoS) for improvements in performance and reduction in energy consumption. Programmers sometimes take advantage of such opportunities, albeit in an ad-hoc manner and often without providing any QoS guarantees. We propose a system called Green that provides a simple and flexible framework that allows programmers to take advantage of such approximation opportunities in a systematic manner while providing statistical QoS guarantees. Green enables programmers to approximate expensive functions and loops and operates in two phases. In the calibration phase, it builds a model of the QoS loss produced by the approximation. This model is used in the operational phase to make approximation decisions based on the QoS constraints specified by the programmer. The operational phase also includes an adaptation function that occasionally monitors the runtime behavior and changes the approximation decisions and QoS model to provide strong statistical QoS guarantees. To evaluate the effectiveness of Green, we implemented our system and language extensions using the Phoenix compiler framework. Our experiments using benchmarks from domains such as graphics, machine learning, signal processing, and finance, and an in-production, real-world web search engine, indicate that Green can produce significant improvements in performance and energy consumption with small and controlled QoS degradation.",CS,EMBEDDED_RT,85,Clear CS paper with focus on Embedded Systems & Real-time Computing
Design Of A Website-Based Internship Information System (Case Study Of Industrial Engineering UPN “Veteran” East Java),"The Industrial Engineering Study Program is one of the study programs at the Faculty of Engineering, UPN ""Veteran"" East Java. In the teaching and learning process, students are required to undertake an internship as a graduation requirement. Currently, the training administration still relies on manual processes, which results in the administration process needing to be more effective and efficient. Given these problems, an internship information system will be produced. This website-based internship information system is implemented so that anyone can access the website anytime and anywhere. Therefore, the relevant parties can access this internship information system more efficiently. The research method for collecting the required data is observation and interviews. This information system uses PHP programming language and MySQL as the database. Based on the results of the tests that have been carried out, this website-based internship information system makes it easier for related parties to administer internship activities. Students can apply for supervisors' internship seminars and upload internship grades, and admins can plot supervisors, provide assignment letters for supervisors, and plot examiners online and systematically",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Wildlife fecal microbiota exhibit community stability across a longitudinal semi-controlled non-invasive sampling experiment,"Wildlife microbiome studies are being used to assess microbial links with animal health and habitat. The gold standard of sampling microbiomes directly from captured animals is ideal for limiting potential abiotic influences on microbiome composition, yet fails to leverage the many benefits of non-invasive sampling. Application of microbiome-based monitoring for rare, endangered, or elusive species creates a need to non-invasively collect scat samples shed into the environment. Since controlling sample age is not always possible, the potential influence of time-associated abiotic factors was assessed. To accomplish this, we analyzed partial 16S rRNA genes of fecal metagenomic DNA sampled non-invasively from Rocky Mountain elk (Cervus canadensis) near Yellowstone National Park. We sampled pellet piles from four different elk, then aged them in a natural forest plot for 1, 3, 7, and 14 days, with triplicate samples at each time point (i.e., a blocked, repeat measures (longitudinal) study design). We compared fecal microbiota of each elk through time with point estimates of diversity, bootstrapped hierarchical clustering of samples, and a version of ANOVA–simultaneous components analysis (ASCA) with PCA (LiMM-PCA) to assess the variance contributions of time, individual and sample replication. Our results showed community stability through days 0, 1, 3 and 7, with a modest but detectable change in abundance in only 2 genera (Bacteroides and Sporobacter) at day 14. The total variance explained by time in our LiMM-PCA model across the entire 2-week period was not statistically significant (p&amp;gt;0.195) and the overall effect size was small (&amp;lt;10% variance) compared to the variance explained by the individual animal (p&amp;lt;0.0005; 21% var.). We conclude that non-invasive sampling of elk scat collected within one week during winter/early spring provides a reliable approach to characterize fecal microbiota composition in a 16S rDNA survey and that sampled individuals can be directly compared across unknown time points with minimal bias. Further, point estimates of microbiota diversity were not mechanistically affected by sample age. Our assessment of samples using bootstrap hierarchical clustering produced clustering by animal (branches) but not by sample age (nodes). These results support greater use of non-invasive microbiome sampling to assess ecological patterns in animal systems.",CS,BIOINFO,85,Clear CS paper with focus on Bioinformatics & Computational Biology
Convolutional Sparse Coding for Time Series Via a ℓ0 Penalty: An Efficient Algorithm With Statistical Guarantees,"ABSTRACTIdentifying characteristic patterns in time series, such as heartbeats or brain responses to a stimulus, is critical to understanding the physical or physiological phenomena monitored with sensors. Convolutional sparse coding (CSC) methods, which aim to approximate signals by a sparse combination of short signal templates (also called atoms), are well‐suited for this task. However, enforcing sparsity leads to non‐convex and untractable optimization problems. This article proposes finding the optimal solution to the original and non‐convex CSC problem when the atoms do not overlap. Specifically, we show that the reconstruction error satisfies a simple recursive relationship in this setting, which leads to an efficient detection algorithm. We prove that our method correctly estimates the number of patterns and their localization, up to a detection margin that depends on a certain measure of the signal‐to‐noise ratio. In a thorough empirical study, with simulated and real‐world physiological data sets, our method is shown to be more accurate than existing algorithms at detecting the patterns' onsets.",CS,BIOINFO,85,Clear CS paper with focus on Bioinformatics & Computational Biology
Teaching and Learning Domain Modeling through Collaboration Patterns: A Controlled Experiment,"Domain models in software engineering—often represented as class diagrams—depict relevant classes in a given problem domain along with necessary relationships among those classes. These models are important because they establish links between the requirements of a given system under development and the subsequent phases of the systems development life cycle. Although the teaching of basic concepts related to domain modeling takes only about 1 or 2 hours, proper application of these concepts to a given problem situation is difficult for students studying software engineering. Due to their insufficient domain knowledge of the problem situation and modeling experience, they often produce domain models that may not adequately represent necessary elements as part of the domain models. Analysis patterns can help them by encoding expert knowledge and offering guidance in the modeling process. This article reports the findings from a controlled experiment conducted to study the effects of collaboration patterns on the domain modeling process by students. Specifically, the study investigated the differences in students’ perceptions of the ease of the domain modeling process and quality of models produced, perceived difficulties, and how collaboration patterns help address domain modeling difficulties and the quality of domain models produced. Findings from this experimental study involving students from a software engineering course indicate that although there is no significant difference in subjects’ perceptions between the control and treatment groups, the subjects from the treatment group produced better-quality domain models. Additionally, the qualitative analysis of the feedback collected from the subjects from the control and treatment groups reveals that that having knowledge of patterns is beneficial, as it addresses the difficulties in domain modeling.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Learning-based controlled concurrency testing,"Concurrency bugs are notoriously hard to detect and reproduce. Controlled concurrency testing (CCT) techniques aim to offer a solution, where a scheduler explores the space of possible interleavings of a concurrent program looking for bugs. Since the set of possible interleavings is typically very large, these schedulers employ heuristics that prioritize the search to “interesting” subspaces. However, current heuristics are typically tuned to specific bug patterns, which limits their effectiveness in practice.  In this paper, we present QL, a learning-based CCT framework where the likelihood of an action being selected by the scheduler is influenced by earlier explorations. We leverage the classical Q-learning algorithm to explore the space of possible interleavings, allowing the exploration to adapt to the program under test, unlike previous techniques. We have implemented and evaluated QL on a set of microbenchmarks, complex protocols, as well as production cloud services. In our experiments, we found QL to consistently outperform the state-of-the-art in CCT.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
On the Statistical Analysis of Dirty Pictures,"SUMMARY A continuous two-dimensional region is partitioned into a fine rectangular array of sites or “pixels”, each pixel having a particular “colour” belonging to a prescribed finite set. The true colouring of the region is unknown but, associated with each pixel, there is a possibly multivariate record which conveys imperfect information about its colour according to a known statistical model. The aim is to reconstruct the true scene, with the additional knowledge that pixels close together tend to have the same or similar colours. In this paper, it is assumed that the local characteristics of the true scene can be represented by a non-degenerate Markov random field. Such information can be combined with the records by Bayes' theorem and the true scene can be estimated according to standard criteria. However, the computational burden is enormous and the reconstruction may reflect undesirable large-scale properties of the random field. Thus, a simple, iterative method of reconstruction is proposed, which does not depend on these large-scale characteristics. The method is illustrated by computer simulations in which the original scene is not directly related to the assumed random field. Some complications, including parameter estimation, are discussed. Potential applications are mentioned briefly.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Computer Science Education in ChatGPT Era: Experiences from an Experiment in a Programming Course for Novice Programmers,"The use of large language models with chatbots like ChatGPT has become increasingly popular among students, especially in Computer Science education. However, significant debates exist in the education community on the role of ChatGPT in learning. Therefore, it is critical to understand the potential impact of ChatGPT on the learning, engagement, and overall success of students in classrooms. In this empirical study, we report on a controlled experiment with 182 participants in a first-year undergraduate course on object-oriented programming. Our differential study divided students into two groups, one using ChatGPT and the other not using it for practical programming assignments. The study results showed that the students’ performance is not influenced by ChatGPT usage (no statistical significance between groups with a p-value of 0.730), nor are the grading results of practical assignments (p-value 0.760) and midterm exams (p-value 0.856). Our findings from the controlled experiment suggest that it is safe for novice programmers to use ChatGPT if specific measures and adjustments are adopted in the education process.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Improving the Safety of a Pyrotechnic Igniter through a controlled experiment,"AbstractOne of the important goals of quality management in today's competitive marketplace is to build quality into products and processes at the development stage. The use of statistically controlled experiments, in which several parameters are studied simultaneously, can help accomplish this goal efficiently. This paper describes a case study that illustrates the utility of controlled experiments in product development. The customer required improvements in the safety of a pyrotechnic igniter used in one of the products they purchased. The safety improvements could be met by replacing the “match‐head” initiator by a type IA/IW no fire initiator. However, there was concern that changing the initiator might degrade other important product performance characteristics. A controlled experiment was designed to compare three different initiators, the booster charge and the main charge. The experiment quickly and efficiently pointed to a superior initiator and the optimal charges to maintain high performance with enhanced product safety.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Understanding Pound-Drever-Hall locking using voltage controlled radio-frequency oscillators: An undergraduate experiment,"We have developed a senior undergraduate experiment that illustrates frequency stabilization techniques using radio-frequency electronics. The primary objective is to frequency stabilize a voltage controlled oscillator to a cavity resonance at 800 MHz using the Pound-Drever-Hall method. This technique is commonly applied to stabilize lasers at optical frequencies. By using only radio-frequency equipment, it is possible to systematically study aspects of the technique more thoroughly, inexpensively, and free from eye hazards. Students also learn about modular radio-frequency electronics and basic feedback control loops. By varying the temperature of the resonator, students can determine the thermal expansion coefficients of copper, aluminum, and super invar.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Design of a Web-based Livestock Feed Sales Information System (Case Study of Lukman Jaya Store),"Lukman Jaya Store is business engaged in the sale of chicken feed and duck feed for both layer and broilers, and chicken / duck breeds. This shop in located in singkalan village RT14 RW06, Balongbendo district, Sidoarjo regency. The purpose of this research is to design a website-based livestock feed sales information system. The system used at lukman jaya stores is still manual, where the media for promotion is only throught banners and stories from one customer to another. The analysis of the system used is media observation, interviews and literature study. The results of this study are to make it easier for customers to buy and know the chicken and duck feed available at the Lukman Jaya Store.",IS,ECOMM_DB,85,Clear IS paper with focus on E-commerce & Digital Business
Cardiogenic programming of human pluripotent stem cells by dose-controlled activation of EOMES,"AbstractMaster cell fate determinants are thought to induce specific cell lineages in gastrulation by orchestrating entire gene programs. The T-box transcription factor EOMES (eomesodermin) is crucially required for the development of the heart—yet it is equally important for endoderm specification suggesting that it may act in a context-dependent manner. Here, we define an unrecognized interplay between EOMES and the WNT signaling pathway in controlling cardiac induction by using loss and gain-of-function approaches in human embryonic stem cells. Dose-dependent EOMES induction alone can fully replace a cocktail of signaling molecules otherwise essential for the specification of cardiogenic mesoderm. Highly efficient cardiomyocyte programming by EOMES mechanistically involves autocrine activation of canonical WNT signaling via the WNT3 ligand, which necessitates a shutdown of this axis at a subsequent stage. Our findings provide insights into human germ layer induction and bear biotechnological potential for the robust production of cardiomyocytes from engineered stem cells.",CS,BIOINFO,85,Clear CS paper with focus on Bioinformatics & Computational Biology
Can guided decomposition help end-users write larger block-based programs? a mobile robot experiment,"Block-based programming environments, already popular in computer science education, have been successfully used to make programming accessible to end-users in domains like robotics, mobile apps, and even DevOps. Most studies of these applications have examined small programs that fit within a single screen, yet real-world programs often grow large, and editing these large block-based programs quickly becomes unwieldy. Traditional programming language features, like functions, allow programmers to decompose their programs. Unfortunately, both previous work, and our own findings, suggest that end-users rarely use these features, resulting in large monolithic code blocks that are hard to understand. In this work, we introduce a block-based system that provides users with a hierarchical, domain-specific program structure and requires them to decompose their programs accordingly. Through a user study with 92 users, we compared this approach, which we call guided program decomposition, to a traditional system that supports functions, but does not require decomposition. We found that while almost all users could successfully complete smaller tasks, those who decomposed their programs were significantly more successful as the tasks grew larger. As expected, most users without guided decomposition did not decompose their programs, resulting in poor performance on larger problems. In comparison, users of guided decomposition performed significantly better on the same tasks. Though this study investigated only a limited selection of tasks in one specific domain, it suggests that guided decomposition can benefit end-user programmers. While no single decomposition strategy fits all domains, we believe that similar domain-specific sub-hierarchies could be found for other application areas, increasing the scale of code end-users can create and understand.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Interview with Jonathan Cohen: Beginning the Legacy of SLOVO Journal,"DIRECTOR OF CONCILIATION RESOURCES, JONATHAN COHEN, IN CONVERSATION WITH SLOVO’S EXECUTIVE EDITOR BORIMIR TOTEV. Jonathan Cohen joined Conciliation Resources in December 1997 and developed the Caucasus programme. In September 2008, he became Director of Programmes overseeing all Conciliation Resources’ regional programmes and was appointed Executive Director in May 2016. Previously he was deputy director of the Foundation on Inter-Ethnic Relations in The Hague, working with the OSCE High Commissioner on National Minorities. Before that he worked for International Alert and the Peace Research Institute Oslo. He has been a board member of the DFID/CAF Partnerships in the Non-Profit Sector Programme for Russia; acted as a consultant to United Nations Volunteers, the Heinrich Boell Foundation, the Berghof Foundation and taught at the London School of Economics. Read full text here.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Putting the ‘theory’ back into grounded theory: guidelines for grounded theory studies in information systems,"AbstractOver the past decade, there has been increasing interest in the use of grounded theory in information systems research. Grounded theory is a qualitative research method that seeks to develop theory that is grounded in data systematically gathered and analysed. The purpose of this paper is to suggest guidelines for grounded theory studies in information systems. Our guidelines are based on a framework for theorizing in grounded theory studies that focuses on conceptualization and theory scope. Our hope is that the guidelines will help to raise the quality and aspirations of grounded theory studies in information systems.",IS,IS_RM,85,Clear IS paper with focus on Information Systems Research Methods
Two‐sample testing for random graphs,"AbstractThe employment of two‐sample hypothesis testing in examining random graphs has been a prevalent approach in diverse fields such as social sciences, neuroscience, and genetics. We advance a spectral‐based two‐sample hypothesis testing methodology to test the latent position random graphs. We propose two distinct asymptotic normal statistics, each optimally designed for two different models—the elementary Erdős–Rényi model and the more complex latent position random graph model. For the latter, the spectral embedding of the adjacency matrix was utilized to estimate the test statistic. The proposed method exhibited superior efficacy as it accomplished higher power than the conventional method of mean estimation. To validate our hypothesis testing procedure, we applied it to empirical biological data to discern structural variances in gene co‐expression networks between COVID‐19 patients and individuals who remained unaffected by the disease.",CS,BIOINFO,85,Clear CS paper with focus on Bioinformatics & Computational Biology
A clustering method for graphical handwriting components and statistical writership analysis,"AbstractHandwritten documents can be characterized by their content or by the shape of the written characters. We focus on the problem of comparing a person's handwriting to a document of unknown provenance using the shape of the writing, as is done in forensic applications. To do so, we first propose a method for processing scanned handwritten documents to decompose the writing into small graphical structures, often corresponding to letters. We then introduce a measure of distance between two such structures that is inspired by the graph edit distance, and a measure of center for a collection of the graphs. These measurements are the basis for an outlier tolerant K‐means algorithm to cluster the graphs based on structural attributes, thus creating a template for sorting new documents. Finally, we present a Bayesian hierarchical model to capture the propensity of a writer for producing graphs that are assigned to certain clusters. We illustrate the methods using documents from the Computer Vision Lab dataset. We show results of the identification task under the cluster assignments and compare to the same modeling, but with a less flexible grouping method that is not tolerant of incidental strokes or outliers.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
Grounded Theory Methodology as Methodical Hermeneutics,"In this article it is argued that the realism-relativism duality addressed by the grounded theory approach to qualitative research is best accounted for when the method is understood to be an inductive approach to hermeneutics. Phenomenology, C.S. Peirce's theory of inference, philosophical hermeneutics, pragmatism and the new rhetoric are drawn upon in support of this argument. It is also held that this formulation of the grounded theory method opens the possibility that the method improves on earlier approaches to methodical hermeneutics. As an outcome of this formulation, the debate on the validity and reliability of returns from the grounded theory approach is cast in a new light. The new methodical hermeneutics is discussed in terms of prior attempts to relate hermeneutics to method.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Two principles of end-user software engineering research,"This paper argues the importance of two principles for end-user software engineering research. The first of these is that not all end-user developers are the same. The second is that research must be grounded in field studies of actual end-user development practice. In keeping with this second principle, our arguments are based on data from our own field studies of practice. These field studies involve a class of end user developer, whom we term 'professional end user developers' and who include scientists, mathematicians and engineers.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Reuse in Contemporary Software Engineering Practices – An Exploratory Case Study in A Medium-sized Company,"Background: Software practice is evolving with changing technologies and practices such as InnerSource, DevOps, and microservices. It is important to investigate the impact of contemporary software engineering (SE) practices on software reuse. Aim: This study aims to characterize software reuse in contemporary SE practices and investigate its implications in terms of costs, benefits, challenges, and potential improvements in a medium-sized company. Method: We performed an exploratory case study by conducting interviews, group discussions, and reviewing company documentation to investigate software reuse in the context of contemporary SE practices in the case company. Results: The results indicate that the development for reuse in contemporary SE practices incurs additional coordination, among other costs. Development with reuse led to relatively fewer additional costs and resulted in several benefits such as better product quality and less development and delivery time. Ownership of reusable assets is challenging in contemporary SE practice. InnerSource practices may help mitigate the top perceived challenges: discoverability and ownership of the reusable assets, knowledge sharing and reuse measurement. Conclusion: Reuse in contemporary SE practices is not without additional costs and challenges. However, the practitioners perceive costs as investments that benefit the company in the long run.",CS,SW_ENG,75,Matched CS paper to Software Engineering & Development
Indexicality,"A lot of research has been done within the area of mobile computing and context-awareness over the last 15 years, and the idea of systems adapting to their context has produced promising results for overcoming some of the challenges of user interaction with mobile devices within various specialized domains. However, today it is still the case that only a limited body of theoretically grounded knowledge exists that can explain the relationship between users, mobile system user interfaces, and their context. Lack of such knowledge limits our ability to elevate learning from the mobile systems we develop and study from a concrete to an abstract level. Consequently, the research field is impeded in its ability to leap forward and is limited to incremental steps from one design to the next. Addressing the problem of this void, this article contributes to the body of knowledge about mobile interaction design by promoting a theoretical approach for describing and understanding the relationship between user interface representations and user context. Specifically, we promote the concept of indexicality derived from semiotics as an analytical concept that can be used to describe and understand a design. We illustrate the value of the indexicality concept through an analysis of empirical data from evaluations of three prototype systems in use. Based on our analytical and empirical work we promote the view that users interpret information in a mobile computer user interface through creation of meaningful indexical signs based on the ensemble of context and system.",CS,HCI,85,Clear CS paper with focus on Human-Computer Interaction
Being invited. Ethics in participant-researcher relationships,"This article examines the act of ‘being invited’ by research participants to explore asymmetrical power relations and research ethics in ethnographies along migration trajectories. It uses the lens of money in research relationships to explore agency and the reversal of power through hospitality and gift-giving. I examine invitations in the research process as (a) a way for researchers to gain access to their research subject and (b) a way for interlocutors to renegotiate and invert the research process. Drawing on a 9-month multi-sited ethnography along the trajectory of undocumented migration from Afghanistan to Germany, I relied on continuous invitations to revisit interlocutors. First, I argue that invitations are the necessary entry point into research sites, but are often excluded from considerations of research ethics. Second, an examination of ‘being invited’ as a concept shows that interlocutors shape the research process and exercise agency through a moral economy of research relations.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Practices and views on fetal heart monitoring: a structured observation and interview study,"Objective  To assess and explain deviations from recommended practice in National Institute for Clinical Excellence (NICE) guidelines in relation to fetal heart monitoring.Design  Qualitative study.Setting  Large teaching hospital in the UK.Sample  Sixty‐six hours of observation of 25 labours and interviews with 20 midwives of varying grades.Methods  Structured observations of labour and semistructured interviews with midwives. Interviews were undertaken using a prompt guide, audiotaped, and transcribed verbatim. Analysis was based on the constant comparative method, assisted by QSR N5 software.Main outcome measures  Deviations from recommended practice in relation to fetal monitoring and insights into why these occur.Results  All babies involved in the study were safely delivered, but 243 deviations from recommended practice in relation to NICE guidelines on fetal monitoring were identified, with the majority (80%) of these occurring in relation to documentation. Other deviations from recommended practice included indications for use of electronic fetal heart monitoring and conduct of fetal heart monitoring. There is evidence of difficulties with availability and maintenance of equipment, and some deficits in staff knowledge and skill. Differing orientations towards fetal monitoring were reported by midwives, which were likely to have impacts on practice. The initiation, management, and interpretation of fetal heart monitoring is complex and distributed across time, space, and professional boundaries, and practices in relation to fetal heart monitoring need to be understood within an organisational and social context.Conclusion  Some deviations from best practice guidelines may be rectified through straightforward interventions including improved systems for managing equipment and training. Other deviations from recommended practice need to be understood as the outcomes of complex processes that are likely to defy easy resolution.",IT,IT_DOC_KM,85,Clear IT paper with focus on IT Documentation & Knowledge Management
From Observation of Teaching Practices to Mentoring and Teacher Development in a Multicultural Setting: A Case Study,"This article examines whether teacher mentoring can be achieved in the Greek EFL multicultural teaching context, within the private sector, mainly through observation and if this could contribute to the teacher (observee) or even mentor (observer) development. Observation explores everyday multicultural classroom situations and results in a trustworthy interaction between the observer and the teacher about how these could be handled. Mentoring is provided by means of feedback and discussion in an attempt to educate the teacher, offer corrective options and promote reflection. Ideally, development is produced either via the mentor’s suggested alternatives or by the teacher’s own introspection. The article concludes with findings that suggest the suitability of the direct observation form for observing teaching practices in a multicultural setting and the effectiveness of the face-to-face provision of feedback. Furthermore, evidence is provided regarding achievement of mentoring in the private sector, within a working environment of professionalism, respect and trust. Finally, it is shown that teacher development is an issue relying mainly on the personal approach, with innovation and change being its fundamental characteristics.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Thematic Analysis through Artificial Intelligence (AI),"Thematic analysis, a well-enforced qualitative analytic method, is likely to continue evolving with the adoption of AI technologies. This how-to report does not delve into the details of thematic analysis itself, as there are ample existing studies on the topic. Instead, it acknowledges the potential impacts, dynamics, and pitfalls of AI in thematic analysis while offering valuable advice, particularly for novice analysts, on how to incorporate and document AI tools in each phase of a thematic analysis. The author underscores the importance of not allowing AI to overshadow the analyst's critical evaluative and interpretive skills but instead supporting the use of AI as an aid in thematic analysis, enhancing the depth and breadth of analysis, provided certain criteria are adhered to. This approach ensures that AI serves as a complementary tool, augmenting rather than replacing human analytical inquiry.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Thematic Analysis and Artificial Intelligence: A Step-by-Step Process for Using ChatGPT in Thematic Analysis,"This study sets out how to use generative artificial intelligence (AI) in the six steps of systematic thematic analysis. It leverages AI to address the limitations of traditional thematic analysis. This paper developed prompts (inputs) for ChatGPT (a generative AI chatbot based on a large language model) that are based on many researchers’ discussions and criticisms of qualitative data analysis. The contributions of this paper are twofold. First, it addresses a critical research gap by showcasing ChatGPT prompts for each step of the six steps of systematic thematic analysis, which also addresses researcher training in thematic analysis. Second, it contributes to the development of input to train AI in thematic analysis, including a description of how to familiarize an AI system with the context of a research study and the researcher’s methodological and theoretical considerations; this approach helps to reduce human bias and improves accountability and transparency in thematic analysis.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Thematic and Methodological Trends of Graduate Thesis in Comparative Education: A Content Analysis,"In this study, it is aimed to examine the graduate theses made between 2016-2021 in Turkey in the field of comparative education. The data used in the study, which was carried out according to the qualitative research design, were reached through document analysis. Within the scope of the study, 133 graduate theses registered in the database of Higher Education Institution National Thesis Center were analyzed by using content analysis technique in line with research problems. As a result of the findings, it was determined that the most thesis in the field of comparative education was made in 2018; It was determined that Hacettepe University and Gazi University were the universities with the highest number of graduate studies. It was determined that the majority of theses were made in the department of educational sciences. It was determined that the qualitative design was widely preferred in the theses examined and the document analysis technique was mostly used in the data collection process. It was concluded that the sample/comparison criteria and comparison approach were not specified in the majority of theses. It was also concluded that while Germany, England, Finland and the USA were the countries with which the most comparisons were made, the majority of theses were made for secondary and undergraduate education levels. In the theses examined, it was concluded that higher education and teacher education were the most frequently compared topics.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Academic publisher guidelines on AI usage: A ChatGPT supported thematic analysis,"Background As Artificial Intelligence (AI) technologies such as Generative AI (GenAI) have become more common in academic settings, it is necessary to examine how these tools interact with issues of authorship, academic integrity, and research methodologies. The current landscape lacks cohesive policies and guidelines for regulating AI’s role in academic research which has prompted discussions among publishers, authors, and institutions. Methods This study employs inductive thematic analysis to explore publisher policies regarding AI-assisted authorship and academic work. Our methods involved a two-fold analysis using both AI-assisted and traditional unassisted techniques to examine the available policies from leading academic publishers and other publishing or academic entities. The framework was designed to offer multiple perspectives, harnessing the strengths of AI for pattern recognition while leveraging human expertise for nuanced interpretation. The results of these two analyses are combined to form the final themes. Results Our findings indicate six overall themes, three of which were independently identified in both the AI-assisted and unassisted, manual analysis using common software tools. A broad consensus appears among publishers that human authorship remains paramount and that the use of GenAI tools is permissible but must be disclosed. However, GenAI tools are increasingly acknowledged for their supportive roles, including text generation and data analysis. The study also discusses the inherent limitations and biases of AI-assisted analysis, necessitating rigorous scrutiny by authors, reviewers, and editors. Conclusions There is a growing recognition of AI’s role as a valuable auxiliary tool in academic research, but one that comes with caveats pertaining to integrity, accountability, and interpretive limitations. This study used a novel analysis supported by GenAI tools to identify themes emerging in the policy landscape, underscoring the need for an informed, flexible approach to policy formulation that can adapt to the rapidly evolving landscape of AI technologies.",IS,IT_ETHICS,85,Clear IS paper with focus on IT Ethics & Social Responsibility
"Determination of Situations of Extra-curricular Practices Carried About Robotics, Coding and Electronics","In the study, it was aimed that the establishment of intramural and extracurricular practices carried regarding robotics, coding and electronics and the determination of teacher opinions on various subjects regarding robotics, coding and electronics. Within the context of this study, data was collected by teachers or administrators from 52 state schools in Avcılar district of Istanbul with the help of an online interview form. For examining the data collected in the study, descriptive analysis which is a kind of qualitative data analysis methods was used. Collected data was analysed in the frame of current situation determination, the determination of schools’ requirements, future plans of schools and the contribution of practices of robotics-coding and electronics into education. According to results of the study, it was determined that the use of practices of robotics, coding and electronics in intramural and extracurricular activities would provide a more effective learning environment for students and that it would contribute in a positive way to grow up individuals who use improving technology more effectively. In addition, common opinion of all schools that receive or want to receive education in these fields is that the practices of robotics and coding will make a positive contribution on students’ academic successes, attitudes and motivations.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Characterizing and Categorizing the Essence of Sport Consumption Behavior: A Thematic Analysis,"Sport consumer decision making is a complex process in which an individual evaluates, acquires, uses, or disposes of sport products or services. This qualitative study aimed to identify, understand, and describe the essential characteristics of sport products consumption behavior among professional athletes in Thailand. In-depth interviewing was carried out on 25 professional athlete consumers who responded to semi-structured questions regarding their sport goods consumption. Thematic analysis was performed on the transcripts to describe the essential characteristics of the consumption and to identify overarching topics. Following social constructionism, three themes (i.e., acquisition, use, and disposal) were analyzed and eight categories (i.e., economy concern, gender independence, internet reliance, inquisitive mind, visibility quest, brand focus, environmental concern, gifting preference) were induced. The findings gave insight into athlete consumers’ behavioral characteristics in term of their sport products consumption. Understanding these points contributes to gaining leverage in planning a domestic marketing strategy for this kind of goods.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
A Thematic Analysis of Faculty Advice for Doctoral Students,"The present study examines advice given by the graduate faculty in a department (n=24) to new Ph.D. students in the department. The thematic analysis employed inductive coding to draw themes from the data, and seven salient themes emerged from the interviews: relationships, openness, individuality, purpose, academic work, self-care, and logistics. Grounded in a theoretical framework of social constructivism, the present study analyzes how knowledge is created as a social artifact that is passed down from faculty to graduate student and highlights the ways in which doctoral students then shape the meaning of said knowledge through their own interpretations and actions. This study analyzes the interview data to examine the ways in which systemic challenges of pressure and power are perpetuated within academia and highlights the many ways in which graduate faculty are truly invested in their students and their well-being. The findings serve as a catalyst for introspection for the various actors in academic systems, while providing an uplifting motif of genuine care for the overall wellness of doctoral students.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Science Curriculum Objectives’ Intellectual Demands: A Thematic Analysis,"Science curriculums and curricular materials are essential guidelines in materializing effective science teaching. The primary goal of the current study aims to present a thematic analysis of the last three elementary and middle school science curriculums objectives released in 2013, 2017, and 2018 to determine whether they provide a base for science teachers to design intellectually demanding instructional tasks. This study conducted an in-depth document analysis to describe the curricular themes and objectives' intellectual demands beyond a mere description. Moreover, a critical document-based thematic analysis achieved a call for an in-depth interrogation of the intended science curricula. The current study reveals that the explored science curriculums mainly include physics-related and biology-related topics and chemistry-related topics. There is less place for the issues related to astrophysics and earth sciences. Although three curricular changes (2013, 2017, and 2018) were actualized to enrich the science curriculums' scope, intellectual capacity, and thematic variation, the conceptual emphasis seemed to be strictly copied over the years. The curriculums under examination appeared to let the teachers design high intellectually demanding tasks to teach science knowledge and epistemic practices, however, to a certain extent. It is concluded that the sharp decreases in the number of objectives observed in the abstraction zone may hinder teachers from generating teaching environments where students can transfer acquired knowledge and practices to external contexts. Educational recommendations are offered in the sense of curriculum development and teacher education.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Investigating Future Learning of Electronic Module Through Thematic Analysis in Secondary School: A Need Analysis,"The Design and Technology (RBT) subject aims to equip student with knowledge, skills, and values in design, fostering critical, creative, and innovative thinking. However, the RBT curriculum, especially in electronic design, necessitates new teaching methods to align with updated Curriculum and Assessment Standard (DSKP) content. Current curriculum gaps include insufficient coverage of visualization skills essential for understanding electronics. This study investigates the need for developing 2-Dimensional (2D) simulation and AR module through qualitative research. Semi-structured interviews with five experienced RBT teachers were conducted, and the data were analysed by thematic analysis. The analysis revealed one major theme: the overview and expectations for the 2D simulation and AR module in the electronic design topic. This theme encompasses four sub-themes: challenges in the electronic design topic, resources for the electronic design module, contents of the module, and instructional strategy. The study underscores the urgent need for 2D simulation and AR modules to address these challenges and enhance students' understanding, and improve engagement. This approach aims to bridge gaps in the educational system, offering significant benefits to teachers, researchers, and the Ministry of Education.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Automated thematic analysis of health information technology (HIT) related incident reports,"In this paper, the authors describe a method for exploring the feasibility of using Natural Language Processing (NLP) and Machine Learning (ML) techniques to analyze patient safety incident database reports for themes. We developed a novel thematic analysis strategy to automatically detect keywords and latent themes that describe HIT-related patient safety incidents. The strategy was applied to patient safety reports to test the approach. The efforts by the automated strategy were compared to the efforts by analysts who manually reviewed and identified key words, topics, and themes for the same reports. The computer-based error themes were also compared to the human-determined themes for crosschecking. The manual thematic analysis took about 150 hours to complete on the patient safety reports. The semi-automated approach took only 10% of that time. 95% of the themes extracted from the automated method were aligned with the themes from the manual process. The findings underscore the utility of NLP and ML in identifying thematic patterns embedded in large numbers of unstructured data. The NLP-ML method therefore represents a valuable addition to the tools of detecting and understanding HIT-related errors.",IS,HIS,85,Clear IS paper with focus on Healthcare Information Systems
Cognitive Radicalization of Right-Wing Radicals: the Experience of Thematic Analysis,"The cognitive radicalization of supporters of right-wing radical ideology is one of the stages on the way to real radicalism, up to involvement in terrorist activities. One of the elements of cognitive radicalization is media consumption, in particular, the study of content in communities of appropriate orientation on social networks. The purpose of this study is to study the thematic features of the content of supporters of right-wing radical ideology in the communities of the VKontakte social network. The study includes an analysis of the content of the most popular posts in these communities. The hypothesis of the study is that the topics of such posts make a significant contribution to the cognitive radicalization of users of social networks. The relevance of posts was determined in accordance with online message metrics (number of likes, reposts, comments). Then, through discourse analysis, the topics of the messages were determined. This allowed us to determine which topics in right-wing communities are most in demand and cause the greatest response from users. The results obtained indicate that the most popular messages are those in which certain enemies of the right are represented, primarily ""external enemies"" (a broad category that included migrants, supporters of leftist ideas, officials of various levels, patriotic bloggers, etc.). The second most popular thematic block can be conditionally described as ""their ownThis includes messages describing the image and behavior of a supporter of right-wing ideology, as well as those expressing approval of the actions of certain persons who do not belong to this subculture. Among other thematic blocks, such as ideology, humor, approved actions of politicians, events, actions, etc., are also distinguished.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
An Interview with David Alderson,"David Alderson has become a leading advocate for formulating the foundations of network science so that its predictions can be applied to real networks. He is an assistant professor in the Operations Research Department at the Naval Postgraduate School in Monterey, Calif., where he conducts research with military officer-students on the operation, attack, and defense of network infrastructure systems. Ubiquity interviewed him to find out what is going on.",CS,CN_DS,85,Clear CS paper with focus on Computer Networks & Distributed Systems
Investigating Factors Influencing the Adoption of IT Cloud Computing Platforms in Higher Education,"Cloud computing has become an emerging IT platform for productive services and efficient use of innovation in higher education. The purpose of this research study is to investigate the factors that influence the adoption of cloud computing in Sub-Saharan Africa under Ethiopian higher education. These factors professed to influence the adoption of cloud computing built on the integrated framework of technology-organization-environment (TOE) framework, diffusion of innovation (DoI) theory, and sociocultural theory. In-depth interviews have been taken with the University of Gondar key professionals in Ethiopia to accomplish this research study. The results show that the organisational factors are more influential factors than the technological, environmental, and sociocultural factors to the adoption of cloud computing (CC) in the Ethiopian higher education (EHE) sector. This qualitative study aids researchers and practitioners in the field of IT technology cloud to employ these key factors suggestive to cloud computing adoption in the higher educational institutions.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
The impact of cloud adoption on talent management: an exploratory study from the “learning organization” perspective,"Purpose Cloud computing, a dominant technology, significantly impacts organizations, necessitating talent management strategies for sustained growth. This study aims to explore the impact of cloud adoption on large French organizations through a “learning organization” perspective.   Design/methodology/approach Interviews were conducted with business and IT stakeholders from 35 multinational organizations in France.   Findings Cloud services have a high impact on large organizations, leading to a demand for cloud-related skills, a power shift from IT to business departments and increased shadow IT activities. Effective utilization requires organizational learning and a change management project, transforming organizations into productive and innovative learning organizations.   Originality/value This paper contributes to cloud computing, organizational learning and talent management literature, offering managers a novel approach to handling cloud services.",IS,DIGITAL_TRANS,85,Clear IS paper with focus on Digital Transformation
Articles on biotechnology teaching: thematic content analysis study,"This tematic content analysis is done about teaching biotechnology which is researched in Turkey. is conducted a thematic content analysis of 64 articles, from a total of 45 magazines published in Turkey from 2003 to 2018. An analysis of the research trends of Turkish researchers based on teaching biotechnology. In this research, it has been tried to guide the researchers by determining the trends in the field and research methods frequently used in this field and research methods used in this field (changes according to publication years and languages, what kind of research problems on qualitative research subjects are emphasized, what research methods are used, data collection tools, sample or working group, data analysis methods). In studies conducted to investigate the biotechnology education in Turkey, it shows that there is not adequately address the issue of biotechnology education. Key Words: Teaching biotechnology, Thematic Content Analysis, Academic Achievement",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Technology Adoption for NGO Management: a Case Study of Atma Foundation,"Mammoth leaps in technology in recent years have revolutionized industry like never before, leading to what is referred to as the 4th Industrial Revolution, or Industry 4.0. The digital transformation of the world has lent new scope to automation, collaboration and man-machine interfaces. According to Haleem et al. (2023)1, Management 4.0 increases efficiency and reduces human error in management processes, through the use of technological advancements like Artificial Intelligence, Virtual Reality, Internet of Things, Robotics etc. In this background, it is relevant to study how Non-Governmental Organisations are adapting, and how well they are poised to harness the possibilities of management 4.0. In India, new regulations by the Government have brought the onus on Non-Governmental Organisations to streamline their management especially with regard to financial transactions, compliance, monitoring etc. This increases the relevance of digital transformation for NGOs in India, and a study on its possibilities and challenges will help NGOs prepare a sustainable roadmap. This paper uses the single case study method to explore how the technological advancements of industry 4.0 are playing out for Non-Governmental Organisations. According to Gilgun, (1994)2 and Takahashi &amp; Araujo, (2019)3 when it is required to do an intensive study of a phenomenon with the aim of understanding its various aspects and repercussions, a single case study method is a useful tool. The organisation chosen here as the ‘case’ is ATMA Foundation, a non-profit voluntary organisation with 17 years’ history, working on different verticals, and hence having a wide scope for adopting better management systems. The methodology used here consists of interviews, observation and review of organisational documents. A checklist of digital tools commonly used by NGOs was prepared, and it was found that ATMA Foundation already uses or is exploring the use of more than 75% of these. The leadership team also expressed their willingness to adopt new technology. The organisational processes were reviewed and a general framework of management needs was prepared. After consultation with experts in digital technologies, the researcher identified new tools which would greatly enhance the efficiency of the organisation. It was found that the organisation is currently making good use of many tools like MS office, cloud storage, emails, Zoom, Google meet, WhatsApp, Tally Prime, Systools and OneDrive. Some of the new and/or AI powered tools identified as suitable for this organisation include Zoho Creator, Zoho CRM, Zoho Projects, Workday, Big Data, MS Office with copilot, Open AI, Bing and Bard. Challenges identified include initial cost, upskilling of employees, realigning management processes and ethical issues. Solutions to overcome these barriers ranged from software donations and pro-bono consultations to conducting change management sessions for leaders and employees. The study concluded that even small NGOs had huge potential to harness technology for Management 4.0, and the challenges involved could be overcome with foresight and planning.",IS,DIGITAL_TRANS,85,Clear IS paper with focus on Digital Transformation
Robodebt: A Socio-Technical Case Study of Public Sector Information Systems Failure,"Large-scale public sector information systems (PSIS) that administer social welfare payments face considerable challenges. Between 2014 and 2023, an Australian government agency conceived and implemented the Online Compliance Intervention (OCI) scheme, widely referred to as Robodebt. The scheme's primary purpose was to apply digital transformation in order to reduce labour costs and increase recovery of overpayments. Among its key features were a simplified, but inherently erroneous, estimation method called income averaging, and a new requirement that welfare recipients produce documentation for income earned years earlier. Failure by welfare recipients to comply with mandates resulted in the agency recovering what it asserted to be overpayments. This article presents a case study of Robodebt and its effects on over 1 million of its clients. The detailed case study relies on primary data through Senate and other government hearings and commissions, and secondary data, such as media reports, supplemented by academic sources. Relevant technical features include (1) the reliance on the digital persona that the agency maintains for each client, (2) computer-performed inferencing from client data, and (3) automated decision-making and subsequent action. This article employs a socio-technical systems approach to understanding the factors underlying a major PSIS project failure, by focusing on the system's political and public service sponsors; its participants (users); the people affected by it (usees); and the broader economic, social, and political context. Practical and theoretical insights are presented, with the intention of highlighting major practical lessons for PSIS, and the relevance of an articulated socio-technical frame for PSIS.",IS,IT_ETHICS,85,Clear IS paper with focus on IT Ethics & Social Responsibility
Case study research: a multi‐faceted research approach for IS,"Abstract.  A wide variety of approaches can be applied under the heading of case study research; this paper explicitly discusses the range of the alternatives. Many papers discussing case study research emphasize one particular variation of case study research only. The current paper provides an overview of the various uses of case study research in the information systems field by describing the different ways in which case study research can be used, using examples from published IS literature for illustration, and providing references to other method papers for more detailed discussion of each alternative. Researchers are reminded that case study research can be used in the positivist and interpretivist traditions, for testing or building theory, with a single or multiple case study design, using qualitative or mixed methods. The range of case study research alternatives makes it a highly versatile research strategy for IS",IS,IS_RM,75,Matched IS paper to Information Systems Research Methods
Multiparty sensemaking: A technology‐vendor selection case study,"AbstractInformation system (IS) procurement history is replete with poorly executed, multimillion dollar procurement decisions. Yet we barely understand what effective IS procurement should look like. IS procurement is highly challenging, as it requires the client to simultaneously select a technology and vendor. This paper explores the technology‐vendor selection process through the sensemaking perspective. Our study develops a sensemaking model for technology‐vendor selection that connects the multiple rounds of client‐vendor communicative actions with the client's sensemaking process. We show how the client reconciles fragmented and sometimes conflicting cues and information through three intertwined cycles: immediate, retrospective, and decision. Sensebreaking occurs as a separate process (and not a communicative action) when disruptive cues occur persistently and from different vendors over multiple rounds of sensemaking. We derive a set of critical factors, on the basis of the sensemaking perspective, for selecting an appropriate vendor and technical solution. These insights in turn help explain many poorly executed IS procurement decisions.",IS,IT_PM,85,Clear IS paper with focus on IT Project Management
Testing geographical information systems: a case study in a fire prevention support system,"PurposeThe purpose of this paper is to describe the development and evaluation of a geographical information system (GIS) testing framework that was used to test a fire prevention support GIS.Design/methodology/approachA year‐long case study was undertaken concerning the testing of a fire prevention support GIS in a UK fire and rescue service.FindingsThe GIS testing framework developed involved testing the different components of a GIS, testing their interactions, and then testing the system as a whole. Since GISs contain different components such as spatial analyses and map‐based output, this supports the adoption of a different testing framework compared to existing types of information systems.Research limitations/implicationsGISs will typically be used by organisations for decision making. Clearly if the information presented by a GIS is inaccurate, unrepresentative, or unreliable, then the decision‐making process can be undermined.Practical implicationsThis is particularly important with regard to GISs used by emergency services (such as the fire and rescue service studied) where lives could potentially be put at risk by erroneous information provided by such systems.Originality/valuePrevious research had indicated that GISs may be inadequately tested. The framework developed for GISs testing provided a systematic testing approach, reducing the likelihood of errors in such systems.",IS,DSS,85,Clear IS paper with focus on Decision Support Systems
A Case Study For Accounting Information Systems A Business Continuity Plan For Protecting Critical Financial Information In The NYC Financial Services Industry,"This case study outlines a project launched by the Wall Street West organization, a data redundancy system in Northeastern Pennsylvania which provides backdrop for financial institutions located in New York City. The purpose of this study is threefold. First, the history on the importance of business continuity plans in a post 9/11 world is explored. Second, the Federal Reserve Board, the Office of the Comptroller of the Currency, and the Securities and Exchange Commission recommendations regarding Disaster Recover, in addition to the requirements of The Sarbanes-Oxley Act, are reviewed. Lastly, an overview of Wall Street Wests effort is provided, looking at some of the strategic advantages to locate in Northeastern Pennsylvania and demonstrating the important resources provided by Wall Street West to protect the nations national security. Conclusions and case use recommendations are presented as this case is ideally suited for use in an Accounting Information Systems course at either the undergraduate or graduate level creating an awareness of the importance of business continuity planning.",IS,IT_GOV_STRAT,85,Clear IS paper with focus on IT Governance & Strategy
Agricultural information systems: a national case study,"PurposeThe purpose of the paper is to investigate the agricultural information system in Turkey, with particular reference to the effectiveness of this system for farmers.Design/methodology/approachA case study based on a review of the literature, established knowledge and national experience to date.FindingsThat, in Turkey, there is insufficient connection between the publishing activities of research institutions and other institutions active in the field. This lack of coordination causes an incomplete distribution of agricultural information to farmers. In particular, this creates an information system in which there is no effective feedback in the “research–publishing–farmer” triangle. Yet distribution of agricultural information to users and reciprocal user feedback is vital, because it is the essential mechanism by which a consistently reliable and effective distribution of information can be maintained.Research limitations/implicationsAlthough the central thesis of the paper is not advanced by reference to original research on the part of the authors, it is based on pre‐existing, well respected research which is intelligently interpreted and authoritatively synthesized by them.Practical implicationsTo solve problems of agricultural information flow, the lack of coordination among the various organizations concerned has to be dealt with effectively, and a single organization has to be set up where information is collected in and distributed from the center. Non‐public publishing and research services have to be supported and encouraged in parallel with this.Originality/valueThe paper advances a clear plan of action for improving the information system in an area of great relevance to all developing countries.",IS,KM,85,Clear IS paper with focus on Knowledge Management
"Online Legal Information Systems in India: a Case Study from the Faculty of Law, University of Delhi","AbstractIn this digital age, users require immediate access to information. To foster the process of research, the legal fraternity demands efficient online legal information systems. Raj Kumar Bhardwaj provides a view from India and reports on a case study that has been conducted on the use of various legal information databases in the Faculty of Law, University of Delhi, India. In his paper, he also reviews and discusses the various aspects relating to legal information retrieval systems, with particular reference to the various essential legal databases that cover Indian law.",IS,KM,85,Clear IS paper with focus on Knowledge Management
Study on an Information Systems for Visualizing the Effectiveness of Local Cleaning Activities: Case Study of Iwate Prefecture in Japan,"The microplastics problem requires consideration according to local characteristics. This study is a case study of Iwate Prefecture in Japan. In order to understand the current conditions of collection and cleaning activities for coastal and river debris, Iwate prefectural residents need to be able to visualize the activities they perform in their daily lives, and the system for this is yet undeveloped. The research team designed the system with reference to the stages in SSM. And based on the systems design, we prototyped two systems that focus on data posting for users. After evaluating two prototype applications, we defined the data sets that users would contribute during and after the activities. This paper is useful for areas with similar characteristics.",IS,SOC_COMP,85,Clear IS paper with focus on Social Computing & Collaboration
Examining Information Systems Outsourcing: A Case Study from the United Kingdom,"This paper examines the issue of outsourcing information services (IS) through the framework of case study-based, grounded research, located in a major UK company. The impact of outsourcing drivers including efficiency, IS alignment and the human resource dimension are explored and balanced against the perceived outsourcing risks. It is also postulated that although managers claim rational economic benefits, when making outsourcing decisions, they may, in fact, be bounded in their rationality by their perception of the quality experienced as users of IS/IT. Hence, it is suggested that the real justification for outsourcing may be that it provides a definitive, albeit drastic, vehicle for change.",IS,IT_GOV_STRAT,85,Clear IS paper with focus on IT Governance & Strategy
"Successfully completing case study research: combining rigour, relevance and pragmatism","The organizational and social issues associated with the development, implementation and use of computer‐based information systems have increasingly attracted the attention of information systems researchers. Interest in qualitative research methods such as action research, case study research and ethnography, which focus on understanding social phenomena in their natural setting, has consequently grown. Case study research is the most widely used qualitative research method in information systems research, and is well suited to understanding the interactions between information technology‐related innovations and organizational contexts. Although case study research is useful as a means of studying information systems development and use in the field, there can be practical difficulties associated with attempting to undertake case studies as a rigorous and effective method of research. This paper addresses a number of these difficulties and offers some practical guidelines for successfully completing case study research. The paper focuses on the pragmatics of conducting case study research, and draws from the discussion at a panel session conducted by the authors at the 8th Australasian Conference on Information Systems, September 1997 (ACIS 97), from the authors' practical experiences, and from the case study research literature.",IS,IS_RM,75,Matched IS paper to Information Systems Research Methods
Information Systems in Bangalore Traffic: A Case Study,"Year 2012, month of March, Monday morning 8.30 a.m. Ramesh, a techie1in his mid 30s with one of the popular software companies in Bangalore was on his way to office. He was waiting for the traffic signal to turn green in one of the busiest junctions of Bangalore city.",IS,BPM,85,Clear IS paper with focus on Business Process Management
Managing information systems’ contingencies in banks: a case study,"As information systems (IS) become indispensable, EDP disaster recovery has to become an essential IS function. In this paper, after discussing the criticality of sound planning for information systems disaster recovery in banks, the approach to contingency planning adopted by Banca Commerciale Italiana, one of the largest banks of Italy, is presented. The most crucial point of the whole decision‐making process was the selection of the most reliable layout configuration. Hence, the steps and the simulation methodology followed to such a purpose are illustrated. Finally the benefits and pitfalls of the solution adopted in Banca Commerciale Italiana are discussed.",IS,ISM,85,Clear IS paper with focus on Information Systems Management
Evaluating Case Study and Action Research Reports: Real-world Research in Cybersecurity,"There is a growing number of scientific papers reporting on case studies and action research published each year. Consequently, evaluating the quality of pilling up research reports is becoming increasingly challenging. Several approaches for evaluation of quality of the scientific outputs exist however they appear to be fairly time-consuming and/or adapted for other research designs. In this paper, we propose a reasonably light-weight structure-based approach for evaluating case study and action research reports (SAE-CSAR) based on eight key parts of a real-world research report: research question, case description, data collection, data analysis, ethical considerations, results, discussion and limitations. To evaluate the feasibility of the proposed approach, we conducted a systematic literature survey of papers reporting on real-world cybersecurity research. A total of N = 102 research papers were evaluated. Results suggest that SAE-CSAR is useful and relatively efficient, and may offer a thought-provoking insight into the studied field. Although there is a positive trend for the inclusion of data collection, data analysis and research questions in papers, there is still room for improvement suggesting that the field of real-world cybersecurity research did not mature yet. The presence of a discussion in a paper appears to affect most its citation count. However, it seems that it is not uniformly accepted what a discussion should include. This paper explores this and other issues related to paper structure and provides guidance on how to improve the quality of research reports.",CS,CYBER_SEC,85,Clear CS paper with focus on Cybersecurity & Information Security
Combining Phenomenology and Grounded Theory in Software Engineering: An Experience,"Phenomenology and grounded theory are two prominent qualitative methods, particularly used in social sciences research. Phenomenology is carried out to understand the individuals’ actual experience regarding a phenomenon. The method describes ""what"" individuals experience and ""how"" they experience it. The focus is on the meaning of the exposures experienced by individuals regarding the phenomenon. Grounded theory on the other hand allows researchers to explore a phenomenon in depth with individuals, by which a theory is then generated. The goal is to go beyond the understanding of phenomenon by producing a theory that describes comprehensively the problem being studied. Although these two methods are initiated by similar motivations, namely to understand a phenomenon, they however employ slightly different approaches during execution. The differences make them fit complementary together to produce a more concrete and holistic outcome. To research that occasionally use qualitative methods such as Software Engineering, these methods bring new and multifaceted experience. Software Engineering research opts for qualitative methods to promote understanding, as many phenomena in the field have yet to be understood by its community. In that respect, grounded theory is becoming quite a norm in Software Engineering research in recent years, phenomenology however is relatively sporadic. This paper shares the experience of employing as well as combining phenomenology and grounded theory in Software Engineering research. The sharing is intended to inspire future research in twofold: more technical fields such as Software Engineering to employ qualitative methods in research; and leveraging the benefits of combining two qualitative methods complementarily in one study.",CS,SW_ENG,85,Clear CS paper with focus on Software Engineering & Development
Nurses as Stakeholders in the Adoption of Mobile Technology in Australian Health Care Environments: Interview Study,"Background The 2017 Australian Digital Health Agency (ADHA) Strategy is based on the underlying assumption that digital technology in health care environments is ubiquitous. The ADHA Strategy views health professionals, especially nurses, as grappling with the complexity of installing and using digital technologies to facilitate personalized and sustainable person-centered care. Yet, ironically, the 2018 debate over how to enroll Australians into the national electronic health record system and its alteration from an opt-in to an opt-out model heightened public and professional concern over what constituted a “safe, seamless and secure” health information system. What can be termed a digital technology paradox has emerged where, although it is widely acknowledged that there are benefits from deploying and using digital technology in the workplace, the perception of risk renders it unavailable or inaccessible at point of care. The inability of nurses to legitimately access and use mobile technology is impeding the diffusion of digital technology in Australian health care environments and undermining the 2017 ADHA Strategy.   Objective This study explored the nature and scope of usability of mobile technology at point of care, in order to understand how current governance structures impacted on access and use of digital technology from an organizational perspective.   Methods Individual semistructured interviews were conducted with 6 representatives from professional nursing organizations. A total of 10 interview questions focused on factors that impacted the use of mobile technology for learning at point of care. Seven national organizations and 52 members from the Coalition of National Nursing and Midwifery Organisations were invited to participate. Interviews were recorded and transcribed verbatim. Data analysis was systematic and organized, consisting of trial coding; member checking was undertaken to ensure rigor. A codebook was developed to provide a framework for analysis to identify the themes latent in the transcribed data. Nurses as stakeholders emerged as a key theme.   Results Out of 6 participants, 4 female (67%) and 2 male (33%) senior members of the nursing profession were interviewed. Each interview lasted between 17 and 54 minutes, which reflected the knowledge of participants regarding the topic of interest and their availability. Two subthemes, coded as ways of thinking and ways of acting, emerged from the open codes. Participants provided examples of the factors that impacted the capacity of nurses to adopt digital technology from an emic perspective. There were contributing factors that related to actions, including work-arounds, attentiveness, and experiences. Nurses also indicated that there were attitudes and influences that impacted thinking regarding access and use of mobile technology at point of care.   Conclusions Nurses are inadequately prepared for the digital future that has now arrived in health care environments. Nurses do not perceive that they are leaders in decision making regarding digital technology adoption, nor are they able to facilitate digital literacy or model digital professionalism.",IS,HIS,85,Clear IS paper with focus on Healthcare Information Systems
Sociocultural Barriers for Female Participation in STEM: A Case of Saudi Women in Cybersecurity,"The participation of women in Science, Technology, Engineering, and Mathematics (STEM) workforces is overwhelmingly low as compared to their male counterparts. The low uptake of cybersecurity careers has been documented in the previous studies conducted in the contexts of the West and Eastern worlds. However, most of the past studies mainly covered the Western world leaving more knowledge gaps in the context of Middle Eastern countries such as Saudi Arabia. Thus, to fill the existing knowledge gaps, the current study focused on women in Saudi Arabia. The aim of the study was to investigate the factors behind the underrepresentation of Saudi women in the cybersecurity space by specifically targeting the existing socio-cultural barriers. The study used a qualitative design that entailed reliance on both primary interview data and additional evidence from prior literature to evaluate the barriers faced by Saudi women in cybersecurity. A sample of 15 Saudi women aged 18 – 30 years with a college education or still in college pursuing a course in IT (Information Technology) or had basic computer literacy skills was purposefully recruited as the most desirable participants. A thematic analysis process was conducted on the primary data to generate theory from the findings, further compared with and verified based on a critical literature review. The themes that were generated from the interviews include lack of autonomy, family responsibilities, female as the weaker gender, and child bearing and caring duties.",IT,IT_SEC_RISK,85,Clear IT paper with focus on IT Security & Risk Management
Barriers and facilitators to the adoption of electronic clinical decision support systems: a qualitative interview study with UK general practitioners,"Abstract Background Well-established electronic data capture in UK general practice means that algorithms, developed on patient data, can be used for automated clinical decision support systems (CDSSs). These can predict patient risk, help with prescribing safety, improve diagnosis and prompt clinicians to record extra data. However, there is persistent evidence of low uptake of CDSSs in the clinic. We interviewed UK General Practitioners (GPs) to understand what features of CDSSs, and the contexts of their use, facilitate or present barriers to their use.  Methods We interviewed 11 practicing GPs in London and South England using a semi-structured interview schedule and discussed a hypothetical CDSS that could detect early signs of dementia. We applied thematic analysis to the anonymised interview transcripts.  Results We identified three overarching themes: trust in individual CDSSs; usability of individual CDSSs; and usability of CDSSs in the broader practice context, to which nine subthemes contributed. Trust was affected by CDSS provenance, perceived threat to autonomy and clear management guidance. Usability was influenced by sensitivity to the patient context, CDSS flexibility, ease of control, and non-intrusiveness. CDSSs were more likely to be used by GPs if they did not contribute to alert proliferation and subsequent fatigue, or if GPs were provided with training in their use.  Conclusions Building on these findings we make a number of recommendations for CDSS developers to consider when bringing a new CDSS into GP patient records systems. These include co-producing CDSS with GPs to improve fit within clinic workflow and wider practice systems, ensuring a high level of accuracy and a clear clinical pathway, and providing CDSS training for practice staff. These recommendations may reduce the proliferation of unhelpful alerts that can result in important decision-support being ignored.",IS,HIS,75,Matched IS paper to Healthcare Information Systems
Barriers and facilitators to the adoption of electronic clinical decision support systems: a qualitative interview study with UK general practitioners,"Background Well-established electronic data capture in UK general practice means that algorithms, developed on patient data, can be used for automated clinical decision support systems (CDSSs). These can predict patient risk, help with prescribing safety, improve diagnosis and prompt clinicians to record extra data. However, there is persistent evidence of low uptake of CDSSs in the clinic. We interviewed UK General Practitioners (GPs) to understand what features of CDSSs, and the contexts of their use, facilitate or present barriers to their use. Methods We interviewed 11 practicing GPs in London and South England using a semi-structured interview schedule and discussed a hypothetical CDSS that could detect early signs of dementia. We applied thematic analysis to the anonymised interview transcripts. Results We identified three overarching themes: trust in individual CDSSs; usability of individual CDSSs; and usability of CDSSs in the broader practice context, to which nine subthemes contributed. Trust was affected by CDSS provenance, perceived threat to autonomy and clear management guidance. Usability was influenced by sensitivity to the patient context, CDSS flexibility, ease of control, and non-intrusiveness. CDSSs were more likely to be used by GPs if they did not contribute to alert proliferation and subsequent fatigue, or if GPs were provided with training in their use. Conclusions Building on these findings we make a number of recommendations for CDSS developers to consider when bringing a new CDSS into GP patient records systems. These include co-producing CDSS with GPs to improve fit within clinic workflow and wider practice systems, ensuring a high level of accuracy and a clear clinical pathway, and providing CDSS training for practice staff. These recommendations may reduce the proliferation of unhelpful alerts that can result in important decision-support being ignored.",IS,HIS,85,Clear IS paper with focus on Healthcare Information Systems
Perspectives on the enablers of e-heath adoption: an international interview study of leading practitioners,"Studies examining the application of information technology to the delivery of health-care services often highlight the anticipated benefits. In consequence, the benefits of health-care information technology adoption, often referred to as ‘e-health’, are widely reported yet there is limited empirical evidence as to how such benefits can be realized. Design and implementation guidelines have been considered from a socio-technical perspective and there is support for the successful application of these principles. There are also some global surveys on the topic, but these often report only statistical data and lack richness of content. This study draws on existing literature to examine whether the principles of health-care information technology adoption are currently applied in practice. The paper presents a timely international analysis of the drivers, critical enablers and successful deployment strategies for e-health from the perspective of leading practitioners. The study considers the adoption of e-health in 15 countries. A qualitative research design was used and semistructured interviews were conducted with 38 thought leaders with expertise in health-care information systems and technology. The study presents a comparative analysis of the lessons learned from implementing, integrating and embedding e-health in practice, and presents a four-phase approach from the perspective of practitioners for the accelerated deployment of e-health systems: (i) develop a strategic approach, (ii) engage the workforce, (iii) capitalize on information technology and (iv) partner with the patient/citizen.",IS,HIS,85,Clear IS paper with focus on Healthcare Information Systems
Perceived barriers and opportunities to the use of 3D printing in a healthcare system with low adoption: A semi-structured interview study,"Background The application of Three-D (3D) printing in medicine is gaining momentum. Although its use is being adopted in healthcare in some regions, other countries have not embraced 3D printing widely, with relatively low adoption. It is important to gain an understanding of the barriers and opportunities to the acceptance of 3D printing technology in low adoption healthcare systems, to understand why usage is low, and to consider how to support its use where appropriate. The region chosen for the study was the Republic of Ireland, a high-income country with low adoption of 3D printing in healthcare. Methods An interview study was conducted to identify barriers and opportunities to the use of 3D printing in this healthcare system and employed a qualitative descriptive approach. Purposeful and snowball sampling was used to recruit participants from diverse stakeholders working in both public and private healthcare systems. Semi-structured interviews were conducted with ten healthcare professionals. Audio recordings were recorded verbatim using Otter AI and thematic analysis was performed using NVivo utilising the Braun and Clarke framework. Respondents included doctors, nurses, occupational therapists, physiotherapists, and a hand therapist. Results Three main themes were identified: Theme 1: Needs within the health service that could be supported by 3D printing, Theme 2: Barriers to adoption of 3D printing in the healthcare service, and Theme 3: Opportunities to support the adoption of 3D printing use in healthcare. Conclusions Research is required on these barriers to ensure patients accessing healthcare systems are provided with the same opportunities to receive personalised, cutting-edge care as their international counterparts who currently adopt this technology.",IS,HIS,85,Clear IS paper with focus on Healthcare Information Systems
Case Study: The Impact Of Emerging Technologies On Cybersecurity Education And Workforces,"A qualitative case study focused on understanding what steps are needed to prepare the cybersecurity workforces of 2026-2028 to work with and against emerging technologies such as Artificial Intelligence and Machine Learning. Conducted through a workshop held in two parts at a cybersecurity education conference, findings came both from a semi-structured interview with a panel of experts as well as small workgroups of professionals answering seven scenario-based questions. Data was thematically analyzed, with major findings emerging about the need to refocus cybersecurity STEM at the middle school level with problem-based learning, the disconnects between workforce operations and cybersecurity operators, the distrust of Non-Traditional Training Programs, and the need to build digital security generalists’ curriculum and training. Recommendations are also made for possible next steps.",IT,IT_TRAINING,85,Clear IT paper with focus on IT Training & Education
Researching ERP adoption: an internet‐based grounded theory approach,"PurposeThis research seeks to investigate the introduction of new information and communication technology systems and to describe the development of a conceptual model of enterprise resource‐planning systems adoption based on the published rationales organizations use to justify their adoption.Design/methodology/approachThe study uses a grounded theory approach to building the conceptual model from electronically distributed documents. These documents were selected from a sample of universities which adopted enterprise resource‐planning systems.FindingsThis paper reports on the use of grounded theory in the internet context. The study found that there were strong similarities between justifications and reported motives. The study noted that justifications concerning financial, work‐life and organisational‐mission issues were relatively minor.Research limitations/implicationsThis model is built on published justifications, which should not be confused with motives. This picture may distort reality by over‐emphasising some rationales and under‐representing others.Practical implicationsThis paper may be of interest to researchers considering the use of grounded theory in their research project.Originality/valueThis paper describes how grounded theory was used to construct a model of the rationales for adopting enterprise resource‐planning systems from electronically sourced documents. The paper is of interest to researchers in information systems and those conducting grounded theory research on the internet.",IS,ES_ERP,75,Matched IS paper to Enterprise Systems & ERP
The digital transformation in the psychology of workplace spirituality,"PurposeThere is evidence that spirituality at the workplace has positive effects on work outcomes, and there are different models conceptualizing the construct. To date, there is no discussion highlighting how digitalization is affecting workplace spirituality and vice versa. The present review tries to close this gap by discussing the psychological dynamics in light of digitalization and spirituality in the context of work.Design/methodology/approachThis is a conceptual discussion based on an extensive narrative review. The conceptual design is further tested with a real-life case study.FindingsThe result is a model that may guide future research, which consists of the four highly interdependent domains, namely psychology (with the dimensions of emotion, cognition and behavior), digitalization (with the dimensions of platforms, data and algorithms), spirituality (with the dimensions of meaning, self-transcendation and belonging), as well as the workplace (with the dimensions of work tasks, location, community and culture and values). The discussion includes implications for the future of work, suggestions for management decisions and potential future research directions.Originality/valueTo date, there are many discussions about digital transformation and a limited amount of them have invested in analyzing psychological dimensions. The application to spirituality and the workplace – especially when the two are combined – is almost wholly absent, which makes the present discussion both innovative and original.",IS,DIGITAL_TRANS,85,Clear IS paper with focus on Digital Transformation
Action research literature 2004-2006,"This review of recent action research books covers the period from about mid-2004 to mid-2006, complementing an earlier review (Dick, 2004). After noting some important recent additions to the action research literature, I address the literature on several different applications of action research including education, community, participatory development, and organizations. There are briefer sections on other topics. Action research journals and special issues of other journals are also identified. Finally, I identify some themes and trends in the action research literature.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Digital transformation preparedness: an exploratory study,"PurposeDigital transformation (DT) harnessing the potential of emerging technology creates opportunities and challenges for organizations worldwide. Senior executives view DT as a key initiative for future competitiveness, a view shared by academic researchers. What may challenge the organization is that the vision may be present while preparedness may be lacking. Organizational preparedness depends on managers and employees charged with implementing DT and their perceptions on preparedness are often not aligned with senior executives.Design/methodology/approachIn this research, the authors explore the perceptions of managers and employees on DT preparedness in an organization by gathering data from 579 participants. This study uses an innovative approach to qualitative data analysis using interactive topic modeling.FindingsFindings in this qualitative study provide valuable insights on the perceptions of these individuals and helps understand (a) how they view DT preparedness and (b) may behave in this context. In general DT is well understood, however managers are not keen to change work processes to take advantage of the new digital tools and there appears that generational gap is a barrier to successful DT.Originality/valueSenior executives play a central role communicating the DT vision necessary to inspire managers and employees. As organizations continue to invest large sums of money to explore value creation for customers and stakeholders by leveraging digital technologies, the information systems (IS) discipline can take the lead by asking the question, what can be done to improve the understanding of DT implementation in an organization?",IS,DIGITAL_TRANS,85,Clear IS paper with focus on Digital Transformation
‘Different isn’t free’: Gender @ work in a digital world,"US society is thoroughly computerized and the majority of its population engages in activities involving computers. Why, then, does computer science and engineering (CSE) remain highly male-dominated and seemingly impervious to desegregation? This study explores how CSE professionals in corporations and universities navigate and subvert male hegemony to persist. I document practices in CSE that reproduce the ideological union between masculinity and competency, including hazing, bragging, and bullying. These practices, much like rites of passage, also serve to indoctrinate CSE workers to the core values in computing knowledge production, including constant observation, combative work styles, and male hegemony, all of which differentially impact women. Women who persist in CSE describe their experiences as wearisome, constrained by a fear of being different, and thus further marginalized. I argue that processes and value systems by which people become computing professionals reflect a gendered, technocratic culture, one that reproduces labor segregation in CSE.",CS,HCI,75,Matched CS paper to Human-Computer Interaction
Grounded theory approach in nursing practice,"Grounded theory is a methodological approach that aims to generate a theory that is grounded in systematically gathered data and its analysis through an inductive process. Annells (1997) defined grounded theory methodology as a qualitative approach to an inquiry that is embedded in relativist ontology and subjectivist epistemology. The purpose of grounded theory is to generate a theory that is rooted in the participant's perspective involved in the study. This paper aims to explicate the ontological and epistemological points of view of the constructivist paradigm. The constructivist paradigm has a relativist ontology and a subjective epistemology (Guba &amp; Lincoln, 1994). A critique of grounded theory methodology is discussed inconsideration of diverse factors such as health and power-related inequalities, environment, social context, culture, gender, and social status. In nursing research, grounded theory is increasingly used by researchers. This paper will highlight the importance of grounded theory research for nursing practice and the generation of theories that are rooted in real clinical practices (Lazenbatt &amp; Elliott, 2005).",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
A Grounded Theory and Collaborative Design Approach to Disability Storytelling on TikTok,"This developing dissertation explores the use of TikTok as a platform for individual and collective storytelling and information creation practices – within a specific online health community of people experiencing painful, invisible, and difficult to diagnose central sensitivity syndromes (CSSs) – to understand and support these embodied, creative, and collective information behaviors. Ongoing data collection indicates that people with CSSs are using TikTok affordances to tell and scaffold complex micro-stories about their expertise and social experiences of disability: by employing iconographic elements to make disability visual; intimate cinematography; audio, visual, and community-specific mimetic options; and platform-specific novel feature use. The research design draws upon critical disabilities studies (CDS) sensitizing concepts. A constructivist grounded theory approach will be employed, by theoretically sampling TikTok micro-videos, their top comments, and, by the time of this presentation, conducting semi-structured interviews with CSS TikTok community members. This poster also discusses these preliminary results, as well as a novel initial sampling approach which addresses both the hashtag and algorithmic logics of the platform, and an implementation of feminist ethics of care in research methods. Then, three codesign workshops with individuals experiencing CSSs will develop creative storytelling materials that can be utilized in various contexts. These workshops promote the inclusion of disabled community members as co-designers and aim to co-design physical and digital storytelling resources such as prompts, templates, and TikTok features.These findings expand storytelling theory into the health domain, introduce and define algorithmically mediated online health communities, and promote critical disability studies perspectives in information science.",IS,SOC_COMP,85,Clear IS paper with focus on Social Computing & Collaboration
Methodological Combinations in Qualitative Research in Music Education: Autobiography and Grounded Theory,"This article explores the theoretical and methodological approaches of (auto)biographical qualitative research and grounded theory, based on their historical and epistemological characteristics. This study in music education aimed to gain an understanding of the process of becoming a primary school music teacher. The Supervised Internship course in teacher education for a music degree was analyzed using the methodological and formative (auto)biographical dispositifs of the Biographical Project Workshop (Delory-Momberguer, 2006) and situational analysis (Clarke, 2005), constructed through the students’ oral, written, and musical narratives based on their experiences as music teachers in the public elementary schools. As a result of the methodological combination, it was found that the research procedure was a collaborative process between the researcher and the participants. This approach allowed for theorizing that did not decontextualize the trainees' narratives. It is recommended to consider using this combination with a postmodern perspective on grounded theory due to epistemological compatibility. Further exploration of the empirical field and reflexivity could contribute to the advancement of qualitative research methodology.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Theory in Emancipative Action: Aligning Action Research in Information Systems Education with Critical Social Research in Information Systems,"As educators, we want to guide our students so that they develop to the best of their ability and are emancipated. As researchers in education, we often use action research. We use proven theories to guide our intervention to emancipate our students. Or do we? Recently, prominent information systems journals have published few papers in the field of information systems education. We demonstrate that the guidelines for action research from a critical social research perspective in information systems are not evident in action research studies in information systems education. The emancipative goals of pure critical social research and reliance on critical social theory to guide our intervention are lacking in these educational studies. Our aim is to provide alignment between educational action research in information systems and information systems research conducted from a critical social theory perspective. Our methodology is to explicitly propose phases of action research from a critical social research perspective, grounded both in information systems and education literature. Then, we demonstrate the value of this approach in a study on the improvement of a data warehousing module. We conclude that by using proven theories and reflecting on the presuppositions in a problem environment, the researcher is able to guide the development of students and the community.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Knowledge systems and value creation,"PurposeThe paper aims to explore the role of knowledge management systems (KMS) in promoting value creation in the construction sector.Design/methodology/approachAn action research methodology using a multiple case study approach, which includes participant observation and semi‐structured interviews.FindingsThe findings indicate that KMS promote value creation when they embed and nurture the social conditions that bind and bond team members together. Also, to be effective KMS should be incorporated within a change management programme that promotes a “participatory” type of culture while taking into account the team‐based structure and discipline‐oriented nature of the construction industry. Therefore, much more consideration should be given to organisational change issues prior to deployment of KMS.Research limitations/implicationsThe study is limited to organisations from the construction industry, but can be generalised to organisations from other sectors that exhibit similar characteristics.Practical implicationsThe findings can be used to guide management teams in deploying KMS to foster value creation as part of a wider change management programme.Originality/valueWhile related research tends to adopt an objectivist or subjectivist approach to knowledge management (KM), the present research argues that a third approach is required where issues related to technology, culture, and organisation must be blended successfully to address complex organisational barriers to effective KM leading to value creation.",IS,KM,85,Clear IS paper with focus on Knowledge Management
Collaborating on Multiparty Information Systems Development Projects: A Collective Reflection-in-Action View,"Growth of Web-based applications has drawn a great number of diverse stakeholders and specialists into the information systems development (ISD) practice. Marketing, strategy, and graphic design professionals have joined technical developers, business managers, and users in the development of Web-based applications. Often, these specialists work for different organizations with distinct histories and cultures. A longitudinal, qualitative field study of a Web-based application development project was undertaken to develop an in-depth understanding of the collaborative practices that unfold among diverse professionals on ISD projects. The paper proposes that multiparty collaborative practice can be understood as constituting a “collective reflection-in-action” cycle through which an information systems (IS) design emerges as a result of agents producing, sharing, and reflecting on explicit objects. Depending on their control over the various economic and cultural (intellectual) resources brought to the project and developed on the project, agents influence the design in distinctive ways. They use this control to either “add to,” “ignore,” or “challenge” the work produced by others. Which of these modes of collective reflection-in-action are enacted on the project influences whose expertise will be reflected in the final design. Implications for the study of boundary objects, multiparty collaboration, and organizational learning in contemporary ISD are drawn.",IS,SOC_COMP,85,Clear IS paper with focus on Social Computing & Collaboration
Group Support Systems and Action Research,"Research in Information Systems, specifically Group Support Systems (GSS), lends itself well to Action Research. From its original definition in 1985, GSSs have included a human component, the facilitator thereby providing an excellent venue for action research. This article proposes that action research acts as a key component in the knowledge accrual process for information systems research. The article demonstrates this proposal by developing a four-phased empirical-action research cycle based on McGrath et al.’s (1982) empirical research cycle, Baskerville’s (1997) action research cycle, and the call to include the practitioner’s view in empirical research as posited by Kerlinger’s (1986) suggested goals for non-experimental studies. Finally, the proposed empirical-action research cycle is applied directly to a GSS field study wherein a group uses a GSS for a real-world problem of business process reengineering. The details of the study are discussed using the four phases in the model.",IS,SOC_COMP,75,Matched IS paper to Social Computing & Collaboration
Ethical Frameworks and Regulatory Governance,"[Purpose] This article analyzes the Colombian Artificial Intelligence (AI) Strategy and seeks to answer the formulation process of Colombia’s digital transformation and Artificial Intelligence strategy. [Methodology/approach/design] The study utilizes a case study analysis approach to explore factors that may have influenced the Colombian AI policy formulation process. We conducted an elite interview and documentary research on public policy documents. [Findings]. This study identifies the inherent challenges of the national strategy for artificial intelligence. The implementation of the Colombian AI Strategy is guided by a market-oriented state model that promotes self-regulation regarding AI. [Practical implications] National strategies are an increasingly important theme of scholarly debate in AI policy. This article aims to contribute to analyzing such strategies, including the challenges inherent in their design and implementation. [Originality/value] This article examines how Latin America’s third most populous country, Colombia, shapes AI governance strategies. The uniqueness of this study lies in its proposition to outline the model of AI governance based on a detailed analysis of public policies.",IS,IT_ETHICS,85,Clear IS paper with focus on IT Ethics & Social Responsibility
“Focus Groups Can Be Fun”: The Use of Activity-Oriented Questions in Focus Group Discussions,"Interest in focus group discussions has grown recently, and so has the recognition of them as a valuable method for qualitative data collection. Despite increasing popularity, they are not an easy option, and moderators must find appropriate ways to approach participants to achieve good-quality data. A path to reach this aim is the inclusion in the focus group agenda of some “exercises” (or activity-oriented questions) that are enjoyable and productive supplements to questions. Exercises provide a different way of gathering information and are beneficial, for instance, for more reflective participants. They can help focus the group's attention on the core study topic and also make subsequent comparative analysis more straightforward. They can also be helpful with young people and to discuss sensitive topics. The author describes and provides suggestions for use and examples of several exercises, illustrating their application in a research project investigating the cultural meaning of youth suicide in university students in Italy, India, and Australia.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Focus groups - main determinants and application possibilities,"A focus group is a 90–120-minute semi-structured conversation between 7-12 participants selected according to some relevant characteristics. The main purpose of the focus group is to determine the attitudes, feelings, beliefs, experiences, and reactions of the participants in a way that is not achievable using other methods. The paper presents a comprehensive overview of the basic features of the focus group as a qualitative research method and an analysis of their application with an emphasis on application in education research. The paper, in addition to theoretical analysis by reviewing the relevant literature, provides a critical review and recommendations for further research on this issue, which contributes to clarifying the purpose and encouraging the application of the focus group method.",IS,ED_TECH_ELEARN,85,Clear IS paper with focus on Educational Technology & E-learning
Users As Developers In Information System Projects,"Users have been described as necessary experts in information system developments. This research introduces a viewpoint that the users are the main actors in development projects and the other participants only give their experience for the use of the actual developers. In addition to the strong involvement of users, our research emphasises the special nature of the information system project with earlier-made specifications. This article suggests that in order to achieve a successful output, a reflective and flexible working process is needed. This suggestion is valid especially in a case that is out of the line of common approaches that are described in the literature. The research approach in this study was qualitative and the empirical material was gathered from a case study. The approach was subjective and it necessitated interpretation when analysing the results. The case included an information system development that was carried out to produce an inter-organisational information system to support certain functionality between organisations. Despite the output was an information system, we argue that the approach with active users is also applicable in the development of any other artefact.",IS,IT_PM,85,Clear IS paper with focus on IT Project Management
Involving lead users in firm’s standardization strategy within action groups: evidence from smart robotics,"PurposeThis research explores how firms manage the complex technologies standardization in action groups. It considers the strategic issues that technology producers face when involving lead users in architecture design. Drawing on the multi-mode standardization literature, this study addresses two dilemmas regarding value creation and appropriation by technology producers within coalitions. The first dilemma is how to create value by developing solutions in compliance with industry standards. The second one is how to appropriate value while ensuring the technology sharing with action groups. The answers to these two dilemmas contribute to filling the research gap on value creation and appropriation in multi-mode standardization.Design/methodology/approachThe research focuses on technology producers participating in action groups where lead users play a crucial role. We conducted a qualitative analysis based on the standardization experience of a Japanese company specializing in smart robotics. Data are collected through semi-structured interviews with key actors. Action groups are defined operationally as a set of stakeholders including competitors of the technology producers, component suppliers, end users, services providers, research centers and academia. The case study is suitable for highlighting specific aspects of the standardization process during its manifestation. It reveals how firms create and appropriate value, providing details about its standardization strategy.FindingsOur findings show that smart robotics standardization is drivn by collaborative models, where the two dilemmas of value creation and appropriation are evident. Firstly, the case revealed that standardization is lead users oriented. Secondly, lead users’ involvement is crucial to customize technologies. Thirdly, the firm’s position is to share a part of the value with the members. The IPR policy is a matter of interest within action groups, since the collaboration is based on open innovation models to share patents and licenses related knowledge.Research limitations/implicationsThis research has some limitations attributable to the limited generalizability of the results due to the qualitative analysis. In addition, this study considers the perspective of technology producers, but should also take into account the perspective of both collective actions itself and the lead users. Findings have some implications in the strategy negotiation. Participating in action groups is not enough to ensure a competitive advantage. Involving lead users is of strategic importance to acquire a competitive advantage. Lead users contribute to the producers’ technology design, helping firms to differentiate solutions from the industry standard and create value from customized technologies.Practical implicationsThis study helps practitioners understand the competitive side of collective actions, clarifying the value capture and appropriability in standardization. The research provides insights to policymakers and standard development organizations committees when they are called to harmonize standards considering the fallouts on the sector’s competitiveness. Findings suggest appropriate property rights policies to manage the issues related to the value appropriability and technology sharing, recognizing action groups members for their contribution in value creation.Originality/valueThis study shows how firms deal within action groups with the two dilemmas of variety versus technology conformity and property rights versus technology sharing. It fills the research gap in collective actions, emphasizing the perspective of the individual firm in the group rather than the coalition strategy itself. This topic highlights the crucial role of lead users within action groups in managing the two dilemmas, offering a new perspective for understanding critical issues of multi-mode standardization. Reflecting on mechanisms and tools to manage the two dilemmas allows firms to protect their competitive advantage in coalitions.",IS,DIGITAL_INNOV,85,Clear IS paper with focus on Digital Innovation & Entrepreneurship
Spectral Estimation with Free Decompression,"Computing eigenvalues of very large matrices is a critical task in many machine learning applications, including the evaluation of log-determinants, the trace of matrix functions, and other important metrics. As datasets continue to grow in scale, the corresponding covariance and kernel matrices become increasingly large, often reaching magnitudes that make their direct formation impractical or impossible. Existing techniques typically rely on matrix-vector products, which can provide efficient approximations, if the matrix spectrum behaves well. However, in settings like distributed learning, or when the matrix is defined only indirectly, access to the full data set can be restricted to only very small sub-matrices of the original matrix. In these cases, the matrix of nominal interest is not even available as an implicit operator, meaning that even matrix-vector products may not be available. In such settings, the matrix is ""impalpable,"" in the sense that we have access to only masked snapshots of it. We draw on principles from free probability theory to introduce a novel method of ""free decompression"" to estimate the spectrum of such matrices. Our method can be used to extrapolate from the empirical spectral densities of small submatrices to infer the eigenspectrum of extremely large (impalpable) matrices (that we cannot form or even evaluate with full matrix-vector products). We demonstrate the effectiveness of this approach through a series of examples, comparing its performance against known limiting distributions from random matrix theory in synthetic settings, as well as applying it to submatrices of real-world datasets, matching them with their full empirical eigenspectra.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback,"Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Performance Analysis and Comparison of Machine and Deep Learning Algorithms for IoT Data Classification,"In recent years, the growth of Internet of Things (IoT) as an emerging technology has been unbelievable. The number of networkenabled devices in IoT domains is increasing dramatically, leading to the massive production of electronic data. These data contain valuable information which can be used in various areas, such as science, industry, business and even social life. To extract and analyze this information and make IoT systems smart, the only choice is entering artificial intelligence (AI) world and leveraging the power of machine learning and deep learning techniques. This paper evaluates the performance of 11 popular machine and deep learning algorithms for classification task using six IoT-related datasets. These algorithms are compared according to several performance evaluation metrics including precision, recall, f1-score, accuracy, execution time, ROC-AUC score and confusion matrix. A specific experiment is also conducted to assess the convergence speed of developed models. The comprehensive experiments indicated that, considering all performance metrics, Random Forests performed better than other machine learning models, while among deep learning models, ANN and CNN achieved more interesting results.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Finding People with Emotional Distress in Online Social Media: A Design Combining Machine Learning and Rule-Based Classification,"Many people face problems of emotional distress. Early detection of high-risk individuals is the key to prevent suicidal behavior. There is increasing evidence that the Internet and social media provide clues of people’s emotional distress. In particular, some people leave messages showing emotional distress or even suicide notes on the Internet. Identifying emotionally distressed people and examining their posts on the Internet are important steps for health and social work professionals to provide assistance, but the process is very time-consuming and ineffective if conducted manually using standard search engines. Following the design science approach, we present the design of a system called KAREN, which identifies individuals who blog about their emotional distress in the Chinese language, using a combination of machine learning classification and rule-based classification with rules obtained from experts. A controlled experiment and a user study were conducted to evaluate system performance in searching and analyzing blogs written by people who might be emotionally distressed. The results show that the proposed system achieved better classification performance than the benchmark methods and that professionals perceived the system to be more useful and effective for identifying bloggers with emotional distress than benchmark approaches.",CS,NLP,85,Clear CS paper with focus on Natural Language Processing
Grid search in hyperparameter optimization of machine learning models for prediction of HIV/AIDS test results,"In this work, we propose hyperparameters optimization using grid search to optimize the parameters of eight existing models and apply the best parameters to predict the outcomes of HIV tests from the Ethiopian Demographic and Health Survey (EDHS), HIV/AIDS dataset. The core challenge of this work is to find the right or optimum parameter values that generate the optimal model and uncertain training computing costs and test predictive models using various values of hyperparameters. To overcome these challenges, we explore the effects of hyperparameters optimizations by applying a proposed grid search hyperparameter optimization (GSHPO) on the considered models to robust the prediction power. An extensive number of experiments are conducted to affirm the feasibility of our proposed methods. These experiments are done in two separate phases. In the first phase, we test our method with the selected models before hyperparameter optimization is applying (using the default parameters). The second phase of the experiment is done after the hyperparameter optimization is applying (using GSHPO). During the experiment, the 10-fold cross validation technique is used to solve the bias of the models. The proposed system helped to tune the hyperparameters using the grid search approach to the prediction algorithms. Several standard metrics are used to assess the method's efficiency, like accuracy, precision, recall, f1-score, AUC-ROC, MAE, RMSE, R2 and confusion matrix to compare results of each experiments. The results obtained by after applying 10-fold cross validation techniques and the proposed GSHPO are promising. Our findings suggest that the hyper-parameters of tuning models have a statistically important positive impact on the models' prediction accuracy.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Calibration in Deep Learning: A Survey of the State-of-the-Art,"Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent advances in calibrating deep models. In this survey, we review the state-of-the-art calibration methods and their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibration methods that we roughly classify into four categories: post-hoc calibration, regularization methods, uncertainty estimation, and composition methods. We also cover recent advancements in calibrating large models, particularly large language models (LLMs). Finally, we discuss some open issues, challenges, and potential directions.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Peer reviews of peer reviews: A randomized controlled trial and other experiments,"Is it possible to reliably evaluate the quality of peer reviews? We study this question driven by two primary motivations – incentivizing high-quality reviewing using assessed quality of reviews and measuring changes to review quality in experiments. We conduct a large scale study at the NeurIPS 2022 conference, a top-tier conference in machine learning, in which we invited (meta)-reviewers and authors to voluntarily evaluate reviews given to submitted papers. First, we conduct a randomized controlled trial to examine bias due to the length of reviews. We generate elongated versions of reviews by adding substantial amounts of non-informative content. Participants in the control group evaluate the original reviews, whereas participants in the experimental group evaluate the artificially lengthened versions. We find that lengthened reviews are scored (statistically significantly) higher quality than the original reviews. Additionally, in analysis of observational data we find that authors are positively biased towards reviews recommending acceptance of their own papers, even after controlling for confounders of review length, quality, and different numbers of papers per author. We also measure disagreement rates between multiple evaluations of the same review of 28% – 32%, which is comparable to that of paper reviewers at NeurIPS. Further, we assess the amount of miscalibration of evaluators of reviews using a linear model of quality scores and find that it is similar to estimates of miscalibration of paper reviewers at NeurIPS. Finally, we estimate the amount of variability in subjective opinions around how to map individual criteria to overall scores of review quality and find that it is roughly the same as that in the review of papers. Our results suggest that the various problems that exist in reviews of papers – inconsistency, bias towards irrelevant factors, miscalibration, subjectivity – also arise in reviewing of reviews.",CS,DL_IR,85,Clear CS paper with focus on Digital Libraries & Information Retrieval
A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks,"Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore, we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of this groundbreaking technology.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Deep Learning-Based Near-Fall Detection Algorithm for Fall Risk Monitoring System Using a Single Inertial Measurement Unit,"Proactively detecting falls and preventing injuries are among the primary keys to a healthy life for the elderly. Near-fall remote monitoring in daily life could provide key information to prevent future falls and obtain quantitative rehabilitation status for patients with weak balance ability. In this study, we developed a deep learning-based novel classification algorithm to precisely categorize three classes (falls, near-falls, and activities of daily living (ADLs)) using a single inertial measurement unit (IMU) device attached to the waist. A total of 34 young participants were included in this study. An IMU containing accelerometer and gyroscope sensors was fabricated to acquire acceleration and angular velocity signals. A comprehensive experiment including thirty-six types of activities (10 types of falls, 10 types of near-falls, and 16 types of ADLs) was designed based on previous studies. A modified directed acyclic graph-convolution neural network (DAG-CNN) architecture with hyperparameter optimization was proposed to predict fall, near-fall, and ADLs. Prediction results of the modified DAG-CNN structure were found to be approximately 7% more accurate than the traditional CNN structure. For the case of near-falls, the modified DAG-CNN demonstrated excellent prediction performance with accuracy of over 98% by combining gyroscope and accelerometer features. Additionally, by combining acceleration and angular velocity the trained model showed better performance than each model of acceleration and angular velocity. It is believed that information to preemptively handle the risk of falls and quantitatively evaluate the rehabilitation status of the elderly with weak balance will be provided by monitoring near-falls.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Prediction on the Urban GNSS Measurement Uncertainty Based on Deep Learning Networks With Long Short-Term Memory,"The GNSS performance could be significantly degraded by the interferences in an urban canyon, such as the blockage of the direct signal and the measurement error due to reflected signals. Such interferences can hardly be predicted by statistical or physical models, making urban GNSS positioning unable to achieve satisfactory accuracy. The deep learning networks, specializing in extracting abstract representations from data, may learn the representation about the GNSS measurement quality from existing measurements, which can be employed to predict the interferences in an urban area. In this study, we proposed a deep learning network architecture combining the conventional fully connected neural networks (FCNNs) and the long short-term memory (LSTM) networks, to predict the GNSS satellite visibility and pseudorange error based on GNSS measurement-level data. The performance of the proposed deep learning networks is evaluated by real experimental data in an urban area. It can predict the satellite visibility with 80.1% accuracy and predict the pseudorange errors with an average difference of 4.9 meters to the labeled errors. Experiments are conducted to investigate what representations have been learned from data by the proposed deep learning networks. Analysis results show that the LSTM layer within the proposed networks may contain representations about the environment, which affects the prediction behavior and can associate with the real environment information.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
"Unveiling Energy Efficiency in Deep Learning: Measurement, Prediction, and Scoring Across Edge Devices","Today, deep learning optimization is primarily driven by research focused on achieving high inference accuracy and reducing latency. However, the energy efficiency aspect is often overlooked, pos-sibly due to a lack of sustainability mindset in the field and the absence of a holistic energy dataset. In this paper, we conduct a threefold study, including energy measurement, prediction, and efficiency scoring, with an objective to foster transparency in power and energy consumption within deep learning across various edge devices. Firstly, we present a detailed, first-of-its-kind measurement study that uncovers the energy consumption characteristics of on-device deep learning. This study results in the creation of three extensive energy datasets for edge devices, covering a wide range of kernels, state-of-the-art DNN models, and popular AI applications. Secondly, we design and implement the first kernel-level energy predictors for edge devices based on our kernel-level energy dataset. Evaluation results demonstrate the ability of our predictors to provide consistent and accurate energy estimations on unseen DNN models. Lastly, we introduce two scoring metrics, PCS and IECS, developed to convert complex power and energy consumption data of an edge device into an easily understandable manner for edge device end-users. We hope our work can help shift the mindset of both end-users and the research community towards sustainability in edge computing, a principle that drives our research. Find data, code, and more up-to-date information at",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
Effect of concept mapping education on clinical decision-making in nursing students: A randomized controlled trial,"Clinical decision-making is a critical and essential skill in clinical care. Nurses' proficiency in proper clinical decision-making significantly improves the quality of nursing care. As an active learning approach, concept mapping education plays a vital role in deep learning and applying critical thinking skills in clinical decision-making. This study aimed to investigate the impact of concept mapping on clinical decision-making in nursing students. The study was an experimental design with two groups, where participants were randomly assigned to the test group (25 students) or the control group (25 students). Data were collected using the clinical decision-making questionnaire. Based on relevant literature, the test group participated in eight concept mapping sessions lasting 60 to 90 minutes each, while the control group received traditional lecture-based education. Covariance analysis, excluding the effect of the pre-test, showed no significant difference (P=0.86, eta=0.01). Still, a paired t-test revealed a significant difference in the test group before and after the intervention (P<0.01). The results demonstrated that concept mapping education enhances clinical decision-making. Therefore, this teaching method, which promotes students' reflection, creativity, and decision-making skills, is recommended in nursing education. Concept mapping is an effective, innovative, and cost-efficient educational strategy.",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Deep Learning for Image Super-Resolution: A Survey,"Image Super-Resolution (SR) is an important class of image processing techniqueso enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
The Ecofisio Mobile App for Assessment and Diagnosis Using Ultrasound Imaging for Undergraduate Health Science Students: Multicenter Randomized Controlled Trial,"Background Generation Z is starting to reach college age. They have adopted technology from an early age and have a deep dependence on it; therefore, they have become more drawn to the virtual world. M-learning has experienced huge growth in recent years, both in the medical context and in medical and health sciences education. Ultrasound imaging is an important diagnosis technique in physiotherapy, especially in sports pathology. M-learning systems could be useful tools for improving the comprehension of ultrasound concepts and the acquisition of professional competencies. Objective The purpose of this study was to evaluate the efficacy and use of an interactive platform accessible through mobile devices—Ecofisio—using ultrasound imaging for the development of professional competencies in the evaluation and diagnosis of sports pathologies. Methods Participants included 110 undergraduate students who were placed into one of two groups of a randomized controlled multicenter study: control group (ie, traditional learning) and experimental group (ie, Ecofisio mobile app). Participants’ theoretical knowledge was assessed using a multiple-choice questionnaire (MCQ); students were also assessed by means of the Objective Structured Clinical Examination (OSCE). Moreover, a satisfaction survey was completed by the students. Results The statistical analyses revealed that Ecofisio was effective in most of the processes evaluated when compared with the traditional learning method: all OSCE stations, P<.001; MCQ, 43 versus 15 students passed in the Ecofisio and control groups, respectively, P<.001. Moreover, the results revealed that the students found the app to be attractive and useful. Conclusions The Ecofisio mobile app may be an effective way for physiotherapy students to obtain adequate professional competencies regarding evaluation and diagnosis of sports pathologies. Trial Registration ClinicalTrials.gov NCT04138511;",NON_COMPUTING,NOT_APPLICABLE,95,Paper is not related to computing or information technology
Statistical Analysis of Design Aspects of Various YOLO-Based Deep Learning Models for Object Detection,"Object detection is a critical and complex problem in computer vision, and deep neural networks have significantly enhanced their performance in the last decade. There are two primary types of object detectors: two stage and one stage. Two-stage detectors use a complex architecture to select regions for detection, while one-stage detectors can detect all potential regions in a single shot. When evaluating the effectiveness of an object detector, both detection accuracy and inference speed are essential considerations. Two-stage detectors usually outperform one-stage detectors in terms of detection accuracy. However, YOLO and its predecessor architectures have substantially improved detection accuracy. In some scenarios, the speed at which YOLO detectors produce inferences is more critical than detection accuracy. This study explores the performance metrics, regression formulations, and single-stage object detectors for YOLO detectors. Additionally, it briefly discusses various YOLO variations, including their design, performance, and use cases.",CS,CV_PR,85,Clear CS paper with focus on Computer Vision & Pattern Recognition
The Effectiveness of Deep Learning Based on the Fullan Approach on Working Memory and Academic Procrastination in First-Year Secondary School Students,"Objective: The objective of this study was to examine the effectiveness of a deep learning intervention based on the Fullan approach on improving working memory and reducing academic procrastination in middle school students. Methods and Materials: A randomized controlled trial was conducted with 30 middle school students (15 in the experimental group and 15 in the control group) from Tehran. Participants were selected through convenience sampling and assessed at three time points: pre-test, post-test, and a five-month follow-up. The experimental group received an 8-session deep learning intervention, each lasting 90 minutes, designed to enhance cognitive and self-regulation skills. Working memory was measured using Daneman and Carpenter’s (1980) working memory test, and academic procrastination was assessed using McCloskey's (2011) procrastination scale. Data were analyzed using analysis of variance (ANOVA) with repeated measures, and post-hoc comparisons were performed using the Bonferroni test. The statistical analysis was conducted using SPSS-27. Findings: The results showed a significant improvement in working memory and a reduction in academic procrastination in the experimental group compared to the control group. In the experimental group, working memory scores significantly increased from pre-test to post-test and remained high at the five-month follow-up. Additionally, academic procrastination scores significantly decreased after the intervention, with lasting improvements at the follow-up. The control group showed no significant changes in either variable. The results of the ANOVA revealed significant interaction effects for both working memory and procrastination (p < 0.05). Conclusion: The deep learning intervention based on the Fullan approach was effective in enhancing working memory and reducing academic procrastination in middle school students. These findings suggest that cognitive training programs incorporating deep learning strategies can significantly improve academic behaviors and cognitive functions in adolescents.",CS,CS_EDU,85,Clear CS paper with focus on Computer Education & Pedagogy
Personalized recurrence risk stratification in surgically resected non-small cell lung cancer: Multicenter deep learning model development and validation.,"e13632 Background: Current adjuvant treatment strategies for surgically resected non-small cell lung cancer (NSCLC) often fail to account for individual patient recurrence risks, and surveillance protocols are typically not tailored to these factors. Precise evaluation of recurrence risk is essential for tailoring adjuvant treatment plans and designing individualized follow-up strategies. This study aimed to develop and validate a deep-learning model leveraging clinicopathological parameters to assess recurrence risk. Methods: Patients with histologically proven pN2 NSCLC who underwent complete resection were enrolled in one academic institution as the training set. Participants in a randomized controlled trial were included as the test set. Patients across another four independent academic medical centers were enrolled as external validation sets. A deep learning algorithm, DeepSurv, was trained using key clinicopathological variables, along with two typical machine learning algorithms, Survival Support Vector Machine (SSVM) and Random Survival Forest (RSF). The performance of all models was evaluated using the concordance index (C-index). Results: The training, test, and external validation datasets comprised 1400, 364, and 841 individuals. Cox regression analysis in the training set showed that sex, age, smoking history, positive lymph node amounts, histology, pathologic tumor stage, trachea invasion, visceral pleural invasion, lymphovascular invasion, and postoperative radiotherapy were predictors of recurrence. Models were trained based on these variables. In the training cohort, DeepSurv achieved a C-Index of 0.77 (CI, 0.75-0.78), outperforming SSVM (0.66, 95%CI, 0.64-0.67) and RSF (0.63, CI, 0.62-0.64). In the testing cohort, DeepSurv scored a C-Index of 0.73 (CI, 0.70-0.75), SSVM at 0.67 (CI, 0.64-0.69), and RSF at 0.57 (CI, 0.55-0.59). In the external validation cohort, DeepSurv maintained its lead at a C-Index of 0.70 (CI, 0.68-0.71), with SSVM at 0.66 (CI, 0.63-0.69) and RSF at 0.58 (CI, 0.56-0.59). Patients were stratified into high- and low-risk recurrence groups based on the DeepSurv model. The high-risk group exhibited a significantly higher recurrence rate (HR, 1.23; CI, 1.06–1.48; P < 0.01). DeepSurv effectively distinguished patients who could benefit from postoperative radiotherapy. Patients who followed DeepSurv treatment recommendations achieved significantly better recurrence-free survival than those who did not (HR = 0.73; CI, 0.56-0.96; P = 0.02). Conclusions: The DeepSurv deep learning model outperformed traditional methods, including SSVM and RSF, in predicting recurrence risk in surgically resected NSCLC using clinicopathological variables. This model shows significant potential for optimizing adjuvant treatment decisions and personalized longitudinal monitoring.",CS,AI_ML,85,Clear CS paper with focus on Artificial Intelligence & Machine Learning
"Developing Secure and Interoperable Health Information Systems Using Blockchain Technology to Enhance Data Privacy, Security, and Accessibility in Healthcare","Healthcare information systems are going through a huge change that will make them more efficient, safe, and easy to use. As more persistent records are digitized and private therapeutic information is shared, it is more critical than ever to create beyond any doubt that security, security, and availability are well secured. Blockchain innovation can be a great way to bargain with these issues since it makes a decentralized, unchangeable record that can be utilized to securely store and send healthcare information whereas securing its security and security. This exposition looks at how blockchain innovation might offer assistance make wellbeing data frameworks more secure and more congruous with each other. Healthcare companies can set up a trusted and unchangeable way to oversee understanding information over different frameworks and parties by utilizing blockchain independent plan, cryptography, and voting instruments. One of the leading things approximately blockchain innovation is that it can make data more private. Cryptographic instruments, like encryption and computerized marks, make it conceivable to store and share private wellbeing information securely with individuals who are permitted to see it, without compromising persistent security. Moreover, since blockchain is unchangeable, once information is recorded, it can't be changed or eradicated. This makes the history of who gotten to and changed information clear and simple to check. In conjunction with ensuring protection, blockchain innovation moreover makes things more secure by bringing down the chances of information spills and unlawful get to. Blockchain-based wellbeing data frameworks make information keeping less centralized and use solid encryption strategies to lower the chances of single focuses of failure and unlawful changes. This makes information more secure and dependable. Keen contracts on blockchain stages can too mechanize and execute get to control rules, making beyond any doubt that as it were individuals who are permitted to can see certain restorative data or do certain activities that have as of now been set up. Interoperability is another important part of modern healthcare information systems that lets different healthcare companies and systems work together and share data easily.",CS,AI_ML,0.85,Extracted from log - paper 1
Security and Privacy of Technologies in Health Information Systems: A Systematic Literature Review,"Health information systems (HISs) have immense value for healthcare institutions, as they provide secure storage, efficient retrieval, insightful analysis, seamless exchange, and collaborative sharing of patient health information. HISs are implemented to meet patient needs, as well as to ensure the security and privacy of medical data, including confidentiality, integrity, and availability, which are necessary to achieve high-quality healthcare services. This systematic literature review identifies various technologies and methods currently employed to enhance the security and privacy of medical data within HISs. Various technologies have been utilized to enhance the security and privacy of healthcare information, such as the IoT, blockchain, mobile health applications, cloud computing, and combined technologies. This study also identifies three key security aspects, namely, secure access control, data sharing, and data storage, and discusses the challenges faced in each aspect that must be enhanced to ensure the security and privacy of patient information in HISs.",CS,AI_ML,0.85,Extracted from log - paper 2
"ZeroTrustBlock: Enhancing Security, Privacy, and Interoperability of Sensitive Data through ZeroTrust Permissioned Blockchain","With the digitization of healthcare, an immense amount of sensitive medical data are generated and shared between various healthcare stakeholders—however, traditional health data management mechanisms present interoperability, security, and privacy challenges. The centralized nature of current health information systems leads to single points of failure, making the data vulnerable to cyberattacks. Patients also have little control over their medical records, raising privacy concerns. Blockchain technology presents a promising solution to these challenges through its decentralized, transparent, and immutable properties. This research proposes ZeroTrustBlock, a comprehensive blockchain framework for secure and private health information exchange. The decentralized ledger enhances integrity, while permissioned access and smart contracts enable patient-centric control over medical data sharing. A hybrid on-chain and off-chain storage model balances transparency with confidentiality. Integration gateways bridge ZeroTrustBlock protocols with existing systems like EHRs. Implemented on Hyperledger Fabric, ZeroTrustBlock demonstrates substantial security improvements over mainstream databases via cryptographic mechanisms, formal privacy-preserving protocols, and access policies enacting patient consent. Results validate the architecture’s effectiveness in achieving 14,200 TPS average throughput, 480 ms average latency for 100,000 concurrent transactions, and linear scalability up to 20 nodes. However, enhancements around performance, advanced cryptography, and real-world pilots are future work. Overall, ZeroTrustBlock provides a robust application of blockchain capabilities to transform security, privacy, interoperability, and patient agency in health data management.",CS,AI_ML,0.85,Extracted from log - paper 3
Leveraging blockchain for sustainable supply chain management: A data privacy and security perspective,"This review examines how blockchain technology can be leveraged to enhance data privacy and security in sustainable supply chain management (SSCM). As global supply chains become increasingly complex and the demand for sustainability grows, ensuring data privacy and security has become a critical concern. Traditional supply chain systems often face challenges such as data breaches, lack of transparency, and difficulty in tracing products and materials. Blockchain technology, with its decentralized, immutable, and transparent architecture, offers a promising solution to these challenges. Blockchain can enhance data security by ensuring that data is tamper-proof, traceable, and encrypted, thus protecting sensitive information across the supply chain. It provides transparency while allowing permissioned access, ensuring that stakeholders can verify data without exposing confidential information. Furthermore, privacy-preserving technologies such as zero-knowledge proofs and homomorphic encryption allow verification of data without compromising its security. Smart contracts enable automated compliance with regulatory frameworks like GDPR, reducing the risk of human error and improving operational efficiency. The integration of blockchain in SSCM can improve traceability, transparency, and accountability, thereby promoting environmental and social sustainability. By tracking the origin and journey of goods, blockchain helps verify ethical sourcing practices and reduce carbon footprints. However, the technology also presents challenges, including scalability, integration with legacy systems, and cost considerations. Through case studies in industries such as food, textiles, and renewable energy, this review highlights the practical applications and benefits of blockchain for SSCM. It concludes that blockchain has the potential to revolutionize supply chain operations, but careful consideration must be given to overcoming its technical and financial barriers to widespread adoption.",CS,AI_ML,0.85,Extracted from log - paper 4
Artificial Intelligence for Secured Information Systems in Smart Cities: Collaborative IoT Computing with Deep Reinforcement Learning and Blockchain,"The accelerated expansion of the Internet of Things (IoT) has raised critical challenges associated with privacy, security, and data integrity, specifically in infrastructures such as smart cities or smart manufacturing. Blockchain technology provides immutable, scalable, and decentralized solutions to address these challenges, and integrating deep reinforcement learning (DRL) into the IoT environment offers enhanced adaptability and decision-making. This paper investigates the integration of blockchain and DRL to optimize mobile transmission and secure data exchange in IoT-assisted smart cities. Through the clustering and categorization of IoT application systems, the combination of DRL and blockchain is shown to enhance the performance of IoT networks by maintaining privacy and security. Based on the review of papers published between 2015 and 2024, we have classified the presented approaches and offered practical taxonomies, which provide researchers with critical perspectives and highlight potential areas for future exploration and research. Our investigation shows how combining blockchain's decentralized framework with DRL can address privacy and security issues, improve mobile transmission efficiency, and guarantee robust, privacy-preserving IoT systems. Additionally, we explore blockchain integration for DRL and outline the notable applications of DRL technology. By addressing the challenges of machine learning and blockchain integration, this study proposes novel perspectives for researchers and serves as a foundational exploration from an interdisciplinary standpoint.",CS,AI_ML,0.85,Extracted from log - paper 5
Analysis and Improvement of Blockchain-Based Multilevel Privacy-Preserving Location Sharing Scheme for Telecare Medical Information Systems,"Patient location sharing is an important part of modern smart healthcare and mobile medical services. Blockchain has many attractive properties and is suitable for managing patient locations in telecare medical information systems (TMIS). Recently, Ji et al. proposed a blockchain-based multilevel privacy-preserving location sharing (BMPLS) scheme for TMIS. In this paper, we show that Ji et al.’s BMPLS scheme does not achieve confidentiality and multilevel privacy-preserving. An adversary outside the system can use an ordinary personal computer to completely break the system within a dozen hours and obtain the location of any patient at any time. The adversary inside the system can use an ordinary personal computer to obtain the location of the designated patient within tens of seconds. Using salting technology, we propose an improved BMPLS scheme to fix our attacks. We also optimized the BMLS scheme to make it correct and executable. The security analysis shows that the improved BMPLS scheme achieves decentralization, untamperability, confidentiality, multilevel privacy-preserving, retrievability, and verifiability. The simulation shows that the improved BMPLS scheme is practical, the computational overhead of the location record phase is within 10 ms, and the computational overheads of the location sharing and location extraction phases are both within 30 ms.",CS,AI_ML,0.85,Extracted from log - paper 6
Searchable Blockchain-Based Healthcare Information Exchange System to Enhance Privacy Preserving and Data Usability,Ensuring the security and usability of electronic health records (EHRs) is important in health information exchange (HIE) systems that handle healthcare records. This study addressed the need to balance privacy preserving and data usability in blockchain-based HIE systems. We propose a searchable blockchain-based HIE system that enhances privacy preserving while improving data usability. The proposed methodology includes users collecting healthcare information (HI) from various Internet of Medical Things (IoMT) devices and compiling this information into EHR blocks for sharing on a blockchain network. This approach allows participants to search and utilize specific health data within the blockchain effectively. The results demonstrate that the proposed system mitigates the issues of traditional HIE systems by providing secure and user-friendly access to EHRs. The proposed searchable blockchain-based HIE system resolves the trade-off dilemma in HIE by achieving a balance between security and the data usability of EHRs.,CS,AI_ML,0.85,Extracted from log - paper 7
Security and Privacy of Patient Information in Medical Systems Based on Blockchain Technology,"The essence of “blockchain” is a shared database in which information stored is un-falsifiable, traceable, open, and transparent. Therefore, to improve the security of private information in medical systems, this article uses blockchain technology to design a method to protect private information in medical systems and effectively realize anti-theft control of private information. First, the Patient-oriented Privacy Preserving Access Control model is introduced into the access control process of private information in medical systems. Next, a private information storage platform is built by using blockchain technology, and information transmission is realized using standard cryptographic algorithms. In this process, file authorization contracts are also used to guarantee the security of private information and further prevent theft of medical private information. Our simulation results show that the storage response time of this method is kept below 1,000 ms, and the maximum information throughput rate reaches 550 kbit/s, which indicates that this method has strong performance in information storage and transmission efficiency. Moreover, the reliability and bandwidth utilization of data transmission across domains is higher, so the method has higher information security control performance and superior overall performance.",CS,AI_ML,0.85,Extracted from log - paper 8
Towards Improving Privacy and Security of Identity Management Systems Using Blockchain Technology: A Systematic Review,"An identity management system (IDMS) manages and organizes identities and credentials information exchanged between users, identity providers (IDPs), and service providers (SPs) to ensure confidentiality and enhance privacy of users’ personal data. Traditional or centralized IDMS rely on a third party to store a user’s personal information, authenticate the user, and organize the entire process. This clearly constitutes threats to the privacy of the user, in addition to other issues, such as single point of failure (SPOF), user tracking, and data availability issues. Blockchain technology has many useful features that can contribute to solving traditional IDMS issues, such as decentralization, immutability, and anonymity. Blockchain represents an attractive solution for many issues related to traditional IDMS, including privacy, third-party control, data leakage, and SPOF, supported by Distributed Ledger Technology (DLT) security features and powerful smart contracts technology. The current study presents a systematic literature review and analysis for recently proposed solutions that adopt the traditional centralized approach, as well as solutions based on blockchain technology. The study also aims to provide a deep understanding of proposed IDMS solutions and best practices, and highlight the research gaps and open issues related to IDMSs and users’ privacy. In particular, the current research focuses on analyzing the blockchain-based solutions and illustrating their strengths and weaknesses, as well as highlighting the promising blockchain technology framework that can be utilized to enhance privacy and solve security issues in a centralized IDMS. Such a study is an important step towards developing efficient solutions that address the pressing needs in the field.",CS,AI_ML,0.85,Extracted from log - paper 9
Blockchain-based privacy and security preserving in electronic health: a systematic review,"In today’s world, health and medicine play an undeniable role in human life. Traditional and current Electronic Health Records (EHR) systems that are used to exchange information between medical stakeholders (patients, physicians, insurance companies, pharmaceuticals, medical researchers, etc.) suffer weaknesses in terms of security and privacy due to having centralized architecture. Blockchain technology ensures the privacy and security of EHR systems thanks to the use of encryption. Moreover, due to its decentralized nature, this technology prevents central failure and central attack points. In this paper, a systematic literature review (SLR) is proposed to analyze the existing Blockchain-based approaches for improving privacy and security in electronic health systems. The research methodology, paper selection process, and the search query are explained. 51 papers returned from our search criteria published between 2018 and Dec 2022 are reviewed. The main ideas, type of Blockchain, evaluation metrics, and used tools of each selected paper are discussed in detail. Finally, future research directions, open challenges, and some issues are discussed.",CS,AI_ML,0.85,Extracted from log - paper 10
Blockchain Technology as a Decentralized Solution for Data Security and Privacy: Applications Beyond Cryptocurrencies in Supply Chain Management and Healthcare,"Decentralized and promising, blockchain technology brings enhanced security for data along with privacy issues particularly in health and supply chains. Here, we aim to show the usage of blockchain towards the secure data of devices that come under Internet of Medical Things or IoMT by providing data integrity and further sensitive medical information protection. A secure, transparent, and tamper-proof system in health care management can be supported by an IoT framework that uses blockchain. Four data security and privacy algorithms based on blockchain have been considered and tested with excellent results to show a clear indication of how much prevention in un-authorized access can be done in ensuring authenticity, tracking products in the medical supply chain. Some experiments were demonstrated where the blockchain-enabled system had reduced the data breach by 95% from traditional systems. Supply chain traceability had improved by 92%. The combination of machine learning and blockchain increased real-time threat detection by 88%. The outcome shows that blockchain can deal with problems like tampering, privacy violation, and counterfeiting in medical products. However, scalability of applications and adoption by regulators are two major challenges towards full integration. This paper aids in the development of a complete holistic framework that may make use of the usage of blockchain to secure health care and other supply chain systems to allow further avenues for digitizing data safety.",CS,AI_ML,0.85,Extracted from log - paper 11
Ballots and Padlocks: Building Digital Trust and Security in Democracy through Information Governance Strategies and Blockchain Technologies,"This research explores the integration of Information Governance (IG) strategies and Blockchain Technologies (BT) in enhancing digital trust and security within democratic processes. Amid concerns about the integrity and vulnerability of electoral systems in the digital era, this study examines how these technologies can collectively safeguard democracy. Utilizing Partial Least Squares Structural Equation Modeling (PLS-SEM), bootstrapping analysis for mediation effects, and the Fornell-Larcker Criterion for discriminant validity, the analysis was conducted on data from 934 participants involved in the electoral process. Key findings demonstrate that IG strategies significantly impact digital trust, indicating the importance of robust data management, legal compliance, and privacy measures for public confidence in electoral systems. Blockchain Technologies positively affect the security of democratic processes due to their decentralized and immutable characteristics. Furthermore, digital trust is identified as a critical mediator between IG strategies, BT, and the security of democratic processes, highlighting the importance of trust in the effectiveness of these technologies. Based on the insights gained, three actionable recommendations are proposed: Electoral authorities should adopt comprehensive IG frameworks to enhance data integrity and transparency; Pilot blockchain projects should be expanded to refine and understand the broader implementation implications for election security; Efforts should be increased to foster digital literacy and trust among the electorate, emphasizing the role of these technologies in securing electoral integrity.",CS,AI_ML,0.85,Extracted from log - paper 12
Blockchain-Based Distributed Information Hiding Framework for Data Privacy Preserving in Medical Supply Chain Systems,"Medical supply chain communication networks engender critical information and data. Notably in the COVID era, inner personal and private information is being shared between healthcare providers regarding the medical supply chain. In recent years, multiple cyber-attacks have targeted medical supply chain communication networks due to their lack of security measures. In the era where cyber-attacks are cheaper and easier due to the computational power and various algorithms available for malicious uses, security, and data privacy requires intensive and higher measures. On the other hand, Information Hiding Techniques (IHT) compromise various advanced methods to hide sensitive information from being disclosed to malicious nodes. Moreover, with the support of Blockchain, IHT can bring higher security and the required privacy levels. In this paper, we propose the implementation of Blockchain and smart contract with the information hiding technique to enhance the security and privacy of data communication in critical systems, such as smart healthcare supply chain communication networks. Results show the feasibility of the framework using Hyperledger smart contract along with the desired security level.",CS,AI_ML,0.85,Extracted from log - paper 13
Intelligent transportation systems trusted user’s security and privacy,"Standard encryption cannot be utilized in practical communications due to time and storage limits. Dedicated lane keeping (DRL) is a technique that helps self-driving cars navigate congested roads by keeping them in a specific lane (CAVs). Researchers have developed separate networks for distinct types of mixed traffic to cut down on the time and effort typically spent on instruction and coordination. A deep reinforcement learning technique boosts the efficiency of each part and the entire fleet. There is a common misconception that the blockchain is a secure database for private information. A distributed database system in which nodes are directly connected to consensus mechanisms. To ensure data integrity, block blocks on a blockchain network use cryptography and other computer safeguards (such as smart contracts and time stamps). Because of its decentralized design, data storage facilitates collaboration. Digitally signed data records can also be checked to ensure they are accurate. Using hashes to connect individual blocks protects data against tampering by hackers. There is no need for a centralized authority or third party to verify the ledger’s accuracy because everyone can access it anytime. The blockchain allows for a transparent, trustworthy, and auditable system sharing information between entities. Like many other industries, transportation may benefit from the broader implementation of blockchain technology. Based on our findings, a state-run blockchain tailored to the transportation industry was developed and made available to the public. Because of blockchain technology, the car-sharing business model may need to be revised. Blockchain technology utilizes a distributed ledger to record transactions in a way that makes it impossible to alter the underlying data while still allowing for fast access for verification and auditing.",CS,AI_ML,0.85,Extracted from log - paper 14
Securing Secrets in Cyber-Physical Systems: A Cutting-Edge Privacy Approach with Consortium Blockchain,"In the era of interconnected and intelligent cyber-physical systems, preserving privacy has become a paramount concern. This paper aims a groundbreaking proof-of-concept (PoC) design that leverages consortium blockchain technology to address privacy challenges in cyber-physical systems (CPSs). The proposed design introduces a novel approach to safeguarding sensitive information and ensuring data integrity while maintaining a high level of trust among stakeholders. By harnessing the power of consortium blockchain, the design establishes a decentralized and tamper-resistant framework for privacy preservation. However, ensuring the security and privacy of sensitive information within CPSs poses significant challenges. This paper proposes a cutting-edge privacy approach that leverages consortium blockchain technology to secure secrets in CPSs. Consortium blockchain, with its permissioned nature, provides a trusted framework for governing the network and validating transactions. By employing consortium blockchain, secrets in CPSs can be securely stored, shared, and accessed by authorized entities only, mitigating the risks of unauthorized access and data breaches. The proposed approach offers enhanced security, privacy preservation, increased trust and accountability, as well as interoperability and scalability. This paper aims to address the limitations of traditional security mechanisms in CPSs and harness the potential of consortium blockchain to revolutionize the management of secrets, contributing to the advancement of CPS security and privacy. The effectiveness of the design is demonstrated through extensive simulations and performance evaluations. The results indicate that the proposed approach offers significant advancements in privacy protection, paving the way for secure and trustworthy cyber-physical systems in various domains.",CS,AI_ML,0.85,Extracted from log - paper 15
Securing Construction Workers’ Data Security and Privacy with Blockchain Technology,"The construction industry, characterized by its intricate network of stakeholders and diverse workforce, grapples with the challenge of managing information effectively. This study delves into this issue, recognizing the universal importance of safeguarding data, particularly amid rising concerns around unauthorized access and breaches. Aiming to harness the potential of blockchain technology to address these challenges, this study used hypothetical biographical and safety data of construction workers securely stored on a Hyperledger Fabric blockchain. Developed within the Amazon Web Services (AWS) cloud platform, this blockchain infrastructure emerged as a robust solution for enhancing data security and privacy. Anchored in the core principles of data security, the model emerges as a potent defender against the vulnerabilities of traditional data management systems. Beyond its immediate implications, this study exemplifies the marriage of blockchain technology and the construction sector, and its potential for reshaping workforce management, especially in high-risk projects and optimizing risk assessment, resource allocation, and safety measures to mitigate work-related injuries. Practical validation through transaction testing using Hyperledger Explorer validates the model’s feasibility and operational effectiveness, thus serving as a blueprint for the industry’s data management. Ultimately, this research not only showcases the promise of blockchain technology in addressing construction data security challenges but also underscores its practical applicability through comprehensive testing, thus heralding a new era of data management that harmonizes security and efficiency for stakeholders’ benefit.",CS,AI_ML,0.85,Extracted from log - paper 16
Privacy Preserving Blockchain with Energy Aware Clustering Scheme for IoT Healthcare Systems,"Due to advancements in information technology, the healthcare sector becomes beneficial and provides distinct methods of managing medical data and enhancing the quality of medical services. The advanced e-healthcare applications are mainly based on the Internet of Things (IoT) and cloud computing platforms. In IoT enabled healthcare sector, the IoT devices usually record the patient data and transfer it to the cloud for further processing. Energy efficiency and security are treated as critical problems in designing IoT networks in the healthcare environment. As IoT devices are limited to energy, designing an effective technique to reduce energy utilization is needed. At the same time, secure transmission of medical data also poses a major challenging design issue. This paper presents a novel artificial intelligence with a blockchain scheme for IoT healthcare systems named AIBS-IoTHS. The AIBS-IoTH model aims to achieve secure and energy-efficient data transmission in IoT networks. The IoT devices are primarily used to collect patients’ medical data. The AIBS- IoTH model involves a metaheuristic-based modified sunflower optimization-based clustering (MSFOC) technique to achieve energy efficiency. Then, the blockchain empowered secure medical data transmission process is carried out for both inter-cluster and intra-cluster communication. At last, the Classification Enhancement Generative Adversarial Networks (CEGAN) model performs the diagnostic process on the secured medical data to determine the existence of the diseases. The design of MSFOC and CEGAN techniques shows the novelty of the work. An extensive experimental analysis of the benchmark dataset pointed out the superior performance of the proposed AIBS-IoTH model over the other compared methods.",CS,AI_ML,0.85,Extracted from log - paper 17
Simulation of the Lightweight Blockchain Technique Based on Privacy and Security for Healthcare Data for the Cloud System,"Information about healthcare is derived from healthcare data. Healthcare data sharing helps make healthcare systems more efficient as well as improving healthcare quality. Patients should own and control healthcare information, one of their most valuable assets, instead of letting data be spread out among health care providers differ. This protects data from being shared between healthcare systems and privacy. Public ledger accompanied by a decentralized network of peer's compromises patient has been demonstrated to be able to achieve trusted, auditable computing by blockchain. The use of access control and cryptographic primitives are insufficient in addressing modern cyber threats all privacy and security concerns associated with a cloud-based environment. In this paper, the authors proposed a lightweight blockchain technique based on privacy and security for healthcare data for the cloud system. The cost-effectiveness of our system's smart contracts is evaluated, as well as the procedures used for data processing in order to encrypt and pseudonymize patient data.",CS,AI_ML,0.85,Extracted from log - paper 18
Privacy Preservation in Patient Information Exchange Systems Based on Blockchain: System Design Study,"Background With the increasing sophistication of the medical industry, various advanced medical services such as medical artificial intelligence, telemedicine, and personalized health care services have emerged. The demand for medical data is also rapidly increasing today because advanced medical services use medical data such as user data and electronic medical records (EMRs) to provide services. As a result, health care institutions and medical practitioners are researching various mechanisms and tools to feed medical data into their systems seamlessly. However, medical data contain sensitive personal information of patients. Therefore, ensuring security while meeting the demand for medical data is a very important problem in the information age for which a solution is required. Objective Our goal is to design a blockchain-based decentralized patient information exchange (PIE) system that can safely and efficiently share EMRs. The proposed system preserves patients’ privacy in the EMRs through a medical information exchange process that includes data encryption and access control. Methods We propose a blockchain-based EMR-sharing system that allows patients to manage their EMRs scattered across multiple hospitals and share them with other users. Our PIE system protects the patient’s EMR from security threats such as counterfeiting and privacy attacks during data sharing. In addition, it provides scalability by using distributed data-sharing methods to quickly share an EMR, regardless of its size or type. We implemented simulation models using Hyperledger Fabric, an open source blockchain framework. Results We performed a simulation of the EMR-sharing process and compared it with previous works on blockchain-based medical systems to check the proposed system’s performance. During the simulation, we found that it takes an average of 0.01014 (SD 0.0028) seconds to download 1 MB of EMR in our proposed PIE system. Moreover, it has been confirmed that data can be freely shared with other users regardless of the size or format of the data to be transmitted through the distributed data-sharing technique using the InterPlanetary File System. We conducted a security analysis to check whether the proposed security mechanism can effectively protect users of the EMR-sharing system from security threats such as data forgery or unauthorized access, and we found that the distributed ledger structure and re-encryption–based data encryption method can effectively protect users’ EMRs from forgery and privacy leak threats and provide data integrity. Conclusions Blockchain is a distributed ledger technology that provides data integrity to enable patient-centered health information exchange and access control. PIE systems integrate and manage fragmented patient EMRs through blockchain and protect users from security threats during the data exchange process among users. To increase safety and efficiency in the EMR-sharing process, we used access control using security levels, data encryption based on re-encryption, and a distributed data-sharing scheme.",CS,AI_ML,0.85,Extracted from log - paper 19
Consortium Blockchain for Security and Privacy-Preserving in E-government Systems,"Since its inception as a solution for secure cryptocurrencies sharing in 2008, the blockchain technology has now become one of the core technologies for secure data sharing and storage over trustless and decentralised peer-to-peer systems. E-government is amongst the systems that stores sensitive information about citizens, businesses and other affiliates, and therefore becomes the target of cyber attackers. The existing e-government systems are centralised and thus subject to single point of failure. This paper proposes a secure and decentralised e-government system based on the consortium blockchain technology, which is a semi-public and decentralised blockchain system consisting of a group of pre-selected entities or organisations in charge of consensus and decisions making for the benefit of the whole network of peers. In addition, a number of e-government nodes are pre-selected to perform the tasks of user and transaction validation before being added to the blockchain network. Accordingly, e-government users of the consortium blockchain network are given the rights to create, submit, access, and review transactions. Performance evaluation on single transaction time and transactions processed per second demonstrate the practicability of the proposed consortium blockchain-based e-government system for secure information sharing amongst all stakeholders.",CS,AI_ML,0.85,Extracted from log - paper 20
"Blockchain, Enterprise Resource Planning (ERP) and Accounting Information Systems (AIS): Research on e-Procurement and System Integration","Accounting information systems (AISs), the core module of any enterprise resource planning (ERP) system, are usually designed as centralised systems. Nowadays, the continuous development and applications of blockchain, or more broadly—distributed ledger technology (DLT), can change the architecture, overcome and improve some limitations of centralised systems, most notably security and privacy. An increasing number of authors are suggesting the application of blockchain technologies in management, accounting and ERPs. This paper aims to examine the emerging literature on this field, and an immediate result is that blockchain applications can have significant benefits. The paper’s innovative contribution and considerable objective are to examine if blockchain can be successfully integrated with AIS and ERPs. We find that blockchain can facilitate integration at multiple levels and better serve various purposes as auditing compliance. To demonstrate that, we analyse e-procurement systems and operations using case study research methodology. The findings suggest that DLT, decentralised finance (DeFI), and financial technology (FinTech) applications can facilitate integrating AISs and ERP systems and yield significant benefits for efficiency, productivity and security.",CS,AI_ML,0.85,Extracted from log - paper 21
Toward a New Era of Smart and Secure Healthcare Information Exchange Systems: Combining Blockchain and Artificial Intelligence,"Healthcare Information Exchange (HIE) is becoming a fundamental operation in current healthcare systems. In such systems, electronic health records (EHRs) are digitally stored inside each medical centers and, sometimes, are required to be shared between various healthcare facilities (HCFs). Indeed, sharing patient information is crucial and might be vulnerable to power outages, data misuse, privacy or security violations, and an audit trail. Hence, researchers have focused recently on cutting-edge technologies to develop secure HIE systems and ensure data privacy during transactions. Among such technologies, blockchain and artificial intelligence (AI) occupy a vital role in researchers’ focuses and efforts to detect risky transactions in HIE systems, thus enhancing their security and privacy. While the blockchain allows HCFs to link to each other without requiring a central authority, AI models offer an additional security layer when sharing patient data between HCFs. This paper presents a survey about HIE systems, and the aim is two-fold: we first present the architecture of HIE systems along with their challenges; then, we categorize and classify the current state-the-art-techniques that show the potential of using blockchain and AI technologies in such systems.",CS,AI_ML,0.85,Extracted from log - paper 22
Challenges In Security And Privacy Posed By Blockchain Technology,"Blockchain technology presents a promising future for protecting personal data. However, it does pose challenges concerning data security and risk that need to be overcome. This research focuses on investigating these challenges posed by blockchain technology and providing feasible solutions that regulators can take into account when drafting regulations on personal data protection guidelines. The security and privacy-related challenges derived from its progressive maturity, complexity, lack of standardization, and diversity of protocols are superimposed on the demands of a vibrant, competitive environment. It is difficult to align it with the GDPR concerning privacy. There is an urgent need to develop multidisciplinary teams that must ensure its participation from the beginning of the legal/regulatory area, cybersecurity, and company information systems.",CS,AI_ML,0.85,Extracted from log - paper 23
Access Control and Privacy-Preserving Blockchain-Based System for Diseases Management,"In many developing countries, the healthcare sector is facing several challenges, mainly due to the lack of personal, institutions, and medications in public health systems. Over the past decade, information and communication technology has proved its ability to improve medical quality, reduce costs, and promote data security. Developing countries can exploit these technologies to improve the healthcare process and ensure remote health monitoring, especially in rural areas. The Internet of Things and smart medical devices are widely used to provide remote patient monitoring. Current systems are based on centralized communication with cloud servers. However, this architecture increases several security and privacy risks. The adoption of a distributed architecture is required to overcome these issues. In this article, we describe a Blockchain-based system for securing Internet-of-Things (IoT) healthcare devices. In addition to data encryption, we propose to use Blockchain technology to enhance security and privacy in healthcare systems. The system is intended to allow remote patient monitoring, particularly for chronic diseases that necessitate regular monitoring. Three important characteristics were taken into account: security, scalability, and processing time. The security concerns are ensured by using the re-encryption proxy in conjunction with Blockchain to encrypt data and control access to it. To ensure Blockchain scalability, data are stored in an InterPlanetary file system (IPFS) off-chain database. We use an Ethereum Blockchain based on proof of authority (PoA) to speed up the data storage. In comparison to existing methods, the experimental system has shown a significant improvement in the security of healthcare systems.",CS,AI_ML,0.85,Extracted from log - paper 24
Secure and Privacy-Preserving Energy Trading With Demand Response Assistance Based on Blockchain,"The large-scale integration of decentralized energy resources has resulted in surgical changes in energy trading systems. Traditional centralized trading systems suffer from high management costs and low efficiency. The recent advance in blockchain technology has enabled the invention of decentralized energy trading systems, which can overcome the limitations of centralized trading systems. However, security and privacy concerns have become obstacles when widely marketing and implementing decentralized energy trading systems. For example, the on-chain transactions are publicly visible, leading to the risk of trading information leakages. Moreover, in the demand response (DR) assistance process, the aggregated report of energy trading results may contain detailed trading information which results in user privacy leakage. To address those problems, this article proposes a blockchain-based privacy-aware energy trading mechanism. We design a stealthy on-chain transmission method to protect user privacy without revealing the consumer-supplier trading relationship and the exact trading data. Furthermore, we employ non-interactive zero-knowledge (NIZK) proof to aggregate reports in a reliable and privacy-preserving manner. Security analysis and experimental results demonstrate that the proposed mechanism can achieve security goals with affordable cost for decentralized energy trading systems.",CS,AI_ML,0.85,Extracted from log - paper 25
Integration of Blockchain technology in biomedical diagnostics: Ensuring data security and privacy in infectious disease surveillance,"The integration of blockchain technology in biomedical diagnostics offers a promising solution to the challenges of data security and privacy in infectious disease surveillance. As the digitalization of healthcare systems accelerates, the need to protect sensitive health information becomes increasingly critical. Blockchain, with its decentralized and immutable nature, provides a robust framework for ensuring the integrity and confidentiality of biomedical data. This abstract explores how blockchain technology can be leveraged to enhance data security and privacy in the context of infectious disease surveillance, where rapid and accurate data sharing is essential for effective public health responses. Infectious disease surveillance relies on the collection, analysis, and dissemination of large volumes of data, often shared across multiple institutions and geographical regions. Traditional systems for managing this data are vulnerable to breaches, unauthorized access, and data tampering, which can compromise public health efforts and patient privacy. Blockchain technology addresses these vulnerabilities by enabling secure, transparent, and tamper-proof data exchanges. Each transaction or data entry is recorded in a distributed ledger, accessible only to authorized participants, thus ensuring that the data remains secure and unaltered. Moreover, blockchain’s inherent transparency allows for real-time monitoring and auditing of data flows, which is crucial in the timely detection and response to infectious disease outbreaks. The use of smart contracts within blockchain networks further enhances the automation and efficiency of data management, ensuring that data is only accessed and shared according to predefined rules and conditions. This not only safeguards patient privacy but also builds trust among stakeholders, including patients, healthcare providers, and public health authorities. In conclusion, the integration of blockchain technology in biomedical diagnostics presents a transformative approach to addressing the critical issues of data security and privacy in infectious disease surveillance. By leveraging blockchain's unique features, healthcare systems can ensure that sensitive diagnostic data is protected, thus supporting more effective and secure public health interventions in the fight against infectious diseases. Keywords: Blockchain, Biomedical Diagnostics, Data Security, Privacy, Infectious Disease Surveillance.",CS,AI_ML,0.85,Extracted from log - paper 26
Efficient and secure privacy protection scheme and consensus mechanism in MEC enabled e-commerce consortium blockchain,"The application of blockchain technology to the field of e-commerce has solved many dilemmas, such as low transparency of transactions, hidden risks of data security and high payment costs. Mobile edge computing(MEC) can provide computational power for blockchain, and can meet the demand for high real-time and low latency in e-commerce transaction systems. However, there are still some constraints in the MEC enabled e-commerce consortium blockchain, such as the leakage of user privacy information, low security of consensus algorithm and other security issues. In this paper, we propose a secure transaction model suitable for MEC enabled e-commerce consortium blockchain, aiming to ensure the efficiency of system transaction processing while improving the security of users’ privacy information and transaction data. The model adopts the lightweight Paillier encryption algorithm to protect the security of user privacy information and transaction data to prevent the leakage of user privacy information, and optimizes the security of leader election phase of Raft consensus algorithm by introducing the shamir secret sharing protocol to improve the anti-Byzantine failure capabilities of Raft consensus algorithm. The effectiveness of the scheme proposed in this paper is demonstrated by experimental simulations.",CS,AI_ML,0.85,Extracted from log - paper 27
Application of Blockchain and Internet of Things (IoT) for Ensuring Privacy and Security of Health Records and Medical Services,"The research’s introduction primarily focuses on the advantages of putting in place a data security framework to protect all health information. The medical associations should employ IoT and a wifi system to create an effective data security system. It is essential to think about all the benefits and the importance of using an information security management system to keep everything documents pertaining to medical facilities secure. In the study’s evaluation of the data, the advantages of utilising a security control system for information to protect health history are primarily the emphasis. All essential benefits and technology, though, must be primarily focused on managing all data security systems. It is crucial to possess all the knowledge necessary to keep healthcare data safe in order to correctly access all patient data. It is essential to create this data security management to keep all medical records secure and private secure. Two tests—a a one-sample T test and a regression test — were run during the data analysis phase, using the year and the prevalence of cryptography in the healthcare sector as independent variables in each test. An enhanced level of medical record security is the dependent variable. The outcomes were contrasted with the calculated values. The threshold for significance is fixed at 0.05.",CS,AI_ML,0.85,Extracted from log - paper 28
A Survey on Security and Privacy in Blockchain-based Central Bank Digital Currencies,"The increasing interest in Central Bank Digital Currencies has heightened the need for the suitable security technologies for preserving the privacy of users of the CBDC. Although the CBDC system architecture is deeply related to the legacy payment system and the public blockchain system, security and privacy issues of the CBDC are completely different from those of the existing systems as the purpose of the CBDC is to achieve auditable privacy. We demonstrate the taxonomy of the security and privacy issues in CBDC system according to the following areas: identity, transaction, consensus and auditability. We also emphasize the research gaps in the ﬁelds stem from the CBDC’s unique characteristics including the authorized audit risk problem and the cross-border payments problem. This study contributes to the current understanding of the security and privacy concerns of CBDCs and addresses the remaining gaps in this ﬁeld of research.",CS,AI_ML,0.85,Extracted from log - paper 29
Blockchain-Based Data Management and Control System in Rail Transit Security Scenario,"During the 14th Five-Year Plan period, China's urban rail transit market has exhibited steady growth, paralleled by increases in passenger volume and emerging safety challenges. The advent of national standards such as GB 51151 has heightened safety requirements, pressing the need for technological advancements in rail transit security systems. Traditional security systems suffer from isolated operations and inefficient information exchanges, necessitating additional human resources for management. We propose integrating blockchain technology to enhance trust and security across disparate systems. Additionally, the introduction of heterogeneous query blockchain middle-ware facilitates cross-chain data interoperability and advanced querying capabilities, further enriching our multimodal, fine-grained blockchain security management system that leverages Fabric's channel isolation for secondary permission control. This system not only ensures secure data transmission and storage but also addresses privacy and trust issues, enabling unified data handling and traceability across rail transit security platforms. The experiment demonstrated the efficacy of our work",CS,AI_ML,0.85,Extracted from log - paper 30
Blockchain-Enabled Trust Management With Location Privacy Preservation in Vehicular Ad Hoc Networks,"With the advancement of intelligent transportation systems, location-based services (LBSs) have been widely applied in vehicular ad hoc networks (VANETs). LBS utilizes mobile devices to gather vehicle location data, which is then processed using relevant technologies. By combining this data with additional information, LBS offers users personalized and intelligent services. However, providing LBS brings critical security issues related to the exposure of vehicle positions, as well as privacy-preserving problems during the process of collecting location information in VANETs. We propose a distributed trust-based k anonymity scheme to address the aforementioned issues. Our proposed scheme adopts a trust framework among vehicles for various types of LBS. This framework involves a multiparty evaluation and consideration of trust value fluctuations to enhance the efficiency of establishing a reliable k anonymous cloaking region. Furthermore, by leveraging the tamper-proof and decentralized nature of blockchain, we employ a lightweight consortium blockchain to maintain the security of the trustworthiness data throughout the entire model. Extensive security analysis and rigorous experiments have been conducted to demonstrate that the scheme exhibits a certain degree of resilience against attacks on various trust models. Additionally, it has the ability to construct anonymous regions with limited time delay, thereby preserving the privacy of vehicle locations. In comparison to other schemes, it exhibits lower computational complexity and enhanced security.",CS,AI_ML,0.85,Extracted from log - paper 31
Data security sharing model based on privacy protection for blockchain‐enabled industrial Internet of Things,"With the widespread application of Industrial Internet of Things (IIoT) technology in the industry, the security threats are also increasing. To ensure the safe sharing of resources in IIoT, this paper proposes a data security sharing model based on privacy protection (DSS‐PP) for blockchain‐enabled IIoT. Compared with previous works, DSS‐PP has obvious advantages in several important aspects: (1) In the process of identity authentication, it protects users' personal information by using authentication technology with hidden attributes; (2) the encrypted shared resources are stored in off‐chain database of the blockchain, while only the ciphertext index information is stored in the block. It reduces the storage load of the blockchain; (3) it uses blockchain logging technology to trace and account for illegal access. Under the hardness assumption of Inverse Computational Diffe–Hellman (ICDH) problem, this model is proven to be correct and safe. Through the analysis of performance, DSS‐PP has better performance than the referred works.",CS,AI_ML,0.85,Extracted from log - paper 32
Advancing Healthcare IoT: Blockchain and Federated Learning Integration for Enhanced Security and Insights,"Fast-growing IoT devices in healthcare have ushered in a new era of data-driven patient care and treatment. These gadgets capture too much sensitive medical data, a major issue. We need to boost data analysis, security, and privacy. This research uses cutting-edge concepts like federated learning and blockchain technology to improve Healthcare IoT analytics and security. This study examines blockchain and federated learning's numerous integration aspects. It begins with data security, employing blockchain's immutability and decentralization to prevent manipulation. Federation learning was utilized to solve privacy concerns while adhering to tight healthcare data security regulations, providing strong data analysis without releasing raw, patient-specific information. This study examines how federated learning and blockchain technologies enhance healthcare IoT systems' analytics. This connection allows safe, collaborative analysis among healthcare professionals and devices, offering more insights for research, diagnostics, and tailored patient care. How to integrate these cutting-edge technologies into healthcare IoT networks is also explored.",CS,AI_ML,0.85,Extracted from log - paper 33
Multiple Layer Public Blockchain Approach for Internet of Things (IoT) Systems,"All of us know that cryptography is an innovative security strategy. Network security construction with authentication techniques of its layers through blockchain technologies is an important field to discuss. The problem that we are trying to solve is the Transaction Privacy Leakage in public Blockchain networks with IOT networks, which has resulted in the publicity of this data on the network as well as synchronizing the information that allowed it to be accessed and propagated between distributed nodes. At the same time, there are some privacy risk concerns associated with public data wherein transactions contain sensitive information about their issuers. Although some previous research introduced models to deal with the problem of Transaction Privacy Leakage like deterministic key generation, mixing services, ring signature, zero-knowledge proof, and quantum-resistant algorithms, the suggested models do not fully achieve prevention or integrity in all cases. This paper presents a developed Multi-Layer Blockchain Security Model (MLBSM) that can be used to protect IoT networks while also facilitating their implementation for protecting IoT networks and similar networks to prevent Transaction Privacy Leakage for all users in the public blockchain network. The clustering concept is utilized to facilitate the multi-layer architecture. By implementing this, we may achieve unprecedented levels of security and transparency in the blockchain network which will protect the privacy of all users in different technologies.",CS,AI_ML,0.85,Extracted from log - paper 34
A Lightweight Blockchain-Based Remote Mutual Authentication for AI-Empowered IoT Sustainable Computing Systems,"Internet of Things (IoT) has led to significant advancements in communication technologies, specifically, concerning IoT-based sustainable information systems. Lately, industry-academic communities have made great strides for the development of security in IoT-based applications, such as traffic management, industrial automation systems, military surveillance systems, transportation, parking, etc. The sustainable IoT converges AI and blockchain technologies for enhancing quality of individual’s life. As a result, emerging IoT applications operate a distributed ledger technology to provide robust-level of encryption and execution for contractual agreement that resolves interoperability and security issues. Thus, this article proposes a blockchain-based remote mutual authentication (B-RMA) that considers smart devices and cloud networks to offer security and privacy. The proposed B-RMA can coexist with the IoT-based smart environment to decentralize the processing of user authentication requests. The prominence of the proposed strategies including security efficiency and privacy protection, is evaluated using informal security analysis. Moreover, a runtime platform “Node.js” was used to analyze the communication metrics, such as execution time, throughput, and overhead ratio, over the concurrent requests. The investigation results prove that the B-RMA achieves a scalable environment, accordingly.",CS,AI_ML,0.85,Extracted from log - paper 35
A Blockchain-based Model for Securing Data Pipeline in a Heterogeneous Information System,"In our digital world, access to personal and public data has become an item of concern, with challenging security and privacy aspects. Modern information systems are heterogeneous in nature and have an inherent security vulnerability, which is susceptible to data interception and data modification due to unsecured communication data pipelines between connected endpoints. This re-search article presents a blockchain-based model for securing data pipelines in a heterogeneous information system using an integrated multi-hazard early warning system (MHEWS) as a case study. The proposed model utilizes the inherent security features of blockchain technology to address the security and privacy concerns that arise in data pipelines. The model is designed to ensure data integrity, confidentiality, and authenticity in a decentralized manner. The model is evaluated in a hybrid environment using a prototype implementation and simulation experiments with outcomes that demonstrate advantages over traditional approaches for a tamper-proof and immutable data pipeline for data authenticity and integrity using a confidential ledger.",CS,AI_ML,0.85,Extracted from log - paper 36
SC-CAAC: A Smart-Contract-Based Context-Aware Access Control Scheme for Blockchain-Enabled IoT Systems,"Integrating blockchain technology with the Internet of Things (IoT) facilitates seamless interaction between IoT devices and systems to securely share, access, and exchange data. However, ensuring adequate access control within blockchain-enabled IoT (BIoT) systems remains a significant challenge. It is often difficult to adapt existing access control mechanisms to the dynamic and context-dependent nature of IoT environments, necessitating a robust context-aware approach to ensure adequate security and the privacy of resources within BIoT systems. In this article, we propose a novel smart contract-enabled context-aware access control (SC-CAAC) scheme for BIoT systems. It utilizes context-aware access control models that consider contextual information, including user profile, purpose, date, time, location, resource, and operating environment specifications, to make access control decisions. Smart contracts dynamically enforce access control policies and manage access permissions, ensuring that sensitive data and resources are accessible only to authorized users. The proposed scheme leverages the immutability, transparency, and decentralization of a blockchain that is shared by multiple participants in a consortium network, removing the need for a central authority to record and audit access control policies and decisions and promoting accountability and trust. The implementation and evaluation of our proposed scheme using the Hyperledger Besu blockchain demonstrates its effectiveness and scalability in real-world scenarios.",CS,AI_ML,0.85,Extracted from log - paper 37
Enhancing Privacy and Data Security across Healthcare Applications Using Blockchain and Distributed Ledger Concepts,"Nowadays, blockchain is developing as a secure and trustworthy platform for secure information sharing in areas of application like banking, supply chain management, food industry, energy, the Internet, and medical services. Besides, the blockchain can be described in a decentralized manner as an immutable ledger for recording data entries. Furthermore, this new technology has been developed to interrupt a variety of data-driven fields, including the health sector. However, blockchain refers to the distributed ledger technology, which constitutes an innovation in the information recording and sharing without a trusted third party. In this paper, blockchain and Distributed Ledger-based Improved Biomedical Security system (BDL-IBS) has been proposed to enhance the privacy and data security across healthcare applications. Further, our goal is to make it possible for patients to use the data to support their care and to provide strong consent systems for sharing data among different organizations and applications, since this includes managing and accessing a high amount of medical information, and this technology can maintain data to ensure reliability. Finally, results show that new blockchain-based digital platforms allow for fast, easy, and seamless interactions between data suppliers to enhance privacy and data security, including for patients themselves.",CS,AI_ML,0.85,Extracted from log - paper 38
Parallel Management of IoV Information Enabled by Blockchain and Decentralized Autonomous Organizations,"With the development of intelligent transportation technologies, the Internet of Vehicles (IoV) faces challenges such as data silos, security and privacy concerns, data quality issues, and collaboration barriers. To address the various challenges, this paper proposes an innovative integration scheme called the IoV Data Management System (IDMS). This system is built upon blockchain and parallel intelligence technologies, aiming to solve the challenges presented in the IoV domain. The proposed system uses the decentralized, immutable and traceable characteristics of blockchain, combined with the incentive mechanism and collaboration model of decentralized autonomous organization (DAO), to build a secure and trusted data sharing platform to solve data problems in IoV. This research combines parallel intelligence, blockchain and DAO technologies to provide an innovative framework for IoV information management. The proposed framework enables parallel management of connected vehicle systems, thereby improving safety, reliability and efficiency. Furthermore, it would promote the development and application of vehicle networking technology, and provide more intelligent, convenient and safe services for people's travel experience. Finally, a parking data sharing case study validates the effectiveness of the designed system and demonstrates its potential to solve IoV data management challenges.",CS,AI_ML,0.85,Extracted from log - paper 39
A Secure and Privacy-Preserving E-Government Framework Using Blockchain and Artificial Immunity,"Electronic Government (e-Government) systems constantly provide greater services to people, businesses, organisations, and societies by offering more information, opportunities, and platforms with the support of advances in information and communications technologies. This usually results in increased system complexity and sensitivity, necessitating stricter security and privacy-protection measures. The majority of the existing e-Government systems are centralised, making them vulnerable to privacy and security threats, in addition to suffering from a single point of failure. This study proposes a decentralised e-Government framework with integrated threat detection features to address the aforementioned challenges. In particular, the privacy and security of the proposed e-Government system are realised by the encryption, validation, and immutable mechanisms provided by Blockchain. The insider and external threats associated with blockchain transactions are minimised by the employment of an artificial immune system, which effectively protects the integrity of the Blockchain. The proposed e-Government system was validated and evaluated by using the framework of Ethereum Visualisations of Interactive, Blockchain, Extended Simulations (i.e. eVIBES simulator) with two publicly available datasets. The experimental results show the efficacy of the proposed framework in that it can mitigate insider and external threats in e-Government systems whilst simultaneously preserving the privacy of information.",CS,AI_ML,0.85,Extracted from log - paper 40
Blockchain-Based Medical Certificate Generation and Verification for IoT-Based Healthcare Systems,"Nowadays, medical certificates are very important for many users as they want to avail health benefits like tax purposes, insurance claims, legal procedures, and many more. Generating, issuing, and maintaining medical certificates remain a significant problem; before the invention of the computer, they were available as hard copies. The digitization of medical certificates and documents leads to potential security issues, such as forging of certificates risks the privacy of healthcare documents. Moreover, individuals still need to be physically present and wait at the issuing healthcare centers to get the certificates. Currently, the infrastructure of any healthcare industry connects the Internet of Things (IoT) devices and application software that communicates with the information technology systems. Blockchain technology with IoT can significantly affect the healthcare industry by improving efficiency, security, transparency, and can provide more business opportunities. Therefore, a privacy-preserving technique has been proposed in this article for IoT-based healthcare systems using blockchain technology. The proposed architecture provides an interface between the users and healthcare centers to generate and maintain healthcare documents. Furthermore, the proposed scheme ensures security by specifying rules with a smart contract. Results and discussion show that the proposed scheme is more efficient than the existing schemes.",CS,AI_ML,0.85,Extracted from log - paper 41
"Security, Privacy and Risks Within Smart Cities: Literature Review and Development of a Smart City Interaction Framework","The complex and interdependent nature of smart cities raises significant political, technical, and socioeconomic challenges for designers, integrators and organisations involved in administrating these new entities. An increasing number of studies focus on the security, privacy and risks within smart cities, highlighting the threats relating to information security and challenges for smart city infrastructure in the management and processing of personal data. This study analyses many of these challenges, offers a valuable synthesis of the relevant key literature, and develops a smart city interaction framework. The study is organised around a number of key themes within smart cities research: privacy and security of mobile devices and services; smart city infrastructure, power systems, healthcare, frameworks, algorithms and protocols to improve security and privacy, operational threats for smart cities, use and adoption of smart services by citizens, use of blockchain and use of social media. This comprehensive review provides a useful perspective on many of the key issues and offers key direction for future studies. The findings of this study can provide an informative research framework and reference point for academics and practitioners.",CS,AI_ML,0.85,Extracted from log - paper 42
Governing Principles of Self-Sovereign Identity Applied to Blockchain Enabled Privacy Preserving Identity Management Systems,"Digital identity is the key element of digital transformation in representing any real-world entity in the digital form. To ensure a successful digital future the requirement for an effective digital identity is paramount, especially as demand increases for digital services. Several Identity Management (IDM) systems are developed to cope with identity effectively, nonetheless, existing IDM systems have some limitations corresponding to identity and its management such as sovereignty, storage and access control, security, privacy and safeguarding, all of which require further improvement. Self-Sovereign Identity (SSI) is an emerging IDM system which incorporates several required features to ensure that identity is sovereign, secure, reliable and generic. It is an evolving IDM system, thus it is essential to analyse its various features to determine its effectiveness in coping with the dynamic requirements of identity and its current challenges. This paper proposes numerous governing principles of SSI to analyse any SSI ecosystem and its effectiveness. Later, based on the proposed governing principles of SSI, it performs a comparative analysis of the two most popular SSI ecosystems uPort and Sovrin to present their effectiveness and limitations.",CS,AI_ML,0.85,Extracted from log - paper 43
Secure Health Information System with Blockchain Technology,"This paper focuses on highlighting the problems that are associated with the absence of privacy and security of medical records in a healthcare system. It seeks to bridge the gap between the currently used security protocols in the management of health information, and encryption algorithms that should be used. Extant health information systems have always been developed with conventional databases. With all the privileges to read, write and execute assigned to the administrator, who has centralised control over all medical records, there is the likelihood of the misuse, distortion and loss of such records in the event that the administrator becomes compromised or inadvertent system failure. To solve this problem, the use of decentralised and distributed databases becomes paramount. Blockchain technology has recently received much attention due to its ability to permit a peer-to-peer network with distributed databases that can be stored locally on each node in the network. Subsequently, all updates on records in a database are communicated to all participating parties, hence addressing the problem of centralised control. In this paper, we propose a health information system on a blockchain to create a trust-free system for both health personnel and patients. From the results obtained, we achieved the decentralisation of the medical records’ database to enhance the security and privacy of data on the modeled peer-to-peer network.",CS,AI_ML,0.85,Extracted from log - paper 44
A Novel Blockchain and Bi-Linear Polynomial-Based QCP-ABE Framework for Privacy and Security over the Complex Cloud Data,"As a result of the limited resources available in IoT local devices, the large scale cloud consumer’s data that are produced by IoT related machines are contracted out to the cloud. Cloud computing is unreliable, using it can compromise user privacy, and data may be leaked. Because cloud-data and grid infrastructure are both growing exponentially, there is an urgent need to explore computational sources and cloud large-data protection. Numerous cloud service categories are assimilated into numerous fields, such as defense systems and pharmaceutical databases, to compute information space and allocation of resources. Attribute Based Encryption (ABE) is a sophisticated approach which can permit employees to specify a higher level of security for data stored in cloud storage facilities. Numerous obsolete ABE techniques are practical when applied to small data sets to generate cryptograms with restricted computational properties; their properties are used to generate the key, encrypt it, and decrypt it. To address the current concerns, a dynamic non-linear polynomial chaotic quantum hash technique on top of secure block chain model can be used for enhancing cloud data security while maintaining user privacy. In the proposed method, customer attributes are guaranteed by using a dynamic non- polynomial chaotic map function for the key initialization, encryption, and decryption. In the proposed model, both organized and unorganized massive clinical data are considered to be inputs for reliable corroboration and encoding. Compared to existing models, the real-time simulation results demonstrate that the stated standard is more precise than 90% in terms of bit change and more precise than 95% in terms of dynamic key generation, encipherment, and decipherment time.",CS,AI_ML,0.85,Extracted from log - paper 45
Intuitive Development to Examine Collaborative IoT Supply Chain System Underlying Privacy and Security Levels and Perspective Powering through Proactive Blockchain,"Undoubtedly, the supply chain management (SCM) system is an important part of many organizations worldwide; over time, the technologies used to manage a supply chain ecosystem have, therefore, a great impact on businesses’ effectiveness. Among others, numerous developments have been made that targeted to have robust supply chain systems to efficiently manage the growing demands of various supplies, considering the underlying requirements and main challenges such as scalability, specifically privacy and security, of various business networks. Internet of things (IoT) comes with a solution to manage a complex, scalable supply chain system, but to provide and attain enough security during information exchange, along with keeping the privacy of its users, is the great inherent challenge of IoT. To fulfill these limitations, this study designs and models a scaled IoT-based supply chain (IoT-SC) system, comprising several operations and participants, and deploys mechanisms to leverage the security, mainly confidentially, integrity, authentication (CIA), and a digital signature scheme to leverage potentially secured non-repudiation security service for the worst-case scenario, and to leverage privacy to keep users sensitive personal and location information protected against adversarial entities to the IoT-SC system. Indeed, a scaled IoT-SC system certainly opens new challenges to manage privacy and security while communicating. Therefore, in the IoT-SC system, each transaction writes from edge computing nodes to the IoT-SC controller is thoroughly examined to ensure the proposed solutions in bi-directional communication, and their robustness against adversarial behaviors. Future research works, employing blockchain and its integrations, are detailed as paces to accelerate the privacy and security of the IoT-SC system, for example, migrating IoT-centric computing to an immutable, decentralized platform.",CS,AI_ML,0.85,Extracted from log - paper 46
Blockchain-based Electronic Medical Record Security Sharing Scheme,"To solve the difficulty of medical data sharing in traditional medical information systems, we proposed an electronic medical record secure-sharing scheme based on the Blockchain technique. The encrypted text of the patient’s electronic medical record is stored in the cloud server while the metadata of the medical record and access strategy is stored in the blockchain system. We employed smart contracts in the blockchain system to achieve user rights management. We used the decentralized, tamper-proof, and traceable features of the blockchain to realize the safe sharing of electronic medical records. The experimental results of security analysis show that the method can defend against potential network attacks while satisfying patient privacy protection and confidentiality. This study verifies the feasibility and great operating efficiency of the blockchain-based electronic medical record security sharing scheme.Clinical relevance— Our proposed blockchain-based electronic medical record-sharing scheme has great potential for the safe access of third-party users to patient data.",CS,AI_ML,0.85,Extracted from log - paper 47
A secure blockchain framework for healthcare records management systems,"Abstract Electronic health records are one of the essential components of health organizations. In recent years, there have been increased concerns about privacy and reputation regarding the storage and use of patient information. In this regard, the information provided as a part of medical and health insurance, for instance, can be viewed as proof of social insurance and governance. Several problems in the past few decades regarding medical information management have threatened patient information privacy. In intelligent healthcare applications, the privacy of patients' data is one of the main concerns. As a result, blockchain is a severe necessity as it can enhance transparency and security in medical applications. Accordingly, this paper uses the design science method to propose a secure blockchain framework for healthcare records management systems. The proposed framework comprises five components: a blockchain network, smart contracts, privacy key management, data encryption, and integration with healthcare information technology. In the proposed framework, healthcare organizations can manage healthcare information securely and privately. Additionally, a secure storage system for electronic records is proposed to meet these organizations' needs. It provides security and privacy for healthcare organizations, especially when managing healthcare information, and also proposes a secure storage system for electronic records to meet the needs of the organizations.",CS,AI_ML,0.85,Extracted from log - paper 48
A Stealthy Communication Model with Blockchain Smart Contract for Bidding Systems,"With the widespread adoption of blockchain technology, its public ledger characteristic enhances transaction transparency but also amplifies the risk of privacy breaches. Attackers can infer users’ real identities and behaviors by analyzing public transaction patterns and address relationships, posing a severe threat to users’ privacy and security, and thus hindering further advancements in blockchain applications. To address this challenge, covert communication has emerged as an effective strategy for safeguarding the privacy of blockchain users and preventing information leakage. But existing blockchain-based covert communication schemes rely solely on the immutability of blockchain itself for robustness and suffer from low transmission efficiency. To tackle these issues, this paper proposes a stealthy communication model with blockchain smart contract for bidding systems. The model initiates by preprocessing sensitive information using a secret-sharing algorithm-the Shamir (t, n) threshold scheme-and subsequently embeds this information into bidding amounts, facilitating the covert transfer of sensitive data. We implemented and deployed this model on the Ethereum platform and conducted comprehensive performance evaluations. To assess the stealthiness of our approach, we employed a suite of statistical tests including the CDF, the Kolmogorov–Smirnov test, Welch’s t-test and K–L divergence. These analyses confirmed that amounts carrying concealed information were statistically indistinguishable from regular transactions, thus validating the effectiveness of our solution in maintaining the anonymity and confidentiality of information transmission within the blockchain ecosystem.",CS,AI_ML,0.85,Extracted from log - paper 49
Enhancing Cloud Communication Security: A Blockchain-Powered Framework with Attribute-Aware Encryption,"The global production of information continuously increases in quantity and variety. However, the tools and technologies developed to handle such large volumes of data have not adequately met the security and privacy requirements. Existing cloud security systems, often managed by a trusted third party, are susceptible to various security risks. To address these challenges and ensure the protection of personal information, blockchain technology emerges as a crucial solution with substantial potential. This research uses the blockchain-powered attribute-aware encryption method to establish a real-time secure communication approach over the cloud. By employing attribute-based encryption technology, data owners can implement fine-grained search permissions for data users. The proposed solution incorporates accessible encryption technology to enable secure access to encrypted data and facilitate keyword searches on the blockchain. This study provides a functional comparison of recently developed attribute-based encryption algorithms. The access control strategy comprises two access tree types and a linear secret-sharing system, serving as the main components. The elliptic curve’s base field was set to 512b, and the bilinear pairing parameter type used was Type-A. This approach involves storing keywords on a remote server and encrypting them using attribute-based encryption. Furthermore, the encrypted data blockchain and the corresponding ciphertext are stored in the blockchain. Numerical experiments were conducted to evaluate the system’s key generation, trapdoor building, and keyword retrieval capabilities.",CS,AI_ML,0.85,Extracted from log - paper 50
"Blockchain-based decentralized trust management in IoT: systems, requirements and challenges","Internet of Things (IoT) vision has astoundingly transcended environmental sensing with integrated computing systems and smart devices, providing seamless connectivity among humans, machines, and their environment to cooperate for convenience and economical benefits. Apart from all the tremendous benefits of IoT, this paradigm still suffers from challenges of security and privacy vulnerabilities and demands a secure system for effective utilization of services in real-world IoT scenarios relying on which the IoT consumers expect secure and trustworthy communications. Trust Management (TM), which is a crucial aspect of security, plays a vital role in ensuring the exchange of information in a secure manner and maintaining the reliability of a system by measuring the degree of trust on IoT devices, reducing the uncertainties and risks involved in the systems. Thus, in recent years, Blockchain technology has been utilized for developing security innovations in TM field for different classes of IoT applications. It can provide tamper-proof data by enabling more reliable trust information and integrity verification, ultimately enhancing its availability and privacy when storing and sharing information. This paper provides a comprehensive survey that aims at analyzing and assessing Blockchain-based decentralized trust management systems (BCDTMS) for IoT. The contributions of this study are threefold; first, we provide the comprehensive and comparative analysis of state-of-the-art BCDTMS devised for different IoT classes such as Internet of Medical of Things (IoMT), Internet of Vehicles (IoV), Industrial IoT (IIoT), and Social IoT (SIoT). To make it an extensive study, we perform a detailed assessment of the existing BCDTMS in the literature in the aspects of Blockchain and TM. Second, we present requirements for developing Blockchain-based TM systems for IoT, and third we have highlighted the challenges in the context of using Blockchain for TM in various IoT applications.",CS,AI_ML,0.85,Extracted from log - paper 51
Blockchain in Healthcare for Achieving Patients’ Privacy,"Heath data are sensitive and valuable for individuals. The patients need to integrate and manage their medical data continuously. Personal Health Record (PHR) is introduced as a solution for managing their health information. It gives patients ownership over their medical data and provides physicians with realignment data. However, it does not achieve reliability, traceability, trust, nor security of patient control. Centralization of any data is vulnerable to the problem of hacking and single failure in addition to control from one organization. So, the centralization of data is the common problem that all current healthcare systems suffer from. Even in the Metaverse world, the application of Metaverse in healthcare services loses users’ privacy as one of its challenges. In this study, we suggest using blockchain in healthcare to improve security and privacy in medical records. The proposed system employs the advantages of blockchain technology to give patients full control over their data with low throughput, high overhead, and latency. We present a security analysis of our suggested architecture as well as blockchain issues in healthcare systems.",CS,AI_ML,0.85,Extracted from log - paper 52
Privacy Protection Scheme for Personal Health Record System Using Blockchain Based on Homomorphic Encryption,"With the advancement and popularization of science and technology, much research explores the provision of health care services or health management with the assistance of information technology in addition to traditional clinical diagnosis. Personal Health Records (PHR) are available for personalized health record information in the autonomous management system. The personal health record system is to improve disease management or strengthen personal health management. However, users are concerned about the safety and confidentiality of PHR in healthcare systems. In 2008, the blockchain architecture was proposed by Satoshi as a peer-to-peer network architecture that contains a Distributed Ledger Technology (DLT). In this study, we proposed a blockchain-based PHR system using the homomorphic encryption to improve the privacy and security of the users. It allows a third party to perform operations on the ciphertext which can be retrieved correctly later, while the privacy and security of the nodes on the chain are ensured and provided for multiple users to protect the security of their information.",CS,AI_ML,0.85,Extracted from log - paper 53
HDM-Chain: A Secure Blockchain-based Healthcare Data Management framework to ensure Privacy and Security in the Health Unit,"In conventional medical data monitoring systems are suffering key challenges in phases of information immutability, traceability, transparency, observation, data validation, access permission, reliability, privacy, and safety. Personal Health Records(PHR) have various advantages globally, but at PHRs data is ruled to essential safety and privacy concerns. This paper suggests a method to implement a reliable clarification to these points. Traditionally sophisticated methods trading with the security of health records ordinarily makes information inaccessible system to patients. Certain methods struggle to adjust information reliability, patient desire, and regular communication with supplier information. Blockchain(BC) resolves the preceding difficulties from it shares data in a decentralized and transactional way. The utilize of BC could support the healthcare division to adjust the accessibility, privacy, and security of PHRs. This document suggests a BC framework to efficiently and securely collect and keep health records. It represents a reliable and skilled means of achieving healthcare data for patients, physicians, and security insurance agencies while defending the patients of information. The goal of this activity is to show how the suggested system fits the safety requirements of participants (patients), physicians, and third performances and discusses privacy and safety attention in the medical division.",CS,AI_ML,0.85,Extracted from log - paper 54
Privacy-Preserving in Healthcare Blockchain Systems Based on Lightweight Message Sharing,"Electronic medical records (EMRs) are extremely important for patients’ treatment, doctors’ diagnoses, and medical technology development. In recent years, the distributed healthcare blockchain system has been researched for solving the information isolated island problem in centralized healthcare service systems. However, there still exists a series of important problems such as the patients’ sensitive information security, cross-institutional data sharing, medical quality, and efficiency. In this paper, we establish a lightweight privacy-preserving mechanism for a healthcare blockchain system. First, we apply an interleaving encoder to encrypt the original EMRs. This can hide the sensitive information of EMRs to protect the patient’s privacy security. Second, a (t,n)-threshold lightweight message sharing scheme is presented. The EMRs are mapped to n different short shares, and it can be reconstructed by at least t shares. The EMR shares rather than the original EMRs are stored in the blockchain nodes. This can guarantee high security for EMR sharing and improve the data reconstruction efficiency. Third, the indexes of the stored EMR shares are employed to generate blocks that are chained together and finally form a blockchain. The authorized data users or institutions can recover an EMR by requesting at least t shares of the EMR from the blockchain nodes. In this way, the healthcare blockchain system can not only facilitate the cross-institution sharing process, but also provide proper protections for the EMRs. The security proof and analysis indicate that the proposed scheme can protect the privacy and security of patients’ medical information. The simulation results show that our proposed scheme is more efficient than similar literature in terms of energy consumption and storage space, and the healthcare blockchain system is more stable with the proposed message sharing scheme.",CS,AI_ML,0.85,Extracted from log - paper 55
A Secure and Privacy-Preserving Medical Data Sharing via Consortium Blockchain,"Medical data sharing is of great significance in promoting smart medicine. However, the heterogeneity of information systems used by various medical institutions makes sharing difficult. In addition, since medical data involves a great deal of sensitive information, sharing it could easily lead to the leakage of personal privacy. Blockchain, gained popularity as a distributed ledger technology, has great potential to connect heterogeneous systems and provides authenticity and integrity guarantees for medical data sharing. Focusing on the issues of medical data sharing and privacy protection, we propose a medical data sharing scheme based on consortium blockchain. To achieve access control, attribute-based access control technique is implemented, where patients preset attribute-specific access policies for their medical records, and record requesters are described by a set of attributes. For patients, we devise a hybrid storage mode to write access policies of medical records on the consortium blockchain network and store encrypted medical records off-chain. Leveraging blockchain and smart contracts, access privilege control and access history tracking can be realized. To enhance the key management, a tree of medical records is constructed for each patient, and by simply keeping the medical record trees, patients can recover their encryption keys at any time. Furthermore, we carry out an extensive analysis to show the high security and efficiency of our proposed scheme. Finally, we build a Quorum consortium blockchain on the Tencent Cloud and deploy smart contracts on the chain to simulate transactions in our scheme. The experiment results indicate the proposed scheme achieves good feasibility.",CS,AI_ML,0.85,Extracted from log - paper 56
Optimizing Heterogeneity in IoT Infra Using Federated Learning and Blockchain-based Security Strategies,"The Internet of Things (IoT) and associated capabilities are becoming indispensable in the planning, operation, and administration of intricate systems of all sizes. High-end learning solutions that go beyond the boundaries of the problem are necessary for addressing the variety of communication concerns (compatibility, secure communication, etc.) in IoT settings. Building machine learning (ML) networks from disparate data sources is a cutting-edge practice known as Federated Learning (FL). In this article, we implement FL between edge-based servers and devices in a sparsely populated cloud to facilitate cohesive learning and the storage of critical information in smart IoT systems. FL enables collaborative training from a common model by aggregating smaller unit models via regulated edge network participants. Further, all the susceptible device’s information and sensitive message transactions are addressed via blockchain technology. Thus, a blockchain-based security mechanism is integrated to secure user privacy and facilitate widespread practical adoption. Finally, a comparison is made between the proposed model and the three best free, open-source Federated Learning models already in use (FedPD, FedProx, and FedAvg). In terms of statistical, and data heterogeneity (>70% SDI, >97% accuracy), the experimental findings suggest that the proposed model performs better than the existing techniques.",CS,AI_ML,0.85,Extracted from log - paper 57
Security of Blockchain-Based Supply Chain Management Systems: Challenges and Opportunities,"Blockchain is a revolutionary technology that is being used in many applications, including supply chain management. Although, the primary motive of using a blockchain for supply chain management is to reduce the overall production cost while providing the comprehensive security to the system. However, current blockchain-based supply-chain management (BC-SCM) systems still hold the possibility of cyber attacks. Therefore, the goal of this study is to investigate practical threats and vulnerabilities in the design of BC-SCM systems. As a starting point, we first establish key requirements for the reliability and security of supply chain management systems, i.e., transparency, privacy and traceability, and then discern a threat model that includes two distinctive but practical threats including computational (i.e., the ones that threaten the functionality of the application) and communication (i.e., the ones that threaten information exchange among interconnected services of the application). For investigation, we follow a unique approach based on the hypothesis that reliability is pre-requisite of security and identify the threats considering (i) design of smart contracts and associated supply chain management applications, (ii) underlying blockchain execution environment and (iii) trust between all interconnected supply management services. Moreover, we consider both academic and industry solutions to identify the threats. We identify several challenges that hinder to establish reliability and security of the BC-SCM systems. Importantly, we also highlight research gaps that can help to establish desired security of the BC-SCM. To the best of our knowledge, this paper is the first effort that identifies practical threats to blockchain-based supply chain management systems and provides their counter measures. Finally, this work establishes foundation for future investigation towards practical security of BC-SCM system.",CS,AI_ML,0.85,Extracted from log - paper 58
ENSURING THE CONFIDENTIALITY OF PERSONAL DATA AND SUPPORTING CYBER SECURITY WITH THE HELP OF BLOCKCHAIN,"The recent increase in security breaches and digital surveillance highlights the need to improve privacy and security, especially of users' personal data. Advances in cybersecurity and new legislation promise to improve the protection of personal data. Blockchain and distributed ledger (DTL) technologies provide new opportunities to protect user data through decentralized identification and other privacy mechanisms. These systems can give users greater sovereignty through tools that allow them to own and control their own data. The purpose of the article is to research blockchain technology and mechanisms for achieving reliability in blockchain for the protection and security of personal data. Decentralized and federated identity systems give users control over what, when and how much of their personal information can be shared and with whom. These systems can also reduce cybersecurity threats. Through various consensus algorithms, blockchain-based privacy solutions allow users to better manage their data and ensure that the data and models derived from it are more accurate, honest and reliable.",CS,AI_ML,0.85,Extracted from log - paper 59
Blockchain Family Deed Certificate for Privacy and Data Security,"The rapid development of technology has caused some systems to have changed; most of them in the Industrial Revolution 4.0 era using new methods from various aspects of people's lives. Family Deed Certificate is a family identity that contains data about arrangements, relationships, and the number of family members. A family certificate is an essential thing for every citizen to have. However, related problems that occur are still using conventional systems that cause problems such as loss of family deed, and various manipulations of identity data. Thus, from this problem emerged a solution to guarantee all data and information security using blockchain technology. Blockchain technology is a technology for recording transactions with modern technology, which can only be added but cannot be changed or replaced. Blockchain technology can support various fields such as banking, education, health, and priorities for governance. For this research, this is applied in the field of government, which is a blockchain technology family certificate, various problems in terms of a family certificate that is a copy of a lot of family member data, and editing a deed of change, is very inflexible. With the family certificate system, blockchain technology, data security can be guaranteed so that there is no data falsification and can replace any loss on the family deed. This system uses the literature method that contains and how blockchain works. The Certificate of Family Deed on the blockchain is expected to impact the digital world positively.",CS,AI_ML,0.85,Extracted from log - paper 60
Blockchain for Email Security: A Perspective on Existing and Potential Solutions for Phishing Attacks,"Email security is critical to all types of businesses, as it represents 80% of the plethora of official communication tools used by most organizations worldwide. Attackers use several techniques to trick users to perform harmful actions, mainly via emails. Identifying such activities or circumventing them is better than relying on the end-user's behavior of being unaware. Traditional e-mail systems use centralized servers to provide services, making them a single point of failure if servers are attacked or at least private information is leaked. Thus, a decentralized e-mail system can provide more trust and reliability. This study is an initial attempt to explore the use of Blockchain-based solutions to improve the security and privacy of traditional e-mail systems. This paper presents two-fold coverage of this problem. First, a summary of common email security architectures is presented, outlined, and criticized for various parameters. Second, we propose a technique for solving the problem of phishing emails by targeting changes in the email system structure using Blockchain technology thereby preventing a considerable number of phishing attempts. We discuss the approach along with its advantages and disadvantages.",CS,AI_ML,0.85,Extracted from log - paper 61
Enhancing the Security for Healthcare Data using Blockchain Technology,"Cyber security is the safest way to protect the data from hackers and unauthorized users. Healthcare technologies nowadays face lots of cyber security issues related with the security of the health information and privacy of the data. Cyber security is one of the popularized ways to protect the data from hackers and spammers. HealthCare is the field where the data is highly sensitive and the security for the systems is low. Protecting the sensitive data is achieved by the blockchain method, which is similar to a database but the difference is the data stored in the blockchain in blocks. The new blocks included are connected to previous blocks from a chain like structure, it is very secure, each block stores data and also the hash of previous blocks. So, data cannot be easily accessed or manipulated. The mechanism used in blockchain for the security of the data consensus mechanism which contains the different methodologies includes Proof of Work (PoW), Proof of Stake (PoS), Proof of Space and Proof of Authority. Enhanced proof of stake is a combination of the PoS and DPoS used to increase the security of the system by eliminating the 51% attack in blockchain and reduces the data theft threats and protects the medical records of patients from hackers.",CS,AI_ML,0.85,Extracted from log - paper 62
Hybrid blockchain–based privacy-preserving electronic medical records sharing scheme across medical information control system,"With the development of big data and medical information control system, electronic medical records sharing across organizations for better medical treatment and advancement has attracted much attention both from academic and industrial areas. However, the source of big data, personal privacy concern, inherent trust issues across organizations and complicated regulation hinder the great progress of healthcare intelligence. Blockchain, as a novel technique, has been used widely to resolve the privacy and security issues in electronic medical records sharing process. In this paper, we propose a hybrid blockchain–based electronic medical records sharing scheme to address the privacy and trust issues across the medical information control systems, rendering the electronic medical records sharing process secure, effective, relatively transparent, immutable, traceable and auditable. Considering the above confidential issues, we use different sharing methods for different parts of medical big data. We share privacy-sensitive couples on the consortium blockchain, while sharing the non-sensitive parts on the public blockchain. In this way, authorized medical information control systems within the consortium can access the data on it for precise medical diagnosis. Institutions such as universities and research institutes can get access to the non-sensitive parts of medical big data for scientific research on symptoms to evolve medical technologies. A working prototype is implemented to demonstrate how the hybrid blockchain facilitates the pharmaceutical operations in a healthcare information control ecosystem. A blockchain benchmark tool Hyperledger Caliper is used to evaluate the performance of hybrid blockchain–based electronic medical records sharing scheme on throughput and average latency which proves to be practicable and excellent.",CS,AI_ML,0.85,Extracted from log - paper 63
Business process improvement by means of Big Data based Decision Support Systems: a case study on Call Centers,"Big Data is a rapidly evolving and maturing field which places significant data storage and processing power at our disposal. To take advantage of this power, we need to create new means of collecting and processing large volumes of data at high speed. Meanwhile, as companies and organizations, such as health services, realize the importance and value of ""joined-up thinking"" across supply chains and healthcare pathways, for example, this creates a demand for a new type of approach to Business Activity Monitoring and Management. This new approach requires Big Data solutions to cope with the volume and speed of transactions across global supply chains. In this paper we describe a methodology and framework to leverage Big Data and Analytics to deliver a Decision Support framework to support Business Process Improvement, using near real-time process analytics in a decision-support environment. The system supports the capture and analysis of hierarchical process data, allowing analysis to take place at different organizational and process levels. Individual business units can perform their own process monitoring. An event-correlation mechanism is built into the system, allowing the monitoring of individual process instances or paths.",CS,AI_ML,0.85,Extracted from log - paper 64
Integrated Decision Support for Disaster Risk Management: Aiding Preparedness and Response Decisions in Wildfire Management,"A central challenge in disaster risk management (DRM) is that there are key dependencies and uncertainty between the decisions made at the mitigation, preparedness, response, and recovery stages. Decision support systems for disaster management require information systems that allow timely and reliable integration of data sources from different domains, including information on hazards and vulnerabilities for risk analysis, as well as organizational and logistical information for decision analysis. We propose an analytics-centered framework that integrates predictive and prescriptive models responding to unique characteristics of DRM. The framework relies on probabilistic risk assessment and uses optimization-based simulation of the response phase as a means to inform decisions at the preparedness stage. This paper presents a case study regarding the analysis of preparedness and response decisions for wildfire control in Uruguay. Numerical results illustrate insights from the risk-informed analyses. For instance, slight reductions in the preparedness budget can lead to disproportionate losses during the response stage, whereas slight increases have little effect unless explicitly directed to control high-consequence scenarios. Motivated by a real-world problem, this case study emphasizes the challenges for integrated information systems that enable the potential of analytical decision support frameworks for DRM.",CS,AI_ML,0.85,Extracted from log - paper 65
A Framework for Effective Big Data Analytics for Decision Support Systems,"Supporting decision makers requires a good understanding of the various elements that affect the outcomes of a decision. Decision Support Systems have provided decision makers with such insights throughout its history of usage with varying degrees of success. The availability of data sources was a main limitation to what decision support systems can do. Therefore, with the advent of improved analytical methods for Big data sources new opportunities have emerged that can possibly enhance how decision makers analyze their problem and arrive at decisions using information systems. This paper analyzed current related works on both Big data and decision support systems to identify clear elements and factors relevant to the subject and identifying possible ways to enhance their joint usage. Finally, the paper proposes a framework that integrates the key components needed to ensure the quality and relevance of data being analyzed by decision support systems while providing the benefits of insights generated over time from past decisions and positive recommendations.",CS,AI_ML,0.85,Extracted from log - paper 66
Using health information technology for clinical decision support and predictive analytics,"JAMIA has been a premier venue for publication of scholarly work on clinical decision support systems since its inception. With an initial emphasis on knowledge-based systems, i.e., encoding of clinicians’ knowledge into rules that would trigger alerts and reminders, JAMIA evolved into a phase of an increasing number of articles reporting on the use of EHR “big data” to build and validate predictive models that recognize patterns in large amounts of data to derive actionable recommendations for clinicians. Interestingly, this is happening at the same time that health information technology (HIT) is still evolving, data quality in EHRs is being improved, and health information exchange (HIE) continues to be evaluated for cost effectiveness. Key recommendations for HIT optimization are proposed by Chresswell (p. 186). Additionally, Wright (p. 192) advocates for more testing of EHR systems, Alexander (p. 69) focuses on IT in nursing homes, and Kharrazi (p. 2) promotes an agenda for population health informatics. HIT use by office-based physicians and healthcare reform programs is described in a study by Heisey-Grove (p. 133). Improving the quality of EHRs is addressed by Van der Bij (p. 84) and Jamieson (p. 126). The latter describes a randomized trial on the quality of admission notes from EHRs. A systematic review of EHR usability is reported by Ellsworth (p. 222), Yadav (p. 143) compares EHRs with paper records in the documentation of physical exams, Denny (p. 165) describes hypertension “phenotyping” from EHRs, and Das (p. 24) proposes how to generate discharge recommendations. HIT is now pervasive in healthcare and this issue of JAMIA features several articles on the use of HIT for predictive modeling: Goldstein reviews risk prediction (p. 202) and describes the challenges in predicting mortality over time horizons (p. 180), GillameBert (p. 48) learns temporal rules that predict instability in patients undergoing continuous monitoring, and Lennon (p. 148) shows predictive value in particular combinations of pathology markers for pancreatic cysts. Manaktala (p. 91) proposes a clinical decision support system for sepsis mortality, and Haslam (p. 13) discovers disease relationships from clinical trial data. HIE systems help clinicians share information from a particular patient with each other. However, the cost effectiveness of various types of HIEs remains hard to measure. Downing (p. 116) reports on the policies of 11 health systems, Dixon (p. 99) describes the characteristics of veterans who enroll in HIE, Vest (p. 39) describes the organizational capacity and utility of clinical event notification, and Slovis (p. 30) studies the rate of duplicative CT exams. Pharmacy informatics is an important sub-field of specialization. Nelson (p. 197) describes the interaction between pharmacists and the EHR, and White (p. 175) reports on outcomes of a computerbased system for Vitamin D prescriptions. Medication reconciliation is systematically reviewed by Marien (p. 231). Also related to pharmacy are reports from Eschmann (p. 62), who describes a system to predict hyperkalemia from drug interactions, Heringa (p. 55), who shows how clustering related drug interaction alerts helps reduce the number of alerts, and Manzi (p. 77), who describes a clinical pharmacogenomics service. Other types of computer-based interaction with providers and patients are reported in this issue of the journal: McGrath (p. 213) systematically reviews computer-aided instruction in oral health, Ratanawongsa (p. 109) studies computer use and literacy in safety net outpatient communications, and Kelly (p. 156) reports how families stay engaged in pediatric care through a patient portal. HIT evolved rapidly in the past decade and is likely to continue to evolve at this pace until its many challenges are overcome. Our responsibility as professionals is to help drive HIT, clinical decision support, and predictive analytics to the next level, and to educate the next generation so that they can fill the many knowledge gaps that still exist.",CS,AI_ML,0.85,Extracted from log - paper 67
Machine Learning–Enabled Clinical Information Systems Using Fast Healthcare Interoperability Resources Data Standards: Scoping Review,"Background Machine learning–enabled clinical information systems (ML-CISs) have the potential to drive health care delivery and research. The Fast Healthcare Interoperability Resources (FHIR) data standard has been increasingly applied in developing these systems. However, methods for applying FHIR to ML-CISs are variable. Objective This study evaluates and compares the functionalities, strengths, and weaknesses of existing systems and proposes guidelines for optimizing future work with ML-CISs. Methods Embase, PubMed, and Web of Science were searched for articles describing machine learning systems that were used for clinical data analytics or decision support in compliance with FHIR standards. Information regarding each system’s functionality, data sources, formats, security, performance, resource requirements, scalability, strengths, and limitations was compared across systems. Results A total of 39 articles describing FHIR-based ML-CISs were divided into the following three categories according to their primary focus: clinical decision support systems (n=18), data management and analytic platforms (n=10), or auxiliary modules and application programming interfaces (n=11). Model strengths included novel use of cloud systems, Bayesian networks, visualization strategies, and techniques for translating unstructured or free-text data to FHIR frameworks. Many intelligent systems lacked electronic health record interoperability and externally validated evidence of clinical efficacy. Conclusions Shortcomings in current ML-CISs can be addressed by incorporating modular and interoperable data management, analytic platforms, secure interinstitutional data exchange, and application programming interfaces with adequate scalability to support both real-time and prospective clinical applications that use electronic health record platforms with diverse implementations.",CS,AI_ML,0.85,Extracted from log - paper 68
A Research Review and Taxonomy Development for Decision Support and Business Analytics Using Semantic Text Mining,"By 2018, business analytics (BA), believed by global CIOs to be of strategic importance, had for years been their top priority. It is also a focus of academic research, as shown by a large number of papers, books, and research reports. On the other hand, the BA domain suffers from several incorrect, imprecise, and incomplete notions. New areas and concepts emerge quickly; making it difficult to ascertain their structure. BA-related taxonomies play a crucial role in analyzing, classifying, and understanding related objects. However, according to the literature on taxonomy development in information systems (IS), in most cases the process is ad hoc. BA taxonomies and frameworks are available in the literature; however, some are excessively general frameworks with a high-level conceptual focus, while others are application or domain-specific. Our paper aims to present a novel semi-automatic method for taxonomy development and maintenance in the field of BA using content analysis and text mining. The contribution of our research is threefold: (1) the taxonomy development method, (2) the draft taxonomy for BA, and (3) identifying the latest research areas and trends in BA.",CS,AI_ML,0.85,Extracted from log - paper 69
An AI-based Decision Support System for Predicting Mental Health Disorders,"Approximately one billion individuals suffer from mental health disorders, such as depression, bipolar disorder, schizophrenia, and anxiety. Mental health professionals use various assessment tools to detect and diagnose these disorders. However, these tools are complex, contain an excessive number of questions, and require a significant amount of time to administer, leading to low participation and completion rates. Additionally, the results obtained from these tools must be analyzed and interpreted manually by mental health professionals, which may yield inaccurate diagnoses. To this extent, this research utilizes advanced analytics and artificial intelligence to develop a decision support system (DSS) that can efficiently detect and diagnose various mental disorders. As part of the DSS development process, the Network Pattern Recognition (NEPAR) algorithm is first utilized to build the assessment tool and identify the questions that participants need to answer. Then, various machine learning models are trained using participants’ answers to these questions and other historical data as inputs to predict the existence and the type of their mental disorder. The results show that the proposed DSS can automatically diagnose mental disorders using only 28 questions without any human input, to an accuracy level of 89%. Furthermore, the proposed mental disorder diagnostic tool has significantly fewer questions than its counterparts; hence, it provides higher participation and completion rates. Therefore, mental health professionals can use this proposed DSS and its accompanying assessment tool for improved clinical decision-making and diagnostic accuracy.",CS,AI_ML,0.85,Extracted from log - paper 70
Comparative review of big data analytics and GIS in healthcare decision-making,"This research explores the confluence of big data analytics and Geographic information systems (GIS) in healthcare decision-making. The comparative review delineates the unique strengths of each technology, showcasing potential synergies. Big data analytics harnesses advanced analytics for predictive modeling and clinical decision support, while GIS introduces a spatial context for health data analysis. Future trends suggest integrations with artificial intelligence, real-time analytics, and wearable technology. However, challenges encompass data privacy, biases, and interdisciplinary collaboration. Ethical considerations emphasize transparency, informed consent, and the responsible use of patient data. As these technologies evolve, their seamless integration holds the promise of precision health, community-oriented interventions, and proactive pandemic response, reshaping the landscape of healthcare decision-making.",CS,AI_ML,0.85,Extracted from log - paper 71
Decision Support Systems in Temporomandibular Joint Osteoarthritis: A review of Data Science and Artificial Intelligence Applications.,"With the exponential growth of computational systems and increased patient data acquisition, dental research faces new challenges to manage a large quantity of information. For this reason, data science approaches are needed for the integrative diagnosis of multifactorial diseases, such as Temporomandibular joint (TMJ) Osteoarthritis (OA). The Data science spectrum includes data capture/acquisition, data processing with optimized web-based storage and management, data analytics involving in-depth statistical analysis, machine learning (ML) approaches, and data communication. Artificial intelligence (AI) plays a crucial role in this process. It consists of developing computational systems that can perform human intelligence tasks, such as disease diagnosis, using many features to help in the decision-making support. Patient's clinical parameters, imaging exams, and molecular data are used as the input in cross-validation tasks, and human annotation/diagnosis is also used as the gold standard to train computational learning models and automatic disease classifiers. This paper aims to review and describe AI and ML techniques to diagnose TMJ OA and data science approaches for imaging processing. We used a web-based system for multi-center data communication, algorithms integration, statistics deployment, and process the computational machine learning models. We successfully show AI and data-science applications using patients' data to improve the TMJ OA diagnosis decision-making towards personalized medicine.",CS,AI_ML,0.85,Extracted from log - paper 72
Information Processing and Data Analytics for Decision Making: A Journey From Traditional to Modern Approaches,"Decision making is required by all organizations; however, the decision making styles may differ. ost commonly used decision styles include: (a) Autocratic (b) Democratic (c) Consensus and (d) Participatory. With the Globalization and expansion of businesses, professionals have become highly dependent upon the technology to support the decision making process and decision support systems have come-up as a fastest growing discipline. Present work discusses the evolution of computerized decision support, considering the: (a) Model Driven (b) Data Driven (c) Communication Driven (d) Document Driven and (e) Knowledge Driven decision support systems. All three different business levels: Operational, Tactical and Strategic have been considered in the present work to review the development of decision support systems. The traditional data analysis based approaches have been compared with the latest data analytics approaches including the social media analytics and web analytics. Examples from the different industry sectors have been incorporated for better illustrations of decision support.",CS,AI_ML,0.85,Extracted from log - paper 73
Predictive analytics for data driven decision support in health and care,"Abstract Due to an ever-increasing amount of data generated in healthcare each day, healthcare professionals are more and more challenged with information. Predictive models based on machine learning algorithms can help to quickly identify patterns in clinical data. Requirements for data driven decision support systems for health and care (DS4H) are similar in many ways to applications in other domains. However, there are also various challenges which are specific to health and care settings. The present paper describes a) healthcare specific requirements for DS4H and b) how they were addressed in our Predictive Analytics Toolset for Health and care (PATH). PATH supports the following process: objective definition, data cleaning and pre-processing, feature engineering, evaluation, result visualization, interpretation and validation and deployment. The current state of the toolset already allows the user to switch between the various involved levels, i. e. raw data (ECG), pre-processed data (averaged heartbeat), extracted features (QT time), built models (to classify the ECG into a certain rhythm abnormality class) and outcome evaluation (e. g. a false positive case) and to assess the relevance of a given feature in the currently evaluated model as a whole and for the individual decision. This allows us to gain insights as a basis for improvements in the various steps from raw data to decisions.",CS,AI_ML,0.85,Extracted from log - paper 74
Revisiting Ralph Sprague's Framework for Developing Decision Support Systems,"Ralph H. Sprague Jr. was a leader in the MIS field and helped develop the conceptual foundation for decision support systems (DSS). In this paper, I pay homage to Sprague and his DSS contributions. I take a personal perspective based on my years of working with Sprague. I explore the history of DSS and its evolution. I also present and discuss Sprague’s DSS development framework with its dialog, data, and models (DDM) paradigm and characteristics. At its core, the development framework remains valid in today’s world of business intelligence and big data analytics. I present and discuss a contemporary reference architecture for business intelligence and analytics (BI/A) in the context of Sprague’s DSS development framework. The practice of decision support continues to evolve and can be described by a maturity model with DSS, enterprise data warehousing, real-time data warehousing, big data analytics, and the emerging cognitive as successive generations. I use a DSS perspective to describe and provide examples of what the forthcoming cognitive generation will bring.",CS,AI_ML,0.85,Extracted from log - paper 75
A Hospital Information Management System With Habit-Change Features and Medial Analytical Support for Decision Making,"A hospital information management system (Doctive) with habit-change features and medial analytical support for decision making is developed in this study to reduce the risks of heart diseases. Doctive is targeted for hospital authorities to monitor patients’ habits and to prescribe medication and advice accordingly. Furthermore, this system provides emergency assistance for patients based on their current location. The proposed system would be beneficial for monitoring and organizing patients’ information to ease data entry, data management, data access, data retrieval and finally decision making. Doctive is tested and evaluated by 41 people who are either medical experts or professionals in the field of data analytics and visualization. The results indicate a high acceptance rate towards using Doctive system in hospitals and very good usability of the system. Doctive can be useful for healthcare providers and developers to track users’ habits for reducing the risk of heart disease. In the future.",CS,AI_ML,0.85,Extracted from log - paper 76
Impact of Predictive Analytics of Big Data in Supply Chain Management on Decision-Making,"The beginning of information technology has led to a burst of data in every sector of operation. Handling huge volume of data to mine useful information to support decision making is one of the current sources of competitive advantage for organizations. However, preceding research literature on predictive analytics has attributed a lack of direct causal influence on predictive analytics in a manner that support Supply Chain Management in utility companies’ performance. This is as a result of huge data posing great challenges to practitioners when incorporating it into their complex decision making which adds business value. The purpose of this study was to introduce predictive analytics in supply chain management framework that enhances decision making in Kenya Power and lighting Company in Kenya. The study was guided by the following research objectives; to assess the existing predictive analytics in Supply Chain Management, to analyse existing supply chain management systems in utility companies in Kenya and to develop an integrated predictive analytics framework for big data in supply chain management for decision making in Kenya Power and lighting Company in Kenya. This research employed the Design Science research design because one of the key outcomes of the research was framework development. The study was carried out in Kenya Power & Lighting Company in Western Region in the republic of Kenya. The target population was 10 regional finance officers, 10 regional procurement officers, 47 county stores in-charges, 47 county project supervisors and 47 county business managers totalling to 161 as the sample size. The main tools for data collection were questionnaires, interview schedules and documentary review.",CS,AI_ML,0.85,Extracted from log - paper 77
Intelligent Clinical Decision Support Systems for Patient-Centered Healthcare in Breast Cancer Oncology,"Breast cancer is the most common type of cancer in women worldwide, with incidence rate being second highest to other types of cancer. In the current clinical setting, multidisciplinary breast units are introduced to improve the quality of the therapeutic decision based on the best evidence-based practices. DESIREE project aims to provide a web-based software ecosystem for personalized, collaborative and multidisciplinary management of primary breast cancer by specialized breast units, from diagnosis to therapy and follow-ups. In order to provide a multi-model decision support to clinicians in present clinical settings, the project develops and integrates three modalities of decision support, namely guideline-based decision support system (DSS), experience-based DSS and case-based DSS. Visual analytics GUI are developed to properly adapt the results of the DSSs and graphically represent them to the clinician in a user-friendly manner. DESIREE information management system (DESIMS), serves as the interface between the user and DSSs for entering patient data and viewing the results in the visual analytics GUI. In this paper, we present the overall architecture, workflow and integration of the three DSSs in the DESIREE platform.",CS,AI_ML,0.85,Extracted from log - paper 78
Machine learning in information systems - a bibliographic review and open research issues,"Artificial Intelligence (AI) and Machine Learning (ML) are currently hot topics in industry and business practice, while management-oriented research disciplines seem reluctant to adopt these sophisticated data analytics methods as research instruments. Even the Information Systems (IS) discipline with its close connections to Computer Science seems to be conservative when conducting empirical research endeavors. To assess the magnitude of the problem and to understand its causes, we conducted a bibliographic review on publications in high-level IS journals. We reviewed 1,838 articles that matched corresponding keyword-queries in journals from the AIS senior scholar basket, Electronic Markets and Decision Support Systems (Ranked B). In addition, we conducted a survey among IS researchers (N = 110). Based on the findings from our sample we evaluate different potential causes that could explain why ML methods are rather underrepresented in top-tier journals and discuss how the IS discipline could successfully incorporate ML methods in research undertakings.",CS,AI_ML,0.85,Extracted from log - paper 79
Internet of things and simulation approach for decision support system in lean manufacturing,"Today, Industry 4.0 concerns a rapid advancement in manufacturing technologies which help industries increase their productivity. To adopt Industry 4.0 concept is still visionary by certain lean manufacturers when the communication technologies interfaces are not fully equipped at the production system. Most of the facilities towards digitalization are also expensive and require many specialists in different fields to manage the technologies. Therefore, most data analytics (DA) engineering is cannot be employed broadly for process enhancement by Industry 4.0 environment. However, starting with Internet of Things (IOT) concepts, Andon system with simulation was enhanced to support decision making in lean manufacturing. The aims of this research paper is to develop a decision support system (DSS) framework which intersects between Andon and simulation through IOT concept. A better decision-making information flow are demonstrated in detail. To illustrate the applicability of the DSS, it has been implemented in lean manufacturing for automotive part assembly. The results indicate that the DSS can easily be adopted in digital factories to support in planned and operational activities.",CS,AI_ML,0.85,Extracted from log - paper 80
Secured Big Data Analytics for Decision-Oriented Medical System Using Internet of Things,"The Internet of Medical Things (IoMT) has shown incredible development with the growth of medical systems using wireless information technologies. Medical devices are biosensors that can integrate with physical things to make smarter healthcare applications that are collaborated on the Internet. In recent decades, many applications have been designed to monitor the physical health of patients and support expert teams for appropriate treatment. The medical devices are attached to patients’ bodies and connected with a cloud computing system for obtaining and analyzing healthcare data. However, such medical devices operate on battery powered sensors with limiting constraints in terms of memory, transmission, and processing resources. Many healthcare solutions are helping the community with the efficient monitoring of patients’ conditions using cloud computing, however, mostly incur latency in data collection and storage. Therefore, this paper presents a model for the Secured Big Data analytics using Edge–Cloud architecture (SBD-EC), which aims to provide distributed and timely computation of a decision-oriented medical system. Moreover, the mobile edges cooperate with the cloud level to present a secure algorithm, achieving reliable availability of medical data with privacy and security against malicious actions. The performance of the proposed model is evaluated in simulations and the results obtained demonstrate significant improvement over other solutions.",CS,AI_ML,0.85,Extracted from log - paper 81
Reference architecture design for farm management information systems: a multi-case study approach,"One of the key elements of precision agriculture is the farm management information system (FMIS) that is responsible for data management, analytics and subsequent decision support. Various FMISs have been developed to support the management of farm businesses. A key artefact in the development of FMISs is the software architecture that defines the gross level structure of the system. The software architecture is important for understanding the system, analysing the design decisions and guiding the further development of the system based on the architecture. To assist in the design of the FMIS architecture, several reference architectures have been provided in the literature. Unfortunately, in practice, it is less trivial to derive the application architecture from these reference architectures. Two underlying reasons for this were identified. First of all, it appears that the proposed reference architectures do not specifically focus on FMIS but have a rather broad scope of the agricultural domain in general. Secondly, the proposed reference architectures do not seem to have followed the proper architecture documentation guidelines as defined in the software architecture community, lack precision, and thus impeding the design of the required application architectures. Presented in this article is a novel reference architecture that is dedicated to the specific FMIS domain, and which is documented using the software architecture documentation guidelines. In addition, the systematic approach for deriving application architectures from the proposed reference architecture is provided. To illustrate the approach, the results of multi-case study research are shown in which the presented reference architecture is used for deriving different FMIS application architectures.",CS,AI_ML,0.85,Extracted from log - paper 82
Machine Learning and Decision Support in Critical Care,"Clinical data management systems typically provide caregiver teams with useful information, derived from large, sometimes highly heterogeneous, data sources that are often changing dynamically. Over the last decade there has been a significant surge in interest in using these data sources, from simply reusing the standard clinical databases for event prediction or decision support, to including dynamic and patient-specific information into clinical monitoring and prediction problems. However, in most cases, commercial clinical databases have been designed to document clinical activity for reporting, liability, and billing reasons, rather than for developing new algorithms. With increasing excitement surrounding “secondary use of medical records” and “Big Data” analytics, it is important to understand the limitations of current databases and what needs to change in order to enter an era of “precision medicine.” This review article covers many of the issues involved in the collection and preprocessing of critical care data. The three challenges in critical care are considered: compartmentalization, corruption, and complexity. A range of applications addressing these issues are covered, including the modernization of static acuity scoring; online patient tracking; personalized prediction and risk assessment; artifact detection; state estimation; and incorporation of multimodal data sources such as genomic and free text data.",CS,AI_ML,0.85,Extracted from log - paper 83
Big Data and Data Analytics Research: From Metaphors to Value Space for Collective Wisdom in Human Decision Making and Smart Machines,"The Big Data and Data Analytics is a brand new paradigm, for the integration of Internet Technology in the human and machine context. For the first time in the history of the human mankind we are able to transforming raw data that are massively produced by humans and machines in to knowledge and wisdom capable of supporting smart decision making, innovative services, new business models, innovation, and entrepreneurship. For the Web Science research, this is a new methodological and technological spectrum of advanced methods, frameworks and functionalities never experienced in the past. At the same moment communities out of web science need to realize the potential of this new paradigm with the support of new sound business models and a critical shift in the perception of decision making. In this short visioning article, the authors are analyzing the main aspects of Big Data and Data Analytics Research and they provide their own metaphor for the next years. A number of research directions are outlined as well as a new roadmap towards the evolution of Big Data to Smart Decisions and Cognitive Computing. The authors do hope that the readers would like to react and to propose their own value propositions for the domain initiating a scientific dialogue beyond self-fulfilled expectations.",CS,AI_ML,0.85,Extracted from log - paper 84
Artificial Intelligence and Big Data Analytics in Support of Cyber Defense,"Cybersecurity analysts rely on vast volumes of security event data to predict, identify, characterize, and deal with security threats. These analysts must understand and make sense of these huge datasets in order to discover patterns which lead to intelligent decision making and advance warnings of possible threats, and this ability requires automation. Big data analytics and artificial intelligence can improve cyber defense. Big data analytics methods are applied to large data sets that contain different data types. The purpose is to detect patterns, correlations, trends, and other useful information. Artificial intelligence provides algorithms that can reason or learn and improve their behavior, and includes semantic technologies. A large number of automated systems are currently based on syntactic rules which are generally not sophisticated enough to deal with the level of complexity in this domain. An overview of artificial intelligence and big data technologies in cyber defense is provided, and important areas for future research are identified and discussed.",CS,AI_ML,0.85,Extracted from log - paper 85
Improving Decision Analytics with Deep Learning: the Case of Financial Disclosures,"Decision analytics commonly focuses on the text mining of financial news sources in order to provide managerial decision support and to predict stock market movements. Existing predictive frameworks almost exclusively apply traditional machine learning methods, whereas recent research indicates that traditional machine learning methods are not sufficiently capable of extracting suitable features and capturing the non-linear nature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending traditional neural network models with additional hidden layers. Indeed, deep learning has been shown to outperform traditional methods in terms of predictive performance. In this paper, we adapt the novel deep learning technique to financial decision support. In this instance, we aim to predict the direction of stock movements following financial disclosures. As a result, we show how deep learning can outperform the accuracy of random forests as a benchmark for machine learning by 5.66%.",CS,AI_ML,0.85,Extracted from log - paper 86
Hospital information systems: experience at the fully digitized Seoul National University Bundang Hospital.,"The different levels of health information technology (IT) adoption and its integration into hospital workflow can affect the maximization of the benefits of using of health IT. We aimed at sharing our experiences and the journey to the successful adoption of health IT over 13 years at a tertiary university hospital in South Korea. The integrated system of comprehensive applications for direct care, support care, and smart care has been implemented with the latest IT and a rich user information platform, achieving the fully digitized hospital. The users experience design methodology, barcode and radio-frequency identification (RFID) technologies, smartphone and mobile technologies, and data analytics were integrated into hospital workflow. Applications for user-centered electronic medical record (EMR) and clinical decision support (CDS), closed loop medication administration (CLMA), mobile EMR and dashboard system for care coordination, clinical data warehouse (CDW) system, and patient engagement solutions were designed and developed to improve quality of care, work efficiency, and patient safety. We believe that comprehensive electronic health record systems and patient-centered smart hospital applications will go a long way in ensuring seamless patient care and experience.",CS,AI_ML,0.85,Extracted from log - paper 87
Design principles for artificial intelligence-augmented decision making: An action design research study,"ABSTRACT Artificial intelligence (AI) applications have proliferated, garnering significant interest among information systems (IS) scholars. AI-powered analytics, promising effective and low-cost decision augmentation, has become a ubiquitous aspect of contemporary organisations. Unlike traditional decision support systems (DSS) designed to support decisionmakers with fixed decision rules and models that often generate stable outcomes and rely on human agentic primacy, AI systems learn, adapt, and act autonomously, demanding recognition of IS agency within AI-augmented decision making (AIADM) systems. Given this fundamental shift in DSS; its influence on autonomy, responsibility, and accountability in decision making within organisations; the increasing regulatory and ethical concerns about AI use; and the corresponding risks of stochastic outputs, the extrapolation of prescriptive design knowledge from conventional DSS to AIADM is problematic. Hence, novel design principles incorporating contextual idiosyncrasies and practice-based domain knowledge are needed to overcome unprecedented challenges when adopting AIADM. To this end, we conduct an action design research (ADR) study within an e-commerce company specialising in producing and selling clothing. We develop an AIADM system to support marketing, consumer engagement, and product design decisions. Our work contributes to theory and practice with a set of actionable design principles to guide AIADM system design and deployment.",CS,AI_ML,0.85,Extracted from log - paper 88
Unity Decision Guidance Management System: Analytics Engine and Reusable Model Repository,"Enterprises across all industries increasingly depend on decision guidance systems to facilitate decisionmaking across all lines of business. Despite significant technological advances, current paradigms for developing decision guidance systems lead to a tight-integration of the analytic models, algorithms and underlying tools that comprise these systems, which inhibits both reusability and interoperability. To address these limitations, this paper focuses on the development of the Unity analytics engine, which enables the construction of decision guidance systems from a repository of reusable analytic models that are expressed in JSONiq. Unity extends JSONiq with support for algebraic modeling using a symbolic computation-based technique and compiles reusable analytic models into lower-level, tool-specific representations for analysis. In this paper, we also propose a conceptual architecture for a Decision Guidance Management System, based on Unity, to support the rapid development of decision guidance systems. Finally, we conduct a preliminary experimental study on the overhead introduced by automatically translating reusable analytic models into tool-specific representations for analysis. Initial results indicate that the execution times of optimization models that are automatically generated by Unity from reusable analytic models are within a small constant factor of that of corresponding, manually-crafted optimization models.",CS,AI_ML,0.85,Extracted from log - paper 89
Supporting Decision-Making for Promoting Teaching and Learning Innovation: A Multiple Case Study,"The quality of the data and the amount of correct information available is key to informed decision-making. Higher education institutions (HEIs) often employ various decision support systems (DSSs) to make better choices. However, there is a lack of systems to assist with decision-making to promote innovation in teaching and learning. In this study, we evaluate an analytic tool called PROF-XXI that supports strategic decision-making of teaching and learning centres (TLCs) by identifying their competencies in teaching and learning innovation. Through a multiple case study conducted with three Latin American universities and supported by quantitative and qualitative data, we observed how this tool is used and how it facilitates strategic decision-making. Our findings indicate that the tool is accessible, user-friendly, and effective in 1) initiating identification and systematic reflection of institutional competency levels in teaching and learning innovation, 2) enhancing understanding of strengths and weaknesses as well as identifying opportunities for innovation, 3) supporting TLCs with short- and long-term decision-making, and 4) continuously evaluating their strategies, programs, and initiatives. This research can benefit policymakers in higher education who are involved in measuring institutional competencies to improve teaching quality or in making strategic decisions related to teaching and learning innovation.",CS,AI_ML,0.85,Extracted from log - paper 90
Visualizing statistical linked knowledge for decision support,"In a global and interconnected economy, decision makers often need to consider information from various domains. A tourism destination manager, for example, has to correlate tourist behavior with financial and environmental indicators to allocate funds for strategic long-term investments. Statistical data underpins a broad range of such cross-domain decision tasks. A variety of statistical datasets are available as Linked Open Data, often incorporated into visual analytics solutions to support decision making. What are the principles, architectures, workflows and implementation design patterns that should be followed for building such visual cross-domain decision support systems. This article introduces a methodology to integrate and visualize cross-domain statistical data sources by applying selected RDF Data Cube (QB) principles. A visual dashboard built according to this methodology is presented and evaluated in the context of two use cases in the tourism and telecommunications domains.",CS,AI_ML,0.85,Extracted from log - paper 91
Not seeing the (moral) forest for the trees? How task complexity and employees’ expertise affect moral disengagement with discriminatory data analytics recommendations,"Data analytics provides versatile decision support to help employees tackle the rising complexity of today’s business decisions. Notwithstanding the benefits of these systems, research has shown their potential for provoking discriminatory decisions. While technical causes have been studied, the human side has been mostly neglected, albeit employees mostly still need to decide to turn analytics recommendations into actions. Drawing upon theories of technology dominance and of moral disengagement, we investigate how task complexity and employees’ expertise affect the approval of discriminatory data analytics recommendations. Through two online experiments, we confirm the important role of advantageous comparison, displacement of responsibility, and dehumanization, as the cognitive moral disengagement mechanisms that facilitate such approvals. While task complexity generally enhances these mechanisms, expertise retains a critical role in analytics-supported decision-making processes. Importantly, we find that task complexity’s effects on users’ dehumanization vary: more data subjects increase dehumanization, whereas richer information on subjects has the opposite effect. By identifying the cognitive mechanisms that facilitate approvals of discriminatory data analytics recommendations, this study contributes toward designing tools, methods, and practices that combat unethical consequences of using these systems.",CS,AI_ML,0.85,Extracted from log - paper 92
Decision Guidance Analytics Language (DGAL) - Toward Reusable Knowledge Base Centric Modeling,"Decision guidance systems are a class of decision support systems that are geared toward producing actionable recommendations, typically based on formal analytical models and techniques. This paper proposes the Decision Guidance Analytics Language (DGAL) for easy iterative development of decision guidance systems. DGAL allows the creation of modular, reusable and composable models that are stored in the analytical knowledge base independently of the tasks and tools that use them. Based on these unified models, DGAL supports declarative queries of (1) data manipulation and computation, (2) what-if prediction analysis, (3) deterministic and stochastic decision optimization, and (4) machine learning, all through formal reduction to specialized models and tools, and in the presence of uncertainty.",CS,AI_ML,0.85,Extracted from log - paper 93
Big Data & Analytics to Support the Renewable Energy Integration of Smart Grids - Case Study: Power Solar Generation,"Smart Grid is the modernization of electrical networks using intelligent systems and information technologies. In smart grid environment, the application of big data analytics based decision support and intelligent control are mainly in the following four aspects: power generation side management, micro grid and renewable energy management, asset management and collaborative operations, and demand side management. The objective of this research is to present a technological infrastructure for the management of large volumes of information through Big Data tools to support the integration of renewable energy. The infrastructure includes a methodological architecture for the acquisition, processing, storage, management, analysis, monitoring and forecast of large amounts of data. The development of a Big Data application for the analysis and monitoring of the information generated by photovoltaic systems is included as a case study. Solar generation technologies have experienced strong energy market growth in the past few years, with corresponding increase in local grid penetration. The goal is to have timely information to make better decisions to improve the integration of renewable energy in the Smart Grid.",CS,AI_ML,0.85,Extracted from log - paper 94
Business Analytics for Decision Making,"Providing decision support for business processes management and decision processes is a crucial but challenging task. Business intelligence and analytics equips analytics experts with the technological capabilities to support decision processes with reliable information and analytic insights, thus potentially raising the quality of managerial decision making. More-so when leveraging e-commerce platforms with rising complex ways of doing business, it is imperative for business organisations to make calculated decisions which are supported by tools in business analytics. This paper provides insights on how e-commerce, supply chain management, human resource management and business process management employ business analytics as an enabler. Additionally, the paper concludes through business analytics initiatives, businesses and organizations gain critical insights from the structured data collected through various enterprise systems and with the emerging trends and new tools in business analytics, more intelligence will be gathered for decision making in fields such as e-government, healthcare, and security.",CS,AI_ML,0.85,Extracted from log - paper 95
Operational Analytics Data Management Systems,"Prior to mid-2000s, the space of data analytics was mainly confined within the area of decision support systems. It was a long era of isolated enterprise data ware houses curating information from live data sources and of business intelligence software used to query such information. Most data sets were small enough in volume and static enough invelocity to be segregated in warehouses for analysis. Data analysis was not ad-hoc; it required pre-requisite knowledge of underlying data access patterns for the creation of specialized access methods (e.g. covering indexes, materialized views) in order to efficiently execute a set of few focused queries.",CS,AI_ML,0.85,Extracted from log - paper 96
Explainable Student Agency Analytics,"Several studies have shown that complex nonlinear learning analytics (LA) techniques outperform the traditional ones. However, the actual integration of these techniques in automatic LA systems remains rare because they are generally presumed to be opaque. At the same time, the current reviews on LA in higher education point out that LA should be more grounded to the learning science with actual linkage to teachers and pedagogical planning. In this study, we aim to address these two challenges. First, we discuss different techniques that open up the decision-making process of complex techniques and how they can be integrated in LA tools. More precisely, we present various global and local explainable techniques with an example of an automatic LA process that provides information about different resources that can support student agency in higher education institutes. Second, we exemplify these techniques and the LA process through recently collected student agency data in four courses of the same content taught by four different teachers. Altogether, we demonstrate how this process—which we call explainable student agency analytics—can contribute to teachers’ pedagogical planning through the LA cycle.",CS,AI_ML,0.85,Extracted from log - paper 97
How Do Small and Medium-Sized Game Companies Use Analytics? An Attention-Based View of Game Analytics,"The widespread adoption of the freemium business model together with the introduction of cost-efficient analytics tools have made the use of analytics pervasive in the game industry. While big data and analytics have drawn extensive scholarly attention, the research focusing particularly on game analytics is scant and largely descriptive. Thus, there is a need for research focusing on how game companies employ analytics. In this study, we analyze data collected through a set of in-depth interviews of small and medium-sized freemium game developers. We identify four main roles of game analytics: 1) sense-making device, 2) decision-support system, 3) communication tool, and 4) hygiene factor. We employ the attention-based view of the firm to discuss how these roles diverge and converge in terms of organizational attention. The study advances the research on the roles and business value of analytics in the game and software industry.",CS,AI_ML,0.85,Extracted from log - paper 98
Business Analytics in the Context of Big Data: A Roadmap for Research,"This paper builds on academic and industry discussions from the 2012 and 2013 pre-ICIS events: BI Congress III and the Special Interest Group on Decision Support Systems (SIGDSS) workshop, respectively. Recognizing the potential of “big data” to offer new insights for decision making and innovation, panelists at the two events discussed how organizations can use and manage big data for competitive advantage. In addition, expert panelists helped to identify research gaps. While emerging research in the academic community identifies some of the issues in acquiring, analyzing, and using big data, many of the new developments are occurring in the practitioner community. We bridge the gap between academic and practitioner research by presenting a big data analytics framework that depicts a process view of the components needed for big data analytics in organizations. Using practitioner interviews and literature from both academia and practice, we identify the current state of big data research guided by the framework and propose potential areas for future research to increase the relevance of academic research to practice.",CS,AI_ML,0.85,Extracted from log - paper 99
Comparing the utility of decision trees and support vector machines when planning inspections of linear sewer infrastructure,"Closed-circuit television inspection technology is traditionally used to identify aging sewer pipes requiring rehabilitation. While these inspections provide essential information on the condition of pipes hidden from day-to-day view, they are expensive and often limited to small portions of an entire sewer system. Municipalities may benefit from utilizing predictive analytics to leverage existing inspection datasets so that reliable predictions of condition are available for pipes that have not yet been inspected. The predictive capabilities of data mining systems, namely support vector machines (SVMs) and decision tree classifiers, are demonstrated using a case study of sanitary sewer pipe inspection data collected by the municipality of Guelph, Ontario, Canada. The modeling algorithms are implemented using open-source software and are tuned to counteract the negative impact on predictive performance resulting from class imbalance common within pipe inspection datasets. The decision tree classifier outperforms SVM for this classification task – achieving an acceptable area under the receiver operating characteristic curve of 0.77 and an overall accuracy of 76% on a stratified test set. Although predicting individual pipe condition is a notoriously difficult task, decision trees are found to be a useful screening tool for planning future inspection-related activities.",CS,AI_ML,0.85,Extracted from log - paper 100
Big Data Analytics in Health: an overview and bibliometric study of research activity.,"OBJECTIVE The study presents an overview of the research activity in Big Data Analytics (BDA) in the field of health and demonstrates the existing knowledge through related examples. The objective is to inform health librarians about the nature and magnitude of the technological innovations in health information analysis tools, its influence, and where and how further material could be searched. METHODS We performed a bibliometric and co-citation analysis within a total of 804 papers published between 2000 and 2016 and retrieved from the Web of Science and Scopus databases. Using the NVivo text analysis software, we identified the stakeholders of BDA in health and innovative decision support systems in the field. RESULTS Our findings show a tremendous increase in published papers after 2014. Most of them are relevant to neurology and medical oncology. The stakeholders are clinicians, researchers, patients, administrators, IT specialists, vendors and policymakers. New BDA tools in medicine are mostly developed for disease monitoring purposes while they utilise visualisation to identify disease patterns and statistical analysis of past data for making predictions. CONCLUSIONS Health analytics provide a unique opportunity for advancing health information research and medical decision making. It provides health information professionals with new tools in problem-solving offering new perspectives in prognosis and diagnosis of diseases.",CS,AI_ML,0.85,Extracted from log - paper 101
Decision Support Basics,"Because of increasing complexity, rapid change and risk, managers have an obligation to shareholders to learn about and understand computerized decision support systems (DSS). Managers must know much more about information technology solutions and especially computerized decision support. This book is targeted to busy managers and MBA students who need to grasp the basics of computerized decision support. Some of the topics covered include: What is a DSS? What do managers need to know about computerized decision support? And how can managers identify opportunities to create innovative DSS? Overall the book addresses 35 fundamental questions that are relevant to understanding computerized decision support. In a short period of time managers can ""get up to speed"" on decision support, analytics and business intelligence. The book then provides a quick reference to important recurring questions. The questions are arranged in a logical order from more general questions to more specific, including specialized questions of interest to managers and future managers.",CS,AI_ML,0.85,Extracted from log - paper 102
Utility of ChatGPT in Clinical Practice,"ChatGPT is receiving increasing attention and has a variety of application scenarios in clinical practice. In clinical decision support, ChatGPT has been used to generate accurate differential diagnosis lists, support clinical decision-making, optimize clinical decision support, and provide insights for cancer screening decisions. In addition, ChatGPT has been used for intelligent question-answering to provide reliable information about diseases and medical queries. In terms of medical documentation, ChatGPT has proven effective in generating patient clinical letters, radiology reports, medical notes, and discharge summaries, improving efficiency and accuracy for health care providers. Future research directions include real-time monitoring and predictive analytics, precision medicine and personalized treatment, the role of ChatGPT in telemedicine and remote health care, and integration with existing health care systems. Overall, ChatGPT is a valuable tool that complements the expertise of health care providers and improves clinical decision-making and patient care. However, ChatGPT is a double-edged sword. We need to carefully consider and study the benefits and potential dangers of ChatGPT. In this viewpoint, we discuss recent advances in ChatGPT research in clinical practice and suggest possible risks and challenges of using ChatGPT in clinical practice. It will help guide and support future artificial intelligence research similar to ChatGPT in health.",CS,AI_ML,0.85,Extracted from log - paper 103
Big data and visual analytics in anaesthesia and health care.,"Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics--the systematic use of data combined with quantitative and qualitative analysis to make decisions--can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.",CS,AI_ML,0.85,Extracted from log - paper 104
Schema on read modeling approach as a basis of big data analytics integration in EIS,"ABSTRACT Big Data analysis is the process that can help organizations to make better business decisions. Organizations use data warehouses and business intelligence systems, i.e. enterprise information systems (EISs), to support and improve their decision-making processes. Since the ultimate goal of using EISs and Big Data analytics is the same, a logical task is to enable these systems to work together. In this paper we propose a framework of cooperation of these systems, based on the schema on read modeling approach and data virtualization. The goal of data virtualization process is to hide technical details related to data storage from applications and to display heterogeneous data sources as one integrated data source. We have tested the proposed model in a case study in the transportation domain. The study has shown that the proposed integration model responds flexibly and efficiently to the requirements related to adding new data sources, new data models and new data storage technologies.",CS,AI_ML,0.85,Extracted from log - paper 105
Addressing the Complexities of Big Data Analytics in Healthcare: The Diabetes Screening Case,"The healthcare industry generates a high throughput of medical, clinical and omics data of varying complexity and features. Clinical decision-support is gaining widespread attention as medical institutions and governing bodies turn towards better management of this data for effective and efficient healthcare delivery and quality assured outcomes. Amass of data across all stages, from disease diagnosis to palliative care, is further indication of the opportunities and challenges to effective data management, analysis, prediction and optimization techniques as parts of knowledge management in clinical environments. Big Data analytics (BDA) presents the potential to advance this industry with reforms in clinical decision-support and translational research. However, adoption of big data analytics has been slow due to complexities posed by the nature of healthcare data. The success of these systems is hard to predict, so further research is needed to provide a robust framework to ensure investment in BDA is justified. In this paper we investigate these complexities from the perspective of updated Information Systems (IS) participation theory. We present a case study on a large diabetes screening project to integrate, converge and derive expedient insights from such an accumulation of data and make recommendations for a successful BDA implementation grounded in a participatory framework and the specificities of big data in healthcare context.",CS,AI_ML,0.85,Extracted from log - paper 106
Modelling road congestion using ontologies for big data analytics in smart cities,"Intelligent Transport Systems are a vital component within Smart Cities but rarely provide the context that is required by the road user or network manager that will help support decision making. Such systems need to be able to collect data from multiple heterogeneous sources and analyse this information, providing it to stakeholders in a timely manner. The focus of this work is to use Big Data analytics to gain knowledge about road accidents, which are a major contributor to non-recurrent congestion. The aim is to develop a model capable of capturing the semantics of road accidents within an ontology. With the support of the ontology, selective dimensions and Big Data sources will be chosen to populate a model of non-recurrent congestion. Initial Big Data analysis will be performed on the data collected from two different sensor types in Greater Manchester, UK to determine whether it is possible to identify clusters based on journey time and traffic volumes.",CS,AI_ML,0.85,Extracted from log - paper 107
Health informatics and analytics - building a program to integrate business analytics across clinical and administrative disciplines,"Health care organizations must develop integrated health information systems to respond to the numerous government mandates driving the movement toward reimbursement models emphasizing value-based and accountable care. Success in this transition requires integrated data analytics, supported by the combination of health informatics, interoperability, business process design, and advanced decision support tools. This case study presents the development of a master's level cross- and multidisciplinary informatics program offered through a business school. The program provides students from diverse backgrounds with the knowledge, leadership, and practical application skills of health informatics, information systems, and data analytics that bridge the interests of clinical and nonclinical professionals. This case presents the actions taken and challenges encountered in navigating intra-university politics, specifying curriculum, recruiting the requisite interdisciplinary faculty, innovating the educational format, managing students with diverse educational and professional backgrounds, and balancing multiple accreditation agencies.",CS,AI_ML,0.85,Extracted from log - paper 108
City Information Modelling: A Conceptual Framework for Research and Practice in Digital Urban Planning,"The digitalization of the urban development process is driven by the need for informed, evidence-based, collaborative and participative urban planning and decision-making, epitomized in the concept of Smart Cities. This digital transformation is enabled by information technology developments in fields such as 3D city models, Digital Twins, Urban Analytics and Informatics, Geographic Information Systems (GIS), and Planning Support Systems (PSS). In this context, City Information Modelling (CIM) has recently emerged as a concept related to these various technological driving forces. In this article, we review the state of the art of CIM (definitions and applications) in the academic literature and propose a definition and a general conceptual framework. By highlighting how the different disciplines are related to each other within this conceptual framework, we offer a context for transdisciplinary work, and focus on integration challenges, for research and development, both in academia and industry. This will contribute to moving forward the debate on digitalization of the built environment development process in the field of Smart Cities.",CS,AI_ML,0.85,Extracted from log - paper 109
Integrated Care and Connected Health Approaches Leveraging Personalised Health through Big Data Analytics,"Integrated care and connected health are two fast evolving concepts that have the potential to leverage personalised health. From the one side, the restructuring of care models and implementation of new systems and integrated care programs providing coaching and advanced intervention possibilities, enable medical decision support and personalized healthcare services. From the other side, the connected health ecosystem builds the means to follow and support citizens via personal health systems in their everyday activities and, thus, give rise to an unprecedented wealth of data. These approaches are leading to the deluge of complex data, as well as in new types of interactions with and among users of the healthcare ecosystem. The main challenges refer to the data layer, the information layer, and the output of information processing and analytics. In all the above mentioned layers, the primary concern is the quality both in data and information, thus, increasing the need for filtering mechanisms. Especially in the data layer, the big biodata management and analytics ecosystem is evolving, telemonitoring is a step forward for data quality leverage, with numerous challenges still left to address, partly due to the large number of micro-nano sensors and technologies available today, as well as the heterogeneity in the users' background and data sources. This leads to new R&D pathways as it concerns biomedical information processing and management, as well as to the design of new intelligent decision support systems (DSS) and interventions for patients. In this paper, we illustrate these issues through exemplar research targeting chronic patients, illustrating the current status and trends in PHS within the integrated care and connected care world.",CS,AI_ML,0.85,Extracted from log - paper 110
"Prediction of In-hospital Mortality in Emergency Department Patients With Sepsis: A Local Big Data-Driven, Machine Learning Approach.","OBJECTIVES Predictive analytics in emergency care has mostly been limited to the use of clinical decision rules (CDRs) in the form of simple heuristics and scoring systems. In the development of CDRs, limitations in analytic methods and concerns with usability have generally constrained models to a preselected small set of variables judged to be clinically relevant and to rules that are easily calculated. Furthermore, CDRs frequently suffer from questions of generalizability, take years to develop, and lack the ability to be updated as new information becomes available. Newer analytic and machine learning techniques capable of harnessing the large number of variables that are already available through electronic health records (EHRs) may better predict patient outcomes and facilitate automation and deployment within clinical decision support systems. In this proof-of-concept study, a local, big data-driven, machine learning approach is compared to existing CDRs and traditional analytic methods using the prediction of sepsis in-hospital mortality as the use case. METHODS This was a retrospective study of adult ED visits admitted to the hospital meeting criteria for sepsis from October 2013 to October 2014. Sepsis was defined as meeting criteria for systemic inflammatory response syndrome with an infectious admitting diagnosis in the ED. ED visits were randomly partitioned into an 80%/20% split for training and validation. A random forest model (machine learning approach) was constructed using over 500 clinical variables from data available within the EHRs of four hospitals to predict in-hospital mortality. The machine learning prediction model was then compared to a classification and regression tree (CART) model, logistic regression model, and previously developed prediction tools on the validation data set using area under the receiver operating characteristic curve (AUC) and chi-square statistics. RESULTS There were 5,278 visits among 4,676 unique patients who met criteria for sepsis. Of the 4,222 patients in the training group, 210 (5.0%) died during hospitalization, and of the 1,056 patients in the validation group, 50 (4.7%) died during hospitalization. The AUCs with 95% confidence intervals (CIs) for the different models were as follows: random forest model, 0.86 (95% CI = 0.82 to 0.90); CART model, 0.69 (95% CI = 0.62 to 0.77); logistic regression model, 0.76 (95% CI = 0.69 to 0.82); CURB-65, 0.73 (95% CI = 0.67 to 0.80); MEDS, 0.71 (95% CI = 0.63 to 0.77); and mREMS, 0.72 (95% CI = 0.65 to 0.79). The random forest model AUC was statistically different from all other models (p ≤ 0.003 for all comparisons). CONCLUSIONS In this proof-of-concept study, a local big data-driven, machine learning approach outperformed existing CDRs as well as traditional analytic techniques for predicting in-hospital mortality of ED patients with sepsis. Future research should prospectively evaluate the effectiveness of this approach and whether it translates into improved clinical outcomes for high-risk sepsis patients. The methods developed serve as an example of a new model for predictive analytics in emergency care that can be automated, applied to other clinical outcomes of interest, and deployed in EHRs to enable locally relevant clinical predictions.",CS,AI_ML,0.85,Extracted from log - paper 111
Business Intelligence and Analytics,"A business intelligence system is a data-driven decision support system. Managing data is especially important for business intelligence and analytics. Data warehouses, marts or data-driven decision support systems are intended to help managers transform data into information and knowledge. Routinely data is moved from source systems to a decision support data store. Some comparison reports include external data on competitors or other relevant data. Analytics refers to quantitative analysis of data. There are three components of business analytics: i) descriptive analytics, ii) predictive analytics and iii) prescriptive analytics. Besides statistical analysis techniques, data mining is the key component of predictive analytics. Data mining is a process of analyzing large amounts of data to identify data content relationships. Cloud computing and “Big” data are changing business intelligence and analytics. Columnar data bases let analysts work with data from web logs and other nonrelational data sources. Keywords: analytics; business intelligence; decision support; decision sciene",CS,AI_ML,0.85,Extracted from log - paper 112
Enhancing Knowledge Management and Decision-Making Capability of China’s Emergency Operations Center Using Big Data,"AbstractEmerging communication and computing technologies such as social media, Internet of Things and big data provide great opportunities to improve information management systems for emergency operations. This paper studies the issues of information management at China’s Emergency Operations Center (EOC), and proposes a data-driven knowledge management system (KMS) to support decision-making, coordination, and collaboration within EOCs and with the public. In the proposed KMS, big data analytics is employed to gather and analyze information from different knowledge domains and track how a crisis evolves in physical world and in cyber space. The proposed system aims at improving situation awareness of public opinions and regulating human behaviors in regards to an emergency. A case study is presented to explain how the proposed system is applied to improve decision-making during emergency.",CS,AI_ML,0.85,Extracted from log - paper 113
Guide to health informatics,"Basic Concepts in Informatics Models Information Information Systems Informatics Skills Communicating Structuring Questioning Searching Making Decisions Information Systems in Healthcare Information Management Systems The Electronic Health Record Designing and Evaluating Information and Communication Systems Implementation Information System Safety Information Economics Guideline- and Protocol-Based Systems Guidelines, Protocols and Evidence-Based Healthcare Computer-Based Protocol Systems Designing, Disseminating and Applying protocols Communication Systems in Healthcare Communication Systems Basics Interlude-the Internet and the World Wide Web Information and Communication Networks Social Networks and Social Media Interventions Telehealth and Mobile Health Language, Coding and Classification Terms, Codes and Classification Healthcare Terminologies and Classification Systems Natural Language and Formal Terminology Clinical Decision Support and Analytics Clinical Decision Support Systems Interlude-Artificial Intelligence in Medicine Computational Reasoning Methods Model Building for Decision Support, Data Analysis and Scientific Discovery Specialized Applications for Health Informatics Patient Monitoring and Control Population Surveillance and Public Health Informatics Bioinformatics Clinical Bioinformatics and Personalized Medicine Consumer Health Informatics Glossary References",CS,AI_ML,0.85,Extracted from log - paper 114
A Big Data Framework for Decision Making in Supply Chain,"The advent of information and communication technologies (ICT) ushers a cost-effective prospect to take care of large volumes of complex data, commonly known as “big data” in the supply chain operational environment. Big data is being generated today by web applications, social media, intelligent machines, sensors, mobile phones, and other smart handheld devices. Big data is characterized in terms of the velocity, volume, and variety with which it produces along the supply chain. This is due to recent advances in telecommunication networks along with centralized and decentralized data storage systems, which are processed thanks to modern digital computational capabilities. There is a growing interest in the use of this large volume of data and advanced analytics for diverse types of business problems in supply chain management (SCM). Such decision-support software applications employ pure mathematical techniques, artificial intelligence techniques, and sometimes uses both techniques to perform analytical operations that undercover relationships and patterns within supply chain generated big data. This chapter proposes a framework for the utilization of big data in SCM decision making. The framework is based on the SCOR (supply chain operations reference) model, which is endorsed by Supply Chain Council (SCC). The proposed framework is influenced by the enterprise potential of augmented reality and virtual reality in supply chain applications, and it identifies key categories of big data analytics applications for the key businesses of SCOR model. Finally, the chapter highlights research issues to extract insight from big data sources for enterprise decision making.",CS,AI_ML,0.85,Extracted from log - paper 115
AI in Healthcare: Revolutionizing Patient Care with Predictive Analytics and Decision Support Systems,"This article explores the transformative impact of Artificial Intelligence (AI) in healthcare, with a specific focus on how predictive analytics and decision support systems are revolutionizing patient care. Predictive analytics enable early disease prevention and diagnosis by identifying patterns and risk factors, contributing to improved patient outcomes and cost-effective healthcare. Machine learning facilitates personalized treatment plans, leveraging individual patient data for tailored interventions that enhance efficacy and minimize adverse effects. AI-driven algorithms in medical imaging enhance diagnostic accuracy, providing rapid and precise assessments. Decision support systems, powered by AI, streamline healthcare workflows by offering real-time insights based on patient data and clinical guidelines, facilitating evidence-based decision-making. Remote patient monitoring, facilitated by AI, allows for proactive healthcare interventions by tracking vital signs and identifying potential health issues in real time. The article also discusses challenges and ethical considerations associated with AI integration in healthcare, emphasizing the importance of responsible deployment and regulatory frameworks. The comprehensive exploration underscores how AI is not only transforming patient care but also shaping the future of healthcare delivery.",CS,AI_ML,0.85,Extracted from log - paper 116
AI in Healthcare: Transforming Patient Care through Predictive Analytics and Decision Support Systems,"This article explores the transformative impact of Artificial Intelligence (AI) in healthcare, with a specific focus on how predictive analytics and decision support systems are revolutionizing patient care. Predictive analytics enable early disease prevention and diagnosis by identifying patterns and risk factors, contributing to improved patient outcomes and cost-effective healthcare. Machine learning facilitates personalized treatment plans, leveraging individual patient data for tailored interventions that enhance efficacy and minimize adverse effects. AI-driven algorithms in medical imaging enhance diagnostic accuracy, providing rapid and precise assessments. Decision support systems, powered by AI, streamline healthcare workflows by offering real-time insights based on patient data and clinical guidelines, facilitating evidence-based decision-making. Remote patient monitoring, facilitated by AI, allows for proactive healthcare interventions by tracking vital signs and identifying potential health issues in real time. The article also discusses challenges and ethical considerations associated with AI integration in healthcare, emphasizing the importance of responsible deployment and regulatory frameworks. The comprehensive exploration underscores how AI is not only transforming patient care but also shaping the future of healthcare delivery.",CS,AI_ML,0.85,Extracted from log - paper 117
SNAPS: Sensor Analytics Point Solutions for Detection and Decision Support Systems,"In this review, we discuss the role of sensor analytics point solutions (SNAPS), a reduced complexity machine-assisted decision support tool. We summarize the approaches used for mobile phone-based chemical/biological sensors, including general hardware and software requirements for signal transduction and acquisition. We introduce SNAPS, part of a platform approach to converge sensor data and analytics. The platform is designed to consist of a portfolio of modular tools which may lend itself to dynamic composability by enabling context-specific selection of relevant units, resulting in case-based working modules. SNAPS is an element of this platform where data analytics, statistical characterization and algorithms may be delivered to the data either via embedded systems in devices, or sourced, in near real-time, from mist, fog or cloud computing resources. Convergence of the physical systems with the cyber components paves the path for SNAPS to progress to higher levels of artificial reasoning tools (ART) and emerge as data-informed decision support, as a service for general societal needs. Proof of concept examples of SNAPS are demonstrated both for quantitative data and qualitative data, each operated using a mobile device (smartphone or tablet) for data acquisition and analytics. We discuss the challenges and opportunities for SNAPS, centered around the value to users/stakeholders and the key performance indicators users may find helpful, for these types of machine-assisted tools.",CS,AI_ML,0.85,Extracted from log - paper 118
Intelligent Decision Support for Energy Management: A Methodology for Tailored Explainability of Artificial Intelligence Analytics,"This paper presents a novel development methodology for artificial intelligence (AI) analytics in energy management that focuses on tailored explainability to overcome the “black box” issue associated with AI analytics. Our approach addresses the fact that any given analytic service is to be used by different stakeholders, with different backgrounds, preferences, abilities, skills, and goals. Our methodology is aligned with the explainable artificial intelligence (XAI) paradigm and aims to enhance the interpretability of AI-empowered decision support systems (DSSs). Specifically, a clustering-based approach is adopted to customize the depth of explainability based on the specific needs of different user groups. This approach improves the accuracy and effectiveness of energy management analytics while promoting transparency and trust in the decision-making process. The methodology is structured around an iterative development lifecycle for an intelligent decision support system and includes several steps, such as stakeholder identification, an empirical study on usability and explainability, user clustering analysis, and the implementation of an XAI framework. The XAI framework comprises XAI clusters and local and global XAI, which facilitate higher adoption rates of the AI system and ensure responsible and safe deployment. The methodology is tested on a stacked neural network for an analytics service, which estimates energy savings from renovations, and aims to increase adoption rates and benefit the circular economy.",CS,AI_ML,0.85,Extracted from log - paper 119
Managing bias and unfairness in data for decision support: a survey of machine learning and data engineering approaches to identify and mitigate bias and unfairness within data management and analytics systems,"The increasing use of data-driven decision support systems in industry and governments is accompanied by the discovery of a plethora of bias and unfairness issues in the outputs of these systems. Multiple computer science communities, and especially machine learning, have started to tackle this problem, often developing algorithmic solutions to mitigate biases to obtain fairer outputs. However, one of the core underlying causes for unfairness is bias in training data which is not fully covered by such approaches. Especially, bias in data is not yet a central topic in data engineering and management research. We survey research on bias and unfairness in several computer science domains, distinguishing between data management publications and other domains. This covers the creation of fairness metrics, fairness identification, and mitigation methods, software engineering approaches and biases in crowdsourcing activities. We identify relevant research gaps and show which data management activities could be repurposed to handle biases and which ones might reinforce such biases. In the second part, we argue for a novel data-centered approach overcoming the limitations of current algorithmic-centered methods. This approach focuses on eliciting and enforcing fairness requirements and constraints on data that systems are trained, validated, and used on. We argue for the need to extend database management systems to handle such constraints and mitigation methods. We discuss the associated future research directions regarding algorithms, formalization, modelling, users, and systems.",CS,AI_ML,0.85,Extracted from log - paper 120
Big data as a value generator in decision support systems: a literature review,"PurposeThis paper aims to analyze how decision support systems manage Big data to obtain value.Design/methodology/approachA systematic literature review was performed with screening and analysis of 72 articles published between 2012 and 2019.FindingsThe findings reveal that techniques of big data analytics, machine learning algorithms and technologies predominantly related to computer science and cloud computing are used on decision support systems. Another finding was that the main areas that these techniques and technologies are been applied are logistic, traffic, health, business and market. This article also allows authors to understand the relationship in which descriptive, predictive and prescriptive analyses are used according to an inverse relationship of complexity in data analysis and the need for human decision-making.Originality/valueAs it is an emerging theme, this study seeks to present an overview of the techniques and technologies that are being discussed in the literature to solve problems in their respective areas, as a form of theoretical contribution. The authors also understand that there is a practical contribution to the maturity of the discussion and with reflections even presented as suggestions for future research, such as the ethical discussion. This study’s descriptive classification can also serve as a guide for new researchers who seek to understand the research involving decision support systems and big data to gain value in our society.",CS,AI_ML,0.85,Extracted from log - paper 121
Development of IoT—enabled data analytics enhance decision support system for lean manufacturing process improvement,"For over three decades, production firms have extensively espoused lean manufacturing (LM) approach for constantly enhancing their operations. Of late, due to the fusion of physical and digital systems within the Industry 4.0 evolution, production systems can upgrade by applying both notions and lift operational excellence to a new high. This is primarily the reason why digital business transformation has gained significance. Moreover, Industry 4.0 that is led by data assures huge strides in output. The sheer volume of pertinent data from the production systems employing servers, sensors, and cloud computing have made the data exchange procedure more gigantic and intricate. However, conventional systems do not extensively support LM in the context of Industry 4.0. Moreover, the previous studies by researchers in the same field, shown that there was no standard platform to manage the new technologies in LM. This study presents a discussion on the interrelated framework about the way Industry 4.0 has transformed production into an industry focusing on connective mechanisms and platforms which utilize data analytics from the real world. The theoretical framework proposed in this paper integrates LM, data analytics, and Internet of Things (IoT) to enhance decision support systems in process improvement. Data analytics in simulation is employed through Internet of Things to improve bottleneck problems by maintaining the principle of LM. The main information flow route within LM decision support system is demonstrated in detail to show how the decision-making process is done. The decision support mechanism has undergone up-gradation and the suggested framework has shown that the assimilated components could function together to augment the output.",CS,AI_ML,0.85,Extracted from log - paper 122
Visual Analytics for Decision Support: A Supply Chain Perspective,"Supply chain (SC) activities generate huge amount of data that can be used in decision making processes. However, proper data analytics techniques are required to combine, organize, and analyze data from different sources and produce required insights available for decision makers. These techniques promote analytical reasoning of the events and patterns hidden in the data using visualizations, so-called Visual Analytics (VA). Although there is a large number of VA systems to facilitate the process of analysis and decision making, there is a lack of an adequate overview of what already exists in this area for SC management. To address that need, we conducted a systematic literature review to analyze the state of the art in SC VA systems. Particularly, we focus on use cases, the type of the decisions that a VA system intended to support, the type of visualizations employed, the type of analytics used, and the data that has been used for analysis. The goal of this study is to provide SC and VA researchers with an overview of the works carried out in the field of SC VA, helping them to observe latest trends and to recognize existing gaps that need further investigation. Consequently, a mapping between decisions of various SC business processes and their reciprocal visualization techniques and tactics have been provided. Adding to that, VA applications and use cases in SC are identified based on the SC Operation Reference (SCOR) model and underlying decision areas are recognized.",CS,AI_ML,0.85,Extracted from log - paper 123
A Human(e) Factor in Clinical Decision Support Systems,"The overwhelming amount, production speed, multidimensionality, and potential value of data currently available—often simplified and referred to as big data —exceed the limits of understanding of the human brain. At the same time, developments in data analytics and computational power provide the opportunity to obtain new insights and transfer data-provided added value to clinical practice in real time. What is the role of the health care professional in collaboration with the data scientist in the changing landscape of modern care? We discuss how health care professionals should provide expert knowledge in each of the stages of clinical decision support design: data level, algorithm level, and decision support level. Including various ethical considerations, we advocate for health care professionals to responsibly initiate and guide interprofessional teams, including patients, and embrace novel analytic technologies to translate big data into patient benefit driven by human(e) values.",CS,AI_ML,0.85,Extracted from log - paper 124
Challenges in Using Big Data to Develop Decision Support Systems for Social Work in Germany,"Abstract Nowadays, big data analytics are increasingly replacing human decision-making processes in practice fields. In the welfare context, however, they are still being explored only marginally. The following theoretical discussion draws on the example of the MAEWIN project to explore the challenges of using big data when developing decision support systems in the context of social work. The project reveals some similarities with the well-known challenges of big data research (e.g., with regard to data protection, bias, and handling probabilities). However, it also has to face further challenges such as the different knowledge approaches within social work.",CS,AI_ML,0.85,Extracted from log - paper 125
Prescriptive Maintenance of Railway Infrastructure: From Data Analytics to Decision Support,"One of the main benefits of the railways digital transformation is the possibility of increasing the efficiency of the Asset Management process through the combination of data-driven models and decision support systems, paving the road towards an Intelligent Asset Management System (IAMS). The paper describes the whole IAMS decisional process based on a real railway signaling use case: from field data acquisition to decision support. The process includes data collection, preparation and analytics to extract knowledge on current and future assets’ status. Then, the extracted knowledge is used within the decision support system to prioritize asset management interventions in a fully-automated way, by applying optimization logics and operational constraints.The target is to optimize the scheduling of maintenance activities, to maximize the service reliability and optimize both usage of resources and possession times, avoiding (or minimizing) contractual penalties and delays.In this context, a real use case related to signaling system and, in particular, to track circuits, is presented, applying the proposed methodology to an Italian urban rail network and showing the usefulness of the approach and its possible further developments.",CS,AI_ML,0.85,Extracted from log - paper 126
Decision Support Systems in Fisheries and Aquaculture: A systematic review,Decision support systems help decision makers make better decisions in the face of complex decision problems (e.g. investment or policy decisions). Fisheries and Aquaculture is a domain where decision makers face such decisions since they involve factors from many different scientific fields. No systematic overview of literature describing decision support systems and their application in fisheries and aquaculture has been conducted. This paper summarizes scientific literature that describes decision support systems applied to the domain of Fisheries and Aquaculture. We use an established systematic mapping survey method to conduct our literature mapping. Our research questions are: What decision support systems for fisheries and aquaculture exists? What are the most investigated fishery and aquaculture decision support systems topics and how have these changed over time? Do any current DSS for fisheries provide real- time analytics? Do DSSes in Fisheries and Aquaculture build their models using machine learning done on captured and grounded data? The paper then detail how we employ the systematic mapping method in answering these questions. This results in 27 papers being identified as relevant and gives an exposition on the primary methods concluded in the study for designing a decision support system. We provide an analysis of the research done in the studies collected. We discovered that most literature does not consider multiple aspects for multiple stakeholders in their work. In addition we observed that little or no work has been done with real-time analysis in these decision support systems.,CS,AI_ML,0.85,Extracted from log - paper 127
Big data analytics for proactive industrial decision support,"Big data technologies offer new opportunities for analyzing historical data generated by process plants. The development of new types of operator support systems (OSS) which help the plant operators during operations and in dealing with critical situations is one of these possibilities. The project FEE has the objective to develop such support functions based on big data analytics of historical plant data. In this contribution, we share our first insights and lessons learned in the development of big data applications and outline the approaches and tools that we developed in the course of the project.",CS,AI_ML,0.85,Extracted from log - paper 128
Intelligent Learning Analytics Dashboards: Automated Drill-Down Recommendations to Support Teacher Data Exploration,"Learning analytics dashboards commonly visualize data about students with the aim of helping students and educators understand and make informed decisions about the learning process. To assist with making sense of complex and multidimensional data, many learning analytics systems and dashboards have relied strongly on AI algorithms based on predictive analytics. While predictive models have been successful in many domains, there is an increasing realization of the inadequacies of using predictive models in decision-making tasks that affect individuals without human oversight. In this paper, we employ a suite of state-of-the-art algorithms, from the online analytics processing, data mining, and process mining domains, to present an alternative human-in-the-loop AI method to enable educators to identify, explore, and use appropriate interventions for subpopulations of students with the highest deviation in performance or learning process compared to the rest of the class. We demonstrate an application of our proposed approach in an existing learning analytics dashboard (LAD) and explore the recommended drill-downs in a course with 875 students. The demonstration provides an example of the recommendations from real course data and shows how recommendations can lead the user to interesting insights. Furthermore, we demonstrate how our approach can be employed to develop intelligent LADs.",CS,AI_ML,0.85,Extracted from log - paper 129
A Malaria Analytics Framework to Support Evolution and Interoperability of Global Health Surveillance Systems,"Malaria is a leading cause of death in Africa. Many organizations, NGO’s, and government agencies are collaborating to prevent, control, and eliminate malaria. In order to succeed in these shared goals, an integrated, consistent knowledge source to empower informed decision-making is required. Malaria surveillance is currently performed using dynamic, interconnected, systems which require rapid data exchange between different platforms. An important challenge these systems must overcome is the occurrence of dynamic changes in one or more interacting components, which can lead to inconsistencies and mismatches between components of the infrastructure. In this paper, we present our efforts toward the design and development of the semantic interoperability and evolution for malaria analytics platform, with the goal of improving data and semantic interoperability for dynamic malaria surveillance and to support the integration of data across multiple scales. The long term target is to deliver transparent and scalable tools for decision making for malaria elimination. Our analysis is focused on sentinel sites in selected African countries, including Uganda and Gabon.",CS,AI_ML,0.85,Extracted from log - paper 130
"Artificial intelligence, real-time feedback and workplace learning analytics to support in situ complex problem-solving: a commentary","To help workers make the right decision, over the years, technological solutions and workplace learning analytics systems have been designed to aid this process (Ruiz-Calleja et al., 2019). Recent developments in artificial intelligence (AI) have the potential to further revolutionise the integration of human and artificial learning and will impact human and machine collaboration during team work (Seeber et al., 2020).,Complex problem-solving has been identified as one of the key skills for the future workforce (Hager and Beckett, 2019). Problems faced by today's workforce emerge in situ and everyday workplace learning is seen as an effective way to develop the skills and experience workers need to embrace these problems (Campbell, 2005; Jonassen et al., 2006).,In this commentary the authors argue that the increased digitization of work and social interaction, combined with recent research on workplace learning analytics and AI opens up the possibility for designing automated real-time feedback systems capable of just-in-time, just-in-place support during complex problem-solving at work. As such, these systems can support augmented learning and professional development in situ.,The commentary reflects on the benefits of automated real-time feedback systems and argues for the need of shared research agenda to cohere research in the direction of AI-enabled workplace analytics and real-time feedback to support learning and development in the workplace.",CS,AI_ML,0.85,Extracted from log - paper 131
Adapted Visual Analytics Process for Intelligent Decision-Making: Application in a Medical Context,"The theoretical and practical researches on Visual Analytics for intelligent decision-making tasks have remarkably advanced in the past few years. Intelligent Decision Support Systems (IDSS) introduce effective and efficient paths from raw data to decision by involving visualization and data mining technologies. Data mining-based DSS produces potentially interesting patterns from data. The transition from extracted patterns to knowledge is a delicate task. In this context, we propose to adapt a common visual analytics process for creating a path that enables the user (decision-maker) to automatically explore and visually extract insights by interacting with the patterns. This proposal is inspired from integrating traditional visual analytics concepts with the mental model of knowledge visualization. The idea is to combine an automatic and visual analysis of patterns to generate knowledge for the purpose of decision-making. To validate our proposal, we have applied it to a medical case study for the fight against Nosocomial Infections in Intensive Care Units. The developed platform was evaluated according to the utility and usability dimensions.",CS,AI_ML,0.85,Extracted from log - paper 132
"Analytics, bias, and evidence: the quest for rational decision making","ABSTRACT Evidence-based decision making seems both desirable and rational. New analytical tools for investigating ‘big data’ promise to provide additional unbiased evidence. Concurrently, technological advances for improving decision making reopen issues related to facts, biases, and beliefs. For many years, decision support systems and technologies have had the goal of enhancing the effectiveness of human decision-making processes, fostering rational thinking, and avoiding biases and errors. Recently, cognitive neuroscience research has highlighted issues of implicit cognition, physiological and naturalistic processes, and the impact of social cues as elements of human thought. Decision support builders and data scientists must consider a broader range of issues, including issues of knowledge and belief, social factors, and technical capabilities when developing cognitive, analytical, and decision support systems.",CS,AI_ML,0.85,Extracted from log - paper 133
Prospects and challenges for clinical decision support in the era of big data.,"Recently, there has been burgeoning interest in developing more effective and robust clinical decision support systems (CDSSs) for oncology. This has been primarily driven by the demands for more personalized and precise medical practice in oncology in the era of so-called Big Data (BD); an era that promises to harness the power of large-scale data flow to revolutionize cancer treatment. This interest in BD analytics has created new opportunities as well as new unmet challenges. These include: routine aggregation and standardization of clinical data; patient privacy; transformation of current analytical approaches to handle such noisy and heterogeneous data; and expanded use of advanced statistical learning methods based on confluence of modern statistical methods and machine learning algorithms. In this review, we present the current status of CDSSs in oncology, the prospects and current challenges of BD analytics, and the promising role of integrated modern statistics and machine learning algorithms in predicting complex clinical endpoints, individualizing treatment rules, and optimizing dynamic personalized treatment regimens. We discuss issues pertaining to these topics and present application examples from an aggregate of experiences. We also discuss the role of human factors in improving the utilization and acceptance of such enhanced CDSSs and how to mitigate possible sources of human error to achieve optimal performance and wider acceptance.",CS,AI_ML,0.85,Extracted from log - paper 134
SUPPORT SYSTEMS FOR HEALTH MONITORING USING INTERNET-OF-THINGS DRIVEN DATA ACQUISITION,"The Digital Health (D-Health) era is expected to be the “next big thing” since the invention of the internet, characterized by inexpensive and widespread medical data acquisition devices, widespread availability of identityremoved health data, and analytics algorithms that provide remote health monitoring feedback to doctors in realtime. Recent years have brought incremental developments in three key technological areas towards the realization of the D-Health era: data acquisition, secure data transmission/storage, and data analytics. i) For data acquisition, the emerging Internet-of-Things (IoT) devices are becoming a viable technology to enable the acquisition of remote health monitoring data. ii) For data storage, emerging system-level and cryptographic mechanisms provide secure and privacy-preserving transmission, storage, and sharing of the acquired data. iii) For data analytics, emerging decision support algorithms provide a mechanism for healthcare professionals to base their clinical diagnoses partially on machine-suggested statistical inferences that rely on a wide corpus of accumulated data. The D-Health era will create new business opportunities in all of these areas. In this paper, we propose a generalized structure for a DHealth system that is capable of remote health monitoring and decision support. We formulate our proposed structure around potential business opportunities and conduct technical feasibility studies.",CS,AI_ML,0.85,Extracted from log - paper 135
Predictive Risk Analytics for Weather-Resilient Operation of Electric Power Systems,"Day-to-day operation of the electricity grid generation, transmission, and distribution is environmentally driven and closely dependent on evolving weather patterns. This paper introduces several new weather-driven analytics for accurate spatial–temporal electricity generation forecasts, asset health and reliability assessment, probabilistic load forecasts, and electricity market simulations. A new risk metric is suggested, which accounts for the weather hazards, grid vulnerability, and financial consequences in the face of changing weather patterns and associated meteorological predictions over time. New mitigation formulations for power system topology control through transmission line switching for fast and timely recovery of the weather-caused electricity outages are suggested. The proposed decision support tool enables the operators to predictively evaluate the high-risk weather threats and consequently plan on how to safeguard the grid when exposed to forecasted weather-driven incidents. The efficiency of the proposed toolset is illustrated by application to a part of the IEEE 73-Bus test system.",CS,AI_ML,0.85,Extracted from log - paper 136
SYMBIOTIC SIMULATION SYSTEM: HYBRID SYSTEMS MODEL MEETS BIG DATA ANALYTICS,"Symbiotic simulation is one of Industry 4.0 technologies that enables interaction between a physical system and the simulation model that represents it as its digital twin. Symbiotic simulation is designed to support decision making at the operational levels by making use of real- or near real- time data that is generated by the physical system, which is used as an input to the simulation model. From the modeling perspective, a symbiotic simulation system comprises a hybrid systems model that combines simulation, optimization and machine learning models as well as a data acquisition module and an actuator. The actuator is needed when the symbiotic simulation system is designed to directly control the physical system without human intervention. This paper reviews the components of a symbiotic simulation system from the perspective of hybrid systems modeling and highlights research questions needed to advance symbiotic simulation study.",CS,AI_ML,0.85,Extracted from log - paper 137
An empirical assessment of enterprise information systems success in a developing country: the Jordanian experience,"PurposeThe paper discusses the assessment of enterprise information systems (EIS) success in an organizational context is one area that is not sufficiently researched. This lack of theoretical attention prompted the authors to measure EIS success in the organizational setting of Jordanian listed firms.Design/methodology/approachBased on the DeLone and McLean success model and by adopting a quantitative approach, survey data were collected through questionnaires. The established questionnaires were distributed to 250 senior IT executives who use EIS and a total of 134 valid questionnaires were empirically tested via the Smart-PLS technique.FindingsThe investigation findings indicate that information and system quality had a positive impact on user satisfaction and individual impact. User satisfaction had a positive influence on individual impact, both of which eventually lead to organizational impact. The results also showed that the model used has adequate convergent and discriminant validities, as well as sufficient reliability.Practical implicationsThe outcomes can help managers and practitioners more effectively understand the factors influencing EIS success among Jordanian listed firms.Originality/valueThis research paper is the very first in assessing EIS success from an organizational perspective in developing country such as Jordan.",CS,AI_ML,0.85,Extracted from log - paper 138
The role of enterprise information systems strategies enabled strategy-making on organizational innovativeness: a resource orchestration perspective,"PurposeThe paper aims to establish the role of enterprise information systems strategies (ISS) enabled by business strategies for attaining organizational innovativeness (ORIN) mediated by performance (decision-making and business processes) under environmental turbulence.Design/methodology/approachThe research framework is developed based on theoretical grounding and validated with the help of 408 responses from Brazil using SmartPLS path modeling.FindingsThe results of the research suggest that the resource orchestra of enterprise information systems strategy-enabled strategy-making can be a viable alternative to enhance innovation activities in the organizations through the mediated role of performance (decision-making and business process).Practical implicationsThe research demonstrates the role of business function (information systems) strategy enabled overall business strategy-making for achieving innovations in the organization. Fortune organizations are exploiting the information systems strategy enabled business strategy for innovations in the organization; such as Amazon, Walmart, Costco, etc.Originality/valueThe proposed and validated model is a contribution to the enterprise information systems strategy theory. This model presents the role of resource orchestras in achieving innovations in organizations.",CS,AI_ML,0.85,Extracted from log - paper 139
Intellectual structure of cybersecurity research in enterprise information systems,"ABSTRACT Enterprises aspire for ongoing and effective information systems security. Cybersecurity frameworks ensure the availability, confidentiality, and integrity of information. Inspired by the omnipresent challenges and ever-increasing spending by enterprises, we identify the state of research on cybersecurity in enterprises. We employ citation, co-citation, centrality, and citation-path analysis to uncover its intellectual core. Our study reveals five core themes of cybersecurity research: (a) artificial intelligence in cybersecurity, (b) grids, networks, and platform security, (c) algorithms & methods, (d) optimisation & modelling, and (e) cybersecurity management. We discuss the implications for EIS and opportunities for research in each of these themes.",CS,AI_ML,0.85,Extracted from log - paper 140
AI-enabled Enterprise Information Systems for Manufacturing,"ABSTRACT This paper considers Enterprise Information Systems functional architecture and carries out review of AI applications integrated in Customer Relationship Management, Supply Chain Management, Inventory and logistics, Production Planning and Scheduling, Finance and accounting, Product Lifecycle Management and Human Resources, with special attention to the manufacturing enterprises. Enhanced capabilities are identified and proposed as AI services. AI-enablement implements improved decision-making or automation by using Machine Learning models or logic-based systems. It is a process of the enterprise transformation leading to the convergence of the four major disruptive technologies, namely Industrial Internet of Things, Agent-based Distributed Systems, Cloud Computing and Artificial Intelligence.",CS,AI_ML,0.85,Extracted from log - paper 141
Role of Emerging Technologies in Accounting Information Systems for Achieving Strategic Flexibility through Decision-Making Performance: An Exploratory Study Based on North American and South American Firms,"Nowadays, accounting departments highly rely on accounting information systems to make decisions based on current, updated, and contemporary data. And, most accounting practices can be enhanced by emerging technologies coupled with accounting information systems. Therefore, contemporary accounting information systems (AIS) coupled with emerging technologies is the highest priority in organizations to make decisions that can contribute to strategic flexibility and performance of the organizations. The objective of the study is to identify the role of information systems infrastructure integration (ISII) on strategic flexibility and innovation (SFI) through the mediated role of information systems (IS)-enabled strategic enterprise management (IS-SEM) practices and decision-making performance (DMP). The study is based on contemporary literature in the field of emerging technologies in accounting information systems particularly business intelligence and analytics (BI &A). Resource-based view had been applied to create novel constructs to test the research framework and hypothesis. The research framework and hypothesis are tested based on 388 organizations from Brazil and USA. The results reflect that information systems infrastructure integration impacts strategic flexibility and innovations in organizations. Further, there is no difference observed between North American and South American organizations. The results of the research suggest that accounting information systems (AIS) practitioners and researchers should look beyond emerging technologies investments and shift their attention to how information systems infrastructure integration (ISII) and information systems-enabled strategic enterprise management (IS-SEM) practices can leverage decision-making performance (DMP) and impact on strategic flexibility and innovation.",CS,AI_ML,0.85,Extracted from log - paper 142
Enabling supply chain analytics for enterprise information systems: a topic modelling literature review and future research agenda,"ABSTRACT This paper provides a literature review of the research within the framework of 1) analytics,2) supply chain management, and 3) enterprise information systems, and relate the findings to competitive enablers. The findings are used to construct a future research agenda. The methodology is a systematic two-stage approach, based on a Smart Literature review framework using topic modelling. The research agenda proposes future research within the themes of 1) context, 2) cross-functional analytics, 3) cross-planning level analytics, 4) implementation and assimilation of analytics in EIS, 5) analytics and big data for SCM, 6) managerial aspects of analytics, and 7) data and system heterogeneity.",CS,AI_ML,0.85,Extracted from log - paper 143
An integrated framework of enterprise information systems in smart manufacturing system via business process reengineering,"Enterprise information systems play a significant role in the Industry 4.0 era and are the crucial component to realize smart manufacturing systems. However, traditional enterprise information systems have some limits: (1) lack of complete information, (2) only satisfy limited business needs, and (3) lack of seamless integration, business intelligence, value-driven processes, and dynamic optimization. Clearly, the existing enterprise information systems are unable to satisfy the requirements for smart manufacturing systems: (1) autonomous operation, (2) sustainable values, and (3) self-optimization. In addition, smart manufacturing systems have become more efficient and effective, demanding for seamless information flow in enterprise information systems, knowledge, and data-driven accurately decision. Therefore, a new enterprise information systems framework is needed to bridge gaps between the requirements for traditional manufacturing system and smart manufacturing system. In this article, the integrative framework is proposed based on the business process reengineering, lean thinking, and intelligent management methods, with inclusion of six enterprise information systems aspects to provide upgrading guidelines from traditional manufacturing to smart manufacturing. The procedure of this method contains three steps: (1) it identifies requirements and acquires best practices using AS-IS model, (2) it redesigns six aspects of enterprise information systems using TO-BE model, and (3) it proposes a new enterprise information systems framework. Finally, the proposed framework is validated by real cases.",CS,AI_ML,0.85,Extracted from log - paper 144
The Latest Information Systems in the Enterprise Management and Trends in their Development,"The idea of digital economy developing covers the whole world. Economy’s integration into the global information space today is one of the determinants of its competitiveness and efficiency. Under such conditions, transformational processes associated with an innovative model formation of the economy focused on scientific high-tech production, sustainable development and infrastructure creation for the intellectualized information space formation are becoming relevant in the national industry. One of the most important elements of modern innovation transformations is the information systems and technologies that are capable of producing large amounts of information and knowledge, transmit them at a distance, accumulate, store and form new intellectual products both within national and international economic systems.Therefore, the purpose of the paper is to investigate the functional potential of the newest information systems in the enterprises’ management, processing their economic information and formation of management processes information provision, as well as solving urgent tasks related to the need for the information systems’ continuous development and appropriate measures planning and their justification with the help of economic and mathematical methods taking into account the “cost and efficiency” ratio.The article highlights the relationship between the managerial process effectiveness and the managerial decisions’ information provision, and substantiates the crucial role of the newest information systems in achievement of the company’s commercial and innovative success, and increasing its profitability. According to the results of the research, the authors emphasized the need to find ways of information systems and technologies’ continuous development and improvement as mandatory conditions for their use. The scientific-methodical approach is proposed for the integral indicator calculation of the effectiveness of the enterprise information system improvement and development measures plan, which provides an opportunity to optimize costs and substantiate the enterprise information system development effectiveness.",CS,AI_ML,0.85,Extracted from log - paper 145
The key for success in enterprise information systems projects: development of a novel ERP readiness assessment method and a case study,"ABSTRACT ERP projects have a significant cost-share with high failure-rates. When the failure factors of ERP projects are evaluated, it is noteworthy that enterprises do not pay enough attention to ERP Readiness and moved directly to implementation. The literature lacks to explain ‘how and with which method’ an ERP Readiness assessment of an enterprise is made. Therefore, this study develops a novel ERP Readiness Assessment Model and Measurement Tool. The developed eight-staged methodology gives the ability for a holistic assessment of companies in transition to ERP systems. Besides, the results are in alignment with two MCDM methods for case company evaluation.",CS,AI_ML,0.85,Extracted from log - paper 146
Enterprise Knowledge Graphs: A Semantic Approach for Knowledge Management in the Next Generation of Enterprise Information Systems,"In enterprises, Semantic Web technologies have recently received increasing attention from both the research and industrial side. The concept of Linked Enterprise Data (LED) describes a framework to incorporate benefits of Semantic Web technologies into enterprise IT environments. However, LED still remains an abstract idea lacking a point of origin, i.e., station zero from which it comes to existence. We devise Enterprise Knowledge Graphs (EKGs) as a formal model to represent and manage corporate information at a semantic level. EKGs are presented and formally defined, as well as positioned in Enterprise Information Systems (EISs) architectures. Furthermore, according to the main features of EKGs, existing EISs are analyzed and compared using a new unified assessment framework. We conduct an evaluation study, where cluster analysis allows for identifying and visualizing groups of EISs that share the same EKG features. More importantly, we put our observed results in perspective and provide evidences that existing approaches do not implement all the EKG features, being therefore, a challenge the development of these features in the next generation of EISs.",CS,AI_ML,0.85,Extracted from log - paper 147
"Gamification in Enterprise Information Systems: What, why and how","With gamification, design elements known from games can be used to increase employees' engagement and improve users' experience. This paper points to Enterprise Information Systems as a viable point of implementation of gamification. There are three relevant questions: what gamification actually consists of, why it is worthwhile to apply it in Enterprise Information Systems, and how to implement it properly. The paper aims to answer them.",CS,AI_ML,0.85,Extracted from log - paper 148
The Effect of Enterprise Systems Implementation on the Firm Information Environment,"This study uses an archival research design to assess the impact of enterprise systems on a firm's internal information environment as reflected in the production of management earnings forecasts. Specifically, the authors hypothesize that, if enterprise systems improve management's access to decision-relevant internal information, higher quality management earnings forecasts should ensue. Consistent with disclosure theory and the purported technical characteristics of enterprise systems, the authors find a positive association between enterprise system implementations and subsequent increases in the likelihood of management forecast issuance and the accuracy of the forecasts. Additional robustness tests support the argument that improvements in management forecasts are due to improvements in the firm's internal information environment rather than to enhancements in management's ability to manage earnings. Beyond accumulating financial reporting information, the authors note that such systems provide management with information to make day-to-day operational decisions. Moreover, the paper provides a basis for considering management forecast qualities as a measurable proxy for improvements in the firm's internal information environment that result from information technology investments.",CS,AI_ML,0.85,Extracted from log - paper 149
A comprehensive survey on machine learning approaches for malware detection in IoT-based enterprise information system,"ABSTRACT The Internet of Things (IoT) is a relatively new technology that has piqued academics’ and business information systems’ attention in recent years. The Internet of Things establishes a network that enables smart devices in an organisational information system to connect to one another and exchange data with the central storage. Android apps are placed on Android apps to enhance the user-friendliness of IoT devices in business information systems, making them more interactive and user-friendly. However, the usage of Android apps makes IoT devices susceptible to all forms of malware attacks, including those that attempt to hack into IoT devices and get access to sensitive information stored in the corporate information system. The researchers offered a variety of attack mitigation approaches for detecting harmful malware embedded in an Android application operating on an IoT device. In this context, machine learning offered the most promising strategies to detect malware attacks in IoT-based enterprise information systems because of its better accuracy and precision. Its capacity to adapt to new forms of malware attacks is a result of its learning capabilities. Therefore, we conduct a detailed survey, which discusses emerging machine learning algorithms for detecting malware in business information systems powered by the Internet of Things. This article reviews all available research on malware detection, including static malware detection, dynamic malware detection, promoted malware detection and hybrid malware detection.",CS,AI_ML,0.85,Extracted from log - paper 150
International Journal of Enterprise Information Systems,"Council (ABDC); Bacon’s Media Directory; Burrelle’s Media Directory; Cabell’s Directories; Compendex (Elsevier Engineering Index); CSA Illumina; DBLP; DEST Register of Refereed Journals; EconLit; Gale Directory of Publications & Broadcast Media; GetCited; Google Scholar; IAOR Online; INSPEC; JournalTOCs; KnowledgeBoard; Library & Information Science Abstracts (LISA); MediaFinder; Norwegian Social Science Data Services (NSD); SCOPUS; The Index of Information Systems Journals; The Standard Periodical Directory; Ulrich’s Periodicals Directory Copyright The International Journal of Enterprise Information Systems (IJEIS) (ISSN 1548-1115; eISSN 1548-1123), Copyright © 2014 IGI Global. All rights, including translation into other languages reserved by the publisher. No part of this journal may be reproduced or used in any form or by any means without written permission from the publisher, except for noncommercial, educational use including classroom teaching purposes. Product or company names used in this journal are for identification purposes only. Inclusion of the names of the products or companies does not indicate a claim of ownership by IGI Global of the trademark or registered trademark. The views expressed in this journal are those of the authors but not necessarily of IGI Global. Research Articles",CS,AI_ML,0.85,Extracted from log - paper 151
Future Work and Enterprise Systems,"From its earliest days, research in business and information systems engineering (BISE) has been dedicated to envisioning how information technology will change the way we work and live. Today, technological innovation happens at a faster pace and reaches users more quickly than ever before. For example, while it took 75 years for the telephone to reach 100 million users, it was 16 years for mobile phones, 7 years for the World Wide Web, four and a half years for Facebook (Dreischmeier et al. 2015), and only a few weeks for Pokemon GO (Moon 2016). The rapid acceleration of technological diffusion confronts BISE researchers, who usually study technological innovations from the perspective of socio-technical systems (Bostrom and Heinen 1977). Work systems are conceptualized as an interplay of tasks, technologies, and people (vom Brocke and Rosemann 2014), systems “in which human participants and/or machines perform work (processes and activities) using information, technology, and other resources to produce specific products/services for specific internal and/or external customers” (Alter 2013, p. 75).",CS,AI_ML,0.85,Extracted from log - paper 152
Integrated design and operation management for enterprise systems,"Enterprise is a mini social-technical-ecological system in that it consists of humans, equipment and machines, and it has a location or site. Its structure follows the substanceinfrastructure (S-I) framework (Zhang and Wang 2016; Zhang and van Luttervelt 2011). There are two types of the S-I framework: the substance drives the infrastructure (Type I) and the infrastructure drives the substance (Type II). The enterprise system belongs to Type II. For instance, to a manufacturing system, the substance refers to goods made of materials, and the infrastructure refers to humans and machines, which produce and deliver goods to customers in response to their demands. To a service system (Wang et al. 2014), the substance refers to data (knowledge and information) (Zhang 1994) or signals or humans, and the infrastructure refers to humans and machines, which generate data, produce signals, or offer services to customers in accordance with their demands. Enterprise is a dynamic system, and it changes in its state and/or structure with respect to time, location, and/or event, and both the substance and infrastructure may change. A change on the part of the structure and/or state, say A, always has a reason or reasons, and this change is further associated with the change of another part of the structure and/or state of the system, say B; B is an independent variable and A is a dependent variable in this case (Zhang et al. 2005). The knowledge that governs the relation of A and B is called principle (Zhang et al. 2005; Zhang and Wang 2016). For instance, B is the force (F) applied on a block and A is the acceleration (a) of the block system, and the knowledge that governs the relation of A and B, in this case, is the Newton’s second law, that is, F = ma, where m is the mass of the block system. A care must be taken that the principle (knowledge) may be hidden or unfolded in data or big data but a correspondence relation of A and B can be built using various machine learning methods, e.g., Artificial Neural Network (ANN) (Zhao and Zhang 2017), various deep learning methods (Zhang et al. 2018), etc. The independent variable is a function of time, location, and/or event, so is the dependable variable, and thus the whole system changes with respect to time, location, and/or event. Design of an enterprise system means to determine its structure (infrastructure and substance) in response to a need or demand in a context (Zhang and Wang 2016). For instance, in response to the need of charging to electric vehicles, a new enterprise idea, the electric charge station enterprise, emerges. To this new enterprise, one needs to determine the charging equipment, number of workers, and so on, which makes sense to the design of an enterprise (Zhang and Wang 2016). Construction of an enterprise follows its design. Design and construction are processes, so it makes sense to say about their management. A good practice of the management of design and construction thus results in a good structure. ENTERPRISE INFORMATION SYSTEMS 2019, VOL. 13, NO. 4, 424–429 https://doi.org/10.1080/17517575.2019.1597169",CS,AI_ML,0.85,Extracted from log - paper 153
Risk management framework for information systems and organizations:,"This publication provides guidelines for applying the Risk Management Framework (RMF) to information systems and organizations. The RMF includes a disciplined, structured, and flexible process for organizational asset valuation; security and privacy control selection, implementation, and assessment; system and control authorizations; and continuous monitoring. It also includes enterprise-level activities to help better prepare organizations to execute the RMF at the system level. The RMF promotes the concept of near real-time risk management and ongoing system authorization through the implementation of continuous monitoring processes; provides senior leaders and executives with the necessary information to make cost-effective, risk management decisions about the systems supporting their missions and business functions; and integrates security and privacy controls into the system development life cycle. Applying the RMF tasks enterprise-wide helps to link essential risk management processes at the system level to risk management processes at the organization level. In addition, it establishes responsibility and accountability for the security and privacy controls deployed within organizational systems and inherited by those systems. The RMF incorporates concepts from the Framework for Improving Critical Infrastructure Cybersecurity that complement the currently established risk management processes mandated by the Office of Management and Budget and the Federal Information Security Modernization Act.",CS,AI_ML,0.85,Extracted from log - paper 154
Are business users social? A design experiment exploring information sharing in enterprise social systems,"In recent years, social technology has changed the ways people collaborate and communicate. With the blurring boundaries between work and personal life, business software vendors have begun to deliberate on the possibilities for enhancing the rather rigid and impersonal structures in enterprise systems (ES) by integrating social features. In doing so, they frequently assume that business users share the same interaction patterns as private users. In this paper, we challenge this belief and explore the factors that stimulate business users to share information in ES environments. By means of a design experiment, we show different use scenarios and explore business users’ attitudes toward open and unconditional information sharing in ES. Our results demonstrate that business users are less ‘social’ and that applying social features in ES is highly context dependent. Based on these findings, we offer recommendations for software vendors and researchers who are interested in the social enhancements of ES.",CS,AI_ML,0.85,Extracted from log - paper 155
Sociotechnical Enterprise Information Systems Design and Integration,"As technology continues to grow as an essential component of small and large enterprises, an ever increasing demand is placed on cutting-edge designs for information systems.Sociotechnical Enterprise Information Systems Design and Integration covers multiple systems and developments in design for businesses and enterprises of all sizes. This publication not only highlights the advancing technology and research in this area, but it also proposes strategic approaches to manage risks and detect errors. Researchers, practitioners, and professionals wishing to expand their knowledge on the most up to date information systems and its designs will benefit from this comprehensive compilation of research.",CS,AI_ML,0.85,Extracted from log - paper 156
Differing Strategies to Meet Information-Sharing Needs: Publicly Supported Community Health Information Exchanges Versus Health Systems' Enterprise Health Information Exchanges.,"POLICY POINTS Community health information exchanges have the characteristics of a public good, and they support population health initiatives at the state and national levels. However, current policy equally incentivizes health systems to create their own information exchanges covering more narrowly defined populations. Noninteroperable electronic health records and vendors' expensive custom interfaces are hindering health information exchanges. Moreover, vendors are imposing the costs of interoperability on health systems and community health information exchanges. Health systems are creating networks of targeted physicians and facilities by funding connections to their own enterprise health information exchanges. These private networks may change referral patterns and foster more integration with outpatient providers. CONTEXT The United States has invested billions of dollars to encourage the adoption of and implement the information technologies necessary for health information exchange (HIE), enabling providers to efficiently and effectively share patient information with other providers. Health care providers now have multiple options for obtaining and sharing patient information. Community HIEs facilitate information sharing for a broad group of providers within a region. Enterprise HIEs are operated by health systems and share information among affiliated hospitals and providers. We sought to identify why hospitals and health systems choose either to participate in community HIEs or to establish enterprise HIEs. METHODS We conducted semistructured interviews with 40 policymakers, community and enterprise HIE leaders, and health care executives from 19 different organizations. Our qualitative analysis used a general inductive and comparative approach to identify factors influencing participation in, and the success of, each approach to HIE. FINDINGS Enterprise HIEs support health systems' strategic goals through the control of an information technology network consisting of desired trading partners. Community HIEs support obtaining patient information from the broadest set of providers, but with more dispersed benefits to all participants, the community, and patients. Although not an either/or decision, community and enterprise HIEs compete for finite organizational resources like time, skilled staff, and money. Both approaches face challenges due to vendor costs and less-than-interoperable technology. CONCLUSIONS Both community and enterprise HIEs support aggregating clinical data and following patients across settings. Although they can be complementary, community and enterprise HIEs nonetheless compete for providers' attention and organizational resources. Health policymakers might try to encourage the type of widespread information exchange pursued by community HIEs, but the business case for enterprise HIEs clearly is stronger. The sustainability of a community HIE, potentially a public good, may necessitate ongoing public funding and supportive regulation.",CS,AI_ML,0.85,Extracted from log - paper 157
A Method for Enterprise Ontology based Design of Enterprise Information Systems,"A method for designing an Enterprise information system, which is based on an implementation independent model of the organization, has been developed. This model, also called an ontological model, consists of both implementation independent business processes and implementation independent information processes. An Enterprise information system (EIS) has to be understood as an implementation scenario of the ontological model. This study contains a specification framework that provides rules for taking into account during engineering the EIS.",CS,AI_ML,0.85,Extracted from log - paper 158
Information Systems Control: A Review and Framework for Emerging Information Systems Processes,"A major stream of information systems (IS) research examines the topic of control, which focuses on attempts to affect employee behavior as a means to achieve organizational objectives. Despite a rich history of IS control research, approximately 90 percent of the publications focus on only three IS processes: managing information systems development, managing IS outsourcing, and managing security. However, the emergence of new IS processes and technologies with distinct control challenges, such as managing enterprise architecture and managing innovation, highlights a need to consider the wider applicability of past control insights. In this paper, we first integrate existing IS control constructs and relationships into a comprehensive IS control model. Second, we apply this model to emerging IS processes to guide future research and practice. We review 65 influential IS control-related journal papers and identify five control dimensions. We then consolidate these dimensions into a single, integrated model to apply past IS control findings to the challenges of emerging information systems by posing a series of related propositions. With this paper, we position current IS control research to be increasingly applicable and relevant to tomorrow’s emerging IS opportunities and challenges.",CS,AI_ML,0.85,Extracted from log - paper 159
Optimization of Enterprise Financial Management and Decision-Making Systems Based on Big Data,"Based on information asymmetry theory, principal-agent theory, and risk management theory, this paper analyzes the mechanism of the impact of big data on financial decision-making, analyzing four dimensions: how big data enhances the information base for forecasting, how big data improves the relevance of decision-making, how big data builds new competitive advantages, and how big data promotes dynamic decision-making. Secondly, through the analysis of specific implementation cases of enterprise big data in financial decision-making, we focus on the real problems faced in management and the effect of solving problems through big data platform. The enterprise realizing the integration of business and finance not only can better lead business expansion, but also can improve the management level within the enterprise, which is conducive to the improvement of core competitiveness. The integration of industry and finance is essentially achieved through the application of various financial management modules to the business operations of enterprises, including budget management, capital management, fixed asset management, and financial accounting. If we want to implement the whole process of financial integration, it is impossible to achieve this manually, and we must first build a powerful information system as a guarantee. Under the guidance of theories of information asymmetry, stakeholders, and management information systems, Internet finance enterprises should build their own integrated business finance system based on the demand for business finance integration in the Internet finance industry, to enhance the matching of business finance data of Internet finance enterprises, improve the efficiency of enterprise management, and realize business finance integration. Finally, through the research of this paper, we hope to provide reference for other similar enterprises to apply big data for financial decision-making. Through the application of big data, higher economic benefits are achieved in procurement management, production control, capital budget, and investment decision compared with the previous ones. It is concluded that in the era of big data, massive data can be used to serve enterprise decision-making in depth, which can break the business and financial barriers, improve the efficiency and quality of decision-making, optimize the organizational structure and personnel, and enhance the prediction and early warning capability. The application of big data tools has become the key to assisting financial decision-making and enhancing enterprise value.",CS,AI_ML,0.85,Extracted from log - paper 160
A Comparative Analysis of Using the Capability Notion for Congruent Business and Information Systems Engineering,"The notion of capability has been gaining a growing attention in the business and information system (IS) engineering community due to a number of reasons: it facilitates focus on business investments, it can be used as a baseline for business planning, and it directly leads to service specification and design. It is not however widely known to what extent capability is considered in different modeling approaches, how it is defined, and what purpose it fulfills. This article analyzes how the notion of capability is included in the frameworks spanning from business-oriented such as Business Architecture and Business Value Modeling, to the alignment-oriented represented by Enterprise Architecture (EA), and Enterprise Modeling (EM). The results of the analysis have shown that capability has widespread presence in the frameworks and that its conceptual meaning is largely similar, while the intentions and the mechanisms of its use differ, which raises stimulating opportunities for new contributions and improvements in the field.",CS,AI_ML,0.85,Extracted from log - paper 161
Enterprise systems: are we ready for future sustainable cities,"Purpose – This paper aims to revisit the adoption reasons of enterprise systems (ES) and supply chain management systems (SCMS) and to explore the new dimensions of sustainability required to be added in the whole process of adoption of these systems. Moreover, it aims to explore the benefits of ES to organizations and to relate these benefits to the ES adoption in future sustainable city settings. Future cities will have micro-industries requiring dynamic interactions and will be dependent on efficient supply chains. The recent developments in information and communications technology (ICT) such as cloud computing through its dynamic, on-demand and service-based delivery are making it possible to achieve those goals for supply chains. The ES in general and more specifically SCMS have integrated organizations into one seamless mesh. Design/methodology/approach – This paper presents a framework for adoption of sustainable ES in a smart city setting. The framework, firstly, is presented at a macro-level, pa...",CS,AI_ML,0.85,Extracted from log - paper 162
Securing IoT devices in smart cities of India: from ethical and enterprise information system management perspective,"ABSTRACT To improve quality of life to the citizens, Government of India has proposed to create 100 Smart Cities in India (SCI). Each entity in SCI (enterprises) remains connected using Enterprise Information System (EIS) management. Citizens in SCI are also expected to use Internet of Things (IoT) enabled devices which will generate huge amount of data (Big Data). These data need to be protected. This would require efficient use of preventive technologies by the enterprises along with human efforts. In this paper some hypotheses and a conceptual model have been provided. These are validated through survey works with 331 usable respondents.",CS,AI_ML,0.85,Extracted from log - paper 163
"Understanding governance, risk and compliance information systems (GRC IS): The experts view","Although Governance, Risk and Compliance (GRC) is an emerging field of study within the information systems (IS) academic community, the concept behind the acronym has to still be demystified and further investigated. The study investigates GRC systems in depth by (a) reviewing the literature on existing GRC studies, and (b) presenting a field study on views about GRC application by professional experts. The aim of this exploratory study is to understand the aspects and the nature of the GRC system following an enterprise systems approach. The result of this study is a framework of particular GRC characteristics that need to be taken into consideration when these systems are put in place. This framework includes specific areas such as: goals and objectives, purpose of the system, key stakeholders, methodology and requirements prior to implementation, critical success factors and problems/barriers. Further discussion about the issues, the concerns and the diverse views on GRC would assist in developing an agenda for the future research on the GRC field.",CS,AI_ML,0.85,Extracted from log - paper 164
Enterprise ontology based development of information systems,"For the development of enterprise information systems, the utilisation of a suitable methodology is essential, providing necessary methods and techniques for modelling the business domain and for designing the supporting information systems. Several methodologies exist and are widely applied in practice nowadays, but most of them lack a theoretical foundation. In this paper, we demonstrate an information system development methodology based on the notions of enterprise ontology and business components, and explain it within the conceptual framework called the generic system development process. The methodology allows for reduction of complexity of domain models and for identification of stable business components.",CS,AI_ML,0.85,Extracted from log - paper 165
Information Security Governance Of Enterprise Information Systems: An Approach To Legislative Compliant,"Enterprises are now operating in the network economy. The network economy is dependent on the information infrastructure via the Internet. Organizations of all types (business, academia, government, etc.) are facing risks resulting from their ever-increasing reliance on the information infrastructure. Because of this, the US government implemented a number of legislations to secure cyberspace. This paper will examine the issue of Information Security Governance (ISG) of an enterprise information system, it will elaborate on the ISG framework, discuss the legislations and finally, assess how ISG can be framed to meet legislations to show due diligence and continuous process monitoring.",CS,AI_ML,0.85,Extracted from log - paper 166
The Power of Noise: Redefining Retrieval for RAG Systems,"Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.",CS,AI_ML,0.85,Extracted from log - paper 167
Enterprise Integration and Information Architecture: A Systems Perspective on Industrial Information Integration,"Introduction Modern Enterprise Solution Emergence of ESs Growth of ESs Brief History of ESs Characteristics of ESs ES Examples ES Applications in the Manufacturing Industry C3P System Reconfigurable Manufacturing Systems Aero-Engine Pipe Routing Assembly Planning ES Applications in Healthcare ES Applications in Managing Dams ES Applications in the Telecommunication Industry ESs in Transportation ES Applications in Other Areas Conclusion References Enterprise Integration Enterprise Integration Manufacturing Integration Brief Description of Manufacturing Enterprises Challenges Facing Manufacturing Enterprises Modularity and Integration to Meet Challenges Manufacturing Integration Engineering Integration Customer Integration Industry-Oriented ERP Introduction Industry-Oriented Enterprise Resource Planning Introduction IERP IERP versus General-Purpose ERP Connotation of IERP-Oriented Componentization Business-Driven Approach to IERP System-Oriented Componentization Levels of Business-Process-Driven Modeling Meta-Model for Business-Process-Driven IERP Componentization Category of Business Components Summary Entire Resource Planning Comprehensive MF Theory Comprehensive MF Theory and ERP Integrating ESs: Future Prospects References Extended Enterprise Integration in Supply Chain Interenterprise Collaboration Supply Chain Collaboration Service-Oriented Architecture RFID and IoT Agent Workflow Management Integrating Supply Chain Extended Enterprise Integration Examples of Recent Research Enterprise Collaboration: An Agent-Based Model VE Collaborative Operation: A Grid-Based Model Summary References Enterprise and Supply Chain Architecture Enterprise Architecture Supply Chain Modeling and the Relationship with EM and EA Modeling SCM Models in Operations Research, Operations Management, and SCM SCM Models Involving Intraorganizational Interoperation SCM Models Involving Interorganizational Interoperation EM and EA Methods Related to SCM Closing the Gaps between Existing SCM, EM, and EA Models Software Architecture: An Example of Recent Research Types of Software Architecture Scenario-Based Software Architecture Analysis: A New Method Modeling and Analysis of Workflow for LSCs: An Example of Integrative Modeling of SCM and EM Lean Supply Chain Assumptions Standardization of Collaborating Business Process between Organizations Modeling and Analysis of Cross-Organizational Workflow Application Example References Information Architecture for Enterprise and Supply Chain: A New Discipline of Industrial Information Integration Intraorganizational Systems Interorganizational Systems Model-Driven Architecture Service-Oriented Architecture Interoperability Models Industrial Information Integration: Examples Multilingual Semantic Interoperation in Interorganizational Enterprise Systems Agricultural Ecosystem Enterprise Information System Water Resource Management Enterprise System Automated Assembly Planning System for Complex Products Railway Signaling Enterprise System Based on IIIE IIIE: A New Discipline of Industrial Information Integration Business Process Management Information Integration and Interoperability EA and EAI Service-Oriented Architecture References Enterprise Process Modeling and Workflow Management Introduction Workflow Basics Intraorganizational Workflows Modeling Perspectives Modeling Techniques Interorganizational Workflows Workflow Modeling between Organizations Interaction Models Routing Approaches Workflow Analysis Qualitative Analysis Quantitative Analysis Empirical Study Future Directions Technical Aspects Managerial Aspect Summary References Enterprise Information Integration Modeling and Integrating Information Flows Data and Information Integration RFID: An Emerging Information Architecture IoT: An Emerging Internet-Based Information Architecture Introduction IoT-Oriented Infrastructure for Manufacturing Systems Enabling Technologies Ubiquitous Computing RFID Wireless Sensor Networks Cloud Computing More on the Enabling Technologies of IoT Standards Current Research Applications Introduction Industrial Deployment Social Internet of Things Healthcare Applications Infrastructure Security and Surveillance Data Cleaning in IoT Applications Challenges Future Work SOA for Internet of Things Open Problems and Future Directions Technical Challenges Standardization Information Security and Privacy Protection Innovation in IoT Environment Development Approaches References Industrial Information Integration Enterprise Application Intraorganizational EA Interorganizational EA Integration Requirements New Technology Requirements Integration Approaches Syntactic Integration Approaches SOA BPM Middleware-Based Techniques Object-Oriented Technology CORBA DCOM and COM Transaction-Based Technology Semantic Integration Approaches Enterprise Application Integration Distributed EA Architectures Integration of Distributed EAs Business Process Layer Integration Remote Method Invocation Message-Oriented Middleware Data Layer Integration Communication Layer Integration Presentation Layer Integration Other Integration Technologies J2EE Net Framework Web Services, SOA, and ESB Enterprise Service Bus Future Perspectives Trends Some Research Challenges Summary References Systems Approach to Industrial Information Integration Complexity Design Science Systems Approach Information Integration: An SSE Perspective Systems Concepts Complex Systems Dimensionality ESs' Subsystem Integration: Workflow Management ESs' Subsystem Integration: Manufacturing Systems Inquiring Systems Methodological Development Interdisciplinary Study Selection of Methods Theory for Integrated Information Systems as a Whole Summary CMFT: A New Theory in Systems Perspectives and Its Implication to IIIE Microscopic Perspectives: BI in ESs Business Intelligence Supervised Learning Methods Unsupervised Learning Clustering Itemset Mining Opportunities Provided by BI to ESs Process Mining Outlier Detection Graph Data Summary Resilient ESs Enterprise Resilience and Resilient Enterprise Systems SSE Serves ESs References Future Evolution Overview Major New Theories Impacting ESs MF Theory Major New Technologies Impacting ESs Internet of Things Cloud Computing Other Methods and Techniques Impacting ESs Software Architecture Methods Networking Summary and Challenges References Index",CS,AI_ML,0.85,Extracted from log - paper 168
Mechanism of formation of industrial enterprise development strategy in the information economy,"The article establishes that under the conditions of the information economy, industrial enterprises need a development strategy that takes into account the peculiarities of world changes in the spheres of production, marketing, management, etc. To solve this problem, a mechanism has been developed for forming a strategy for the development of an industrial enterprise in the information economy, based on the model of harmonization of specialized business processes with business process management, integration of management systems of specialized business processes into the general system of management of an industrial enterprise, as well as informatization and automation of business process management of the enterprise. Implementation of the developed mechanism into the practice of managing the development of an industrial enterprise provides an opportunity to increase its competitiveness, increase sales and reduce the cost of production.",CS,AI_ML,0.85,Extracted from log - paper 169
Critical factors for successful implementation of enterprise systems,"Enterprise resource planning (ERP) systems have emerged as the core of successful information management and the enterprise backbone of organizations. The difficulties of ERP implementations have been widely cited in the literature but research on the critical factors for initial and ongoing ERP implementation success is rare and fragmented. Through a comprehensive review of the literature, 11 factors were found to be critical to ERP implementation success – ERP teamwork and composition; change management program and culture; top management support; business plan and vision; business process reengineering with minimum customization; project management; monitoring and evaluation of performance; effective communication; software development, testing and troubleshooting; project champion; appropriate business and IT legacy systems. The classification of these factors into the respective phases (chartering, project, shakedown, onward and upward) in Markus and Tanis’ ERP life cycle model is presented and the importance of each factor is discussed.",CS,AI_ML,0.85,Extracted from log - paper 170
"Enterprise Information Systems Design, Implementation and Management: Organizational Applications","Enterprise Information Systems Design, Implementation and Management: Organizational Applications investigates the creation and implementation of enterprise information systems. Covering a wide array of topics such as flow-shop scheduling, information systems outsourcing, ERP systems utilization, Dietz transaction methodology, and advanced planning systems, it is an essential reference source for researchers and professionals alike.",CS,AI_ML,0.85,Extracted from log - paper 171
Enterprise Information Systems and Implementing IT Infrastructures: Challenges and Issues,"Enterprise Information Systems and Implementing IT Infrastructures: Challenges and Issues aims at identifying potential research problems and issues in EIS, allowing research scholars and practitioners to develop suitable strategies and operational policies for EIS, thus improving communication within organizations. The cutting-edge research discussed in this book intends to spark the discussion of new ideas and developments in the field of EIS amongst researchers and practitioners.",CS,AI_ML,0.85,Extracted from log - paper 172
Secure computation protocols under asymmetric scenarios in enterprise information system,"ABSTRACT It has become acentral question to guarantee the security of sensitive data in enterprise information systems. Actually, different enterprises may grasp asymmetric information due to their scales. How to guarantee the computation security under such scenario is acrucial problem for enterprise information system. In this paper, we propose asecure computation protocol under asymmetric information scenario in the presence of rational parties. We also work towards the achievement of security properties in enterprise information systems. Our new definition of utilities is proved to be coincident (and compatible) with the former definitions under asymmetric information scenario.",CS,AI_ML,0.85,Extracted from log - paper 173
Influential Characteristics of Enterprise Information System User Interfaces,"ABSTRACT End-user acceptance is considered as a significant factor influencing the success of enterprise information system (EIS) implementations and operations. This study conceptualizes three aspects of EIS user interfaces (UIs), namely information overload, control familiarity, and UI fit, and proposes a model to understand their effect on two major factors that are considered to influence the end-user acceptance of these systems: EIS end user’s performance expectancy and effort expectancy. We developed a theoretical model and multiitem scales for the proposed EIS UI characteristics and tested the model empirically with data from a survey performed with a sample 98 EIS end users. The results from our test provide evidence for the key role that EIS UI design plays in the end user’s performance and effort expectancy.",CS,AI_ML,0.85,Extracted from log - paper 174
Policy-Level Specifications in REA Enterprise Information Systems,"The Resource‐Event‐Agent (REA) enterprise model is a widely accepted framework for the design of the accountability infrastructure of enterprise information systems. Policy‐level specifications define constraints and guidelines under which an enterprise operates, and they are an extension to the REA enterprise model, adding the “what should, could, or must be” to the “what is.” This paper aims both at comprehensive understanding of policy‐level definitions as part of REA enterprise systems and at understanding of the semantic constructs that enable such definitions. We first explore two distinctive semantic abstractions essential to policy‐level specifications: typification and grouping. The typification abstraction links instances of an object class to concepts for which they are concrete realizations, while the grouping abstraction aggregates objects into collections. We next present a number of patterns for the semantic modeling of policies. Following, we look at policy‐level applications for REA enter...",CS,AI_ML,0.85,Extracted from log - paper 175
Enterprise Information Systems: Contemporary Trends and Issues,"This book analyzes various aspects of enterprise information systems (EIS), including enterprise resource planning, customer relationship management, supply chain management systems, and business process reengineering. It describes the evolution and functions of these systems, focusing on issues related to their implementation and upgrading. Enhanced with pedagogical features, this book can be read by graduate and undergraduate students, as well as senior management and executives involved in the study and evaluation of EIS.",CS,AI_ML,0.85,Extracted from log - paper 176
Global Implications of Modern Enterprise Information Systems: Technologies and Applications,"Many companies have encountered pitfalls in their attempts to successfully implement enterprise resource planning (ERP) that have proven nearly insurmountable.Global Implications of Modern Enterprise Information Systems: Technologies and Applications presents useful strategies, techniques, and tools for the successful design, development, and implementation of enterprise information systems (EIS). Through an international collaboration of perspectives, this collection presents cutting-edge research findings on modern enterprise information systems.",CS,AI_ML,0.85,Extracted from log - paper 177
ANALYSIS OF CABLE NETWORK READINESS FOR THE IMPLEMENTATION OF ENTERPRISE RESOURCE PLANNING INFORMATION SYSTEMS AT THE FACULTY OF INDUSTRIAL ENGINEERING,"With the rapid advancement of information and communication technology, organizations increasingly adopt integrated systems to enhance efficiency and productivity. One widely adopted technology is Enterprise Resource Planning (ERP), a comprehensive management system that integrates various business functions, including finance, manufacturing, inventory, and human resources. Implementing an ERP system requires a robust network infrastructure, particularly in terms of quality of service (quality of service). This study aims to evaluate the readiness of the cable network infrastructure across three buildings at the Faculty of Industrial Engineering, Telkom University, to implement an Odoo-based ERP system. The research employs the Network Development Life Cycle (NDLC) methodology, focusing on crucial quality of service parameters such as throughput, delay, jitter, and packet loss. Data were collected through observations, interviews, and network analysis using Wireshark, with tests conducted at different times (low, peak, and intermediate). The results show that the TULT Building, Mangudu Building, and Building B Cacuk networks are generally prepared for ERP implementation. For instance, in the TULT Building, the average throughput without filters at low, peak, and intermediate times was 45.296 Kbps, 50.923 Kbps, and 61.399 Kbps, respectively. Packet loss averaged 0.56%, 0.50%, and 0.65% without filters. Despite jitter values ranging from 103.73 ms to 582.40 ms, below the TIPHON standard, the ERP system remains functional as it is not highly sensitive. The study concludes that the existing network infrastructure is sufficient mainly for the Odoo-based ERP implementation, with recommendations for further improvements to address jitter issues.",CS,AI_ML,0.85,Extracted from log - paper 178
A Decision-Making Model With Intuitionistic Fuzzy Information for Selection of Enterprise Resource Planning Systems,"Involvement of small and medium-scale enterprises is important for developing countries like India. Henceforth, enhancing their competency with the adoption of a suitable enterprise resource planning system (ERPS) is a vital way to enhance their competitiveness. The choice of an appropriate ERPS can be considered as a complex multicriteria decision-making (MCDM) problem where several alternatives (ERPS packages) are assessed based on certain types of characteristics, namely domain knowledge of the supplier, system reliability, service and support, functionality, compatibility, information security, cross-module integration, etc. One critical issue associated with the evaluation of ERPSs is the handling of uncertain information. In this article, we first develop a new methodology to resolve the issue of MCDM problems. In this methodology, the weights of decision experts are systematically calculated with the extended variance approach on intuitionistic fuzzy sets. For calculating the criteria weights, an optimization model based on cross-entropy is proposed, and for the aggregation of the criteria values, a new model called intuitionistic fuzzy improved measurement alternatives and ranking based on the compromise solution is developed, which allows both the vector and linear normalizations and having the advantages of comprising two kinds of aggregation models. Next, an ERPS cloud vendor assessment problem is deliberated with three alternatives (A1: Systems, Applications, and Products (SAP), A2: Oracle Corporation, A3: Microsoft Corporation) and 16 criteria to interpret the reasonableness of the introduced framework. According to the outcomes, SAP is the most suitable alternative. Next, a sensitivity analysis is exhibited with diverse parameter values to inspect the permanence of the presented approach. The advantage of the presented approach is discussed with the comparative investigation.",CS,AI_ML,0.85,Extracted from log - paper 179
Implementation of business intelligence considering the role of information systems integration and enterprise resource planning,"The aim of this research is the implementation of business intelligence, considering the role of information systems integration and enterprise resource planning on it. According to the objectives of this research, it is practical research, and the work process is based on descriptive, survey, and exploratory research. The study population of the qualitative part of this research includes experts (information technology and communications managers from Tehran Stock Exchange companies and professors). Twenty-five interviews were performed by a non-random and targeted method, until a theoretical saturation of the questionnaire was reached. The study population of the quantitative part includes all the personnel of 167 companies where business intelligence is implemented in their organizations. Two questionnaires were used for gathering the required data for evaluating and measuring the studied variables. Validity is confirmed by experts' opinions. Finally, seven issues of structural factors, behavioral factors, environmental factors, processes, output, consequence, and the effect and their subcomponents are identified as effective items in business intelligence success. Regarding the outcome, importance, and the model coefficient of the main factors, the processes have the most impact on the results. So, organizations should pay more attention to their working processes to improve business intelligence success. Overall, the results regarding the effective factors on successful implementation of business intelligence reflect best practices of firms that have successfully implemented BI systems and provide insights for BI stakeholders that may increase the chances of successful implementation. This study shows the value of integrated information systems and enterprise resource planning in the success of business intelligence implementation. The findings of this study provide an opportunity for other researchers to study a cost optimization approach. It also suggests it is time to investigate suitable approaches by a focus on the appropriate factors for successful business intelligence implementation and by comparative analysis of ways to boost business intelligence preparation. This study also found further factors, in addition to enterprise resource planning and information systems integration, that can be used to select and rank more factors of business intelligence implementation. Furthermore, a model that examines the integration of business intelligence and the other information systems in the company is proposed for future research.",CS,AI_ML,0.85,Extracted from log - paper 180
ADCOMSITE SEBAGAI PENDUKUNG ADMINISTRASI DAN KOMITE DENGAN MENGGUNAKAN ENTERPRISE RESOURCE PLANNING INFORMATION SYSTEM DI MTs PPKP SAMPIT,"Information technology and education are very closely related to one another. In the field of education, information technology also has benefits not only as a teaching and learning tool but can also be used for administrative information systems and school committees. One of the schools that has not yet implemented technology in its administration and school committees is MTs PPKP. In 2021, MTs PPKP has 465 students. 158 new students for the 2021-2022 academic year. For that many students, it is necessary to have support who can help handle the administration of new students and even for committee payments for both old and new students. In this study, the ERP (Enterprise Resource Planning) module is an alternative that can be used to solve this problem. In this study, the ERP (Enterprise Resource Planning) module used is Inventory Management and Sales and Distribution. Where the Inventory Management module used in this study focuses on managing school attribute data for new students. Meanwhile, the Sales and Distribution module focuses on managing student data as well as administrative and school committee payments. Based on the research that has been done, the method used can be used to support administration and school committees.",CS,AI_ML,0.85,Extracted from log - paper 181
"Assessing the Impact of System Quality, Information Quality, and Service Quality on Enterprise Resource Planning (ERP) Systems","Globally, governments are taking steps to help them increase their income generation margin by implementing tax administrative ERP systems. However, the impacts on the internal system users of these ERP system quality features have not drawn the attention needed. This study, therefore, examines the relationship between the information systems' (IS) quality and individual impact using the theoretical foundation of the DeLone and McLean IS success model and, secondly, addresses the interrelationships between the quality constructs of information systems (IS). The authors also used the structural equation modeling technique of partial least squares to evaluate and analyze the data. The results show that system quality, the information quality, and the service quality characteristics of the tax administrative ERP system have a strong positive impact on the success of the IS at the individual level. There is also a positive relationship between the information systems' (IS) quality construction. The results provide additional empirical observations and consequences for management.",CS,AI_ML,0.85,Extracted from log - paper 182
The Effect of Accounting Information Systems (AIS) on Enterprise Resource Planning (ERP),The purpose of this study is to know the impact of accounting and administrative information systems on the performance of institutions. It also aims at finding the relationship between the variables of the accounting information system and the performance variables of the institutions. The study data were collected from 60 employees and faculty members within the College of Computer Science and Information Technology at the University of Basra in Iraq. The project management process was carried out through the PMP program and analysis of the project was conducted through the SPSS program. The study showed the importance of management and planning followed by constraints on efficiency and procedures related to the performance of institutions. The study achieved its objectives and determined the relationship between variables. The use of accounting and management information systems is very important within the organization in the context of technological development in the world.,CS,AI_ML,0.85,Extracted from log - paper 183
Mechanisms for successful management of enterprise resource planning from user information processing and system quality perspective,"Enterprise resource planning (ERP) systems are now ubiquitous in modern organizations. A number of previous studies have focused only on system factors and perceptions, there is a noticeable shortfall in research that concurrently addresses technological factors and human roles in explaining user satisfaction. This study aimed to identify these variables from the perspectives of information systems, technology, and human participation, thereby addressing this knowledge gap. The focus of the study was a large shipbuilding and marine company utilizing an ERP system. The participants, a sample of 234 ERP users, were carefully selected by the company’s executives and practitioners, and data was collected through online questionnaires. They were selected through purposive sampling from among employees who use ERP systems in large-scale shipbuilding and marine engineering companies. The study aimed to clarify the relationships between user satisfaction and perceived ease of use, perceived usefulness, system quality, service quality, participation, and information quality. A partial least squares structural equation modeling (PLS-SEM) was used to analyze the collected data. The results indicated that perceived ease of use, system quality, service quality, and participation positively influenced user satisfaction, whereas perceived usefulness did not have a significant impact. Interestingly, participation was found to lessen the effects of perceived usefulness on satisfaction. The findings of this study suggest that to enhance ERP user satisfaction, managers should strive to make the ERP system easy-to-use and stable, encourage employee participation in the decision-making process, and bolster the role of the support team. It should be noted, however, that the study has limitations as it did not consider all possible factors, such as training and support. Future research should take a broader view of the variables involved in the operation of an enterprise-wide information system.",CS,AI_ML,0.85,Extracted from log - paper 184
Enhancing PLS-SEM-Enabled Research with ANN and IPMA: Research Study of Enterprise Resource Planning (ERP) Systems’ Acceptance Based on the Technology Acceptance Model (TAM),"PLS-SEM has been used recently more and more often in studies researching critical factors influencing the acceptance and use of information systems, especially when the technology acceptance model (TAM) is implemented. TAM has proved to be the most promising model for researching different viewpoints regarding information technologies, tools/applications, and the acceptance and use of information systems by the employees who act as the end-users in companies. However, the use of advanced PLS-SEM techniques for testing the extended TAM research models for the acceptance of enterprise resource planning (ERP) systems is scarce. The present research aims to fill this gap and aims to show how PLS-SEM results can be enhanced by advanced techniques: artificial neural network analysis (ANN) and Importance–Performance Matrix Analysis (IPMA). ANN was used in this research study to overcome the limitations of PLS-SEM regarding the linear relationships in the model. IPMA was used in evaluating the importance and performance of factors/drivers in the SEM. From the methodological point of view, results show that the research approach with ANN artificial intelligence complements the results of PLS-SEM while allowing the capture of nonlinear relationships between the variables of the model and the determination of the relative importance of each factor studied. On other hand, IPMA enables the identification of factors with relatively low performance but relatively high importance in shaping dependent variables.",CS,AI_ML,0.85,Extracted from log - paper 185
The Adoption of Enterprise Resource Planning and Business Intelligence Systems in Small and Medium Enterprises: A Conceptual Framework,"The widespread globalism of services and the fast technological progress brought about by IT have made organizations today competitive in offering new products and services. One of the highlighted innovations is the enterprise resource planning and business intelligence system (ERPB), which has captivated the interest of industry analysts and policymakers due to its potential to provide more intellectual information for decision making and ensure its continuity. Firms must use ERPBI appropriately to flourish, as its misuse leads to failure. Even though research has focused on the factors influencing ERPBI adoption, most studies have ignored many factors that influenced the behavioral intention to adopt variable that significantly affects the adoption and use of technology. Ensuring the proper adoption of ERPBI is critical to corporate success. If ERPBI is misused, this will not serve its objective. Although a few studies have studied factors affecting the success of ERPBI adoption, the majority have failed to discover drivers of continuity and sustainability. This article proposed a conceptual framework for ERPBI adoption using the TOE and UTAUT models. Four main hypotheses were formulated for the three dimensions of technology, organization, and environment. The proposed framework is vital in adopting the ERPBI and could shape the direction in successfully adopting new technologies in SMEs. This study could fill in the gap in ERPBI research because previous studies have not focused on ERPBI adoption factors, ERPBI adoption, and business continuity.",CS,AI_ML,0.85,Extracted from log - paper 186
Determination the Factors that Affect the Use of Enterprise Resource Planning Information System through Technology Acceptance Model,"In order to survive in today’s global competitive environment, businesses have to use information systems during management and production processes. Technology Acceptance Model (TAM) is developed by Davis (1989). According to this model, the use of a new information system depends on perceived ease of use and perceived usefulness variables. This study analyzes the factors that affect the use of Enterprise Resource Planning (ERP) software, which is an information system, in the frame of TAM variables. 236 companies that use these information systems participated in the survey. It is determined that, eleven independent variables, which are taken into consideration in the scope of user characteristics, innovative characteristics, organizational and environmental characteristics factors, have statistically meaningful and positive effect on perceived ease of use and perceived usefulness which represent TAM parameters. Similarly, it is determined that, perceived ease of use and perceived usefulness have a positive and statistically meaningful effect on the use of ERP.",CS,AI_ML,0.85,Extracted from log - paper 187
Using Information Systems for Enhancing Internal Operation: Enterprise Resource Planning Systems,"In this chapter, we will review functional information systems, explain why there is a trend towards cross-functional information systems, identify the major cross-functional information systems in business and their main roles, explain the need for enterprise resource planning (ERP) systems in business, discuss implementation issues of ERP systems, and describe emerging trends of ERP systems.",CS,AI_ML,0.85,Extracted from log - paper 188
Moving enterprise resource planning (ERP) systems to the cloud: the challenge of infrastructural embeddedness,"Cloud enterprise resource planning (ERP) solutions allow organizations to support and coordinate key business processes by leveraging virtualization. Nevertheless, moving ERPs to the cloud is not straightforward, and organizational cloud ERP initiatives raise multiple concerns. We conducted an in-depth systematic review of relevant research literature and identified six key concerns related to cloud ERP implementation: a) the introduction of new ERP work arrangements, b) the migration of legacy data, c) the assurance of compliance with extant rules and regulations for security, d) the continuous alignment between ERP functionality and business processes, e) the ongoing integration between ERPs and the rest of the organization’s application portfolio, and f) the establishment of adequate reliability levels. The identified concerns are associated with both transition management and operations supported by cloud ERPs. All the identified concerns are also related to the need to achieve infrastructural embeddedness. This need sets ERPs apart from other types of cloud-based applications, such as office automation solutions that do not have as many dependencies and exchanges with other systems and repositories within an organization’s information infrastructure. We argue that the challenge of embeddedness has different implications for organizations of different sizes, and we call for further empirical research.",CS,AI_ML,0.85,Extracted from log - paper 189
"The Past, Present, and Future of Enterprise Resource Planning","Enterprise Resource Planning (ERP) is a software system designed to facilitate the automation and management of essential business operations inside businesses, with the aim of achieving optimum performance. ERP software facilitates the integration and synchronization of data across various business processes inside an organization. This results in the establishment of a unified and reliable source of information, which in turn optimizes the efficiency and effectiveness of operations throughout the whole firm. The platform has the capability to integrate a company's human resource functions, financials, manufacturing, supply chain, operations, reporting and commerce into a unified system. The objective of this article is to provide a comprehensive examination of the historical progression and evolution of ERP systems, while also highlighting contemporary advancements facilitated by digital breakthroughs such as cloud computing. The research methodology used in this work included doing a comprehensive evaluation of scholarly literature sourced from academic publications as well as industry sources. The study revealed that ERP systems have undergone significant transformations compared to their predecessors, like MRP (material resource mapping) systems and IC (integrated control) packages. The ongoing transformations are seen in conjunction with the emergence of cloud computing and advancements in modern technologies such as artificial intelligence. Numerous scholarly publications have already examined the progression of Enterprise Resource Planning (ERPs). However, the distinctive value of this particular article lies in its comprehensive coverage of the latest advancements in the field, including debates on cloud-based ERPs and postmodern ERPs.",CS,AI_ML,0.85,Extracted from log - paper 190
User Acceptance of Enterprise Resource Planning (ERP) Systems in Higher Education Institutions: A Conceptual Model,"It has become common practice for higher education institutions (HEIs) to replace existing computer systems, called legacy administrative information systems, with new ones to cope with the continuously changing demands in the context of education. At the top of these systems is enterprise resource planning (ERP) systems that integrate HEIs' business processes, functions, and data to improve their overall productivity and effectiveness. However, many studies on ERP adoption have shown that organizations frequently face several barriers, and the failure rate is high. In addition, various research projects have concluded that, quite often, HEIs do not obtain the expected advantages from the adoption of the ERP system. This research aims to explore the factors that affect the behavioral adoption and acceptance of an ERP system in the context of HEIs. Based on works from literature and authors' observations of the PeopleSoft system (ERP system) implementation at Yanbu University College (YUC) in Yanbu City, Saudi Arabia (SA), a conceptual model of users' acceptance of ERP systems has been proposed. The framework is based on the unified theory of acceptance and use of technology (UTAUT) model and is extended with additional constructs. The present study offers a theoretical contribution by extending the UTAUT model to provide a richer understanding of users' adoption behavior of ERP systems in the HEI context. In addition, according to the authors' knowledge, this is the first paper that to address ERP users' adoption perspective in HEIs in SA.",CS,AI_ML,0.85,Extracted from log - paper 191
Factors of Success in Implementation of Enterprise Resource Planning Systems,"Implementing complex and yet sound and effective accounting information systems known as enterprise resource planning systems, is an enormous project for the firms that want to streamline their information flow and increase their value. As the effects of these systems on the performance of the entities are both financial and non-financial, their success is of crucial importance. This study is an exploratory factorial analysis to identify the factors of successful implementation of ERP systems in the medium and big companies in Albania. We use in depth surveys and interviews with firms that have implemented enterprise resource planning systems and include twenty initial variables in the survey. Later these variables are combined in three factors that have the biggest impact on the success rate of ERP implementation: the overall quality and end-user satisfaction with the ERP system; the cost-benefit ratio of the ERP and the support from the ERP provider and ERP implementing consultant. This study contributes not only theoretically to the empirical literature, but also practically because it helps businesses of the region that are considering implementing ERPs in the future to pay attention to the most critical factors of success with ERPs.",CS,AI_ML,0.85,Extracted from log - paper 192
Managing Records in Enterprise Resource Planning Systems,"Enterprise resource planning (ERP) systems are increasingly being used for the management of business processes and to integrate tasks within institutions in real time. While managing and integrating processes, ERP systems generate and are expected to manage enormous amounts of data and information that should be managed in trustworthy manner. This article draws from a multi-year ERP implementation project by the United Nations to highlight some recordkeeping challenges.",CS,AI_ML,0.85,Extracted from log - paper 193
Sustainable Construction through Resource Planning Systems Incorporation into Building Information Modelling,"The latest industrial revolution 4 enabled significant performance improvement through technological advancements. Simultaneously, the industry is setting high-level expectations for changing business practices toward long-term benefits in all three sustainability dimensions. The concept of sustainability embraces all production and operation processes in the Architecture, Engineering, and Construction (AEC) industry. This study systematically explores the literature on sustainability with Enterprise Resource Planning (ERP) and Building Information Modelling (BIM) technologies in the AEC industry and the sustainability vision for their integration. The different types of ERP and BIM implementations have similarities in addressing the broad scope of functionalities. The emergence and proliferation of ERP and BIM have brought crucial changes to the business environment. Further evolution to cloud-based operations is transforming companies from technology-oriented practices to data-centric decision-making smart infrastructures. The narrative literature review investigates the sustainability insights and ideas in ERP and BIM solutions, presenting state of the art on systems integration topics. The relevant literature was retrieved to achieve the research objectives which were qualitatively analyzed to generate the basis for further research.",CS,AI_ML,0.85,Extracted from log - paper 194
Enterprise Resource Planning Systems: The Business Backbone,"Enterprise Resource Planning is a software system that integrates data and information management of the entire functional company that includes finance, accounting, production, sales, purchasing, human resources and other functions. These functions are separated by software modules, but are connected to one integrated data center. Modules in the ERP system include financial accounting, control, investment management, enterprise controlling, treasury, logistics execution, sales distribution, materials management, production planning, plant maintenance, quality management, project systems. The benefits of ERP in the company can help business process automation, Single point of information, High efficiency, Sources of information sufficient to conduct analysis.",CS,AI_ML,0.85,Extracted from log - paper 195
Enterprise Resource Planning Systems and XBRL Reporting Quality,"Enterprise resource planning (ERP) systems are indispensable for a majority of larger and midsize firms, and have changed the way accounting information is collected, stored, processed, and disseminated. Although most ERP systems integrate an eXtensible Business Reporting Language (XBRL) component in their core modules, little research has examined how ERP systems affect the quality of XBRL filings. Using unique data from branch-level ERP implementation, we find that the degree of ERP adoption among a firm’s branches is negatively associated with the firm’s XBRL filing errors and positively associated with XBRL reporting comparability, which in turn facilitates external users’ access to the firm’s XBRL filings in the SEC’s EDGAR. These results suggest that ERP systems improve XBRL reporting quality. Moreover, our results indicate that ERP can mitigate the negative effect of extension taxonomies on XBRL reporting quality, which highlights the importance of the ERP system in a complicated XBRL reporting environment.",CS,AI_ML,0.85,Extracted from log - paper 196
Factors affecting post-implementation success of enterprise resource planning systems: a perspective of business process performance,"ABSTRACT The study investigates critical factors which are important to evaluate enterprise resource planning (ERP) in the post-implementation stage. A conceptual framework is proposed with a set of relevant hypotheses and a structural equation modeling is used to analyze the survey data using Smart-PLS package program. The results illustrate that post-implementation success factors are significant for assessing an overall impact of ERP post-implementation. Likewise, the possibility of business process performance is higher in a condition of when the systems are employed in a coordinated way. The findings may assist ERP professionals and developers in other countries for ERP implementation in future.",CS,AI_ML,0.85,Extracted from log - paper 197
Developing an ARIS-House-Based Method from Existing Information Systems to Project-Based Enterprise Resource Planning for General Contractor,"In recent years, general contractors in the construction industry have gradually begun to implement a system called enterprise resource planning (ERP). During the ERP implementation process, contractors performed required analyses on daily operation functions demanded by the enterprise. The analyses focused on function mapping to ensure that ERP satisfies all the requirements, including the functions of existing information systems, and meets future requirements. The process of function mapping in the construction industry typically involves a series of lengthy and time-consuming meetings, and face-to-face discussions; systematic analysis procedure was lacking. This research will propose a novel function mapping approach, the Architecture of Integrated Information Systems (ARIS)-house-based (AHB) method, to enhance the effectiveness of meetings and improve the efficiency of discussions. In addition, AHB method will use the structure of ARIS-house diagram to guide the function mapping process, streamline existing information systems, meet future requirements, and successfully implement ERP. Finally, this research will use a case study to verify the effectiveness of the AHB method for contractor to implement ERP.",CS,AI_ML,0.85,Extracted from log - paper 198
Environmental information systems based on enterprise resource planning,"This paper describes the consequences of the integration of environmental information within enterprise resource planning (ERP) systems. The state‐of‐the art of dedicated environmental information systems is briefly discussed. Essentials and peculiarities of environmental information are highlighted. The role of environmental management systems and their relationship with other dedicated management systems is positioned within this field. The need for information following from this is compared with the information available in current ERP systems. The common features of both information systems are discussed and the importance of physical relationships within them is demonstrated. A physical approach is argued as the very base of future extended ERP systems. An outline of the special character of physical information is presented for this purpose. Subsequently, some restrictions connected to the current approach in ERP are analysed. This results in a number of recommendations. The most crucial aspects are the integration of the process and discrete manufacturing orientation by applying a multiple‐input multiple‐output approach to all processes, and a similar consideration of co‐products, by‐products, wastes and emissions.",CS,AI_ML,0.85,Extracted from log - paper 199
Quality And Effectiveness Of Enterprise Resource Planning - Customer Relationship Management Systems: Implications For Information Systems Marketing Strategies,"The present research examines the success of Enterprise Resource Planning (ERP) systems in Greece, employing Delone and McLean’s model. The questionnaires of the study were e-mailed to the 1,049 largest organizations based on turnover and assets. The final research sample consists of 105 Greek enterprises. The list of organizations was compiled by using the Index of Companies and Products Directory (ICAP). Results show that users have more positive attitude towards system quality and information quality and less positive attitude towards service quality. Moreover, users believe that ERP systems enhance their performance and can contribute to the control of management. Furthermore, present paper proved that D&M model constitutes a causal model with the exception of the variable of system use that is only related to user satisfaction, and ERP system quality.The findings of this research trigger many theoretical and managerial implications and create lots of potential for future research in the fields of Information Systems, Management, and Marketing.",CS,AI_ML,0.85,Extracted from log - paper 200
Impact of Top Management Support on Accounting Information System: A Case of Enterprise Resource Planning (ERP) System,"Accounting Information System (AIS) is a system that processes financial and nonfinancial transactions that directly affect the organizations operations processing of financial activities such as changes to customers' names and addresses to keep such files current. The purpose of this study was to illustrate the impact of top management support on accounting information systems, a case of Enterprise Resource Planning (ERP) Systems and, further, to analyze the impact of accounting information systems on organizational performance. Online questionnaire was the main tool used for data collection and data was analyzed using descriptive statistics, inferential statistics, as well as while qualitative data was analyzed using thematic analysis. The study established that in the 3 university ERP had already been adopted and was being implemented. Also the Top Management supported the implementation of ERP. The ERP system was not adequately utilized to realize its full potential. It was extensively used on matters related to finance. Especially students finance and managing of the universities payrolls. The systems had not yet been extensively used on managing human resource matters and communication that it had capacity to achieve much. The findings of this study are important as they provide insight that could awaken the institutions that had adopted the ERP systems on how they could optimally benefit from the system. It also highlights the role played by top management on implementation of a new system like ERP. Finally it gives knowledge for fellow scholars interested in the field of Accounting Information Systems generally.",CS,AI_ML,0.85,Extracted from log - paper 201
Extended Version of Linguistic Picture Fuzzy TOPSIS Method and Its Applications in Enterprise Resource Planning Systems,"The main objective of the proposed research in this paper is introducing an extended version of the linguistic picture fuzzy TOPSIS technique and then solving the problems in enterprise resource planning systems. In this article, we use the uncertain information in terms of linguistic picture fuzzy numbers; the decision maker provides membership, neutral, and nonmembership fuzzy linguistic terms to represent uncertain assessments information of alternatives in linguistic multicriteria decision making (LMCDMs). In order to introduce the extended version of TOPSIS method, we defined a new hamming distance measure between two linguistic picture fuzzy numbers. Further, we apply the proposed method to problem of enterprise resource planning systems and discuss numerical implementation of the proposed method of LMCDM.",CS,AI_ML,0.85,Extracted from log - paper 202
A structural equation model for analyzing the relationship between enterprise resource planning and digital supply chain management,"Digitization and the internet are new phenomena which have greatly changed the way in which modern-day businesses are run. Enterprise resource planning (ERP) is one such system developed as a result of digitization in order to improve firm supply chain efficiency. This study investigated the relationship between digital and sustainable supply chain (in this case the ERP system) and firm competences of supply chain management. 33 respondents from e-commerce firms volunteered to take part in the data collection process of the study, with a majority of them being information technology (IT) managers. The hypothesized relationships were analyzed using structural equation modelling (SEM). Findings of the study depict that all the hypothesized relationships were significant (p<.001) at the 1 % level of significance, implying that there exists a positive effect of the ERP systems on firm performance.",CS,AI_ML,0.85,Extracted from log - paper 203
A decision-making framework for evaluating enterprise resource planning systems in a high-tech industry,"ABSTRACT In recent years, the adoption of enterprise resource planning (ERP) is becoming essential for achieving fast and transparent information exchange, avoiding unnecessary waste and maintaining coordination within a firm and among partners in a supply chain. However, the implementation of an ERP system often fails, and one major reason is that the selected ERP system is not suitable for the firm. Therefore, a good evaluation framework for selecting the most appropriate ERP system is necessary. This study proposes a framework, which integrates decision-making trial and evaluation laboratory (DEMATEL), analytic network process (ANP), VlseKriterijumska Optimizacija I Kompromisno Resenje in Serbian (VIKOR) and fuzzy set theory, for the evaluation of ERP systems. First, the fuzzy DEMATEL is used to understand the direct and indirect relationships among the criteria. Second, the fuzzy ANP is adopted to calculate the importance weights of the sub-criteria. Finally, the most appropriate ERP system is obtained by the fuzzy VIKOR. The proposed decision-making framework is applied to a firm in the high-tech industry in selecting the most appropriate ERP system. The results show that the proposed framework can help firms evaluate ERP systems effectively by collecting experts’ opinions in an uncertain environment.",CS,AI_ML,0.85,Extracted from log - paper 204
Enterprise Resource Planning Systems: Digitization of Healthcare Service Quality,"The purpose of this study is to evaluate the perception of healthcare professionals in improving the quality of services in healthcare centers by deploying the platform of Enterprise Resource Planning (ERP). Individual attributes, organizational impression, information, and the system quality of ERP have been used to evaluate the overall influence of integrated planning systems on health care service quality. A mixed methods approach is used to collect and examine data through triangulation. Data for the empirical study was collected from 279 medical professionals of five healthcare organizations operating in the city of Lahore, Pakistan, through a self-administered questionnaire. Descriptive statistics squared multiple correlations and reliability coefficients were used as data analysis tools. Moreover, the goodness of fit test of the structural model was conducted through AMOS 20. All given dimensions of ERP are postulated to have a positive effect on healthcare service quality. The results reveal that the use of an enterprise planning system has a positive impact on individuals, organizational information quality, and system quality in healthcare services. The study further concludes that a well implemented ‘Enterprise Resource Planning System’ results in better system output and enables healthcare professionals to provide better healthcare service quality.",CS,AI_ML,0.85,Extracted from log - paper 205
Effect of Enterprise Resource Planning Systems and Forms of Management Control on Firm’s Competitive Advantage,"In the brick of digitalization industry revolution era, this study signifies the pertinent role of Enterprise Resource Planning Systems (ERPs) towards assisting the organization towards attaining the firm’s mission and goal. This study extends the knowledge by exploring the relationship between ERPs and management control (MC), which in turn enhances firm’s competitive advantage. Realizing the limited empirical work on ERPs from management accounting and control perspective, the discussion would be drawn from business stakeholder’s perspective, instead of from information technology standpoint. The study views ERPs as an important resource in creating the capability to control the business operations and combination of both factors creates the firm’s competitive advantage. Survey questionnaires were administered via email to 972 randomly selected manufacturing firms listed in Federation of Malaysian Manufacturer Directory. Based on the 114 usable responses, the data was analyzed using a structural equation modeling (SEM) approach through partial least square (PLS) software. The findings provide empirical evidence on the significance of ERPs in determining firm’s MC approaches, both technocratic and socio-ideological forms of control. Evidently, these variables do associate positively with competitive advantage. Additionally, the analysis demonstrates that only technocratic form of MC mediates the relationship between ERPs and competitive advantage, but not for socio-ideological control. These findings provide an insight on the relationship among ERPs, form of MC and firm’s competitive advantage, which may be an input for businesses in facing the industrial digitalization era.",CS,AI_ML,0.85,Extracted from log - paper 206
Optimization of Resource Allocation and Task Allocation with Project Management Information Systems in Information Technology Companies,"This study proposes a novel approach to designing an integrated Resource Allocation and Task Allocation Optimization System (RATAOS) using Enterprise Architecture (EA) to improve project management efficiency. The proposed approach integrates Project Management Information System (PMIS) with an optimization system designed using a random forest model and Natural Language Programming (NLP). The integrated system optimizes resource and task allocation, reducing operational costs by 14% and planning phases by 88.7%. Project completion time increased by 50.80%, demonstrating the effectiveness of the integrated system. The purpose of this study is to find a solution for PMIS to be used as an automatic data-driven Resource and Task Allocation Optimization System. The technique used in this study is service integration between the existing PMIS with the Resource and Task Allocation Optimization System.",CS,AI_ML,0.85,Extracted from log - paper 207
Conceptual model of enterprise resource planning and business intelligence systems usage,"Businesses have invested considerable resources in the usage of enterprise resource planning ERP and business intelligence BI systems. These systems were heavily studied in developed countries while there are a few and narrowly focused studies in developing ones. However, studies on the integration of ERP and BI have not been given enough attention hereafter ERPBI. There are many challenges facing the ERPBI usage in term of the steadily increasing speed with which new technologies are evolving. In addition, there are a number of factors that affecting this usage. Based on the finding of the literature, a model from a critical success factors CSFs perspective that examine the relationship between ERPBI usage and organisational performance is proposed. The conceptual model provides a foundation for more research in the future. The expected results of the study will improve the business outcome and help design strategies based on an investigation between ERPBI usage and organisational performance.",CS,AI_ML,0.85,Extracted from log - paper 208
Do Enterprise Resource Planning Systems (ERPs) Constrain Real Earnings Management?,"ABSTRACT The examination of the beneficial impact of enterprise resource planning systems (ERPs) on firm performance appears in extensive literature. Prior studies also examine how ERP implementations impact the timeliness of financial information. Few studies, however, address the question of whether the increase in managers' access to accounting data differentially influences managerial behavior. We investigate the association of ERP implementation with managers' flexibility to deviate from normal operating practices to present better financial results. Our findings suggest that after the implementation of an ERP, earnings management through real activities declines. These results particularly indicate that ERP implementations enhance the quality of financial reporting by constraining opportunistic managerial behavior.",CS,AI_ML,0.85,Extracted from log - paper 209
Information System Assurance for Enterprise Resource Planning Systems: Unique Risk Considerations,"Enterprise Resource Planning (ERP) systems inherently present unique risks due to tightly linked interdependencies of business processes, relational databases, and process reengineering. Knowledge of such risks is important in planning and conducting assurance engagements of the reliability of these complex computer systems. Yet, there is little empirical evidence on this issue. To examine this topic, a semi‐structured interview study was conducted with 30 experienced information systems auditors (from 3 of the Big 5 firms) who specialize in assessing risks for ERP systems. This approach allowed us to obtain detailed information about participants' views and client experiences. The results indicate that the implementation process of ERP systems has an important impact on system reliability. Further, interviewees identified a number of common implementation problems (e.g., improperly trained personnel and inadequate process reengineering efforts) that result in heightened risks. Interviewees also reported ...",CS,AI_ML,0.85,Extracted from log - paper 210
Measuring Enterprise Resource Planning (ERP) Systems Effectiveness in Indonesia,"Refining DeLone and McLean’s (D&M) information system model and technology-organisation-environment (TOE) framework, this research identifies the prominent factors that determine ERP system success. Hypotheses are also drawn based on supporting theories to evaluate the causal relationship between the success determinants. The level of achievement is measured by system quality, information quality, service quality, external quality and top management support, which intermediated by perceived usefulness and user satisfaction towards business benefits. To provide empirical evidence, 86 valid samples out of 156 were collected using a web survey that targeted ERP users in Indonesia. Furthermore, Partial Least Squares–Structural Equation Modelling (PLS-SEM) algorithm were applied to check the proposed hypotheses. The results suggest system quality, information quality and service quality significantly affect user satisfaction, whereas they moderately impact on perceived usefulness. Interestingly, external pressures were reported as being the biggest influence on user satisfaction and positively impacted on perceived usefulness. Despite being fairly predictive to perceived usefulness, top management support along with general perceptual factors ultimately promote system success by elevating business benefits.",CS,AI_ML,0.85,Extracted from log - paper 211
"Enterprise Resource Planning: Past, Present, and Future","ABSTRACT The purpose of this article is to provide a broad overview of the history and development of ERPs and outline recent developments with the advent of digital innovations like cloud computing. The research approach in this article was to review literature from both academic journals and industry reports. The article found that ERPs have changed dramatically from precursor systems like integrated control (IC) packages and material resource planning (MRP) systems. They continue to change with the advent of cloud computing, as well as digital innovations like artificial intelligence. While several articles have addressed the evolution of ERPs, this article’s unique contribution is that it covers the most recent developments, including discussions on cloud ERPs and postmodern ERPs.",CS,AI_ML,0.85,Extracted from log - paper 212
Koha enterprise resource planning system and its potential impact on information management organizations,"Purpose The purpose of this paper is to focus on Koha enterprise resource planning system and its potential impact on information management organizations in Kenya against the risks and myths associated with the solution. Design/methodology/approach The study focused on selected scan and analysis of information management organizations using Koha enterprise resource planning system in addition to document or desk review analysis. Expert opinions and ideas of information professionals, especially information systems and information leaders, also provided vital knowledge. Findings Koha enterprise resource planning system as the premier and leading free and open-source software is transforming and integrating information services in knowledge-based organizations. Driving significant forces include economical benefits, global customer base, free use and distribution, technical and online support, compatibility and integration with other technologies and global access to information and organization of knowledge. The study also established that the risks and myths associated with the system such as compatibility and integration with other technological solutions are no longer possible challenges. Koha provides business intelligence and cloud computing solutions over proprietary or commercial systems for managing and supporting information in organizations. Research limitations/implications This study purposively focused on selected information and knowledge management organizations using the Koha free and open-source enterprise resource planning system in addition to expert opinions and ideas of stakeholders in the information industry. Practical implications Koha enterprise resource planning system provides alternative solutions to information organizations already burdened with limited financial resources. In addition, it proves to the entire world that the solution that was developed for rural and small-based information organizations has developed into a leading knowledge enterprise resource planning solution. In information management organizations, Koha enterprise resource planning system is the strategic asset whose growth and use has increased tremendously across the globe. Social implications Potential widespread application and usage of the Koha enterprise resource system that was initially developed for rural and small information organizations is proof enough that the free and open-source software movement can produce best solutions that are economically viable over proprietary systems. Significant impact of the enterprise system across the globe indicates that information management organizations, information professionals and leaders are satisfied with the system. Originality/value In the knowledge-based economy, where technological systems and solutions are fundamental for quality delivery of services to the customers, information professionals must and as always provide the necessary technology, leadership and management qualities. Across the world, Koha free and open-source enterprise resource planning system is increasingly gaining momentum in information management organizations, and Kenyan information professionals are no exception.",CS,AI_ML,0.85,Extracted from log - paper 213
Acceptance of homegrown enterprise resource planning (ERP) systems in Ethiopia,"In the current competitive global market, organizations are implementing information and communication technology (ICT) that could add value to their products, processes, and satisfaction of their users. The adoption, implementation and use of homegrown enterprise resource planning (ERP) systems is one of these mechanisms being globally used for recording, processing, storing, and exchanging organizational information anytime anywhere. Although organizations have been utilizing ERP systems, the acceptance of homegrown ERP systems is given less attention as compared to commercial off-the shelf (COTS) software. Hence, this research studied factors that determine acceptance of homegrown ERP through the extension of unified theory of acceptance and use of technology (UTAUT) model. The finding revealed that performance expectancy, effort expectancy, social influence, competitive advantage, cost effectiveness, and facilitations functions are determinants of homegrown ERP system acceptance in Ethiopia. Moreover, experience and voluntariness are found to be significant moderators of the study.",CS,AI_ML,0.85,Extracted from log - paper 214
Developing business advantages from the technological possibilities of enterprise information systems,"Organizations are increasingly implementing Enterprise Information Systems (EIS), and Enterprise Resource Planning (ERP) systems in particular. Despite the notable studies on the advantages of an EIS, many organizations are not satisfied with the benefits or advantages gained. At the same time, it is assumed that such systems with increasing innovations and technological enhancements would generate abundant business advantages, if organizations exploited these opportunities. The investigation in this work drew on the sociomateriality perspective, using imbrication notion, and focused on a telecomm case study to examine how organizations can exploit the technological possibilities of an EIS to create business benefits. The study findings suggest that business benefits can be achieved when the EIS as a technical system is interwoven with the organizational work in which both dynamically change in practice (not from the technical features of the system), when the system provides interesting and beneficial technological possibilities that attract organizations, and when the firm has the organizational capabilities to translate these possibilities into real business benefits.",CS,AI_ML,0.85,Extracted from log - paper 215
The Evolution of Enterprise Resource Planning Systems,"Management of organizations needs efficient information systems to improve competitiveness by cost reduction and better logistics. It is universally recognized by large and small to medium-size enterprises (SME) that the capability of providing the right information at the right time brings tremendous rewards to organizations in a global competitive world of complex business practices. ERP (Enterprise Resource Planning) can be defined as a framework for organizing, defining and standardizing the business processes necessary to effectively plan and control an organization so the organization can use its internal knowledge to seek external advantage. This paper presents the growth and success of ERP adoption and development through history. The evolution of ERP systems closely followed the spectacular developments in the field of computer hardware and software systems. There is still a never-ending process on the ERP market, of reengineering and development, bringing new products and solutions. The consolidations continue to occur and the key players continue to build out their products. The next phase of ERP systems will be the merged products.",CS,AI_ML,0.85,Extracted from log - paper 216
"Digital Government Information Platform Construction: Technology, Challenges and Prospects","The construction of digital government information platforms plays a crucial role in modern governance. This paper provides an in-depth exploration of the technology, challenges, and prospects associated with these platforms. Firstly, it analyzes the technological aspects, emphasizing the utilization of advanced technologies such as big data, cloud computing, and artificial intelligence to build efficient and integrated government information systems. The integration of these technologies can significantly enhance the accessibility and usability of public services, contributing to the overall effectiveness of governmental operations. Secondly, it delves into the challenges that accompany the development and implementation of digital government information platforms, including data security, privacy protection, and addressing the digital divide among citizens. Overcoming these challenges is essential to ensure the successful deployment and widespread adoption of digital government platforms. Lastly, the paper discusses the prospects, highlighting the potential for improved public services, enhanced transparency, and more effective decision-making through the utilization of digital government information platforms. By examining the opportunities offered by these platforms, such as increased citizen engagement and streamlined administrative processes, this research contributes to an understanding of their comprehensive impact on governance and public administration.",CS,AI_ML,0.85,Extracted from log - paper 217
Advancements and forecasts of digital taxation information systems usage and its impact on tax compliance: does trust and awareness make difference?,"Purpose The main purpose of the current study was to develop a new research model in the hope of providing a further understanding of Digital Taxation Information Systems (DTIS) usage and its impact on tax compliance by investigating the mediating role of trust in e-government services (TIE) and the moderating role of awareness (AW) toward these systems. Design/methodology/approach A quantitative research method approach with Partial Least Squares-Structural Equation Modelling (PLS-SEM) was employed to analyze the data collected. Findings The results indicated that DTIS usage is influenced by perceived usefulness (PU), perceived ease of use (PEU), attitude (ATT), knowledge (KN), subjective norm (SN), AW and TIE. Contrary to what is expected, AW does not moderate the association between SN and DTIS usage. Eventually, the results also revealed that TIE has mediated the association between trust in government (TIG) and DTIS usage. Originality/value This study provides thought-provoking empirical pieces of evidence about understanding the situation of DTIS usage and its impact on tax compliance among academic professors in Jordan. Furthermore, the study outcomes and discussion presented will help the Jordanian government improve and comprehensively formulate strategies to increase the tax compliance procedure.",CS,AI_ML,0.85,Extracted from log - paper 218
Determinants of user satisfaction with financial information systems in the digital transformation era: insights from emerging markets,"Purpose Most of the previous studies agree about the significance of user satisfaction in ensuring the endurance of information systems (ISs). Accordingly, it is crucial to investigate the effect of e-Government systems on individual end-user satisfaction as more and more countries adopt and deploy such Government Financial Management Information Systems (GFMIS) in the era of digital transformation. Because of this, the purpose of this study is to investigate the factors that contribute to the success of GFMIS in Jordan and ultimately the satisfaction of its users. Design/methodology/approach The IS success model developed by DeLone and McLean (2003) serves as the theoretical underpinning for the current research. Adding training quality as a new variable to the proposed model has been found to further increase the satisfaction of GFMIS users. A total of 104 GFMIS users in Jordan provided the data used to verify the model. The partial least squares-structural equation modelling was used to test the hypotheses. Findings The empirical findings indicated that GFMIS user satisfaction is significantly affected by information quality, service quality and perceived usefulness; meanwhile, system quality is only partially supported. The research also showed that the level of satisfaction among Jordanian GFMIS users was related to the quality of training they received. Originality/value This study fills a crucial literature gap by developing a research model that can help improve GFMIS usage towards attaining greater performance amongst government agencies in Jordan.",CS,AI_ML,0.85,Extracted from log - paper 219
Legacy Systems Modernisation for Citizen-Centric Digital Government: A Conceptual Model,"Information technology and communication (ICT) plays an important role as a catalyst for organisational development and innovation. However, old information systems that are known as legacy systems often expose organisations to the risk of business failure. These systems are not only impeding the advancement in technology strategy but also hindering the organisations’ business competitiveness. Nevertheless, legacy systems are essential in supporting critical functions in organisations including the public sector and could not be scrapped easily. These systems need to be given a new strength through modernisation to continue providing the best service in line with global trends. Modernisation is a complex task that involves several related aspects. In the context of the public sector, legacy systems involve a complicated information relationship, environment, and culture, while ensuring the citizens are of high priority. The implementation of a digital government represents the transformation of the public service delivery to the citizens that emphasises a citizen-centric design. This study, therefore, aims to address this concern by reviewing the factors involved and suggesting a guideline in the form of a conceptual model to assist in the modernisation of legacy systems for a citizen-centric digital government. Data from the theoretical study were analysed using content analysis. The results show that the legacy systems’ modernisation comprised four main aspects, namely human, process, product, and organisation aspects, with related factors and elements. This model contributes as a reference for the public sector and provides overall guidance in performing legacy systems modernisation.",CS,AI_ML,0.85,Extracted from log - paper 220
"Power, politics, and the institutionalisation of information systems for promoting digital transformation in the public sector: A case of the South African’s government digital transformation journey","The institutionalisation of new technologies, information systems, norms, practices and other innovations for improving governance, planning, operational efficiency and service delivery in the public sector remain a challenge. Power dynamics, and politics have also been recognised as playing a critical role in the institutionalisation of information systems for promoting digital transformation of the public sector. This study used data collected through an extensive review of literature and empirical data from a case study of South Africa’s government digital transformation journey. The study explored power dynamics and the role of politics in the institutionalisation of reforms and deinstitutionalisation of institutionalised practices that constrain transformation in institutions. Power dynamics, and politics in institutions were found to have a significant bearing on the institutionalisation of reforms that include information systems in the digital transformation of government. The study found that the digital transformation of the public is more than the implementation technology and requires the holistic view of institutions as social, economic, and political structures.",CS,AI_ML,0.85,Extracted from log - paper 221
Digital Transformation of Local Government: Design and Development of the Pakuhaji District Community Service Information System Website,"In the era of digital transformation, local governments are increasingly pursuing innovation to improve the quality of public services. This research aims to carry out digital transformation in the government of Pakuhaji District, Tangerang Regency, by designing and building an efficient and responsive public service information system website. Through a systems development approach, this research identifies community needs, evaluates existing service processes, and designs information technology-based solutions. The designed information system covers various aspects of community services, such as submitting correspondence, public service information, and managing population data. The use of web technology enables better accessibility for the public, increases administrative efficiency, and speeds up the handling of service requests. The application of responsive design principles and intuitive interfaces is also a focus in website development to ensure optimal user experience. The research methodology includes a community needs survey, analysis of existing service processes, and development of a website prototype. It is hoped that the results of this research can make a positive contribution to the effectiveness and efficiency of community services in Pakuhaji District, creating a local government that is more responsive and connected to the needs of its citizens. It is hoped that this digital transformation can become the basis for further development in realizing a technology-oriented government for the welfare of society.",CS,AI_ML,0.85,Extracted from log - paper 222
C-suite Leadership of Digital Government,"Despite decades of digitalization in day-to-day government operations and in the governance of the public sector, there is a major research gap in understanding the nature of digital government leadership (DGL) and the diversity in how top managers are leading the digital transition and transformation of government. Based on a structured literature review and in-depth inductive analysis of previous research within the domains of e-government, information systems, and public administration research, this article explores how the C-suite level of government is leading the digitalization. In the article, we propose a definition of DGL and a leadership framework to capture the nexus and direction of leadership. Also, the article proposes distinct leadership roles and actions to forward digital government.",CS,AI_ML,0.85,Extracted from log - paper 223
Digital Government in Social Sciences Discipline: Mapping Pivotal Features and Proposed Theoretical Model,"The research aimed to describe the trend issues, identify the key features, and propose a theoretical model of digital government. A comprehensive search was used to find eligible articles in the Academic Scopus Database. Further, the quality of the study was assessed during the screening phase, where it met 115 journal-related articles on digital government within the social sciences discipline. Further, this literature was analyzed by NVivo 12 Plus via a hierarchy diagram, cluster analysis, word frequency analysis, and the VOSViwer tool to visualize the data via a network, overlay, and density analysis. The findings revealed a term network formed by digital government and trend issues, resulting in several growing concerns, such as e-government, open government, and technology adoption. Furthermore, key features were reported following proportional analysis, such as systems, development, services, models, information, public, policy, management, and networks. Another point is that a proposed theoretical model has been constructed and selected for future research.",CS,AI_ML,0.85,Extracted from log - paper 224
Factors influencing village information systems adoption in Indonesia: A qualitative study,"Like other developing countries, the Indonesian government is pursuing digital transformation to achieve good governance at the central and micro levels. One of the strategies for achieving digital government transformation at the micro level is implementing village information systems (VIS), information systems that village officials manage. Unfortunately, not all villages in Indonesia are thriving in adopting VIS. Therefore, this study aims to answer an overarching puzzle: Why did some village governments successfully adopt VIS while others failed? Using a case‐study approach to VIS adoption in Gunungkidul Regency, Indonesia, this study fills the gap in the literature from the technological‐organizational–environmental perspective that affects e‐government adoption at the village government level. We found four main factors that influence the success or failure of village information systems adoption: (1) VIS interoperability (technology context), (2) the workload of village officers (organizational context), (3) the role of civil society, and (4) the role of a vendor (environmental context). This research enriches the literature by identifying these four factors within the TOE framework, still rarely present in e‐government adoption studies, especially in the context of village governments in developing countries. This research has practical implications for the successful adoption of VIS as a village government effort to gain data sovereignty.",CS,AI_ML,0.85,Extracted from log - paper 225
Privacy concerns and digital government: exploring citizen willingness to adopt the COVIDSafe app,"ABSTRACT Contact tracing is a key public health intervention during the coronavirus pandemic. While government contact tracing apps (e.g., COVIDSafe) may enforce personal information protection, privacy concerns remain among citizens. To date, few studies have investigated the adoption of contact tracing technology and corresponding citizen information privacy concerns. To address this gap, we propose a research model to explore the impact of individual privacy concerns, trust, and risk perceptions on citizen’s willingness to download a federal contact tracing app. To test the model, we administer a survey to Australian citizens to assess their perceptions of the government’s “COVIDSafe” app. The results of this study indicate that relative advantage, compatibility and trusting beliefs increase adoption intentions. The study provides recommendations for governments tackling COVID-19 and guidance for contact tracing strategies in preparation for future pandemics.",CS,AI_ML,0.85,Extracted from log - paper 226
Digital Government and the Circular Economy: Towards an Analytical Framework,"Circular economy is high on the political agenda, with governments at all levels setting ambitious goals to move away from traditional linear production models, where goods are used and disposed as waste, towards a future with less use of virgin raw materials, and where valuable materials at a product end-of-life are returned as raw materials or in an environmentally-friendly way to the biosphere. While circular economy is gaining a lot of attention on a policy level, the role that digital government can play to facilitate the circular economy transition is largely unexplored. We carry out a review of existing literature in the fields of digital government and Information Systems (IS) to identify the roles played by digital government in the circular economy. Based on an analysis of 54 empirical research articles, we identify foci and gaps in relation to the different types of roles played by government (nodality, authority, treasure, and organization), to stages of the Product Life Cycle (pre-use, in-use, and post-use), and to types of digital technology focused on. Based on these findings, we present an analytical framework to guide future research on digital government in relation to the circular economy, and exemplify the use of the framework drawing on examples from circular economy initiatives in the automotive industry.",CS,AI_ML,0.85,Extracted from log - paper 227
Access to Government Information and Inclusive Stewardship of North America’s Archaeological Heritage,": The Digital Index of North American Archaeology (DINAA) gazetteer works to enrich understanding of the human presence on the landscape of North America since the late Pleistocene by connecting hundreds of thousands of archaeological and historical sites to related tribal and other government bodies, museum, library, archive, and scientific datasets, as well as repositories of scientific literature. This chapter explores how open data, if applied appropriately in partnership with tribal authorities and experts, can help serve the interests of Indigenous peoples. Currently, Native American tribes face daunting obstacles in obtaining data documenting ancestral territories. Relevant data are often siloed within opaque and under-resourced government systems. DINAA makes key descriptive information about North America’s rich cultural heritage available for inspection, evaluation, and use by descendant communities, historically marginalized from administrative and political processes. This “open government” focus helps make cultural heritage management more accountable to wider constituencies. Making these data linked and accessible can be part of larger efforts to enable sovereign tribal nations to effectively manage and protect their ancestral cultural heritage.",CS,AI_ML,0.85,Extracted from log - paper 228
"Open government, civic tech and digital platforms in Latin America: A governance study of Montevideo's urban app ‘Por Mi Barrio’","Digital technologies have a recognised potential to build more efficient, credible, and innovative public institutions in Latin America. Despite progress, digital transformation in Latin American governments remains limited. In this work, we explore a peculiar yet largely understudied opportunity in the region: pursuing digital government transformation as a collaborative process between the government and civil society organisations. To do so, we draw from information systems research on digital government and platforms for development, complemented with governance theory from political science and conduct an interpretive in‐depth case study of an urban reporting platform in Montevideo called ‘Por Mi Barrio’. The study reveals three mutually reinforced orders of governance in the trajectory of the project and explain how the collaboration unfolded over time: (i) a technical decision to use open platform architectures; (ii) the negotiation of formal and informal rules to make the project thrive and (iii) a shared, long‐term ideology around the value of open technologies and technical sovereignty grounded in years of political history. Using a contextual explanation approach, our study helps to improve our understanding on the governance of collaborative digital government platforms in Latin America, with specific contributions to practice.",CS,AI_ML,0.85,Extracted from log - paper 229
Trustworthiness of digital government services: deriving a comprehensive theory through interpretive structural modelling,"ABSTRACT Having its origin in public administration, trustworthiness is a significant concept in digital government research, influencing the relationships between citizens and governments. However, the interrelationships between the facets of trustworthiness are given inadequate attention. Therefore, the aim of this research was to develop a theory detailing the factors affecting citizens’ perceptions of e-government trustworthiness. A comprehensive review of public administration and information systems literature highlighted 20 pertinent variables. The interrelationships of these variables were identified and categorized according to their driving and dependence power by employing interpretive structural modelling. The proposed model was then drawn based on the level partitioning of variables and interrelationships of the variables determined using the final reachability matrix. The findings reveal that current conceptualizations of digital government trustworthiness take a too narrow view. The findings can help government policy makers with understanding the interrelated factors associated with trustworthiness in the context of digital government services and implement them in effective strategic planning.",CS,AI_ML,0.85,Extracted from log - paper 230
Digital government and geographic information systems,"The focus of this chapter is to examine how government agencies are deploying geographic information systems (GIS) to enhance the delivery of digital government. We will explain how critical technological advances are enabling government agencies to use GIS in web-based applications In addition, we will illustrate the approaches that state and local governments in the United States are taking to deploy GIS for e-government applications using examples from Indianapolis, Indiana, Tucson, Arizona, Washington D. C. and the State of Oregon's Department of Environmental Protection. While these examples greatly improve service delivery performance and enhance public decision-making, we raise the issue that e-government GIS applications may be more broadly deployed in organizations that are better adept at dealing with the managerial and technical issues related to using GIS.",CS,AI_ML,0.85,Extracted from log - paper 231
Realizing the Promise: Government Information Systems and the Fourth Generation of Information Technology,"Interoperability is more than “digital plumbing”—making sure that computers talk so that bits of data flow properly. Fundamentally, interoperability is people talking and sharing information. Sharing information reduces the “paperwork burden”on the citizen, streamlines work processes, and enriches the formulation, implementation, and evaluation of policy. Building on prior theory and research, this research has developed empirically derived, practical findings and recommendations to support thedevelopment of appropriate interoperable systems.",CS,AI_ML,0.85,Extracted from log - paper 232
Measuring optimization of digital military programs: an innovation of information and communication systems in industrial digitalization 4.0,"This research examines military digital optimization as an information and communication system innovation in the industrial digitalization era 4.0. Advances in information and communication technology especially on the industrial revolution 4.0. The real impact is seen in several aspects of human- life. The industrial revolution 4.0 also provided a change in the government system into good governance. Technological advances also have an impact on the defense system or military system in Indonesia, especially for the Indonesian National Army, especially the (TNI AD) through the use of an e-military application system to facilitate the search for internal information related to TNI personnel. The research method is descriptive qualitative with a sampling technique that is purposive sampling. Kodam IX / Udayana, Denpasar, Bali as a location of this study. Data collection uses a structured interview method to a number of informants who have been willing to engage a number of 13 people.",CS,AI_ML,0.85,Extracted from log - paper 233
A proposed conceptual success model of citizen-centric digital government in Malaysia,"The emergence of Digital Government throughout the world is reflecting how governments are trying to find innovative digital solutions towards empowering social, economic and political advantage. Effective service delivery to citizens through Information Communication Technology application such as integrated citizen service information systems is a prerequisite to achieve citizen-centric digital government. Measuring success of such systems is a growing concern. However, very few studies have attempted to find success factors using Information Systems theoretical approach in the context of digital government, particularly in Malaysia. Therefore, this study is designed to bridge the gap by identifying such factors and propose a conceptual model. This study addresses success factors from system and personal traits’ perspectives, behavioral intention, satisfaction, trust and citizen empowerment as determinants of digital government success. Keywords: Digital government; e-government; trust; digital services; information systems",CS,AI_ML,0.85,Extracted from log - paper 234
Technological Revolution in the Field: Green Development of Chinese Agriculture Driven by Digital Information Technology (DIT),"According to the Plan for Rural Development of Digital Agriculture (2019–2025) ,accelerated integration of digital technologies and agriculture is crucial to promoting high-quality agriculture in China. The application of DIT in agricultural activities will not only help improve the efficiency of agricultural production, but also promote the green development of agriculture and the achievement of the Dual Carbon Target (DCT). In order to further clarify the comprehensive effects of the application of DIT in agricultural systems and provide routes for government decision-makers to assist in reducing agricultural emissions by DIT, this paper adopts the logical deductive method and starts with the application status to draw out the specific paths of low-carbon transformation in DIT-driven agriculture, while further discussing the potential issues in the process and corresponding solutions. DIT is a double-edged sword. It can promote the green and low-carbon transformation of agriculture by implementing precision operation, environmental monitoring, optimizing carbon emission accounting, and supervising the carbon market. However, at the same time, it may face problems such as unbalanced rural development and excessive financialization of the carbon market. Therefore, we should be optimistic but cautious about the application of DIT in reducing agricultural emissions. We can address potential problems by strengthening government-led investment, broadening channels for capital investment, strengthening skills training for farmers, and enhancing the regulation of trading in carbon sink markets.",CS,AI_ML,0.85,Extracted from log - paper 235
"Interoperability, Trust Based Information Sharing Protocol and Security: Digital Government Key Issues","Improved interoperability between public and private organizations is of key significance to make digital government newest triumphant. Digital Government interoperability, information sharing protocol and security are measured the key issue for achieving a refined stage of digital government. Flawless interoperability is essential to share the information between diverse and merely dispersed organisations in several network environments by using computer based tools. Digital government must ensure security for its information systems, including computers and networks for providing better service to the citizens. Governments around the world are increasingly revolving to information sharing and integration for solving problems in programs and policy areas. Evils of global worry such as syndrome discovery and manage, terror campaign, immigration and border control, prohibited drug trafficking, and more demand information sharing, harmonization and cooperation amid government agencies within a country and across national borders. A number of daunting challenges survive to the progress of an efficient information sharing protocol. A secure and trusted information-sharing protocol is required to enable users to interact and share information easily and perfectly across many diverse networks and databases globally. This article presents (1) literature review of digital government security and interoperability and, (2) key research issue trust based information sharing protocol for seamless interoperability among diverse government organizations or agencies around the world. While trust-based information access is well studied in the literature, presented secure information sharing technologies and protocols cannot offer enough incentives for government agencies to share information amid them without harming their own national interest. To overcome the drawbacks of the exiting technology, an innovative and proficient trust-based security protocol is proposed in this article for sharing of top secret information amid government intelligence agencies globally. The trust protocol intended assures the enhanced interoperability of any modern digital government by sharing secure and updated information among government intelligence agencies to avoid any threatening deeds.",CS,AI_ML,0.85,Extracted from log - paper 236
An attempt to understand complexity in a government digital transformation project,"Digital transformation projects will become one of the dominating tools for mastering digital transformation in governments. Studies show that such projects are complex undertakings and increasingly difficult to manage. The purpose of the paper is to provide a better understanding of the factors that cause complexity in government digital transformation projects. The authors use an in-depth case study approach to investigate factors of complexity in an ongoing digital transformation project. The results indicate that complexity in this project is rooted in dynamic relationships between multiple dimensions of organization, technologies, and innovation. The authors conclude that when organizational structuring, the introduction of new technology, and efforts to innovate and create added value for citizens and businesses operate in tandem, the pervasive complexity associated with delivering government digital transformation projects becomes increasingly difficult to manage.",CS,AI_ML,0.85,Extracted from log - paper 237
Ethical Issues and Citizen Rights in the Era of Digital Government Surveillance,"Government information systems are big business (costing over 1 per cent of GDP a year). They are critical to all aspects of public policy and governmental operations. Governments spend billions on them for instance, the UK alone commits £14 billion a year to public sector IT operations. Yet governments do not generally develop or run their own systems, instead relying on private sector computer services providers to run large, long-run contracts to provide IT. Some of the biggest companies in the world (IBM, EDS, Lockheed Martin, etc) have made this a core market. The book shows how governments in some countries (the USA, Canada and Netherlands) have maintained much more effective policies than others (in the UK, Japan and Australia). It shows how public managers need to retain and develop their own IT expertise and to carefully maintain well-contested markets if they are to deliver value for money in their dealings with the very powerful global IT industry. This book describes how a critical aspect of the modern state is managed, or in some cases mismanaged. It will be vital reading for public managers, IT professionals, and business executives alike, as well as for students of modern government, business, and information studies.... Download ebook, read file pdf IT Corporations, the State, and e-Government",CS,AI_ML,0.85,Extracted from log - paper 238
How does digital payment transform society as a cashless society? An empirical study in the developing economy,"Purpose After analyzing these uncountable benefits of digital or cashless payment, many European countries like Sweden, Finland and Canada has been trying to convert their payment system into cashless. Following these developed countries, the Bangladesh Government has taken a decision to transfer society as a cashless society by using information technologies for adopting the fourth industrial revolution over the world. Digital payment system is among the various options available for transforming a cashless society. First, this empirical study presents demographic information and digital payment characteristics on the basis of income levels. This study identifies influential factors of adopting digital payment systems. Finally, this study aims to justify how digital payments transform the Bangladeshi economy into a cashless society in developing countries. Design/methodology/approach The study was administered to a sample of 1,000 Bangladeshi customers who had engaged in online banking transactions for the purpose of acquiring items and services through both social media platforms in Google Form format and face-to-face interactions in hard copy format. Among these, 647 questions were deemed usable and were used for data analysis, where the response rate was 68%. The SmartPLS is used to create and validate the structural equation modeling model presented for the research, as well as to evaluate the hypothesized correlations between the different constructs. Findings This cross-sectional study conducted the extended technology acceptance model (TAM) with perceived security (PS) and personal innovation (PI) variables to identify the influencing adoption factors of digital payment systems. This study finds that perceived ease of use, PI and perceived usefulness have a favorable impact on individuals’ attitudes toward adopting digital payment methods (DPMs). The study also indicated that PS did not influence negatively the adoption of digital payment system. Besides this, the adoption of digital payment will help to transform society into a cashless society in the future. Research limitations/implications Increasingly prevalent across the nation. Several variables are required to facilitate the transition toward a cashless society. This study exclusively focuses on DPMs. Additionally, the data has been obtained exclusively from a single urban area. The adoption of DPMs has become increasingly prevalent across the nation. Practical implications This study would help policymakers, marketers and bankers understand which factors affect digital payment infrastructure expansion. So, they can produce digital payment apps that are compatible with different devices, have fast transactions, are user-friendly, easy to use and highly secure to maintain good attitudes toward digital payment systems. Social implications Few studies have examined how DPMs affect cashless societies in developing countries like Bangladesh. According to researchers, to the best of the authors’ knowledge, this is the first study to explore how digital payments affect cashless society in Bangladesh and raise awareness about it. Originality/value The study extended the TAM model to PS and PI. This paper is also unique in the conceptual arguments and the subject theme of the research area.",CS,AI_ML,0.85,Extracted from log - paper 239
E-government and digital transformation in Libyan local authorities,"This article reports on e-government in local authorities in Libya, and discusses the issues involved in digital transformation. The study builds upon existing models and frameworks to establish a technology-organisation-process (TOP) maturity model for assessing e-government status in three case studies in Libya, which reveal major problems in adopting e-government in Libya. The current technology deployment remains basic, with inadequate information systems and networks, out of date personal computers and office software, and unreliable access to the internet. Organisational capabilities, skill levels, lack of funding, management support and process inefficiencies are other factors hampering progress in the adoption of e-government. A step-change to digital government that employs emergent technologies such as artificial intelligence, big data, analytics and cloud computing is currently out of reach. The TOP maturity model provides a framework for assessing e-government readiness in a developing world environment and gives a multi-dimensional perspective on local authority capabilities.",CS,AI_ML,0.85,Extracted from log - paper 240
A strategic framework for digital preservation in the context of e-government in Botswana public service,"The purpose of the study was to assess the digital preservation capability maturity readiness in the context of e-government in Botswana Public Service with a view to developing a strategic framework that ensures digital continuity. The study adopted a pragmatic paradigm and case study which were deployed in each of the six selected ministries as a unit of analysis. The target population was 102 respondents from six key purposively sampled ministries. Seventy-nine questionnaires were distributed, of which 55 were completed and returned. Interviews were conducted with 21 staff being senior managers, managers for human resources and administration, heads of divisions for records management units, archives unit, ICT managers and senior records managers. Formal participatory observations of documents were conducted. The study’s findings showed that the Botswana public service has no unified national information systems to manage public sector records, which led to some ministries adopting their own electronic records management systems. Currently, few ministries have implemented the electronic records management system and most digital records are not preserved due to lack of preservation guidelines and strategies. In that regard, the study developed a strategic framework to safeguard digital continuity and make sure that e-government is sustained for the benefit of an open government and increased participatory citizenry.",CS,AI_ML,0.85,Extracted from log - paper 241
The Effect of Auditor Competence and Remote Audit Support on Audit Quality through Digital-Based Governance with Information Technology as Moderating Variable in State Financial Audit,"This paper aims to explore and conceptualize the effect of auditor competence and remote audit support on audit quality through digital-based governance with information technology as a moderating variable in state financial audits. Information Technology with digital-based governance can increase the effectiveness of the audit process through digital integration and transformation that replaces paper-based systems in auditing by increasingly adopting sophisticated and high-tech audit support systems to increase the effectiveness and efficiency of audit procedures using remote audits and high auditor competence support. Thus, to keep going guard audit quality, support remote audit with IT-Based use as base development internal audit integration sector government needed in making its easy supervision. Utilization of technology information and build system warning early (early warning system), switch from approach conventional going to approach based on technology information. The proposed method is a quantitative method with a population of auditors from the State Audit Board (BPK). It is hoped that this conceptual paper will provide insight and information for future empirical studies.",CS,AI_ML,0.85,Extracted from log - paper 242
Information Control and Public Support for Social Credit Systems in China,"Critics see China’s social credit system (SCS) as a tool of surveillance and repression. Yet opinion surveys in China find considerable public support for the SCS. We explain this puzzle by focusing on citizens’ lack of knowledge regarding the repressive nature of digital surveillance in dictatorships, which can be attributed to (1) invisible and targeted repression associated with digital surveillance and (2) government propaganda and censorship further concealing its repressive potential. A field survey experiment on 750 college students in three Chinese regions shows that revealing the SCS’s repressive potential significantly reduces support for the system, but emphasizing its social-order-maintenance function does not increase support. Observational evidence from the field survey and a nationwide survey of 2,028 Chinese netizens show that the support is higher if citizens knew about the SCS through state media. Our findings highlight the role of information and framing in shaping public opinion on digital surveillance.",CS,AI_ML,0.85,Extracted from log - paper 243
e-Government Systems Success and User Acceptance in Developing Countries: The Role of Perceived Support Quality,─Abstract ─ This paper proposes a conceptual model to explain user acceptance of eGovernment systems considering the diverse layers of user groups. Due to digital division developing countries are providing e-Government services to heterogeneous user groups including non-educated and less skilful citizens for using computer based systems. Therefore this paper considers support quality of eGovernment systems is one of critical success factors and integrates the factor in a widely adopted user acceptance and success model of information systems. The unified theory of acceptance and use of technology is integrated with information systems success model to explain how the quality of e-Government systems is linked to the acceptance of the systems by citizens. Support quality is added as additional dimension of information systems success and relevant hypotheses are developed under e-Government contexts. Finally a description about data collection and future works are provided.,CS,AI_ML,0.85,Extracted from log - paper 244
Meningkatkan Transparansi dan Akuntabilitas Pemerintah Melalui Teknologi Digital dan Partisipasi Publik dalam Upaya Pemberantasan Korupsi,"Corruption is one of the biggest challenges faced by many countries, including Indonesia. Corrupt practices not only harm the state’s finances but also erode public trust in the government. This study aims to explore how digital technology and public participation can be effectively used to enhance government transparency and accountability in efforts to combat corruption. This research employs a descriptive qualitative method with a literature review and document analysis approach. Data is collected from various relevant literature sources and official documents to analyze the use of digital technology and public participation in enhancing government transparency and accountability in anti-corruption efforts. The results show that digital technologies, such as e-government, e-procurement, blockchain, big data analytics, open data, whistleblowing systems, and digital payment systems, can facilitate broader and faster access to information and increase public participation in government oversight. Public participation, through active involvement in reporting and monitoring, can raise public awareness of corruption issues and strengthen anti-corruption efforts. This study concludes that the combination of digital technology and public participation has great potential to create a more transparent and accountable government. Recommendations include the development of better technological infrastructure, increased digital literacy among the public, and the promotion of public participation in oversight processes. By implementing these strategies, it is hoped that a cleaner and more trustworthy government can be achieved, ultimately improving public welfare.",CS,AI_ML,0.85,Extracted from log - paper 245
Digital transformation in the Indian government,"I M A G E B Y K O N S T A N T I N F A R A K T I N O V tion of the Indian government. NIC is the driving force of the Digital India program and has also helped the government be in the forefront in the use of information technology. It has been working with the government for over four decades, providing statement mail, GIS infrastructure, the public finance management system, and digital payments are key pieces which help provide a foundation for government departments to build IT systems that deliver services to citizens.",CS,AI_ML,0.85,Extracted from log - paper 246
Information Technology/Systems Adoption in the Public Sector: Evidence From the Illinois Department of Transportation,"State government has been moving from manual and paper-based processes to digital services. However, digital divide, declining trust in technology, and low IT/IS adoption rates by public sector employees are important challenges for successful delivery of e-government services to citizens. Previous studies in the area of IT/IS adoption and e-government have mainly focused on citizens. This paper examines IT/IS adoption by employees rather than citizens and the focus is on non-market environment and state government agencies. A research model has been proposed based on the theory of planned behavior (TPB) and technology acceptance model (TAM) which has been extended to include digital divide related constructs and trust in technology. To test the proposed model, a survey was conducted among early adopters of Office 365 at Illinois Department of Transportation (IDOT) in Springfield and Chicago. The paper contributes to research on IT/IS adoption in public sector. The findings also provide insightful design and practical implications for successful IT/IS deployment in public sector.",CS,AI_ML,0.85,Extracted from log - paper 247
Open government data: A systematic literature review of empirical research,"Open government data (OGD) holds great potential for firms and the digital economy as a whole and has attracted increasing interest in research and practice in recent years. Governments and organizations worldwide are struggling in exploiting the full potential of OGD and require a comprehensive understanding of this phenomenon. Although scientific debates in OGD research are intense and heterogeneous, the field lacks theoretical integration of OGD topics and their systematic consideration in the context of the digital economy. In addition, OGD has been widely neglected by information systems (IS) research, which promises great potential for advancing our knowledge of the OGD concept and its role in the digital economy. To fill in this gap, this study conducts a systematic literature review of 169 empirical OGD studies. In doing so, we develop a theoretical review framework of Antecedents, Decisions, Outcomes (ADO) to unify and grasp the accumulating isolated evidence on OGD in context of the digital economy and provide a theory-informed research agenda to tap the potential of IS research for OGD. Our findings reveal six related key topic clusters of OGD research and substantial gaps, opening up prospective research avenues and particularly outlining how IS research can inform and advance OGD research.",CS,AI_ML,0.85,Extracted from log - paper 248
Digital Community Management Mobile Information System Based on Edge Computing,"Edge computing refers to an open platform that integrates network, computing, storage, and application core capabilities on the side close to the source of things or data and provides nearest-end services nearby. “Digital Community” refers to the use of various information technologies and methods to integrate community resources and build a network platform for interactive exchanges and services between the government, property service agencies, residents, and various intermediary organizations within the community. In order to reduce the management tasks of community managers, reduce their community management burden, and improve the management efficiency of community managers, this article is based on the description of several keywords such as distributed edge computing, edge computing business usage scenarios, and digital community management. Edge computing technologies such as centralized traffic model and attribute-based trust evaluation avoid waste of network resources, reduce time delay and system energy consumption, and improve system operation efficiency. A digital community management mobile information management system is designed and developed. 102 owners of the community conducted visits and questionnaire surveys. After investigation and research, it was found that this system has good performance, meets the management needs of most communities, and can improve the management efficiency of community managers by 4%–6%; it has also improved community residents’ satisfaction with digital community management by 2-3%.",CS,AI_ML,0.85,Extracted from log - paper 249
Building a Secure Platform for Digital Governance Interoperability and Data Exchange Using Blockchain and Deep Learning-Based Frameworks,"A secured platform is a critical component of digital governance, as it helps to ensure the privacy, security, and reliability of the electronic platforms and systems used to manage and deliver public services. Interoperability and data exchange are essential for digital governance, as they enable different government agencies and departments to share data, information, and resources seamlessly, regardless of the platforms and technologies they use. In this paper, we build a secure platform to enhance the trustworthiness of digital governance interoperability and data exchange using blockchain and deep learning-based frameworks. Initially, an optimal blockchain leveraging approach is designed using the bonobo optimization algorithm to authenticate data generated from smart city environments. Furthermore, we introduce the integration of a lightweight Feistel structure with optimal operations to enhance privacy preservation. This integration provides two levels of security and ensures interoperability and double-secured data exchange in digital governance systems. In addition, we utilize a deep reinforcement learning (DRL) model to detect and prevent intrusions such as fraud/corruption in the smart city data. This approach enhances transparency and accountability in accessing the data and shows its predominance over other cutting-edge techniques on two benchmark datasets, BoT-IoT and ToN-IoT. Furthermore, the effectiveness of the framework in real-time scenarios has been demonstrated through two case studies. Overall, our proposed framework provides a trustworthy platform for digital governance, interoperability, and data exchange, addressing the challenges of privacy, security, and reliability in managing and delivering public services.",CS,AI_ML,0.85,Extracted from log - paper 250
Public Sector Digital Transformation: Challenges for Information Technology Leaders,"The digital transformation journey in the public sector has become a common agenda for elected leaders, public administrators as well as academics and researchers in the past few years. However, evidence suggests that the efforts to achieve the anticipated benefits from digital transformation proved challenging. Prior studies suggest that several issues related to the introduction of new information systems have unfavourably affected digital public service delivery processes. This paper presents the result of a single case study conducted at one of the most digitalised Ministries of the Ethiopian Federal Government. Using interviews and publicly available documents, we identified a list of factors that could determine the success of digital transformation in public organisations. The findings indicate that the Ministry is struggling from a lack of clearly articulated and shared IT strategic vision and conducive organisational structure fostering digital transformation. Besides, the dysfunctional communications between the IT and remaining departments, lack of information security awareness and measures to mitigate information security risks, the incomplete utilisation of IT solutions due to low skill sets or non-existing culture encouraging digital literacy have all contributed to the bumpy digital transformation journey. The result of our study contributes to research and practice by pointing out various areas of concern that need to be monitored as digital services are continuously rolled out.",CS,AI_ML,0.85,Extracted from log - paper 251
Mastering Electronic Government in the Digital Age,"The ultimate goal of electronic government (e-government) is to offer the increased portfolio of public services to citizens in a cost-effective manner. Most governments make tremendous efforts to deliver the online services to citizens (Roy, Chartier, Crete, & Poulin, 2015). The operation of information and communications technology (ICT) has been the major development of e-government in the past decade (Reddick & Anthopoulos, 2014). ICT has altered public administration by transforming the internal processes and external interactions (Meijer & Bekkers, 2015). E-government services must be redesigned to ensure that the benefits of ICT systems are completely employed (Kasemsap, 2016). The most significant role of ICT is to drive the organizational innovation through information systems and solve the crucial problems that the government cannot solve on its own (Sindelar, Mintz, & Hughes, 2010). E-government has emerged as an effective method of delivering government services to citizens (Weerakkody, Dwivedi, & Kurunananda, 2009). The diffusion of e-government is an international phenomenon (Carter & Weerakkody, 2008). As an integral part of administration modernization (Stier, 2015), e-government is one of the most important ways to bridge the digital platform in developing countries (Venkatesh, Sykes, & Venkatraman, 2014), acts as an effective exploration of government innovation (Wu & Guo, 2015), and can improve the government performance and create the new public value for citizens and businesses (Wang, 2014). The success of e-government system lies with its cost savings in implementation, adoption, benefits provided to the recipients of the system, and associated risks in operating the system (Weerakkody, Irani, Lee, Osman, & Hindi, 2015). This article aims to bridge the gap in the literature on the thorough literature consolidation of e-government. The extant literature of e-government provides a contribution to practitioners and researchers by describing the multifaceted applications of e-government to appeal to the different segments of e-government in order to maximize the public sector impact of e-government in the digital age.",CS,AI_ML,0.85,Extracted from log - paper 252
"Design of Secure Electronic Disposition Applications by Applying Blowfish, SHA-512, and RSA Digital Signature Algorithms to Government Institution","Many Government institutions still applies manual dispositions so that there are still some obstacles including the speed of delivery, accessibility, search, and the absence of security against disposition so that the disposition is prone to damage. In this study an electronic secure disposition application will be developed in accordance with the Regulation of the Minister of State for Administrative Reform and Bureaucratic Reform number 6 of 2011 concerning Regulation of Electronic Service Manuscripts to overcome manual disposition problems in Government Institution. The application will apply the Blowfish algorithm as its encryption method. and digital signatures with SHA-512 hash functions and RSA digital signatures in the attached file. The methodology used is Prototyping and modeling with UML (Unified Modeling Language). The results obtained in the form of a safe electronic disposition system are called Electronic Disposition Information System (e-disposition) of government institution. The system can meet the management of dispositions according to the needs in Government institution with guaranteed confidentiality, data integrity, authentication, and non-denial",CS,AI_ML,0.85,Extracted from log - paper 253
Critical Success Factors of Data Integration on Digital Human Capital Information System to Support Digital Transformation - A Case Study at PTXYZ,"The Human Capital paradigm, used as a catalyst for digital transformation, aims to increase the company's competitive value. The paradigm is outlined in the form of a Digital Human Capital Information System (HCIS) to support digital transformation at PT XYZ. However, this implementation raises a gap with legacy systems that have been running before, so data integration between the two systems is needed. This study aims to determine the critical success factor (CSF) of data integration in HCIS implementation. This study uses the HOT-fit model and TOE framework to determine the factors that influence the success of implementation, then the analytical hierarchy process (AHP) is used to determine the factor that has the greatest influence. The results found that these factors are classified into four criteria: human, technology, organization, and environment. Among the four criteria, the environment is the most important criterion, followed by human, technology, and organization. CSF in each criterion are government regulations and support, innovativeness of senior executives, perceived compatibility, and relative advantage.",CS,AI_ML,0.85,Extracted from log - paper 254
The digital rise and its economic implications for China through the Digital Silk Road under the Belt and Road Initiative,"This research, based on a review of secondary information, explores how the government of China and the country's leading technological enterprises are working together to develop infrastructure for next-generation digital technologies, e.g. artificial intelligence, cloud computing, quantum computing, 5G networks, navigation satellites, and fiber optic cables; to establish technical norms and standards; and to provide services and digital content, e.g. digital messaging applications, mobile payment systems, and e-commerce platforms, to emergent markets; as well as how digital corporate giants of China like Alibaba, Huawei, Baidu, ZTE, China Telecom, China Mobile, China Unicom, and Tencent have been challenging the prevailing status quo. Beijing seeks to assert its dominant role in world affairs through the Digital Silk Road (DSR) to globally influence and control a sizable part of the digital economy. The DSR has significant potential for enhancement of digital interdependence with the underdeveloped and some advanced economies by bridging the gap created by the absence of a critical infrastructure of global digital technology. There is no viable competitor to the DSR's exciting and long-term vision of a globally connected digital future for facilitating mutual growth and collaboration that will ultimately push for a dependency of other countries on DSR under the Belt and Road Initiative.",CS,AI_ML,0.85,Extracted from log - paper 255
Politics and Information Technology Investments in the U.S. Federal Government in 2003-2016,"Information technologies (IT) act as an enabler for policy implementation in the U.S. federal government. While federal agencies increasingly rely on advanced digital technologies to execute new policy initiatives, many agencies are struggling with maintaining decades old legacy systems. This study investigates how national politics affects IT investment profiles in U.S. federal agencies. Drawing on a range of literature from the political science, public administration, and information systems (IS) disciplines, we hypothesize that a federal agency’s capacity-building IT investments are associated with (i) legislative approval for the chief executive, (ii) government dividedness, and (iii) the agency’s ideological characteristic. With a panel data set from 135 federal agencies and bureaus in 2003–2016, our empirical analyses produce several intriguing findings. For instance, when the U.S. Senate and the House of Representatives are controlled by the President’s ruling party, federal agencies are predicted...",CS,AI_ML,0.85,Extracted from log - paper 256
"A Comprehensive, Longitudinal Study of Government DNS Deployment at Global Scale","Within the Domain Name System (DNS), government domains form a particularly valuable part of the names-pace, representing trusted sources of information, vital services, and gateways for government personnel to engage in their duties. As the COVID-19 pandemic has unfolded, governments’ digital resources have become increasingly important to provide support to populations largely in isolation. The accessibility of these resources relies largely on the trustworthiness of the domains that represent them. In this paper, we conduct an extensive measurement study focused on the availability and legitimacy of DNS records in the authoritative nameservers of government domains for over 190 countries. Our measurements reveal that thousands of domains do not use replicated authoritative name-servers, as well as a substantial increase in the trend of more domains relying on a single third-party DNS services provider. We also find more than 1,000 domains vulnerable to hijacking due to defective delegations. Our work shows that although robust overall, the deployments of authoritative nameservers in government domains still contain a non-trivial number of configurations that do not meet RFC requirements, leading to poor performance and reduced reliability that may leave domains vulnerable to hijacking.",CS,AI_ML,0.85,Extracted from log - paper 257
"Citizens’ Trust in Open Government Data: A Quantitative Study about the Effects of Data Quality, System Quality and Service Quality","Previous research assumes that poor quality of Open Government Data (OGD), OGD portals, and the services provided for OGD may result in reduced trust of citizens in OGD. However, studies that empirically test this assumption are scarce. Using the Information Systems (IS) Success Model as a theoretical basis, this study aims to examine the effects of data quality, system quality, and service quality on citizens’ trust in OGD. We used Structural Equation Modeling (SEM) to analyze the 200 responses to our online questionnaire. We found that trust in OGD can be predicted by citizens’ perceptions of OGD system quality and service quality. Furthermore, citizens’ perception of service quality positively influences their perceptions of data and system quality, whereas citizens’ perception of system quality positively influences their perception of data quality. This study is among the first that quantitatively examines the effects of data quality, service quality, and system quality on citizen's trust in OGD. It contributes to the scientific literature by providing an operationalization of elements of the IS Success Model in the context of OGD and by developing and applying a model of factors influencing citizen's trust in OGD. While previous research finds that perceived data quality is the most crucial driver for trust in OGD, our study finds that citizens’ perception of OGD service quality is a more important driver for trust in OGD. With regard to the practical contributions of this study, open data policymakers should be aware that citizens’ perceptions on data quality can be greatly improved when appropriate human services are provided (e.g., designated civil servants offering support or help to data users) in addition to the provision of OGD portal functionalities (e.g., data visualization and comparison tools).",CS,AI_ML,0.85,Extracted from log - paper 258
"The Challenges and Opportunities in Adopting AI, IoT and Blockchain Technology in E-Government: A Systematic Literature Review","Over the past decades, governments around the world are implementing a variety of digital solutions in order to enhance the effectiveness, efficiency, and transparency of public services. By using modern information and communication technologies (ICT), citizens are provided with an easier, faster, and more secure way of accessing different types of data. Previous studies on e-government have given important insights into the differences between traditional government systems and digital, e-government. The research was conducted by the systematic literature review guidelines by Kitchenham for papers published between 2016 and 2021 and aims to identify main technologies that are adequate to be used in the context of e-government, by presenting key challenges, as well as opportunities and possible risks of adopting such solutions in public sector services. The results obtained suggest that pairing governance with modern information systems, such as artificial intelligence (AI), internet of things (IoT) and blockchain technology, if used properly, can contribute to achieving a higher level of quality of public services and encouragement of citizens' trust and further political activity. Therefore, the present study concluded that there is a significant dependency between a successful e-government implementation and the country's established legal foundations, available technical infrastructures, perceived public trust, as well as social and economical conditions.",CS,AI_ML,0.85,Extracted from log - paper 259
E-Services: Implementation of Digital-Based Public Services in The 4.0 Era,"Globalization has numerous significant impacts that have an effect on people's lives. One of these impacts is the advancement of information and communication technology, which has many promising benefits, such as time efficiency, speedy delivery of information, transparency, and affordability. Technological advancements have infiltrated various sectors, including the government. In today's era of regional autonomy, it is essential to achieve good governance, which is balanced with the use of information and communication technology, commonly referred to as e-government. The use of technology and information indirectly encourages the adoption of electronic-based service systems (e-Services) in government institutions. The development of e-services is a media innovation that supports the provision of excellent services for the community, particularly in rural areas. Furthermore, the development of more varied service features such as letter submissions, complaint services, village information, and administrative procedures is expected to accelerate and facilitate access for service users more efficiently and transparently. This study aims to 1) create a public service e-service design; 2) fulfill the wishes of the community in efficient and effective village government public services; 3) understand the transparency of the village government, which can be accessed digitally. The development of letter disposition e-services in Sriharjo Village has improved the quality of public services and increased",CS,AI_ML,0.85,Extracted from log - paper 260
A Systematic Literature Study to Unravel Transparency Enabled by Open Government Data: The Window Theory,"Abstract The opening of data has been credited for improving transparency and for providing a window on government functioning. Although this relationship is intuitively apparent, it is in fact complex and the mere opening of data might not actually yield transparency. In this paper, a comprehensive model of determinants that enable or impede transparency enabled by open government data and the expected effects have been derived by surveying public administration and information systems literature. Public administration literature tends to be focused on factors such as participation and trust, whereas information systems literature focuses on factors such as user interface, user experience, and data quality. Digital government literature attempts to bridge these elements. The Window Theory is introduced, in order to unify existing models by integrating a broad range of factors within a single model. The Window Theory can be used to develop context-dependent models that are both comprehensive and parsimonious.",CS,AI_ML,0.85,Extracted from log - paper 261
Crisis as driver of digital transformation? Scottish local governments’ response to COVID-19,"Abstract The response to the COVID-19 pandemic has, from the outset, been characterized by a strong focus on real-time data intelligence and the use of data-driven technologies. Against this backdrop, this article investigates the impacts of the pandemic on Scottish local government’s data practices and, in turn, whether the crisis acted as a driver for digital transformation. Mobilizing the literatures on digital government transformation, and on the impacts of crises on public administrations, the article provides insights into the dynamics of digital transformation during a heightened period of acute demands on the public sector. The research evidences an intensification of public sector data use and sharing in Scottish local authorities, with focus on health-related data and the integration of existing datasets to gather local intelligence. The research reveals significant changes related to the technical and social systems of local government organizations. These include the repurposing and adoption of information systems, the acceleration of inter and intraorganizational data sharing processes, as well as changes in ways of working and in attitudes toward data sharing and collaborations. Drawing on these findings, the article highlights the importance of identifying and articulating specific data needs in relation to concrete policy questions in order to render digital transformation relevant and effective. The article also points to the need of addressing the persistent systemic challenges underlying public sector data engagement through, on one hand, sustained investment in data capabilities and infrastructures and, on the other, support for cross-organizational collaborative spaces and networks.",CS,AI_ML,0.85,Extracted from log - paper 262
Digital Government Security Infrastructure Design Challenges,"Information security pervades all such needs. In a DG environment, secure interoperation ensures confidentiality when individuals, private organizations, and government agencies access information. Electronic transactions and delivery systems must be secure to ensure protection against fraud and other vulnerabilities. The government’s archived information should be protected from tampering yet remain accessible under proper authorizations. Among all government functions, maintaining collective security remains the most crucial element, requiring that security concerns be addressed at each level of the government’s information infrastructure. In general, the concepts and ideas we describe here—although applied to DG uses—are applicable Digital Government Security Infrastructure Design Challenges",CS,AI_ML,0.85,Extracted from log - paper 263
Coproduction of Government Services and the New Information Technology: Investigating the Distributional Biases,"This article investigates how communications advances affect citizens’ ability to participate in coproduction of government services. The authors analyze service requests made to the City of Boston during a one-year period from 2010 to 2011 and, using geospatial analysis and negative binomial regression, investigate possible disparities by race, education, and income in making service requests. The findings reveal little concern that 311 systems (nonemergency call centers) may benefit one racial group over another; however, there is some indication that Hispanics may use these systems less as requests move from call centers to the Internet and smartphones. Consistent with prior research, the findings show that poorer neighborhoods are less likely to take advantage of 311 service, with the notable exception of smartphone utilization. The implications for citizen participation in coproduction and bridging the digital divide are discussed.",CS,AI_ML,0.85,Extracted from log - paper 264
Prescription Tablets in the Digital Age: A Cross-Sectional Study Exploring Patient and Physician Attitudes Toward the Use of Tablets for Clinic-Based Personalized Health Care Information Exchange,"Background To reduce the cost of health care while increasing efficiency and quality, health systems are seeking innovative means to engage and empower patients. Improved use of information technology and electronic health record (EHR) infrastructure is essential, and required for “meaningful use” as mandated by the federal government. Providing personalized health information using tablets at the point of care could enhance the clinical experience and enable efficient collection of patient reported outcome measures to guide clinical decision making. Objective The aim of this study is to explore patient and provider attitudes and interest in a proposed clinic-based tablet system for personal health information exchange. To provide a context to understand patients’ use of tablets during their clinic visit, we also examine patients’ current activities and time spent in the waiting room, and their use of health information resources. Methods Surveys were administered to 84 patients in the waiting room of a community health center affiliated with Massachusetts General Hospital (MGH) in Boston, MA. This survey included a vignette and illustration describing a proposed tablet-based system in which the patient, upon sign in at the clinic, receives a tablet loaded with personalized information tailored to their specific medical conditions and preferences. Patients were queried about their interest in such a system in comparison to traditional forms of patient education as well as their current health information seeking behaviors and activities and time spent in the waiting room. Interviews with five MGH-affiliated health care providers were conducted to assess their opinions regarding the proposed tablet system. Results The majority (>60%) of patients were “very” or “extremely” interested in the proposed tablet system and thought it would improve their knowledge about their medical condition (60%), assist them in making healthy choices (57%), and help them to feel more comfortable talking with their provider (55%). Patients thought the system would be more motivating, informative, and engaging than traditional printed health education materials. The tablet system was not considered more effective than face-to-face interaction with providers, though 44% thought it would improve their relationship with their physician. Overall, 91% of respondents were willing to learn how to use a tablet and 75% reported being “very” or “extremely” confident they could use one. Four of the five providers believed that the proposed tablet system would improve clinical workflow and patient education. Patients and providers were concerned about privacy and security of data collected using the tablets. Conclusions Both patients and providers were highly amenable to integrating tablets into the clinical experience, and tablets may be useful in improving patients’ health knowledge, the collection of patient reported outcome measures, and improved patient-provider communication. Further research into operationalizing such systems and their validation is necessary before integration into standard clinical practice.",CS,AI_ML,0.85,Extracted from log - paper 265
Case Studies on Digital Government,"IT practitioners in government and computer science programs represent a significant population of professionals constantly attempting to obtain information on the latest research in public management, digital government, and other governmental information management subject matter. Case Studies on Digital Government is the first book dedicated to detailed case studies of information management in government. Case Studies on Digital Government includes cases from local, state, Federal, and international governments, covering a wide variety of technologies such as geographic information systems, enterprise resource planning, Web-Based customer response systems, and cross-agency shared systems. It contains case studies written by practitioners, while most case studies in the public sector have been written by researchers. The practitioners' in-depth knowledge bring a reality to the cases that readers will find stimulating as well as instructive.",CS,AI_ML,0.85,Extracted from log - paper 266
Digital Enterprise Architecture for Green SPBE in Indonesia,"SPBE (Electronic Based Government System) is a legal protection as new breakthroughs in the reform of Indonesian government bureaucracy. The issuance of Presidential Regulation (Perpres) 95/2018 concerning SPBE is expected to be a reference for the transformation from e-Government into i-Government (integrated Government). In the meantime, the government through the Ministry of Administrative and Bureaucratic Reform (PANRB) is drafting an academic paper on the SPBE Bill. Of the 10 elements contained in Presidential Regulation (Perpres) 95/2018, the second element namely SPBE architecture is a concept known in the world of Information Systems as Enterprise Architecture.Enterprise architecture is a conceptual framework that describes how an enterprise is constructed by defining its primary components and the relationships among these components. In SPBE, the main component is defined as domain, consisting of 6 parts, namely: business process architecture domain, data and information, infrastructure, SPBE applications, security, and SPBE services. Unfortunately, the Presidential Regulation (Perpres) 95/2018 has not regulated the concept of Digital Enterprise Architecture, since between Enterprise Architecture (EA) and Digital Enterprise Architecture (DEA) are two things that are significantly different. If EA merely focuses on structuring the company based on the main frame of reference, then DEA focuses on utilizing digital repositories to create living documents as according to the EA framework so that they are easily accessed, modified and managed at any time following the company's development. This study created a DEA model for SPBE in Indonesia. The model created is adapted to the SPBE architecture by carrying out the concept of a digital repository. With digital repositories, time efficiency, paper savings and change management will be easier to achieve. The model created in this study is expected to be utilized to make SPBE much more efficient and green-minded.",CS,AI_ML,0.85,Extracted from log - paper 267
A brief history of the emergence of digital government in the United States,"Digital government may be regarded as the most recent development in the evolving application of electronic information technology to the performance of governmental functions. In the United States, that evolutionary progression is rooted in the Federal, state, and local government use of such information technologies as the telegraph and the telephone. This history, however, considers more than the mere introduction and adaptation of such technologies by governmental entities. Other important aspects include the development and migration of the technologies, as well as imaginative applications of information technology in support of government operations. Also, new policies have been fashioned to ensure the proper management of these technologies and the systems they serve, their protection from physical harm, and the security and privacy of their information. These matters are concisely explored in this overview.",CS,AI_ML,0.85,Extracted from log - paper 268
"Digital profile: the concept, regulatory mechanisms and enforcement problems","The subject of the research is the legal nature of the digital profile of a citizen, as well as a set of legal norms regulating digital profiling relations in Russia.The comparative method, the method of system analysis, as well as the method of legal modeling are used in the article.The purpose of the article is to confirm or disprove the hypothesis that legal regulation is not the only mechanism for regulating relations in the field of digital profiling.The main results, scope of application. The article studies the phenomenon of digital profile, the main approaches to the digital profiling as well as the circumstances that have caused the state's interest in digital profiling. The creation and operation of a digital profile should be aimed at achieving the goal set out in the legislation. The digital profile is a set of relevant, reliable information about individuals and legal entities formed in the unified identification and authentication system or other information systems of state and local government authorities. The formation of a digital profile is carried out in order to provide data to authorities, legal entities and persons who have requested access to this information through the digital profile infrastructure. The analysis of the Russian legal regulation of relations in the field of digital profiling is presented, the problems of enforcement practice are identified. The analysis revealed the main differences between the digital profile and related categories, including social scoring, the unified population register and others. The comparison of a digital profile with a digital avatar and a digital character was carried out. It is extremely important to pay close attention to the problems of digital profiling both at the level of fundamental and applied scientific research. At the state level, it is important to strategically determine what a digital profile is, as well as formulate the main directions of the digital profiling development, challenges and risks. The importance of the development of digital profiling for unified system of public authorities in the Russian Federation is outlined.Conclusions. The analysis of the emerging practice of digital profiling in contemporary society shows that legal regulation does not always allow us to keep up with the rapidly developing relations in this area. The possibility of using other mechanisms should be considered. The use of mechanisms of regulatory experiments can also be considered as special mechanisms for regulating relations in the field of digital profiling. The goal of the research has been achieved, the legal nature of the digital profile has been revealed, approaches to regulating this phenomenon in the conditions of digital transformation have been proposed.",CS,AI_ML,0.85,Extracted from log - paper 269
E-Government and Foreign Direct Investment: Evidence From Chinese Cities,"Along with the rapid development of digital information technology, e-government is of great potential because it is a new form of conducting public administration and a way of demonstrating governmental innovation. The literature suggests that foreign direct investment (FDI) is increasingly associated with the continuing development of e-government in China. Using the Annual Census of Industrial Enterprises and e-government scores of government portals, this study examines the effects of e-government on FDI and how government subsidies mediate relationships between e-government and FDI. The results show that e-government positively affects FDI, and government subsidies have a positive effect on foreign enterprises’ investment efficiency by playing a mediating role between e-government efficiency and FDI. The findings make both theoretical and practical implications related to the role, provision, and acquisition of government subsidies; e-government systems and FDI; e-government scores and government efficiency; and firms maintaining focus on areas that are government priorities. The results show significant this of the",CS,AI_ML,0.85,Extracted from log - paper 270
Public Governance in Digital Transformation: from Electronic Document Exchange to Digital Ecosystems,"This article explores the changes arising from the introduction of information and communication technologies in public administration using an institutional approach. Practical examples are used to consider transformations in the way government institutions interact with citizens and organizations, as well as new emerging opportunities in the provision of public and socially important services. The author analyses both the retrospective formation of centralised government information systems in countries that have reached digital maturity, the formation of electronic, digital and smart governments, and the emerging trends of the next round of digital transformation - the development of interstate digital ecosystems and regional digital environments of megacities and urban agglomerations. In particular, the article examines the steps taken by the US government in the early 21st century to establish electronic interaction between government agencies and to create a system for delivering public services that excludes personal contact by citizens and companies. It then identifies technologies that have subsequently enabled other countries to make the transition to fully digital governmental information. A significant innovation that, in the author's opinion, determined the further vector of transformation of administrative processes was the integration of payment and government information systems. Such a technological solution, implemented through a closer partnership between government and corporate entities, combined with the spread of individual mobile digital devices – smartphones and tablets – has become the basis for the development of unified digital platforms designed to optimize both the provision and receipt of virtually all socially important services. The article also examines the role of government information systems in the development of the data economy and highlights emerging models of government regulation in this area. The following trends of digitalization in the public sector are analysed as relevant: regulators of digital ecosystems acquiring the ability to provide digital services to extraterritorial communities, scaling digital infrastructure in territories with high population concentration when implementing economic development projects and introducing digital technologies in the social sphere.",CS,AI_ML,0.85,Extracted from log - paper 271
E-Government Insights to Smart Cities Research: European Union (EU) Study and the Role of Regulations,"The recent evolution of smart cities research has initiated a holistic dialogue for the integration of past initiatives promoting e-government at European Union Level. At the same moment Future Smart Cities research, is justified as a multi-disciplinary and inter-disciplinary research domain. Within this context this research work provides insights for the integration of Economics, E-government, Information Systems, and Social Sciences. The study addresses the e-government process as one of the most important application of smart cities in our current societies. More specifically, we have examined the existence of $\beta $ -convergence between EU member states in terms of e-government services, confirming the hypothesis that low performance countries record higher growth rate than developed countries. In order to move closer to the factors with high impact on e-government development, we have analyzed other variables for testing the differences between countries, in terms of education, digital skills and access to internet for controlling the hypothesis that countries initially have different development conditions and they will not probably converge to the same steady state. We found that the opportunity of using e-services, particularly e-government services, is less accessible to citizens with low overall digital skills and, as is technically normal, to those with low access to internet access. In this context, at EU level it is needed to rethink and design the e-government services in order to be adapted not only to needs of the citizens, but also to their digital skills. The main contribution of this research is two-fold: From one side provides an integrated study with emphasis on the impact of social sciences and economics research to Future Smart Cities Research and from the other side it brings forward several soft factors for the adoption of Smart Cities services in the context of government transformation and provision of ubiquitous e-services to citizens.",CS,AI_ML,0.85,Extracted from log - paper 272
Towards Sustainable Software for Public Sector Information Systems,"One distinctive characteristic of software is its malleability and succeeding opportunity and also need for a constant change. However, in certain types of software, many agencies in the public sector are bound by the laws on competition and financed by governmental budgeting processes. Consequently, the acquiring agencies adapt their processes to what the existing systems allow due to complexities in making changes and modifications. In this paper, we study the relation of public sector software acquisition and delivery, and the continuously changing nature of software. Then, we analyse pain points of evolving software in public sector information systems, and propose technical artefacts to improve the status quo. Finally, to add flexibility to public sector information systems’ acquisition and evolution, we elaborate a novel architectural pattern called Mosaic architecture, based on macroservices, which allows independent subcontracting, development, and deployment. The approach is demonstrated with a real-world industrial study from a public service that has been sketched applying the macroservice principles.",CS,AI_ML,0.85,Extracted from log - paper 273
DeLone and McLean’s reformulated information systems success model: a systematic review of available literature in public sector (2011-2022),"Purpose DeLone and McLean (D&M) has been amongst the most widely used models to assess the success elements of information systems (IS) since 1992. A decade later in 2003, D&M proposed a revised design that included several components which have been left out of original proposed model. This study aims to rely on the review of a number of papers casing the era 2011–2022 that satisfies a specific set of requirements to identify the research gaps in this area and to prepare a future research agenda. Design/methodology/approach This study is a systematic review: a technique that identifies and evaluates pertinent research. This gathers and analyses data from selected studies with objective to catalogue all empirical research studies, which responds to a certain study topic or hypothesis to meets predetermined inclusion criteria. Data collection method was divided into four stages, and selection details are determined through flow chart. Findings This research discovered D&M 2003 model use in many contexts, countries and cultures to better comprehend the topic and addresses its gaps, particularly with regard to public sector and its particularities. Publications have frequently emphasized the significance of this idea while studying public sector information systems by using associated variables-related items. Findings also include a summary of key components and dimensions used in reviewed studies in relation to each of the seven variables, and associated particularities in government sector over the world. Originality/value To the best of the authors’ knowledge, this is the first ever effort in this developing country in discipline of Information Management to execute such kind of study to review D&M ISS model in detail at this stage to evaluate nonacademic public sector information systems around the world.",CS,AI_ML,0.85,Extracted from log - paper 274
Development of public sector information management systems: challenges and promising practices,"Purpose More and more governmental organizations are switching to information systems to enhance their operations and reduce cost but the development of these systems involves a lot of challenges. This paper aims to find out the challenges that project managers have to face during the development of such systems and the practices they can adopt to address these challenges. Design/methodology/approach To identify the challenges, data have been gathered across six key project management areas. The six targeted key areas are project integration management, project scope management, project time management, project cost management, project human resource management and project communications management. The authors have coordinated with 11 managerial-level IT professionals using semi-structured interviews and have gone through their communication archives. Findings Findings prove that practices such as focusing on cost reduction, informal congregations, trainings and frequent communication between vendor and client help in addressing the challenges. Furthermore, learning from management experiences of the managers can assist managers in similar role to create a pattern of success while working with governmental projects. Originality/value The authors peek into the development life cycle of a public sector project named as prison information management system. The project has encountered numerous challenges and has been accomplished in significantly delayed time than designated.",CS,AI_ML,0.85,Extracted from log - paper 275
Evaluating Public Sector Information Systems: More Than Meets the Eye,"mation systems. T he proliferation of personal computers, networked terminals, and powerful workstations throughout the public sector makes information systems and their information products readily accessible to most government workers. In many cases, the use of information systems seems indispensable for public program success in service delivery, oversight, policy formulation, and accountability functions. The knowledge and skills for effective evaluation of these crucial information systems are not nearly as pervasive as the systems or the range of users. Guidance on effective information-system evaluation is not even readily available for private-sector evaluators, and the governmental environment presents different sorts of challenges for those intent on ensuring effective evaluation.",CS,AI_ML,0.85,Extracted from log - paper 276
The impact of public sector scorecard adoption on the effectiveness of accounting information systems towards the sustainable performance in public sector,"Abstract The effectiveness of accounting information system (AIS) in public sector organization (PSO) is well recognized to play an important part in obtaining sustainable performance (SP). Nevertheless, the effectiveness of AIS cannot achieve the SP on its own due to the rapid changing in the global economic world. Accordingly, there is an increasing demand on a framework of evaluation which is considered to be appropriate with the characteristics of PSO to orientate, manage, and assess operations of AIS toward attaining SP. Thus, this study is undertaken with the aim at exploring the relationship between the impact of public sector scorecard (PSS) implementations and effectiveness of AIS toward SP enhancement with evidence gathered from 883 PSOs’ accountants in Mekong Delta region. Data analyzed by the Structural Equation Modeling (SEM) highlight that PSS adoption has a significant impact on the effectiveness of AIS. It also provides a reliable basis for the association between the effectiveness of AIS and SP. Although the topic of research is still immature, these findings are predicted to serve as a catalyst for scholars, practitioners, and policymakers to inquire and adopt because of its potential benefits in PSOs’ operations.",CS,AI_ML,0.85,Extracted from log - paper 277
Successful Implementation of Information Systems in Public Sector Organizations,"This study aims to determine the success of the implementation of the Regional Financial Management Information System (SIPKD) in the Regional Work Unit (SIPKD) in the Sumedang District Government, by examining the effect of system quality, information quality and service quality on user satisfaction and its implications for net benefits. This study uses primary data from questionnaires given to leaders of agencies, operators, and administrators of SIPKD. The data analysis method used is path analysis. The results of this study indicate that the quality of the system, the quality of information, and the quality of service affect user satisfaction and impact on net benefits both partially and simultaneously. The results of this study contribute to adding literature on the implementation of information systems in public sector organizations, and influencing factors can be used as a reference for public sector organizations in assessing the application of information systems.",CS,AI_ML,0.85,Extracted from log - paper 278
A Synthesis on SWOT Analysis of Public Sector Healthcare Knowledge Management Information Systems in Pakistan,"Healthcare is a community service sector and has been delivering its services for the betterment of civic health since its establishment at communal level. For working efficiently and effectively, this sector profoundly relies on correct and complete health information of people and a proficient integrated healthcare knowledge management information system (HKMIS) to manage this information. The performance of Healthcare organizations has significantly augmented by inception of Information and Communications Technology (ICT) in HKMIS in developed countries, but is yet to exhibit its full potential in developing countries specifically those with huge populations like Pakistan. An exploratory qualitative research methodology was adopted to conduct this study. The purpose and objective of this study was to determine and investigate the internal and external factors that influence the performance of HKMIS by performing SWOT analysis on two of the largest public-sector healthcare organizations of Pakistan. The findings of this study will certainly help authorities to devise methods of improvement in Pakistani HKMIS eventually paving ways towards a better and improved healthcare in the future.",CS,AI_ML,0.85,Extracted from log - paper 279
The Impact of Human Resource Information Systems: An Exploratory Study in the Public Sector,"Various authors have advocated that the use of a Human Resource Information System (HRIS) should lead to valuable outcomes for the organization. Decreased costs, improved communication, and decreases in time spent on mundane activities should create an environment wherein the Human Resources (HR) department would play a more strategic role in the organization. This study is an initial attempt to determine whether HRIS has reached these potential benefits. Based on responses from a sample of HR directors of from public universities we found that, while valuable, HRIS has not yet reached its full potential in this environment.",CS,AI_ML,0.85,Extracted from log - paper 280
Public-sector Financial Management and E-government: The Role Played by Accounting Systems,"ABSTRACT The paper investigates the relationship between two central features of public-sector financial management reforms: accounting innovations and e-government. It focuses on the quality and ease of access to information provided by different accounting systems and e-government strategies. Based on a sample of 33 Organization for Economic Cooperation and Development (OECD) countries, the findings underscore the need to adopt a holistic perspective that considers not only technological issues but also the quality and integrity of information, its international comparability, and the socioeconomic context. The results suggest that both technical and social factors should be considered in adopting e-government.",CS,AI_ML,0.85,Extracted from log - paper 281
Value–Based Guiding Principles for Managing Cognitive Computing Systems in the Public Sector,"Abstract Cognitive Computing Systems (CCSs) are increasing in prominence in the public sector. This paper develops a framework drawing on public value and information technology service management literature to guide the management of CCSs in the public sector. We draw on academic literature, gray literature, legislation and government reports, and examples on CCS initiatives in the public sector to develop insights for research and practice. We then outline the themes and present the insights in the form of guiding principles and specific (detailed) recommendations. These include guiding principles and recommendations for establishing legitimacy, understanding the required capabilities, executing capabilities, creating and measuring public value.",CS,AI_ML,0.85,Extracted from log - paper 282
The role of data and information security governance in protecting public sector data and information assets in national government in South Africa,"Background: The deployment of information and communications technology (ICT) in the public sector, has been exposed to increasing security breaches and cyber-related crimes that have resulted in unauthorised access, theft, fraud and misuse of highly confidential, classified and sensitive public sector data and information (PSDI) assets. The government, as one of the biggest collectors and distributors of PSDI assets, needs to be constantly aware of the risks associated with the collection, classification, storage and dissemination of critical PSDI assets. The lack of sufficient data and information security measures could pose significant security risks that could impact on state security, thus causing national working relationships to be strained, which presents gaps and opportunities for external intruders to capitalise on the mistrust of the government to infiltrate further attacks on critical Information Technology (IT) infrastructure and systems. In order to mitigate and counteract critical and sensitive data and information-related crimes, the government must understand and analyse the importance of data and information security governance (DISG) and how it should be institutionalised through an integrated approach to improve and protect PSDI assets. Aim: The aim of this article is to analyse the institutionalisation of DISG measures government has implemented towards the protection of PSDI assets. Setting: The research setting is in three national government departments, namely the Department of Energy (DoE), the Department of Environmental Affairs (DEA) and the Department of Science and Technology (DST). This study investigates how the strategic combination of data governance (DG) and information security governance (ISG) practices and principles could be implemented and incorporated as one of the various approaches in public sector institutions to improve the DISG management functions of an organisation’s overall data and information systems and processes. Methods: The research approach is qualitative, and the research methodology includes a multiple case study design. Data were collected through semi-structured interviews and was triangulated with literature review. Primary data was analysed using thematic analysis. Results: The research findings are presented according to the McKinsey 7S model, which served as the analytical framework in the study. The research findings indicate that the institutionalisation of DISG management practices and functions in the South African public sector context are very limited, and there is a dominant focus on IT and IT security. It was also identified that DISG policies, practices, and systems have been found to be lacking in public sector management and governance functions. Conclusion: The study concludes that there is currently a lack of sufficient DISG policies, management practices and systems, particularly in the national sphere of government.",CS,AI_ML,0.85,Extracted from log - paper 283
A Framework for the Adoption of Blockchain-Based e-Procurement Systems in the Public Sector,"Public procurement constitutes a core government function for providing goods and services to citizens. The overall success of a digitized public-procurement function yields progress and economic growth for the nation. In this research, we analyze the potential of blockchain-based systems to enhance effectiveness, ease, and transparency in public procurement in the case of Nigeria and identify the current challenges facing public procurement, i.e., lack of trust and transparency among critical stakeholders in the procurement process, systems that only weakly support transaction recording and documentation, complex process structures, corruption in institutions involved in the procurement process. To address these issues, a blockchain-based framework is developed to enable interoperability of information-systems involved in the procurement process, increase citizen participation in eliciting project requirements and to enable a more transparent project monitoring and auditing. We apply the framework to a case study with respect to identifying on-chain activities that enable system interoperability, e-participation and project auditability.",CS,AI_ML,0.85,Extracted from log - paper 284
Critical Success Factors Affecting Information System Satisfaction in Public Sector Organizations: A Perspective on the Mediating Role of Information Quality,"Many studies have investigated technology adoption in western countries and ignored the Arab region. The available Arab studies focused on the technology adoption model (TAM) and its subsequent variations while leaving important factors such as information quality, user involvement, availability of training and top management support on the success of information systems (IS). Despite that these factors were studied scantly in some past studies, this research attempts to fill this gap and develop a more integrative model of IS success. Results indicated the existence of four critical success factors, three organizational factors (management support, training, user involvement), and an information system factor (information quality), that affect IS success (use and satisfaction). Results found that information quality for the first time mediates the effect of the three organizational factors on IS success, while TAM components (perceived usefulness and perceived ease of use) have no effect.",CS,AI_ML,0.85,Extracted from log - paper 285
How is the Use of Performance Information Related to Performance of Public Sector Professionals? Evidence from the Field of Academic Research,"ABSTRACT There is inconclusive evidence as to how performance management is actually related to performance, particularly in subfields of the public sector where professional work prevails. We propose that the association between the use of performance information and performance of public sector professionals varies with the targets of management control. We test our hypotheses in the field of academic research, a prime example of professionalism in the public sector. The overall results of an online survey with 1,976 observations suggest that performance management is positively related to publication performance when performance information is used for the control of input targets. In contrast, we find negative associations of performance information with performance when used to control output targets. Public managers in professional fields should consider these countervailing relationships when they compose and use control systems.",CS,AI_ML,0.85,Extracted from log - paper 286
Big Data in Health Information Systems,"In the healthcare sector, Information Systems are fundamental for decision-making at all levels. This process can be strengthened with the implementation of Big Data analytics. In this context, the present study aims to describe the experiences, benefits, and applications of Big Data in Health Information Systems through a systematic literature review. The research reviewed 22 studies on the use of Big Data in the healthcare sector, applying inclusion and exclusion criteria to select relevant studies. The results of these studies indicated that the use of Big Data in healthcare can improve the quality of teleassistance services for patients, as well as logistics and financial services. It can also prevent diseases and improve patient care in public health information systems. Additionally, it can provide new knowledge and actionable information from new data sources, and promote the natural transformation of descriptive research into predictive and prescriptive research. The studies also highlighted the importance of Artificial Intelligence for data to be useful for research and medical purposes. In general, the research concluded that Big Data has a favorable impact on the healthcare sector, especially in biomedical research, and that its use can improve the efficiency and quality of medical care.",CS,AI_ML,0.85,Extracted from log - paper 287
Information technology systems in public sector health facilities in developing countries: the case of South Africa,"BackgroundThe public healthcare sector in developing countries faces many challenges including weak healthcare systems and under-resourced facilities that deliver poor outcomes relative to total healthcare expenditure. Global references demonstrate that information technology has the ability to assist in this regard through the automation of processes, thus reducing the inefficiencies of manually driven processes and lowering transaction costs. This study examines the impact of hospital information systems implementation on service delivery, user adoption and organisational culture within two hospital settings in South Africa.MethodsNinety-four interviews with doctors, nurses and hospital administrators were conducted in two public sector tertiary healthcare facilities (in two provinces) to record end-user perceptions. Structured questionnaires were used to conduct the interviews with both qualitative and quantitative information.ResultsNoteworthy differences were observed among the three sample groups of doctors, nurses and administrators as well as between our two hospital groups. The impact of automation in terms of cost and strategic value in public sector hospitals is shown to have yielded positive outcomes with regard to patient experience, hospital staff workflow enhancements, and overall morale in the workplace.ConclusionThe research provides insight into the reasons for investing in system automation, the associated outcomes, and organisational factors that impact the successful adoption of IT systems. In addition, it finds that sustainable success in these initiatives is as much a function of the technology as it is of the change management function that must accompany the system implementation.",CS,AI_ML,0.85,Extracted from log - paper 288
Investigating the effect of information systems factors on trust in e-government initiative adoption in palestinian public sector,"The study conducted a primary investigation into the relationship between information system and trust in e-government followed by a secondary one into the relationship between trust in e-government and intention to use. It followed a quantitative approach of data collection through the use of questionnaire survey. The findings revealed that information quality, system quality, service quality, perceived usefulness, perceived ease of use, and security- privacy contributed positively to trust in e-government while the latter impacted intention to use. Moreover, the study's findings supported the rationale behind the IS theories (TAM and IS Success Model) and security-privacy were proven to illustrate trust in e-government adoption initiative. The findings theoretical and practical implications and limitation and suggestions for future research were discussed towards the end of the study.",CS,AI_ML,0.85,Extracted from log - paper 289
Emerging forms of inter-organizational and cross-sector collaborations in e-government initiatives: Implications for participative development of information systems,"Purpose – This study aims to explore recent public sector trends, inter-organizational and cross-sectorcollaborations, and analyzes these in terms of implications for participative development of i ...",CS,AI_ML,0.85,Extracted from log - paper 290
Integrated Financial Management Information Systems: Guidelines for effective implementation by the public sector of South Africa,"Background: Integrated Financial Management Information Systems (IFMIS) can improve public sector management by providing real-time financial information to managers in order to enhance their decision-making capabilities. The South African Public Service is currently busy with the implementation of an IFMIS. However, the implementation of such a project has proved to be a very demanding undertaking and has not been met with resounding success. Objectives: The research was conducted in order to identify the challenges and risks that are involved in the implementation of the IFMIS in South Africa. After identification of the challenges and risks, solutions or guidelines were developed that may make the implementation more successful. Method: The methodology that was used is that of a literature study where theories were explored and used to solve a research problem. Based on the theoretical research, solutions and guidelines were developed to solve challenges and risks experienced. Results: The results indicated that there are a number of challenges involved with the implementation of an IFMIS. A set of best practice guidelines was developed that may make the implementation more successful. Conclusion: The sheer size and complexity of an IFMIS poses significant challenges and a number of risks to the implementation process. There are, however, critical success factors or best practices that can be used for the project to succeed. It is recommended that these best practices be used by the South African Public Service.",CS,AI_ML,0.85,Extracted from log - paper 291
Understanding of blockchain-based identity management system adoption in the public sector,"PurposeThis study aims to understand the benefits and challenges associated with the adoption of a blockchain-based identity management system in public services by conducting an academic literature review, and to explore the design of such a system that can be applied to the Korean government.Design/methodology/approachThis study explores the adoption of a blockchain-based identity management system using a literature review and an actual design case intended for use by the government sector.FindingsBlockchain-based identity management systems can significantly improve transparency, accountability, and reliability in the user control of one's own data while reducing the time and cost needed to deliver public services, as well as increasing administrative efficiency. However, it is not always easy to implement such systems, and introducing new technologies in the government field requires a complicated, time-consuming process. There is currently an appetite for research extending beyond the typical technology-driven approach to elucidate the government adoption of new technologies and explore its implications.Practical implicationsThe idea behind this system is that by storing and managing personal information on the blockchain and providing mobile apps to customers, users can log in or retrieve previously authenticated personal information without having to go through an authentication process. Since users do not need to go through the verification process every time, it is expected that they will be able to access only the necessary personal information more quickly and conveniently without having to deal with unnecessary details. In addition, the blockchain-based operation of a public service effectively increases the transparency and reliability of that service and reduces the social costs caused by personal information leakage.Originality/valueThis study introduces the design of a blockchain-based identity management system that can be used in public services, specifically in the Korean government sector for the first time. Along with a literature review, the implications that this study gleans from these real-world use cases can contribute to this field of research.",CS,AI_ML,0.85,Extracted from log - paper 292
Efficiency creep and shadow innovation: enacting ambidextrous IT Governance in the public sector,"ABSTRACT The current push towards increased innovation within the public sector calls for new approaches to IT Governance. However, recent findings highlight the aim to avoid trade-offs between innovation and efficiency through organisational ambidexterity. This paper reports a case study of ambidextrous IT Governance in two large government agencies. According to the findings, ambidextrous IT Governance is enacted through two separate but interrelated mechanisms that emerge simultaneously. In terms of exploitation, the “efficiency creep” mechanism creates a bias for efficiency – rather than innovation-oriented investments. In terms of exploration, the “shadow innovation” mechanism involves unsanctioned innovation activities. These two mechanisms interplay, in the enactment of ambidextrous IT Governance. The contribution of this study lies in theorising about how ambidextrous IT Governance is enacted in public sector organisations, and how efficiency creep and shadow innovation influence each other. This contribution aids future research and practice on public sector innovation and IT Governance.",CS,AI_ML,0.85,Extracted from log - paper 293
IT value creation in public sector: how IT-enabled capabilities mitigate tradeoffs in public organisations,"ABSTRACT Governments today are striving to improve services in the public sector through digital transformation programs but face tremendous pressures from multiple fronts (economy, national security, healthcare, education, etc.). Even when worldwide enterprise IT spending for the government and education markets has been increasing and is expected to surpass $652 billion in 2023 to cater to such transformation programs, 80% of the government transformation efforts failed to achieve expected results. A plausible reason for this lacklustre performance could be the presence of tradeoffs or conflicts that is particularly salient in public organisations. To better understand the mechanisms by which IT enables or inhibits capabilities of the public organisations in attaining public value, we adopt a conflict resolution lens to study how information technology (IT) enabled capabilities to mitigate these tradeoffs. Using a dataset collected from public organisations in a European country unreeling from a financial crisis, we examine the processes by which IT enables public organisations to manage the tradeoffs arising from conflicting value-based goals. We identify three mitigation strategies facilitated via IT-enabled organisational capabilities – bias, tunnelling and hybridisation. This paper contributes to the understanding of how IT mitigates value-based tradeoffs in public organisations to achieve public value.",CS,AI_ML,0.85,Extracted from log - paper 294
Exploring smartness in public sector innovation - creating smart public services with the Internet of Things,"ABSTRACT The use of the term “smartness” in the context of public service delivery indicates an ambition of the public sector to become more agile and resilient through the adoption of emerging technologies. The Internet of Things (IoT) is an emerging technology that will be key to the realisation of smart public services. The research presented in this paper explored the role of IoT in public sector innovation through a qualitative study of how IoT technology can be leveraged to create and deliver smart winter road maintenance services. We use an existing smartness framework – based on the dimensions of efficiency, effectiveness, transparency and collaboration – to examine the consequences of introducing IoT-based innovation to road maintenance services. The findings suggest that IoT enables public sector innovation and that smartness is created through the combination of technology, people and organisations. The realisation of smartness in public sector innovation requires sufficient management capabilities and robust technology strategies, along with a willingness to explore and adopt new work practices rather than simply implement emerging technologies.",CS,AI_ML,0.85,Extracted from log - paper 295
Why do eGovernment Projects Fail? Risk Factors of Large Information Systems Projects in the Greek Public Sector: An International Comparison,"This paper presents an empirical study of the risk factors of large governmental information systems IS projects. For this purpose the Official Decisions of the Greek Government Information Technology Projects Advisory Committee ITPAC concerning 80 large IS projects have been analyzed and interviews with its members have been conducted. From this analysis 21 risk factors have been identified, and further elaborated and associated with inherent particular characteristics of the public sector, extending existing approaches in the literature. A categorization of them with respect to origin revealed that they are associated with the management, the processes, and the content of these projects. Results show that behind the identified risk factors there are political factors, which are associated with intra-organizational and inter-organizational politics and competition, and can be regarded as 'second level' risk sources. The risk factors identified in this study are compared with the ones found by similar studies conducted in Hong Kong, Finland, and the United States, and also with the ones mentioned by OECD reports. Similarities and differences are discussed.",CS,AI_ML,0.85,Extracted from log - paper 296
"Role of perceived ease of use, usefulness, and financial strength on the adoption of health information systems: the moderating role of hospital size","Adoption of a health information system is always a challenge for hospitals. It is because most of the medical staff do not have enough skills to use the new technology and due to the sensitivity of medical data. These factors pose a challenge for the successful adoption of health information system in hospitals. The aim of this research is to find out the factors which influence the adoption of information systems in hospitals. The study investigated the impact of the Financial status of the Hospital; Perceived Usefulness and Perceived Ease of Use on the adoption of health information systems through a questionnaire survey. Data was collected from 602 healthcare workers from 20 hospitals through close-ended questionnaire in Pakistan, where the adoption of health information systems is very slow. PLS-SEM was used for the analysis. The findings show that the Financial status of the Hospital; Perceived Usefulness and Perceived Ease of Use have positive and significant role in the adoption of Health Information Systems. The finding also shows that hospital size moderates the relationship of Perceived ease of use and the adoption of health information systems and interestingly it does not moderate the relationship among perceived usefulness and financial strength toward the adoption of health information systems. The study concludes that perceived ease of use, perceived usefulness and financial strength are the main factors, necessary for the adoption of health information systems. The findings of the study have useful implications for policy makers, medical professionals to successfully adopt health information systems in hospitals. It also provides new avenues for researchers to explore other factors and test this framework in other countries.",CS,AI_ML,0.85,Extracted from log - paper 297
Barriers and facilitators to health technology adoption by older adults with chronic diseases: an integrative systematic review,"Background In recent years, healthcare systems have progressively adopted several technologies enhancing access to healthcare for older adults and support the delivery of efficient and effective care for this specific population. These technologies include both assistive technologies designed to maintain or improve the independence, social participation and functionality of older people at home, as well as health information technology developed to manage long-term conditions. Examples of such technologies include telehealth, wearable devices and mobile health. However, despite the great promise that health technology holds for promoting independent living among older people, its actual implementation remains challenging. Methods This study aimed to conduct an integrative systematic review of the research evidence on the factors that facilitate or hinder the adoption of different types of technology by older individuals with chronic diseases. For this purpose, four electronic databases (PsycArticles, Scopus, Web of Science and PubMed) were queried to search for indexed published studies. The methodological quality of the selected papers has been assessed using the Mixed Methods Appraisal Tool (MMAT). Results Twenty-nine articles were selected, including 6.213 adults aged 60 or older. The studies have been synthesised considering the types of technological interventions and chronic diseases, as well as the main barriers and facilitators in technology acceptance. The results revealed that the majority of the selected articles focused on comorbid conditions and the utilisation of telemedicine tools. With regard to hindering and facilitating factors, five main domains were identified: demographic and socioeconomic, health-related, dispositional, technology-related and social factors. Conclusion The study results have practical implications not only for technology developers but also for all the social actors involved in the design and implementation of healthcare technologies, including formal and informal caregivers and policy stakeholders. These actors could use this work to enhance their understanding of the utilisation of technology by the ageing population. This review emphasises the factors that facilitate technology adoption and identifies barriers that impede it, with the ultimate goal of promoting health and independent living. Supplementary Information The online version contains supplementary material available at 10.1186/s12889-024-18036-5.",CS,AI_ML,0.85,Extracted from log - paper 298
Factors that affect health information technology adoption by seniors: A systematic review.,"The number of seniors and prevalence of chronic conditions are increasing worldwide, resulting in more pressure on health systems. Health Information Technologies (HIT) present opportunities to support the healthcare needs of seniors. Although prior studies have investigated HIT and seniors, it remains unclear what factors significantly affect the adoption of different HIT by elderly people in the community. A Systematic Review (SR) was conducted between December 2017 and February 2018 (with a search update in 2018-2019) to critically appraise and synthesise existing evidence on HIT adoption factors among seniors. Following the PRISMA guidelines, five major databases were consulted (PubMed, Medline, CINAHL, Scopus and Web of Science). The inclusion criteria consisted of empirical studies, published in English, and reporting impacts of specific factors on HIT adoption among seniors in the community. A total of 41 studies were included in this review, mostly published between 2014 and 2017 in Europe and the US; the level of evidence in these studies was low to moderate. The factors that affect HIT adoption did not differ across types of technologies or age groups. The findings reveal that seniors adopt HIT that are perceived as useful and requiring low effort commitment; price/cost value were reported as adoption barriers. Social influence, facilitating conditions, senior-friendly product design, self-efficacy, Intrinsic Technology Quality, experience/training and technology anxiety may affect HIT adoption by seniors, although the evidence on these impacts remains weak and limited. Mixed and inconclusive evidence was observed on the impacts of socio-demographic variables, health condition, habit and privacy/security. Given the reported low level of HIT adoption among seniors, we call for more rigorous research in this area using a 'senior-centred' approach, which takes into account the discourse/interaction between seniors and their collective environment to better understand the factors that affect their technology adoption and address their needs.",CS,AI_ML,0.85,Extracted from log - paper 299
Implementing Vertical Integration in the Industry 4.0 Journey: Which Factors Influence the Process of Information Systems Adoption?,"One of the key principles of Industry 4.0 is the implementation of vertical integration, which considers the integration of information systems from different hierarchical levels in a company to support decision-making with real-time data flows. Companies face challenges when they want to implement vertical integration, which is not trivial due to the risks inherent to the decision stages of adoption. We investigate the main factors influencing the different stages of adoption of vertical integration to provide a clearer view of what managers should consider at each stage. We adopt a multi-case study approach based on the investigation of ten companies that followed this adoption process. We develop a framework with 22 factors deployed in the three stages of decision (knowledge, persuasion, and final decision) and three main dimensions of analysis: technology, organization, and environment. We analyze the potential tensions between these factors and show how managers should balance such factors during the decision stages.",CS,AI_ML,0.85,Extracted from log - paper 300
"Adoption, Adaptation, Use and Impact of Information Systems during Pandemic Time and Beyond: Research and Managerial Implications","ABSTRACT The COVID-19 pandemic has increased use of technology in our daily and work lives, through digital transformation which has both challenges and opportunities. This brief commentary highlights some of the issues, faced during the pandemic time, that require empirical research and hold strong managerial implications. As such, we identify research implications involving adoption, adaptation, use, and impact of information systems and technology during the COVID-19 period and beyond.",CS,AI_ML,0.85,Extracted from log - paper 301
Adoption of health information systems: Health professionals perspective,"ABSTRACT The healthcare industry has embraced Information and Communication Technology in the performance of its duties to provide quality healthcare services. Though this system provides numerous benefits, research has shown that adoption and use of such systems are limited in the developing world. This study examines factors that will motivate healthcare personnel to adopt and use Health Information Systems using the Unified Theory of Acceptance and Use of Technology (UTAUT2) model. One hundred and ten (110) respondents were selected for the study. The data collected was analysed using Partial Least Square (PLS) with a standard error of 0.10. The findings indicate that Performance Expectancy, Habit and Hedonic Motivation are the main factors users considered to influence their behavioural intention to adopt and use health information system. The results show that when users perceive Health Information systems to improve their performance, they are more likely to adopt and use the system. Stakeholders should consider system quality issues, such as response time and interface design, when developing a new system as these serve as motivational factors towards adoption and use of health information system. This study is among the first few attempts to understand adoption and use of health information systems in Sub-Saharan Africa.",CS,AI_ML,0.85,Extracted from log - paper 302
Health Information Technology Adoption in the Emergency Department.,"OBJECTIVE To describe the trend in health information technology (IT) systems adoption in hospital emergency departments (EDs) and its effect on ED efficiency and resource use. DATA SOURCES 2007-2010 National Hospital Ambulatory Medical Care Survey - ED Component. STUDY DESIGN We assessed changes in the percent of visits to EDs with health IT capability and the estimated effect on waiting time to see a provider, visit length, and resource use. PRINCIPAL FINDINGS The percent of ED visits that took place in an ED with at least a basic health IT or an advanced IT system increased from 25.2 and 3.1 percent in 2007 to 69.1 and 30.6 percent in 2010, respectively (p < .05). Controlling for ED fixed effects, waiting times were reduced by 6.0 minutes in advanced IT-equipped EDs (p < .05), and the number of tests ordered increased by 9 percent (p < .01). In models using a 1-year lag, advanced systems also showed an increase in the number of medications and images ordered per visit. CONCLUSIONS Almost a third of visits now occur in EDs with advanced IT capability. While advanced IT adoption may decrease wait times, resource use during ED visits may also increase depending on how long the system has been in place. We were not able to determine if these changes indicated more appropriate care.",CS,AI_ML,0.85,Extracted from log - paper 303
Information Seeking Behavior and Technology Adoption: Theories and Trends,"With the increasingly complex and ubiquitous data available through modern technology, digital information is being utilized daily by academics and professionals of all disciplines and career paths. Information Seeking Behavior and Technology Adoption: Theories and Trends brings together the many theories and meta-theories that make information science relevant across different disciplines. Highlighting theories that had their base in the early days of text-based information and expanding to the digitization of the Internet, this book is an essential reference source for those involved in the education and training of the next-generation of information science professionals, as well as those who are currently working on the design and development of our current information products, systems, and services.",CS,AI_ML,0.85,Extracted from log - paper 304
A framework development for the adoption of information and communication technology web technologies in higher education systems,"Background: The adoption of information and communication technology (ICT) tools into educational systems has been at the forefront of the educational sector for decades. The integration of Web 2.0 and Web 3.0 technologies is progressively being encouraged worldwide across several universities to support teaching and learning processes and to offer students the possibility of learning experiences and engagements to suit their digital needs. Objectives: This article probes a framework development for the adoption of ICT web technologies in higher education systems (HES) and further suggests a framework for adoption with the aim of enhancing the mode of education delivery and improving business processes. An understanding of the benefits associated with Web 2.0 and Web 3.0 tools adoption is gained to support collaboration between students and educators and to build social presence through interactive learning. South African universities continue to experience circumstances in which many learners who enrol are novice users of Web 2.0 and Web 3.0 tools and require optimal support to bridge the gaps and the knowledge and skills exposure required. The problem with educators’ inability to incorporate Web 2.0 and Web 3.0 tools in their teaching and learning practices exists. Method: A mixed-method approach was applied in this study. The researchers conducted 15 separate interviews with educators coupled with randomly distributed questionnaires to students across three universities (North-West University [NWU], University of South Africa [UNISA] and University of Pretoria [UP]), a total of 969 was recoverable and analysed using analytical tool ATLAS.ti and SPSS. The researchers further validated the data consolidating both techniques used to generate a holistic assessment of the data analysed from the quantitative to support the qualitative findings. Results: Findings revealed that these tools are useful and will have a positive effect on the pedagogical environment, although there are challenges that may be considered during the adoption. These challenges relate to human factors (e.g. technophobia and cultural beliefs), security issues (e.g. privacy and intellectual property rights [IPRs]), ethical and legal issues, ICT infrastructures (e.g. cost implication, risk and ICT teaching facilities); and university policy frameworks. Conclusion: Despite these challenges, Web 2.0 and Web 3.0 technologies in HES offer varieties of teaching and learning platforms and an improved business administration process.",CS,AI_ML,0.85,Extracted from log - paper 305
The Success of Information Systems and Sustainable Information Society: Measuring the Implementation of a Village Financial System,"The purpose of this study is to advance the information society literature research by examining and developing the adoption of information systems within the Village Financial System (SISKEUDES) to improve the sustainable information society (SIS). The models include the DeLone and McLean model and trust theory, which involves eight variables: system information quality, information quality, service quality, trust in government organizations, trust in technology, usage, user satisfaction, net benefits, and sustainable information society. A survey questionnaire was used, and data was collected from SISKEUDES users in Bali, Indonesia, which were statistically analyzed using Partial Least Square (PLS) to understand the phenomena of Information System (IS) adoption and sustainable information society. The research findings reveal that system information quality, information quality, and trust in technology have a significant impact on usage and user satisfaction, whereas service quality and trust in government organizations do not have such an effect. The usage and user satisfaction variables have a significant effect on net benefits, and they have a significant effect on the sustainability of the SIS. This study’s findings can provide e-government practitioners with deeper insights into how to overcome problems with user satisfaction and increase trust in mandatory e-government services in realizing SIS and the “smart village”.",CS,AI_ML,0.85,Extracted from log - paper 306
A Systematic Review of Digital Technology Adoption in Off-Site Construction: Current Status and Future Direction towards Industry 4.0,"Off-site construction (OSC) is known as an efficient construction method that could save time and cost, reduce waste of resources, and improve the overall productivity of projects. Coupled with digital technologies associated with the Industry 4.0 concept, OSC can offer a higher rate of productivity and safety. While there is a rich literature focusing on both OSC and Industry 4.0, the implementation of associated digital technologies in the OSC context has not been fully evaluated. This paper intends to evaluate the current literature of digital technology applications in OSC. Scientometric analyses and a systematic review were carried out evaluating fifteen typical digital technologies adopted by OSC projects, including building information modelling (BIM), radio frequency identification devices (RFID), global positioning systems (GPS), the Internet of Things (IoT), geographic information systems (GIS), sensors, augmented reality (AR), virtual reality (VR), photogrammetry, laser scanning, artificial intelligence (AI), 3D printing, robotics, big data, and blockchain. This review formulates a clear picture of the current practice of these digital technologies and summarizes the main area of application and limitations of each technology when utilized in OSC. The review also points out their potential and how they can be better adopted to improve OSC practice in the future.",CS,AI_ML,0.85,Extracted from log - paper 307
Investigating the Key Factors Influencing on Management Information Systems Adoption among Telecommunication Companies in Yemen: The Conceptual Framework Development,"Organizations invest in information technology and systems because they provide economic value to the business. While recognizing the importance of the management information systems in the organization, the majority of Arab countries in Middle East and underdeveloped countries are still dealing with issues in adopting the technologies. Research in technology adoption among the Middle East countries more focused on internet banking, e-government and e-commerce area. Notwithstanding that individuals in the Middle East are known as late adopter, organizations in Middle East are unable to infuse management system into their business processes. To fully adopt the management information systems, organization must understand the three dimensions of information system; organization, people and technology itself. Thus, this study aims to shed some light on the management information systems adoption among the employees at telecommunication companies in Yemen. The study of the country is still inadequate and less exposure. Hence, the purpose of this study is to provide a conceptual framework for analyzing the factors affecting management information systems adoption. As the organization responsible for providing internet and mobile phones, telecommunication companies should be the first to adopt the latest technology in the market to better provide good infrastructures. In accordance, this study may help managers and policy makers identify the factors in their effort to attract more employees and to adopt the management information systems in the organization.",CS,AI_ML,0.85,Extracted from log - paper 308
Cloud computing technology adoption: an evaluation of key factors in local governments,"PurposeThe significance of cloud services in information technology (IT) is increasing as a means of achieving enhanced productivity, efficiency and cost reduction. Through cloud-based service, the reliability and scalability of an organization’s systems can be enhanced since organizations such as local governments are able to concentrate on their main business strategies. This research seeks to identify critical factors that may have an impact on the acceptance of cloud-based services, where the organizational context is based on local governments in Australia.Design/methodology/approachTo formulate a more comprehensive IT innovation adoption model for cloud technology, factors from the technology-organizational-environment framework, desires framework and diffusion of innovation model were integrated. Data was obtained from 480 IT staff working in 47 local government organizations.FindingsThe research results show that the factors which had a statistically significant and positive impact on the adoption of cloud-based services in local governments were compatibility, complexity, cost, security concerns, expected benefits and organization size. It is likely that the outcomes from this research will provide insights to any organization seeking to make investment decisions on the adoption of cloud-based services.Research limitations/implicationsLimitations include generalizability of the findings since the data is restricted to local government areas in Queensland, Australia. Further, the sample mostly included individuals with managerial positions and may not completely capture the cloud adoption factors relevant for front line IT employees. Another limitation is the possible omission of factors that may be relevant but not considered due to the selected theories. Lastly, this research did not differentiate between different types of cloud adoption such as private, public, community and hybrid models that are possible in this context.Originality/valueThe paper provides a combination framework of cloud-based service adoption based on a literature review on cloud adoption from an IS perspective. It adapts integrated model to establish a more comprehensive innovation adoption framework for cloud technology.",CS,AI_ML,0.85,Extracted from log - paper 309
An Empirical Study of Factors Influencing Accounting Information Systems Adoption,"This study investigates the factors that influence accounting information systems (AIS) adoption among accountants. Drawing on the unified theory of acceptance and use of technology (UTAUT), the task–technology fit (TTF) model and the institutional theory, we developed a research model for AIS adoption by accountants. Data was collected from 216 accountants and multiple linear regression was employed to test the research model. The results showed that five key factors, namely effort expectancy, perceived technology fit, facilitating conditions, self-efficacy and coercive pressure are able to influence the likelihood that accountants would adopt the AIS. This research confirms the need to integrate UTAUT, TTF and institutional theory when studying AIS adoption factors. The findings from this study are useful for senior management, technology consultants, software vendors and accounting professional bodies in promoting the adoption of AIS.",CS,AI_ML,0.85,Extracted from log - paper 310
The Supply Chain Has No Clothes: Technology Adoption of Blockchain for Supply Chain Transparency,"Blockchain technology, popularized by Bitcoin cryptocurrency, is characterized as an open-source, decentralized, distributed database for storing transaction information. Rather than relying on centralized intermediaries (e.g., banks) this technology allows two parties to transact directly using duplicate, linked ledgers called blockchains. This makes transactions considerably more transparent than those provided by centralized systems. As a result, transactions are executed without relying on explicit trust [of a third party], but on the distributed trust based on the consensus of the network (i.e., other blockchain users). Applying this technology to improve supply chain transparency has many possibilities. Every product has a long and storied history. However, much of this history is presently obscured. Often, when negative practices are exposed, they quickly escalate to scandalous, and financially crippling proportions. There are many recent examples, such as the exposure of child labor upstream in the manufacturing process and the unethical use of rainforest resources. Blockchain may bring supply chain transparency to a new level, but presently academic and managerial adoption of blockchain technologies is limited by our understanding. To address this issue, this research uses the Unified Theory of Acceptance and Use of Technology (UTAUT) and the concept of technology innovation adoption as a foundational framework for supply chain traceability. A conceptual model is developed and the research culminates with supply chain implications of blockchain that are inspired by theory and literature review.",CS,AI_ML,0.85,Extracted from log - paper 311
A Model for Examining Challenges and Opportunities in Use of Cloud Computing for Health Information Systems,"Health Information Systems (HIS) are becoming crucial for health providers, not only for keeping Electronic Health Records (EHR) but also because of the features they provide that can be lifesaving, thanks to the advances in Information Technology (IT). These advancements have led to increasing demands for additional features to these systems to improve their intelligence, reliability, and availability. All these features may be provisioned through the use of cloud computing in HIS. This study arrives at three dimensions pertinent to adoption of cloud computing in HIS through extensive interviews with experts, professional expertise and knowledge of one of the authors working in this area, and review of academic and practitioner literature. These dimensions are financial performance and cost; IT operational excellence and DevOps; and security, governance, and compliance. Challenges and drivers in each of these dimensions are detailed and operationalized to arrive at a model for HIS adoption. This proposed model detailed in this study can be employed by executive management of health organizations, especially senior clinical management positions like Chief Technology Officers (CTOs), Chief Information Officers (CIOs), and IT managers to make an informed decision on adoption of cloud computing for HIS. Use of cloud computing to support operational and financial excellence of healthcare organizations has already made some headway in the industry, and its use in HIS would be a natural next step. However, due to the mission′s critical nature and sensitivity of information stored in HIS, the move may need to be evaluated in a holistic fashion that can be aided by the proposed dimensions and the model. The study also identifies some issues and directions for future research for cloud computing adoption in the context of HIS.",CS,AI_ML,0.85,Extracted from log - paper 312
Mobile health technology adoption across generations: Narrowing the digital divide,"Mobile health (m‐health) technologies offer many benefits to individuals, organizations, and health professionals alike. Indeed, the utilization of m‐health by older adults can foster the development of proactive patients, while also reducing financial burden and resource pressures on health systems. However, the potentially transformative influence of m‐health is limited, as many older adults resist adoption leading to the emergence of an age‐based digital divide. This study leverages protection motivation theory and social cognitive theory to explore the factors driving resistance among older adults. This mixed methods study integrates survey findings with insights from qualitative interviews to highlight that the m‐health digital divide is deepening due to older adults' perceived inability to adopt and their unwillingness to adopt stemming from mistrust, high risk perceptions, and strong desire for privacy. The paper contributes to the privacy and social inclusion literature by demonstrating that while many older adults have access to m‐health, they are currently excluded and require careful consideration by technology organizations and researchers. The study provides recommendations for narrowing the m‐health digital divide through inclusive design and educational efforts to improve self‐efficacy, develop privacy literacy, and build trust, thereby ensuring that older citizens are both capable and willing to adopt.",CS,AI_ML,0.85,Extracted from log - paper 313
Review of Technology Adoption Models and Theories to Measure Readiness and Acceptable Use of Technology in a Business Organization,"It has been evident that innovative technologies have evolved in the past few decades, notably, in the computing discipline. As a result, technology adoption models and theories are deemed vital to business organizations to assess if end users are ready and accept new technologies. Therefore, requirements to review prevailing technology adoption models and theories which seek to assist business organizations to assess the readiness and acceptance of these new technologies, otherwise, such initiatives become obsolete and do not support core business processes, as it was originally intended. This paper reviews three technology adoption models relevant to information systems and information technology studies on the latest technologies such as ERP, cloud computing, and other systems implemented in business organizations. The study could assist to analyse the acceptance and utilization of new technologies. It can also be used to improve such models and theories in the adoption of new technologies.",CS,AI_ML,0.85,Extracted from log - paper 314
Advanced manufacturing technology adoption and performance: the role of management information systems departments,This study uses information obtained from the advanced manufacturing technology (AMT) literature to develop a conceptual framework that seeks to illustrate the impact of the management information systems (MIS) department on the different facets of AMT adoption. A detailed survey instrument was administered to a cross‐section of manufacturing firms in the USA to collect the data required to test five hypotheses relating to the efficacy of this framework. The results of this study indicate that the proposed framework is particularly useful in explaining the role of MIS departments in firms that are attempting to integrate advanced process and information technologies. This finding and other results of this study and their implications are discussed.,CS,AI_ML,0.85,Extracted from log - paper 315
"Information Systems, Technology Adoption and Innovation Translation","A new Information System must be adopted before it can be used. New Information Systems can be seen as innovations and viewed through the lens of innovations theory. In this article I will argue that much Information Systems research falls into this category, and that its explanation could benefit from application of innovation theory. One of the difficulties facing any such investigation, however, is that not all of these innovations are adopted in the form in which they were proposed – not all are adopted without change, and many approaches to technological innovation have trouble in explaining this. I will propose that Innovation Translation, informed by Actor-Network Theory, can offer a useful way of examining technological innovations, and particularly in explaining adoptions involving only some parts of the innovation. In this article I give several examples of how I have used Innovation Translation to shed light onto various technological innovations.",CS,AI_ML,0.85,Extracted from log - paper 316
What Drives the Adoption of the Blockchain Technology? A Fit-Viability Perspective,"ABSTRACT Blockchain technology has the promise of transforming security and trust in digital transactions. However, concerns about technical complexity and the benefits of deployment have blunted its adoption. We examine factors that influence managerial intention to adopt blockchain technology. We extend the fit-viability model (FVM) and develop a value-based technology adoption model through an empirical study of 242 managers mostly in medical and financial industries. Managers in such organizations are likely to consider fit and viability in adopting blockchain technology to store and protect data. Drawing upon Fit-Viability and Task-Technology Fit models, and the Unified Theory of Acceptance and Use of Technology (UTAUT), we test a model with Partial Least Squares (PLS) to assess managers’ intention to adopt blockchain technology. Our findings indicate that functional and symbolic benefits have positive impact on managers’ assessment of task-technology fit. Furthermore, viability is an important criterion in adopting blockchain technology.",CS,AI_ML,0.85,Extracted from log - paper 317
Adoption of Hospital Information System Among Nurses: a Technology Acceptance Model Approach,"Introduction: The successful implementation of Hospital Information Systems (HIS) depends on user acceptance. Nurses are the largest group of HIS users in hospitals. This study aims to evaluate some factors may affect the utilization of the Hospital Information System. Aim: To explore factors that contribute to using of Hospital Information System. Methods: In this cross-sectional study, 325 nurses from training Hospitals affiliated with Lorestan University of Medical Sciences (LUMS) were chosen. A valid and reliable structured questionnaire based on Technology Acceptance Model 1&2 and Unified Theory of Acceptance and Use of Technology was used as the data collection tool. Descriptive statistics, Correlations analysis, multiple regression analysis, path analysis technique, Structure Equation Model using AMOS software was used to examine factors that influenced the Adoption of Hospital Information System. Results: The findings indicate a significant direct relationship between Management Support and Perceived Usefulness of HIS. Perceived Usefulness has a significant effect on attitudes. While there was no significant effect of perceived ease of use on attitude. Attitude has a significant effect on behavioral intention. Conclusion: This research provides a tool to realize what factors undertake the behavioral intention of healthcare professionals to use hospital information system and how this may affect future use.",CS,AI_ML,0.85,Extracted from log - paper 318
Utilizing the Technology Acceptance Model to Assess the Employee Adoption of Information Systems Security Measures,"In this study, the factors that affect employee acceptance of information systems security measures were examined by extending the Technology Acceptance Model. Partial least squares structural equation modeling was applied to examine these factors. 174 valid responses from employees from companies in various industry segments in the United States and Canada were analyzed. The results of the statistical analysis indicate that subjective norm moderated by management support showed the strongest effect on intention to use information systems security measures.",CS,AI_ML,0.85,Extracted from log - paper 319
Exploring the motivators of technology adoption in healthcare,"ABSTRACT This paper seeks to determine the motivators that affect the adoption of information technology (IT), specifically in modern healthcare systems, based on a case study in the United Arab Emirates (UAE). The aim is to identify, examine, and place in a hierarchical structure, based on a two-step exploratory methodology, the motivators for IT adoption of four main categories of healthcare stakeholders (employees, patients, UAE citizens and residents, and accredited foresight experts) via an analytic hierarchy process (AHP) model. AHP revealed the relative importance weights of the seven IT adoption motivators and their 41 sub-motivators. Government support was found to be the most important group of motivators, followed by knowledge sharing, infrastructure, green management, lean management, internal/external environment, and social sustainability. The paper facilitates improving technology adoption generally, specifically in the UAE’s healthcare system. A Pareto chart visually presents the ranking of all sub-factors to help supply-chain practitioners better allocate resources for IT implementation. Little research has examined the combined perspectives of various stakeholders and no study has hitherto investigated the significant motivators of IT adoption in the UAEs’ healthcare industry. Including foresight experts is new in this domain.",CS,AI_ML,0.85,Extracted from log - paper 320
A Systematic Review of Social Media Acceptance From the Perspective of Educational and Information Systems Theories and Models,"The study of social media acceptance and adoption is not a new research topic. However, the analysis of the educational and information systems (IS) theories/models that are used to examine the social media acceptance and adoption is considered an important research direction. To examine these theories/models and provide researchers with a clear vision of this research topic, we should be aware of the leading educational and IS theories/models used in this line of research. To this end, this systematic review retrieved and analyzed 2,382 articles. The retrieved articles were then critically examined to meet the inclusion and exclusion criteria, in which 122 articles published between 2009 and 2018 were eventually selected for further critical analysis. The main findings indicated that the uses and gratifications theory (U&G) and the social constructivism theory were considered the most widely used educational theories in social media. Besides, the technology acceptance model (TAM) and the unified theory of acceptance and use of technology (UTAUT) were considered the most extensively used IS models in studying the social media acceptance and adoption. These results afford a better understanding of social media studies related to the educational and IS theories/models and form a constructive reference for future research.",CS,AI_ML,0.85,Extracted from log - paper 321
Exploring information technology adoption in the classroom: case of online learning technology,Information technology and information systems (IT/IS) have been widely adopted in the US educational institutions during the last three decades. Recently the internet-based educational technology has emerged and become an important education tool. WebCT/blackboard is an online learning system implemented by Portland State University (PSU). The purpose of this study is to empirically investigate the factors influencing instructors' use of WebCT/blackboard online learning system. The data is based on a survey conducted in spring 2008 by the Advisory Committee on Academic Information Technologies (ACAIT) of PSU. An ordinal logistic regression model is formulated to analyse the factors' influence. Results show that user training (UT) and user satisfaction (US) are the significant factors influencing instructor's intention to use WebCT/blackboard system.,CS,AI_ML,0.85,Extracted from log - paper 322
The impact of information technology and communication systems on the agility of supply chain management systems,"PurposeThe impact of information technology (IT) on the agility of supply chain management (SCM) systems is very noticeable in the business world nowadays. Competition and constant changes, including product/technological innovations, decreasing product lifestyles and product proliferation, create pressure that affects the business environment. Organizations are required for answering the changes in the market to gain a competitive advantage and business success. The organizations are able to answer to unexpected market changes through supply chain market, and these changes are converted to business opportunities. Using IT to achieve the agility of SCM is one of the important factors to help the organizations. Therefore, the adoption of IT and its efficient implementation can improve the cooperation between supply chain agility through the rapid transfer, the distribution of accurate information and the use of information. This paper aims to investigate the impact of IT on the agility of SCM.Design/methodology/approachA total of 120 employees of the Golasal firm are involved in collecting data using a questionnaire. Measurements were performed in all questionnaires using a five-point Likert scale. The causal model is evaluated by structural equationmodeling technique, which is used to examine the reliability and validity of the model.FindingsThe results have shown that IT has positive influences on the agility of SCM systems. In addition, the obtained results have shown that four variables, namely, IT skills and knowledge, IT-based systems integration, IT infrastructure and design of global position system and geographic information systems, affect the agility of SCM systems.Originality/valueIn this paper, the agility of SCM systems is pointed out and the approach to resolve the problem is applied into a practical example. The presented model provides a complete framework to examine the impact of IT on the agility of SCM systems.",CS,AI_ML,0.85,Extracted from log - paper 323
Ontology Middleware for Integration of IoT Healthcare Information Systems in EHR Systems,"Healthcare sectors have been at the forefront of the adoption and use of IoT technologies for efficient healthcare diagnosis and treatment. Because healthcare IoT sensor technology obtains health-related data from patients, it needs to be integrated with the electronic healthcare records (EHR) system. Most EHR systems have not been designed for integration with IoT technology; they have been designed to be more patient-centric management systems. The use of the IoT in EHR remains a long-term goal. Configuring IoT in EHR can enhance patient healthcare, enabling health providers to monitor their patients outside of the clinic. To assist physicians to access data resources efficiently, a data model that is semantic and flexible is needed to connect EHR data and IoT data that may help to provide true interoperability and integration. This research proposes a semantic middleware that exploits ontology to support the semantic integration and functional collaborations between IoT healthcare Information Systems and EHR systems.",CS,AI_ML,0.85,Extracted from log - paper 324
Specialized Information Systems for the Digitally Disadvantaged,"A number of specialized information systems for the digitally disadvantaged (SISD) have been developed to offset the limitations of people less able to participate in the information society. However, contributions from social identity theory and social markedness theory indicate that SISD can activate a stigmatized identity and thus be perceived unfavorably by their target audience. We identify two mechanisms by which functional limitations affect a digitally disadvantaged person’s adoption decision: (1) adoption decision as shaped through technology perceptions (i.e., perceived usefulness, perceived ease of use, and perceived access barriers), and (2) adoption decision as shaped through marked status awareness (i.e., stigma consciousness). We test our contextualized research model on digitally disadvantaged users with physical and/or sensory disabilities. Results of our mediation analysis show that the individuals who have the most to gain from SISD use (i.e., those with greater perceived functional limitations) are doubly disadvantaged: as a group, they find it more challenging to use SISD and are also more sensitive to the fear of being marked as disadvantaged or vulnerable.",CS,AI_ML,0.85,Extracted from log - paper 325
Adoption and Use of Learning Management Systems in Education: The Role of Playfulness and Self-Management,"This article investigates the factors affecting primary and secondary education teachers’ behavioral intention to adopt learning management systems (LMSs). Information technology (IT) innovations have the power to change the way we work, educate, learn, and basically the way we live. The effect of IT innovations on education makes it critical to understand the current usage situation of LMSs and the factors affecting their adoption by teachers. The unified theory of acceptance and use of technology (UTAUT) was extended with factors from education and game-based learning literature. In order to see the effect of individual- and organizational-level characteristics, multi-group structural equation modeling (SEM) analysis was conducted and discrepancies in relationships were reported. Evaluation of users and non-users and teachers of different fields were also compared to each other. The findings of this study not only contribute to theory through the development and testing of a thorough model relating technology features and individual characteristics to behavioral intention to use, but also offer strong implications for practitioners who would like to increase LMS usage and create a more effective learning environment.",CS,AI_ML,0.85,Extracted from log - paper 326
B2B technology adoption in customer driven supply chains,"Purpose – The purpose of this article is to develop and propose a comprehensive framework that identifies the factors that influence a company’s decision to adopt business to business (B2B) technologies. Design/methodology/approach – The authors review the literature regarding technology adoption from multiple disciplines including: Supply Chain Management, Logistics, Sociology, Information Systems, Marketing and Economics. A synthesis of the review provides the foundation for developing a comprehensive model of inter-firm technology adoption. Findings – The review and synthesis finds inconsistencies in the theoretical models and constructs used in previous studies of inter-firm technology adoption. The comprehensive framework presented identifies four major categories of antecedents to technology adoption: characteristics of a technology, organizational factors, external factors and relationships. The presented model focuses attention on the inclusion of relational factors that affect the adoption of B2B...",CS,AI_ML,0.85,Extracted from log - paper 327
Blockchain technology and its applications in digital accounting systems: insights from Jordanian context,"Purpose The recent progress of digital accounting has significantly affected businesses’ sustainable production process. Businesses generally use digital accounting applications to automate their operational procedures and increase their corporate efficiencies through improved output quality and sustainability. Consequently, the purpose of this study is to look into the antecedent factors that directly and indirectly influence blockchain technology adoption in the context of digital accounting systems. Design/methodology/approach The data of the current study were obtained from 346 accountants working in information technology companies. Partial least squares structural equation modeling was used to test the research proposal model. Findings The empirical results confirmed that the adoption of blockchain technology is most considerably impacted by perceived usefulness, whereby it was also revealed that perceived ease of use has a direct and indirect effect on blockchain technology adoption. Originality/value According to the researchers’ knowledge, this study addresses a vital research gap in the literature by suggesting a comprehensive research model that can help garner enhanced usage of blockchain technology and its implications in digital accounting systems in the Jordanian context.",CS,AI_ML,0.85,Extracted from log - paper 328
"Smart Farming Technology Trends: Economic and Environmental Effects, Labor Impact, and Adoption Readiness","Farming faces challenges that increase the adverse effects on farms’ economics, labor, and the environment. Smart farming technologies (SFTs) are expected to assist in reverting this situation. In this work, 1064 SFTs were derived from scientific papers, research projects, and industrial products. They were classified by technology readiness level (TRL), typology, and field operation, and they were assessed for their economic, environmental, and labor impact, as well as their adoption readiness from end-users. It was shown that scientific articles dealt with SFTs of lower TRL than research projects. In scientific articles, researchers investigated mostly recording technologies, while, in research projects, they focused primarily on farm management information systems and robotic/automation systems. Scouting technologies were the main SFT type in scientific papers and research projects, but variable rate application technologies were mostly located in commercial products. In scientific papers, there was limited analysis of economic, environmental, and labor impact of the SFTs under investigation, while, in research projects, these impacts were studied thoroughly. Further, in commercial SFTs, the focus was on economic impact and less on labor and environmental issues. With respect to adoption readiness, it was found that all of the factors to facilitate SFT adoption became more positive moving from SFTs in scientific papers to fully functional commercial SFTs, indicating that SFTs reach the market when most of these factors are addressed for the benefit of the farmers. This SFT analysis is expected to inform researchers on adapting their research, as well as help policy-makers adjust their strategy toward digitized agriculture adoption and farmers with the current situation and future trends of SFTs.",CS,AI_ML,0.85,Extracted from log - paper 329
Determinants of information technology adoption: an extension of existing models to firms in a developing country,"Advances in new information technology and changes in the global environment have made it increasingly difficult for organizations to make decisions regarding information technology adoption. Moreover, information systems in a global environment are influenced by different cultures, laws, information technology infrastructure, and the availability and role of skilled personnel. Information systems research has traditionally focused on organizations in US and UK without considering how these frameworks and models can be applied and extended to developing countries. In this study of 46 firms we examine the determinants of process-based information technology adoption in the Indian manufacturing sector. Although there are many differences like the type of organizations, and the technology available, between developing and developed countries, we found that factors that influence information technology adoption are similar. Our results showed that organizational factors like a firmâ€™s culture and size, and environmental factors like competition faced by firms, government policies, and market forces like exchange rates and computer prices, have a significant impact on information technology adoption decisions made by firms. We also found that the role of management information systems personnel has a negative impact on adoption.",CS,AI_ML,0.85,Extracted from log - paper 330
"Technology Acceptance Model' Concepts, Contribution, Limitation, and Adoption in Education","Numerous models have been provided by researchers in the past to determine and assess the success of Information Systems. Every model has been evaluated. Consequently, each model has pros and cons related to it. Here we are studying the Technology acceptance Models (TAM) and debating their features, contributions, and the limits of the earlier version of this theory. The TAM model, created by Davis (1989), is applied to determine the acceptance, adoption, and utilization of information technology. It is prevalent two constructs are used in TAM, perceived ease of use and perceived usefulness. The TAM model gained extensive popularity between the researchers, and it is among the most effective models. TAM is dissimilar to other models as it does not measure success. However, it is employed to investigate and predict the users' intention to use Information technology. Since technology acceptance in an academic environment is becoming well-known, what had resulted in hubs of learning describes as an online learning community. Therefore, this paper gives a review of the present state of research on the Technology Acceptance Model. It offers a concise entry point to the theory's background and its adoption in education, which might be purposely advantageous for novice readers. The knowing of these models allows us to know the factors that impact the adoption of IS in the education environment to take advantage of this tool.",CS,AI_ML,0.85,Extracted from log - paper 331
Information technology adoption in health care: when organisations and technology collide.,"The implementation of advanced information systems is enabling great social and organisational changes. However, health care has been one of the slowest sectors to adopt and implement information technology (IT). This paper investigates why this is so, reviewing innovation diffusion theory and its application to both health organisations and information technology. Innovation diffusion theory identifies variables that influence the 'innovativeness' of organisations and the rate at which a technology diffuses. When analysed, these variables show why IT implementation has progressed at a slower rate in health compared with other industry sectors. The complexity of health organisations and their fragmented internal structure constrain their ability to adopt organisation wide IT. This is further impacted upon by the relative immaturity of strategic health IT which is complicated and unable to show quantifiable benefits. Both organisational and technological factors lead to the slow adoption of strategic IT. On the other hand, localised IT solutions and those providing measurable cost reductions have diffused well.",CS,AI_ML,0.85,Extracted from log - paper 332
A Temporal Model of Institutional Interventions for Information Technology Adoption in Less-Developed Countries,"This empirical study of Internet adoption in four Latin American countries delineates a gradual but progressive course of institutional actions and suggests a temporal ordering of the actions--including knowledge building, subsidy, knowledge deployment, innovation directive, and standard setting. The temporal model reveals how each country sustained the momentum of its evolving strategy, grew in competence to forge technological solutions, and gained access to the Internet. The four countries' original goals changed, but through experience they perceived new opportunities and established evolving Internet strategies that form the bases of new technological services provided at the national level.",CS,AI_ML,0.85,Extracted from log - paper 333
Organic pest management decisions: a systems approach to technology adoption,"Organic farmers make system‐level crop protection decisions that combine complementary insect, disease, nematode, and weed management strategies. Data from a 1997 national survey of U.S. organic farmers were used in a multivariate count data model to identify the farm and regional factors influencing the adoption of practices across the linked pest management categories. The results show that weed management requires the greatest management effort by organic farmers. More intensive information‐seeking and on‐farm experimentation, higher educational attainment, and intensity of commitment to organic farming are positively related to the number of weed control strategies adopted. Predictions of adoption based on this model and customized to farm and region specifications will give information providers lead time to develop technical support for reduced chemical pest management systems.",CS,AI_ML,0.85,Extracted from log - paper 334
Transfer and adoption of advanced information technology solutions in resource-poor environments: the case of telemedicine systems adoption in Ethiopia.,"The study of the adoption of information technology (IT) by individuals has taken two approaches, one emphasizing rationalistic goal-oriented behavior and the other focusing on poignant forces that influence an individual's reaction to a new IT. These approaches are not necessarily mutually exclusive. Individuals' acceptance and subsequent usage of a new IT is predicated on both. Additionally, the tendency in past studies has been to examine either the rational or the poignant factors in the context of a ""resource-rich"" environment-one in which there is an abundance of IT, adequate infrastructure, and a high level of acculturation to technology solutions. Consequently, there is a clear need for the examination of these factors in resource-poor environments, where assumptions on technology abundance and technology culturation do not hold. We empirically test a model that explains the intention of physicians in a resource-poor environment (epitomized by rural Ethiopia) to adopt telemedicine systems. This model integrates the rational factors driving goal-oriented behavior with the poignant/emotive factors that are an innate part of each adopter's reaction to the new technology. We use the model to expose salient contextual factors that explain the acceptance behavior of individuals toward complex information and communications technology (ICT) solutions and implications of these on the management of technology transfer initiatives in a resource-poor environment. The model is parsimonious, yet explains 28% of the variance in the intention to adopt telemedicine systems and 58% in perceived ease of use. The theoretical and practical implications of this model are discussed. Namely, Sub-Saharan African, in general, and Ethiopian culture, in particular, plays an integral role in the adoption of ICT solutions. Organizational positions and roles among physicians, clinical professionals, and superiors stand to impact the adoption of telemedicine and other healthcare applications. Last, the degree to which users perceive that ICT is easy to use (i.e., ease of use) can be a function of technology experience and can influence perceived usefulness on behalf of users and healthcare organizations.",CS,AI_ML,0.85,Extracted from log - paper 335
Enhancing the Effectiveness of Consumer-Focused Health Information Technology Systems Through eHealth Literacy: A Framework for Understanding Users' Needs,"Background eHealth systems and applications are increasingly focused on supporting consumers to directly engage with and use health care services. Involving end users in the design of these systems is critical to ensure a generation of usable and effective eHealth products and systems. Often the end users engaged for these participatory design processes are not actual representatives of the general population, and developers may have limited understanding about how well they might represent the full range of intended users of the eHealth products. As a consequence, resulting information technology (IT) designs may not accommodate the needs, skills, cognitive capacities, and/or contexts of use of the intended broader population of health consumers. This may result in challenges for consumers who use the health IT systems, and could lead to limitations in adoption if the diversity of user attributes has not been adequately considered by health IT designers. Objective The objective of this paper is to propose how users’ needs and competences can be taken into account when designing new information and communications technology solutions in health care by expanding the user-task-context matrix model with the domains of a new concept of eHealth literacy. Methods This approach expands an existing method for supporting health IT system development, which advocates use of a three-dimensional user-task-context matrix to comprehensively identify the users of health IT systems, and what their needs and requirements are under differing contexts of use. The extension of this model involved including knowledge about users’ competences within the seven domains of eHealth literacy, which had been identified based on systematic engagement with computer scientists, academics, health professionals, and patients recruited from various patient organizations and primary care. A concept map was constructed based on a structured brainstorm procedure, card sorting, and computational analysis. Results The new eHealth literacy concept (based on 7 domains) was incorporated as a key factor in expanding the user-task-context matrix to describe and qualify user requirements and understanding related to eHealth literacy. This resulted in an expanded framework and a five-step process, which can support health IT designers in understanding and more accurately addressing end-users’ needs, capabilities, and contexts to improve effectiveness and broader applicability of consumer-focused health IT systems. It is anticipated that the framework will also be useful for policy makers involved in the planning, procuring, and funding of eHealth infrastructure, applications, and services. Conclusions Developing effective eHealth products requires complete understanding of the end-users’ needs from multiple perspectives. In this paper, we have proposed and detailed a framework for modeling users’ needs for designing eHealth systems that merges prior work in development of a user-task-context matrix with the emerging area of eHealth literacy. This framework is intended to be used to guide design of eHealth technologies and to make requirements explicitly related to eHealth literacy, enabling a generation of well-targeted, fit-for-purpose, equitable, and effective products and systems.",CS,AI_ML,0.85,Extracted from log - paper 336
The Social Component of Information Systems - How Sociability Contributes to Technology Acceptance,"Research Article Iris Junglas Florida State University iris.junglas@gmail.com Lakshmi Goel University of North Florida l.goel@unf.edu Chon Abraham College of William and Mary chon.abraham@mason.wm.edu Blake Ives University of Houston bives@mac.com The adoption of information systems is often explained in terms of usefulness and ease of use. Lately, researchers have begun to recognize that a hedonic streak in human beings provides a further contributing factor in the adoption and acceptance of information systems. Embedded in this streak is a broader social aspect that incorporates not only the solitary, individual pleasure one gets from using the system, but also a pleasure that one gets from interacting and socializing with others through the system. This becomes particularly evident in virtual environments that support high levels of interaction with others and with artifacts embedded in an immersive context. By drawing on IS theories of technology acceptance and IS success, and on theories of social interaction from evolutionary psychology, activity theory, situated action, and distributed cognition, we test the construct of sociability and its antecedents in Second Life—a popular virtual environment. Our results support that, in addition to an information and system component, a social component contributes to IS usage.",CS,AI_ML,0.85,Extracted from log - paper 337
Consumer Acceptance and Use of Information Technology: A Meta-Analytic Evaluation of UTAUT2,"Despite being regarded as the most comprehensive theory in understanding individual technology adoption – UTAUT2 theory with growing number of citations and impetus beyond IS domain face strong criticism on usage of the model in its entirety. This study located UTAUT2 based empirical studies in the Scopus and Web of Science bibliographic database through citied reference search in order to evaluate appropriate usage of UTAUT2 constructs. The meta-analysis results spanning across 60 studies with more than 122,000 cumulative observations found BI➔UB as the strongest path with all significant values. PE➔BI emerged as the most utilized path with most significant values underscoring the emphasis placed by consumers on utilitarian value. Meanwhile, with most non-significant path values the future usage of EE➔BI path is been cautioned and questioned. Finally, trust, personal innovativeness, perceived risk, attitude, and self-efficacy were found as the five topmost UTAUT2 extensions.",CS,AI_ML,0.85,Extracted from log - paper 338
Design Solutions for User-Centric Information Systems,"Continuous improvements in technological applications have allowed more opportunities to develop systems with user-focused designs. This not only leads to higher success in day-to-day usage, but it increases the overall probability of technology adoption. Design Solutions for User-Centric Information Systems provides a comprehensive examination of the latest strategies and methods for creating technological systems with end users as the focal point of the design process. Highlighting innovative practices and applications across a variety of areas, such as cloud-based computing services, e-government adoption, and logistics evaluation, this book is an ideal reference source for computer engineers, practitioners, project managers, graduate students, and researchers interested in the enhancement of user-centric information system development.",CS,AI_ML,0.85,Extracted from log - paper 339
An extension of the technology acceptance model for understanding travelers’ adoption of variable message signs,"Understanding travelers’ acceptance of Advanced Traveler Information Systems (ATIS) is crucial to the implementation of Intelligent Transportation Systems (ITS) capable of mitigating traffic congestion and improving network performance. This paper adopted an extended Technology Acceptance Model (TAM) to predict and explain road users’ intention to use Variable Message Sign (VMS) information. In addition to the traditional parsimonious TAM constructs (perceived usefulness, perceived ease of use and behavioral intention), the model examined the effects of attitude towards route diversion, familiarity with road network and information quality on road users’ acceptance of VMS. 762 drivers were interviewed and the obtained data were processed using Structural Equation Modeling. The results showed that travelers’ attitude towards route diversion had a positive effect on perceived usefulness and intention to use VMS. Information quality had a positive direct effect on perceived usefulness, perceived ease of use and attitude towards route diversion. Familiarity with the network had a positive effect on attitude towards route diversion and a negative effect on the perceived usefulness of VMS information. Perceived ease of use significantly and positively affected perceived usefulness and intention to use VMS. Perceived usefulness also had a positive effect on intention. Several academic and practical implications were also discussed.",CS,AI_ML,0.85,Extracted from log - paper 340
Development of an Instrument to Measure the Perceptions of Adopting an Information Technology Innovation,"This paper reports on the development of an instrument designed to measure the various perceptions that an individual may have of adopting an information technology IT innovation. This instrument is intended to be a tool for the study of the initial adoption and eventual diffusion of IT innovations within organizations. While the adoption of information technologies by individuals and organizations has been an area of substantial research interest since the early days of computerization, research efforts to date have led to mixed and inconclusive outcomes. The lack of a theoretical foundation for such research and inadequate definition and measurement of constructs have been identified as major causes for such outcomes. In a recent study examining the diffusion of new end-user IT, we decided to focus on measuring the potential adopters' perceptions of the technology. Measuring such perceptions has been termed a ""classic issue"" in the innovation diffusion literature, and a key to integrating the various findings of diffusion research. The perceptions of adopting were initially based on the five characteristics of innovations derived by Rogers 1983 from the diffusion of innovations literature, plus two developed specifically within this study. Of the existing scales for measuring these characteristics, very few had the requisite levels of validity and reliability. For this study, both newly created and existing items were placed in a common pool and subjected to four rounds of sorting by judges to establish which items should be in the various scales. The objective was to verify the convergent and discriminant validity of the scales by examining how the items were sorted into various construct categories. Analysis of inter-judge agreement about item placement identified both bad items as well as weaknesses in some of the constructs' original definitions. These were subsequently redefined. Scales for the resulting constructs were subjected to three separate field tests. Following the final test, the scales all demonstrated acceptable levels of reliability. Their validity was further checked using factor analysis, as well as conducting discriminant analysis comparing responses between adopters and nonadopters of the innovation. The result is a parsimonious, 38-item instrument comprising eight scales which provides a useful tool for the study of the initial adoption and diffusion of innovations. A short, 25 item, version of the instrument is also suggested.",CS,AI_ML,0.85,Extracted from log - paper 341
Towards a systemic model on information systems' adoption using critical systems thinking,"Purpose – This paper aims to propose an extended version of systems development life cycle (SDLC) based on critical systems thinking for information system (IS) adoption in an organizational context from a management perspective.Design/methodology/approach – The model integrates traditional SDLC with the ongoing process of “phase‐stakeholders‐identification” or “pha‐stak‐ification”. The emerging systemic stakeholder networks is proposed to be applied with network mechanisms to influence stakeholders' attitudes towards IS adoption. The authors, considering IS adoption as a multi‐phase innovation project, argue that boundary considerations using a multiple stakeholder perspective (boundary critique) provides an alternative focus for IS adoption.Findings – The study suggests that the proposed model has the capacity to serve as a roadmap for a smooth IS adoption by facilitating organizational learning and change.Research limitations/implications – The study has not been tested empirically.Originality/value – ...",CS,AI_ML,0.85,Extracted from log - paper 342
Information Systems and Healthcare XXXIII: An Institutional Theory Perspective on Physician Adoption of Electronic Health Records,"With the recent legislation providing financial incentives to physicians who acquire electronic health record systems, we will be afforded an opportunity to study incentivized adoption of technology coupled with the threat of future penalties for non-adoption. This research uses institutional theory to propose factors that are expected to influence the adoption of electronic health records (EHRs) by independent physician practices in the coming years. The study presents a model describing the role of coercive, mimetic, and normative forces on adoption intent. Payer incentives/penalties as well as dominant healthcare delivery partners will exert coercive pressures on physician practices. Additionally, since physicians identify with their own specialties, it is expected that they will also be subject to mimetic forces resulting from successful adoption by similar specialists, particularly given their concerns about expected benefits from these systems. Finally, normative forces resulting from the successful interoperation of electronic health records among regional providers should influence physician adoption. The ability to partner with other physicians and healthcare providers or vendors adopting the same system should increase individual practice adoption intent in the presence of coercive, mimetic, and/or normative forces.",CS,AI_ML,0.85,Extracted from log - paper 343
An extension of the technology acceptance model for business intelligence systems: project management maturity perspective,"Business intelligence systems (BISs) refer to a wide range of technologies and applications useful for retrieving and analyzing a large amount of information with the goal to generate knowledge useful for making effective business decision. In order to investigate adoption of BISs in companies, we propose a model based on the technology acceptance model (TAM) that is expanded by variables representing the concept of a project management maturity (PMM). The survey on the sample of USA companies has been conducted with the chief information officer (CIO) as the main informant. A structural equation model has been developed in order to test the research model. Results indicate that TAM expanded with the notion of PMM is useful in increasing understanding of BISs adoption in companies.",CS,AI_ML,0.85,Extracted from log - paper 344
"The role of UTAUT, DOI, perceived technology security and game elements in internet banking adoption","Purpose The increasing innovation and urgent need of up-to-date and convenient information systems have gained high importance in financial sector. Several banks have deployed internet banking in order to reduce cost while improving customer services. Therefore, the growth of internet banking is limited and in many cases fallen short of expectations. The purpose of this paper is to develop an integrated model that combines technology, innovative and environmental factors altogether in order to understand customer’s intention to adopt, and intention to recommend internet banking in social networks. Design/methodology/approach In all, 398 valid responses were collected from customers of commercial banks, using convenience sampling approach. Data were analyzed using the structural equation modeling. Findings The findings show that integrated model has good explanatory power (78.3 percent) to predict customer’s intention to adopt internet banking. Findings also revealed that the interaction effect of gamification between user’s intention to adopt and user’s intention to recommend internet banking will be stronger when gamification effect is higher. Importance performance matrix analysis (IPMA) revealed that innovativeness and perceived technology security were the most important factors in order to determine user’s intention with regard to adoption of internet banking. Practical implications For policy-makers, it is suggested that they should focus on innovative characteristics and must ensure the possible environment for carrying out internet banking transaction. Advertising about new technology with adequate information may produce positive influence on user’s intention. Enjoyable internet banking website with reward system will help to improve user’s intention to adopt and intention to recommend internet banking with others, thus developers should introduce game features on internet banking website. Originality/value This study provides basis for further refinement of individual technology acceptance models and enrich the e-commerce literature adding innovative and game elements in interne banking adoption context. Additionally, the proposed model makes an important contribution in emerging e-commerce literature especially in the context of innovative and gamified internet banking.",CS,AI_ML,0.85,Extracted from log - paper 345
Exploring the contribution of the design characteristics of Information Systems' user interface to the adoption process,"Today, the user interface component of Information Systems (ISs) is attracting great attention, being improved continuously and contributing more to the success of technology adoption. Even though the user interface is deemed to be the tip of the iceberg, the capability of the user interface adds a lot to the Information Technology (IT) adoption process. This paper aims at developing a better understanding of the interface design characteristics and technological architecture that affect smoother user adoption. The authors investigated the technology adoption characteristics by performing a qualitative analysis that included semistructured in-depth interviews. A total of nine IS users and five IS developers participated in the study. Many interface design characteristics that are evaluated in the literature were also considered. This paper also presents a technology adoption taxonomy for classifying the factors that influence user technology adoption. The findings of the interviews, the proposed technology adoption taxonomy and the implications of these findings are discussed. Also, further research opportunities are described.",CS,AI_ML,0.85,Extracted from log - paper 346
"The Impact of Gartner’s Maturity Curve, Adoption Curve, Strategic Technologies on Information Systems Research, with Applications to Artificial Intelligence, ERP, BPM, and RFID","ABSTRACT: How does technology maturity and adoption affect samples, research issues, and use of methodologies in information systems? What is a source of some research issues in strategic and emerging technologies? This paper addresses these questions and others using some frameworks generated by a well-known corporate research group. Gartner Group has been an icon to its corporate clients. However, Gartner has received only limited attention by academics. This paper examines three related frameworks used by Gartner for analyzing information systems (IS) and accounting information systems (AIS) research. Although researchers have previously examined the adoption curve, they generally have ignored the impact of the technology maturity curve and the interaction of the two curves. The paper generates a number of findings, including the finding that where a technology is on the maturity curve limits and facilitates the type of research questions that can be addressed regarding that technology. In addition, Ga...",CS,AI_ML,0.85,Extracted from log - paper 347
Metaverse system adoption in education: a systematic literature review,"The evaluation of information systems (IS) models, which are employed to research the adoption or acceptance of metaverse systems, is thought to be a subject of major significance. Studying the adoption or acceptability of the metaverse system is not a recent study area, and many academics have taken on the task. We should be acquainted with the leading IS models used in this study trend to assess these models and give academics a comprehensive understanding of this study trend. The primary goal of this research, in contrast to previous reviews, is to systematically evaluate the metaverse research in education from the viewpoint of IS theories/models to offer a thorough pointer that might help the scholars to carry out additional research in metaverse acceptance. A total of 41 research that was published between 2011 and 2022 were examined in the present systematic review. The main study results showed that the Technology Acceptance Model (TAM) is recognized as the most widely used model in forecasting people’s intentions to uphold the metaverse system. Furthermore, it was discovered that SmartPLS (PLS-SEM) is a typical tool for validating metaverse models. In addition, the key research purpose covered in the bulk of the reviewed research is to study how students adopt or accept the metaverse system and the technology that supports it. Additionally, most of the research that was gathered was done in China, Taiwan, and the USA, accordingly. Additionally, in most of the evaluated research, it was discovered that university students were the primary respondents concerning data acquisition. These findings are anticipated to significantly improve both our comprehension of metaverse system study and the utilization of IS models.",CS,AI_ML,0.85,Extracted from log - paper 348
Technology acceptance model of the Indonesian government financial reporting information systems,"This study applies the technology acceptance model to explore the government financial reporting information systems (GFRIS) in Indonesia. The model empirically tested using data gathered from 73 respondents of chief finance administration officer, revenue treasurer and expenditure treasurer in Surabaya City Government of Indonesia. Sampling technique used in this research is clustered sampling. The research model was tested by using the partial least squares structural equation modelling (PLS-SEM) approach. The results showed that all constructs in the TAM model were statistically significant. The findings suggest that experience has a positive impact on ease of use and usefulness. Conversely, gender as external variables has no effect on the ease of use and the usefulness of GFRIS. Findings from this study contribute to the literature on user acceptance of information systems and provide insights to assess the user acceptance by focusing on user experience in the government and financial context.",CS,AI_ML,0.85,Extracted from log - paper 349
User Acceptance of Hedonic Information Systems,"This paper studies the differences in user acceptance models for productivity-oriented (or utilitarian) and pleasure-oriented (or hedonic) information systems. Hedonic information systems aim to provide self-fulfilling rather than instrumental value to the user, are strongly connected to home and leisure activities, focus on the fun-aspect of using information systems, and encourage prolonged rather than productive use. The paper reports a cross-sectional survey on the usage intentions for one hedonic information system. Analysis of this sample supports the hypotheses that perceived enjoyment and perceived ease of use are stronger determinants of intentions to use than perceived usefulness. The paper concludes that the hedonic nature of an information system is an important boundary condition to the validity of the technology acceptance model. Specifically, perceived usefulness loses its dominant predictive value in favor of ease of use and enjoyment.",CS,AI_ML,0.85,Extracted from log - paper 350
Understanding Technology Acceptance of Government Information Systems from Employees' Perspective,"The purpose of this paper is to test the unified theory of acceptance and use of technology (UTAUT) in the government-to-government environment and determine the influence of different moderating factors of technology acceptance by government employees. By means of a thorough review of the literature in this area, the paper investigates the application of UTAUT through empirical research on the technology acceptance of the E-recovery system by government employees in different governmental organizations. The empirical data from 384 respondents revealed that performance expectancy and social influence determine employees' behavioral intention. Moreover, gender invariance was detected, but employees' age and experience difference was detected as a moderating factor of the model. The findings of the paper can help those involved in information system development, implementation, and use to take the right actions to achieve maximum adoption of the technology, especially if the use of the technology is mandatory for the users.",CS,AI_ML,0.85,Extracted from log - paper 351
Information Technology to the Rescue? Explaining the Acceptance of Emergency Response Information Systems by Firefighters,"Improving the efficacy of emergency responses with digital means is receiving increasing attention. Currently, several innovative information technologies and systems are being developed to raise the situation awareness of first responders like firefighters. Among them, emergency response information systems (ERIS) appear to provide a particularly promising platform, which helps to gather, analyze, and share relevant information during emergencies. However, the conditions under which firefighters accept or reject such systems remain unclear. Existing theories explain the acceptance of information technologies only on a general level that does not consider the specific usage constraints existing in the firefighter domain. To fill this literature gap, we propose a detailed, domain-specific acceptance model with factors that explain the acceptance of ERIS by firefighters. It combines findings of the user satisfaction and the technology acceptance literature and was developed based on the input of 82 domain experts. An evaluation of the acceptance model in a survey with 212 firefighters from Germany indicates that it is effective in predicting a firefighter's intention to use an ERIS. The identified acceptance factors provide guidance for the design and evaluation of ERIS, enabling the so far mostly theoretical benefits of ERIS to be transferred into practical applications more effectively.",CS,AI_ML,0.85,Extracted from log - paper 352
"The Use of a Technology Acceptance Model (TAM) to Predict Patients’ Usage of a Personal Health Record System: The Role of Security, Privacy, and Usability","Personal health records (PHR) systems are designed to ensure that individuals have access and control over their health information and to support them in being active participants rather than passive ones in their healthcare process. Yet, PHR systems have not yet been widely adopted or used by consumers despite their benefits. For these advantages to be realized, adoption of the system is necessary. In this study, we examined how self-determination of health management influences individuals’ intention to implement a PHR system, i.e., their ability to actively manage their health. Using an extended technology acceptance model (TAM), the researchers developed and empirically tested a model explaining public adoption of PHRs. In total, 389 Saudi Arabian respondents were surveyed in a quantitative cross-sectional design. The hypotheses were analysed using structural equation modelling–partial least squares (SEM-PLS4). Results indicate that PHR system usage was influenced by three major factors: perceived ease of use (PEOU), perceived usefulness (PU), and security towards intention to use. PHR PEOU and PHR intention to use were also found to be moderated by privacy, whereas usability positively moderated PHR PEOU and PHR intention to use and negatively moderated PHR PU and PHR intention to use. For the first time, this study examined the use of personal health records in Saudi Arabia, including the extension of the TAM model as well as development of a context-driven model that examines the relationship between privacy, security, usability, and the use of PHRs. Furthermore, this study fills a gap in the literature regarding the moderating effects of privacy influence on PEOU and intention to use. Further, the moderating effects of usability on the relationship between PEOU, PU, and intention to use. Study findings are expected to assist government agencies, health policymakers, and health organizations around the world, including Saudi Arabia, in understanding the adoption of personal health records.",CS,AI_ML,0.85,Extracted from log - paper 353
The Application of Acceptance Models to Human Resource Information Systems: A Literature Review,"Technology acceptance by users has been extensively studied in recent years in various fields such as technologies for learning, e-commerce, and business technologies. This review focuses specifically on Human Resource Information Systems (HRIS) and its acceptance by users. Given their widespread use in organisations, HRIS acceptance has been researched but not synthesised in any way. This article aims to review the effectiveness of the classical TAM and UTAUT models commonly used for new technologies and to identify the variables added to these models to better predict HRIS acceptance by employees. It also highlights the importance of the human-machine-organisation relationship to contribute to the understanding of HRIS acceptance in professional environments. This review confirms the effectiveness of the TAM and UTAUT models and proposes to develop them by (a) variables reffering to technological characteristics (security, system response time, and the data quality implemented in the system), (b) user satisfaction with the system, and (c) organisational variables (expected role of the HR department). The discussion focuses on the retroaction possibilities between the different Human-Machine-Organisation relation levels.",CS,AI_ML,0.85,Extracted from log - paper 354
Trends on Using the Technology Acceptance Model (TAM) for Online Learning: A Bibliometric and Content Analysis,"The technology acceptance model (TAM) is an information systems model that models how consumers use and accept technology. Scholars have implemented TAM widely to examine the effectiveness and ease of use of online learning. Therefore, this analysis comprehensively analyses TAM’s role in accepting online learning platforms by conducting bibliometric and content analysis based on the PRISMA framework. Insights into technology acceptance models were determined by bibliometrics analysis with VosViewer and content analysis. Methods: This study expanded all research from 2002 to 2020. A sum of 120 publications was analysed in January 2022 as documented in the Scopus database after applying the including and excluding criteria in addition to the manual evaluation. Results: This review’s findings identified the most compelling subjects covered by the journal. Most prolific countries, educational institutions, Journals, and authors were identified. Additionally, the results demonstrate several significant models for technology acceptance; several online learning environments were outlined (MOOC, Moodle, E-learning, flipped learning, and blended learning). Conclusion: The research presents a roadmap for potential researchers, concentrating on critical areas where success is possible. However, more research is required to utilize the TAM model and incorporate different online learning environments.",CS,AI_ML,0.85,Extracted from log - paper 355
Machines that feel: behavioral determinants of attitude towards affect recognition technology—upgrading technology acceptance theory with the mindsponge model,"The rise of emotional AI signals a new era in human-machine relations where intelligent machines not only feel but also feed on human emotions as statistical fodder with the goal of reshaping our behavior. Unlike many smart technologies, emotion-recognition systems sense, monitor, harvest and analyze data extracted from a person’s non-conscious or psycho-physical state, often without their knowledge or consent. As a far more invasive manner of surveillance capitalism, the technological adoption of emotional AI is problematized by a myriad of legal, ethical, cultural, and scientific issues. To better understand the behavioral factors determining an individual’s attitude towards this emerging technology, we first identify five major tensions that may impinge on adoption. Second, we extend the Technological Acceptance Model (TAM) (Davis, 1989 ) model with insights from the mindsponge model of information filtering (Vuong and Napier, 2015 ) along with quantitative affordances offered by the Bayesian computational approach. Our analysis was conducted based on a multi-national dataset surveying perceptions of 1015 young adults (age 18–27) regarding emotional AI applications and their socio-cultural characteristics such as income, region, religiosity, and home country politics. These characteristics are fed into our Bayesian multi-level models as varying intercepts so that we can systematically measure and compare the effects of various behavioral determinants on the attitudes of respondents towards non-conscious data harvesting by government and private sector actors. Critically, this study finds respondents who feel more familiar with, and perceive more utilities in AI technologies, as well as rate themselves as more restrained from heated arguments on social media, feel less threatened by the practice of non-conscious data harvesting by both government and private sector actors. Our findings offer a fertile platform for further exploration of the intersection between psychology, culture, and emotion-recognition technologies as well as important insights for policymakers wishing to ensure design and regulation of the technology serve the best interests of society.",CS,AI_ML,0.85,Extracted from log - paper 356
"User Acceptance of Information Technology: System Characteristics, User Perceptions and Behavioral Impacts","Abstract Lack of user acceptance has long been an impediment to the success of new information systems. The present research addresses why users accept or reject information systems and how user acceptance is affected by system design features. The technology acceptance model (TAM) specifies the causal relationships between system design features, perceived usefulness, perceived ease of use, attitude toward using, and actual usage behavior. Attitude theory from psychology provides the rationale for hypothesized model relationships, and validated measures were used to operationalize model variables. A field study of 112 users regarding two end-user systems was conducted to test the hypothesized model. TAM fully mediated the effects of system characteristics on usage behavior, accounting for 36% of the variance in usage. Perhaps the most striking finding was that perceived usefulness was 50% more influential than ease of use in determining usage, underscoring the importance of incorporating the appropriate functional capabilities in new systems. Overall, TAM provides an informative representation of the mechanisms by which design choices influence user acceptance, and should therefore be helpful in applied contexts for forecasting and evaluating user acceptance of information technology. Implications for future research and practice are discussed.",CS,AI_ML,0.85,Extracted from log - paper 357
Modified technology acceptance model for hospital information system evaluation – a case study,"Hospital Information Systems (HIS) has been widely used in Indonesia. This implementation in hospital needs comprehensive, integrated information system designed to manage all the aspects of a hospital's operation. One of the factors that currently plays an important role in the successful HIS’s application is the user factor. The hospital can evaluate HIS from user factor, so that it becoming better, perfect and can support the objectives, vision and mission of the hospital. This study aims to use the Modified Technology Acceptance Model created in Structural Equation Modeling (SEM) and tested using the software Amos 20 to determine the variables that most influence in HIS’s application using in several hospitals in Sleman, Daerah Istimewa Yogyakarta Province, Indonesia. Data collected by distributing questionnaires to employees of D grade hospitals in Sleman district. The results from this study are seven variables that may influence the user in using HIS (Subjective Norm, Perceived Usefulness, Perceived Ease of Use, User Satisfaction, Behavior Intention to Use, Attitude Toward Using, Actual System Usage) and significant or insignificant relationship among them.",CS,AI_ML,0.85,Extracted from log - paper 358
Applying the technology acceptance model to the introduction of mobile healthcare information systems,"This study presents an extended technology acceptance model (TAM) for exploring the factors that affect users’ intention to employ mobile healthcare information systems. This study aims at exploring the utilisation of mobile phones in healthcare environments and investigating the expectations and intentions regarding the implementation of mobile healthcare information systems in Jordan. Therefore, a questionnaire was designed using a Likert-scale method. The questionnaire was distributed through convenience sampling. The proposed model was empirically tested using data collected from a survey of 25 questions. Through this survey, 450 questionnaires were distributed and 366 ones were returned, or about (81.3%) were valid for analysis. The partial least squares (PLS) structural equation analysis (Ringle et al., 2005) was used to evaluate the causal model. The researchers find that the perceived ease of use has the most significant influence on the respondents’ behavioural intention to use mobile healthcare information systems. This study provides quantified indicators about mobile healthcare information systems and suggests a model that might help in understanding mobile healthcare information systems environment in Jordan.",CS,AI_ML,0.85,Extracted from log - paper 359
“Assessment of the social influence and facilitating conditions that support nurses’ adoption of hospital electronic information management systems (HEIMS) in Ghana using the unified theory of acceptance and use of technology (UTAUT) model”,"BackgroundHospital electronic information management systems (HEIMS) are widely used in Ghana, and hence its performance must be carefully assessed. Nurses as clinical health personnel are the largest cluster of hospital staff and are the pillar of healthcare delivery. Therefore, they play a crucial role in the adoption and assessment of HEIMSs in Ghana. This report sought to assess the “Social Influence” (SI) and “Facilitating Conditions” (FC) that support Nurses’ Acceptance of HEIMS in Ghana using the “Unified Theory of Acceptance and Use of Technology” (UTAUT) model.MethodsThis study applied a non-experimental survey design. An electronic platform questionnaire on smartphones was used to collect data on 660 nurses. Statistically, AMOS Structural Equation Modelling (SEM) version 22.0 was employed to examine the research model.Results“Behavioral Intention” (BI) to HEIMS use was significantly predicted by SI and FC (p < 0.001). Notably, both SI and FC had an influence on nurses’ use behavior (UB) with behavioral intention (BI) as the mediator, which explains a total of 42.1% variance in the intention of nurses to use HEIMS. Likewise, UB of HEIMS was also significantly predicted by SI (R2 = 43.2) and BI (R2 = 0.39.6) with both constructs explaining a total of 51.7% of the variance in nurses’ acceptance to use HEIMS.ConclusionNurses’ adoption of HEIMS in terms of the UB was influenced by SI and BI, whiles SI and FC had the strongest influence on BI (serving as mediator) of UB to adopt and use HEIMS among the nurses in Ghanaian hospitals.",CS,AI_ML,0.85,Extracted from log - paper 360
Factors that affect acceptance and use of information systems within the Maritime industry in developing countries,"Purpose Although information and communication technology has become a significant driver for organizational efficiency and effectiveness, there is inadequate empirical research on technology acceptance in the maritime industry especially in developing countries. Literature on how behavior and attitude influence technology acceptance is non-existent. This study therefore aims to augment existing literature on technology acceptance in developing countries with particular emphasis on the maritime industry. Design/methodology/approach The study extended the unified theory of acceptance and use of technology (UTAUT) model to investigate the factors that affect the acceptance and use of INTTRA: a multi-carrier booking and shipping system designed to facilitate ocean trade worldwide. Responses from 198 subjects, collected through a questionnaire, were analyzed using partial least square structural equation modeling. Findings The research model confirmed significant influences of performance expectancy, facilitating conditions, anxiety and attitude towards use on users’ intention to use INTTRA. In contrast, social influence, effort expectancy and self-efficacy did not significantly influence intention to use. Although these findings confirm some proposed relationships in the UTAUT model, it contradicted the cultural dimension argument that developing countries with higher degrees of femininity pay less attention to performance and high attention to social influence. Research limitations/implications The study contributes to knowledge in the area of information systems and technology acceptance in developing countries. Particularly, it seeks to expand literature on adoption within the maritime industry. The study is limited to the sample used for the study, as it used participants from only one country. However, the findings are not generalized for the entire maritime industry but rather Ghana. Originality/value The originality of the study is derived from the provision of literature on adoption within the maritime industry in developing countries. It also provided evidence that challenges existing knowledge on characteristics of countries that exhibits high level of femininity culture as proposed by Hofstede.",CS,AI_ML,0.85,Extracted from log - paper 361
Cloud-based accounting information systems usage and its impact on Jordanian SMEs’ performance: the post-COVID-19 perspective,"Purpose The purpose of this study is to examine the factors influencing the usage of cloud-based accounting information systems (AIS) in the crisis era (i.e. the COVID-19 pandemic) by expanding the unified theory of acceptance and use of technology (UTAUT) with new related critical factors. Design/methodology/approach A quantitative research approach based on a cross-sectional online questionnaire was used for collecting empirical data from 438 potential and current users of cloud-based AIS. Structural equation modeling based on analysis of a moment structures 25.0 was applied in the data analysis. Findings The outcome of the structural path revealed that performance expectancy, social motivation, COVID-19 risk (COV-19 PR) and trust (TR) were significantly influencing users’ behavioral intention (BI) toward using cloud-based AIS and explained 71% of its variance. While, contrary to what is expected, the impact of effort expectancy and perceived security risk (SEC) on BI was insignificant. In addition, BI was revealed to influence the actual usage behaviors and explained 74% of its variance. The outcome factors: communication quality (CQ) and decision quality (DQ) were significantly influenced by the usage of cloud-based AIS. Practical implications The current research would be valuable for small- and medium-sized enterprises officials and policymakers to illustrate the relatively low rates of cloud-based AIS and formulate strategies to boost the acceptance and use of cloud-based AIS by Jordanian users, where cloud-based services are still deemed as an innovation. Originality/value To the best of the authors’ knowledge, the current study is the first academic paper that extends the UTAUT by integrating additional factors: TR, SEC and COV-19 PR. In addition to two outcome variables: CQ and DQ, to study the cloud-based AIS in the Jordanian setting beyond the COVID-19 pandemic. The current research contributes to the academic knowledge on information technology information system adoption by considering cloud accounting acceptance and use and integration into the work practices of users through the BIs and actual use of cloud-based AIS in Jordan.",CS,AI_ML,0.85,Extracted from log - paper 362
An overview of patient acceptance of Health Information Technology in developing countries: a review and conceptual model,"The potential to improve the quality, efficiency, outcomes, patient safety and reduce cost of healthcare by Health Information Technology (HIT) has been established by researchers. But unfortunately HIT systems are not properly utilized or are not widely available. This problem is even more glaring in developing countries. This article presents a review of some available HIT systems in order to assess the level of their presence and the technology used in developing them. Works related to acceptance of HIT systems were also reviewed so as to study the gaps in this area and propose a solution in order to fill the gaps identified. The problems discovered from this review include lack of availability of these systems especially in developing countries, low rate of HIT systems acceptance and insufficient works on patient acceptance of HIT systems. Studying the factors that affect the acceptance of HIT systems by patients and considering the factors while developing the systems will play a significant role in getting over the aforementioned limitations. As Technology Acceptance Model (TAM) is one of the most popular models for studying users' perception and acceptance of Information System (IS)/Information Technology (IT), we proposed a conceptual model of HIT acceptance in developing countries based on TAM.",CS,AI_ML,0.85,Extracted from log - paper 363
"Determinants of Perceived Ease of Use: Integrating Control, Intrinsic Motivation, and Emotion into the Technology Acceptance Model","Much previous research has established that perceived ease of use is an important factor influencing user acceptance and usage behavior of information technologies. However, very little research has been conducted to understand how that perception forms and changes over time. The current work presents and tests an anchoring and adjustment-based theoretical model of the determinants of system-specific perceived ease of use. The model proposes control (internal and external--conceptualized as computer self-efficacy and facilitating conditions, respectively), intrinsic motivation (conceptualized as computer playfulness), and emotion (conceptualized as computer anxiety) as anchors that determine early perceptions about the ease of use of a new system. With increasing experience, it is expected that system-specific perceived ease of use, while still anchored to the general beliefs regarding computers and computer use, will adjust to reflect objective usability, perceptions of external control specific to the new system environment, and system-specific perceived enjoyment. The proposed model was tested in three different organizations among 246 employees using three measurements taken over a three-month period. The proposed model was strongly supported at all points of measurement, and explained up to 60% of the variance in system-specific perceived ease of use, which is twice as much as our current understanding. Important theoretical and practical implications of these findings are discussed.",CS,AI_ML,0.85,Extracted from log - paper 364
Predicting User Intentions: Comparing the Technology Acceptance Model with the Theory of Planned Behavior,"Information systems IS cannot be effective unless they are used. However, people sometimes do not use systems that could potentially increase their performance. This study compares two models that predict an individual's intention to use an IS: the technology acceptance model TAM and the theory of planned behavior TPB. The comparison was designed to be as fair as possible, not favoring one model over the other. Both TAM and TPB predicted intention to use an IS quite well, with TAM having a slight empirical advantage. TAM is easier to apply, but only supplies very general information on users' opinions about a system. TPB provides more specific information that can better guide development.",CS,AI_ML,0.85,Extracted from log - paper 365
Applying the Technology Acceptance Model and Flow Theory to Online Consumer Behavior,"In this study, we consider the online consumer as both a shopper and a computer user. We test constructs from information systems (Technology Acceptance Model), marketing (Consumer Behavior), and psychology (Flow and Environmental Psychology) in an integrated theoretical framework of online consumer behavior. Specifically, we examine how emotional and cognitive responses to visiting a Web-based store for the first time can influence online consumers' intention to return and their likelihood to make unplanned purchases. The instrumentation shows reasonably good measurement properties and the constructs are validated as a nomological network.A questionnaire-based empirical study is used to test this nomological network. Results confirm the double identity of the online consumer as a shopper and a computer user because both shopping enjoyment and perceived usefulness of the site strongly predict intention to return. Our results on unplanned purchases are not conclusive. We also test some individual and Web site factors that can affect the consumer's emotional and cognitive responses. Product involvement, Web skills, challenges, and use of value-added search mechanisms all have a significant impact on the Web consumer. The study provides a more rounded, albeit partial, view of the online consumer and is a significant steptowards a better understanding of consumer behavior on the Web. The validated metrics should be of use to researchers and practitioners alike.",CS,AI_ML,0.85,Extracted from log - paper 366
Investigating a theoretical framework for e-learning technology acceptance,"E-learning has gained recognition and fame in delivering and distributing educational resources, and the same has become possible with the occurrence of Internet and Web technologies. The research seeks to determine the factors that influence students' acceptance of E-learning and to find out the way these factors determine the students' intention to employ E-learning. A theoretical framework was developed based on the technology acceptance model (TAM). To obtain information from the 270 university students who utilized the E-learning system, a questionnaire was formulated. The results revealed that “social influence, perceived enjoyment, self-efficacy, perceived usefulness, and perceived ease of use” are the strongest and most important predictors in the intention of and students towards E-learning systems. The outcomes offer practical implications for practitioners, lawmakers, and developers in effective E-learning systems implementation to improve ongoing interests and activities of university students in a virtual E-learning atmosphere, valuable recommendations for E-learning practices are given by the research findings, and these may turn out to be as guidelines for the efficient design of E-learning systems.",CS,AI_ML,0.85,Extracted from log - paper 367
The Acceptance of Using Information Technology for Disaster Risk Management: A Systematic Review,". The numbers of natural disaster events are continuously affecting human and the world economics. For coping with disaster, several sectors try to develop the frameworks, systems, technologies and so on. However, there are little researches focusing on the usage behavior of Information Technology (IT) for disaster risk management (DRM). Therefore, this study investigates the affecting factors on the intention to use IT for mitigating disaster’s impacts. This study conducted a systematic review with the academic researches during 2011-2018. Two important factors from the Technology Acceptance Model (TAM) and others are used in describing individual behavior. In order to investigate the potential factors, the technology platforms are divided into nine types. According to the findings, computer software such as GIS applications are frequently used for simulation and spatial data analysis. Social media is preferred among the first choices during disaster events in order to communicate about situations and damages. Finally, we found five major potential factors which are Perceived Usefulness (PU), Perceived Ease of Use (PEOU), information accessibility, social influence, and disaster knowledge. Among them, the most essential one of using IT for disaster management is PU, while PEOU and information accessibility are more important in the web platforms.",CS,AI_ML,0.85,Extracted from log - paper 368
Theories Integrated With Technology Acceptance Model (TAM) in Online Learning Acceptance and Continuance Intention: A Systematic Review,"Since its inception, Technology Acceptance Model (TAM) has been a commonly adopted theory for understanding users’ acceptance of various types of information systems (e.g., online learning systems). Over the years, different information systems theories have been integrated into TAM to further the understanding of users’ intention to accept online learning. To examine the literature, four databases were utilized to discover research articles examining the online learning acceptance and continuance intention of users (e.g., students and teachers). The findings of the systematic review revealed that Task Technology Fit and Theory of Planned Behavior are the most integrated and educationally successful theories into TAM. Meanwhile, course information, satisfaction, perceived usefulness, attitude, system quality, perceived ease of use, and academic performance are the essential drivers for the acceptance or continuance usage of online learning systems. These findings serve as an evidence and reference for educational institutions in developing policies and strategies for the implementation of an online education.",CS,AI_ML,0.85,Extracted from log - paper 369
A Systematic Review of the Technology Acceptance Model in Health Informatics,"Abstract Background One common model utilized to understand clinical staff and patients' technology adoption is the technology acceptance model (TAM). Objective This article reviews published research on TAM use in health information systems development and implementation with regard to application areas and model extensions after its initial introduction. Method An electronic literature search supplemented by citation searching was conducted on February 2017 of the Web of Science, PubMed, and Scopus databases, yielding a total of 492 references. Upon eliminating duplicates and applying inclusion and exclusion criteria, 134 articles were retained. These articles were appraised and divided into three categories according to research topic: studies using the original TAM, studies using an extended TAM, and acceptance model comparisons including the TAM. Results The review identified three main information and communication technology (ICT) application areas for the TAM in health services: telemedicine, electronic health records, and mobile applications. The original TAM was found to have been extended to fit dynamic health service environments by integration of components from theoretical frameworks such as the theory of planned behavior and unified theory of acceptance and use of technology, as well as by adding variables in specific contextual settings. These variables frequently reflected the concepts subjective norm and self-efficacy, but also compatibility, experience, training, anxiety, habit, and facilitators were considered. Conclusion Telemedicine applications were between 1999 and 2017, the ICT application area most frequently studied using the TAM, implying that acceptance of this technology was a major challenge when exploiting ICT to develop health service organizations during this period. A majority of the reviewed articles reported extensions of the original TAM, suggesting that no optimal TAM version for use in health services has been established. Although the review results indicate a continuous progress, there are still areas that can be expanded and improved to increase the predictive performance of the TAM.",CS,AI_ML,0.85,Extracted from log - paper 370
Direct and Indirect Associations of Personality With Audiovisual Technology Acceptance Through General Self-Efficacy,"Increasing consensus among information systems researchers suggests that personality accounts for the effective use of several technologies, yet less is known about the process through which personality affects user perceptions of technology acceptance. This study, therefore, examined whether personality is associated with student perceptions of audiovisual technology acceptance, and whether general self-efficacy mediates this association. In total, 244 students completed an online survey including measures of personality, general self-efficacy, and perceptions of audiovisual technology acceptance. Canonical correlation uncovered significant associations between personality and student beliefs about technology use. Results further revealed that general self-efficacy fully mediated the effects of openness to experience and neuroticism on Perceived Ease of Use, whereas the association between agreeableness and Perceived Usefulness was partially mediated by self-efficacy beliefs. The fact that personality influences students’ perceptions of technology acceptance both directly and indirectly should not remain unnoticed, especially when designing intervention programs to enhance their academic performance.",CS,AI_ML,0.85,Extracted from log - paper 371
A Theoretical Integration of User Satisfaction and Technology Acceptance,"In general, perceptions of information systems (IS) success have been investigated within two primary research streams--the user satisfaction literature and the technology acceptance literature. These two approaches have been developed in parallel and have not been reconciled or integrated. This paper develops an integrated research model that distinguishes beliefs and attitudes about the system (i.e., object-based beliefs and attitudes) from beliefs and attitudes about using the system (i.e., behavioral beliefs and attitudes) to build the theoretical logic that links the user satisfaction and technology acceptance literature. The model is then tested using a sample of 465 users from seven different organizations who completed a survey regarding their use of data warehousing software. The proposed model was supported, providing preliminary evidence that the two perspectives can and should be integrated. The integrated model helps build the bridge from design and implementation decisions to system characteristics (a core strength of the user satisfaction literature) to the prediction of usage (a core strength of the technology acceptance literature).",CS,AI_ML,0.85,Extracted from log - paper 372
How nostalgic feelings impact Pokémon Go players – integrating childhood brand nostalgia into the technology acceptance theory,"ABSTRACT The augmented reality smartphone game Pokémon Go is one of the biggest commercial successes in the last years, posing the question concerning the factors contributing to the game’s success. An apparent distinction to other games is the strong brand Pokémon. We derive a research model based on the established theory of technology acceptance, which includes an established construct for nostalgic feelings – childhood brand nostalgia – and theorise on how it is related to beliefs about technology characteristics and the intention to play the game. For this purpose, we adapt one of the most prominent technology acceptance models for the consumer context and for hedonic information systems, the UTAUT2 model. Based on our model, we conduct a study with 418 active German players aged between 18 and 35. Our results indicate that the effect of childhood brand nostalgia on behavioural intention is fully mediated by the belief constructs. Thus, nostalgic feelings about Pokémon influence the intention of users through altering beliefs concerning Pokémon. We include nostalgic feelings in a technology acceptance model for the first time, therefore contributing to the theoretical advance in the IS domain. The results can be used to enhance the technology acceptance of newly designed products.",CS,AI_ML,0.85,Extracted from log - paper 373
Technology Acceptance Model (TAM) for the Implementation of Knowledge Acquired model for SME,"SMEs are business units that contribute greatly to the country. The existence of SMEs in each country is one of the pillars of the nation's economy. The limited knowledge of newcomer SMEs about the use of information technology has become their main obstacle in developing their business. Research methods through observation activities to investigate the process of acquiring knowledge of SMEs. And use the technology acceptance model (TAM) by adding collaboration and curriculum to the knowledge management system (KMS) model that will change the newcomer SMEs paradigm of information technology, especially knowledge management systems for SMEs. The results of this study are knowledge management system models for small and medium-sized businesses.",CS,AI_ML,0.85,Extracted from log - paper 374
Considerations for Cloud Security Operations,"Information Security in Cloud Computing environments is explored. Cloud Computing is presented, security needs are discussed, and mitigation approaches are listed. Topics covered include Information Security, Cloud Computing, Private Cloud, Public Cloud, SaaS, PaaS, IaaS, ISO 27001, OWASP, Secure SDLC.",CS,AI_ML,0.85,Extracted from log - paper 375
Security of Cloud FPGAs: A Survey,"Integrating Field Programmable Gate Arrays (FPGAs) with cloud computing instances is a rapidly emerging trend on commercial cloud computing platforms such as Amazon Web Services (AWS), Huawei cloud, and Alibaba cloud. Cloud FPGAs allow cloud users to build hardware accelerators to speed up the computation in the cloud. However, since the cloud FPGA technology is still in its infancy, the security implications of this integration of FPGAs in the cloud are not clear. In this paper, we survey the emerging field of cloud FPGA security, providing a comprehensive overview of the security issues related to cloud FPGAs, and highlighting future challenges in this research area.",CS,AI_ML,0.85,Extracted from log - paper 376
Interoperability and Standardization of Intercloud Cloud Computing,"Cloud computing is getting mature, and the interoperability and standardization of the clouds is still waiting to be solved. This paper discussed the interoperability among clouds about message transmission, data transmission and virtual machine transfer. Starting from IEEE Pioneering Cloud Computing Initiative, this paper discussed about standardization of the cloud computing, especially intercloud cloud computing. This paper also discussed the standardization from the market-oriented view.",CS,AI_ML,0.85,Extracted from log - paper 377
Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management,"In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low resource utilization and unbalanced load in the cloud environment, this study proposes a comprehensive solution, including optimization methods such as deep learning and genetic algorithm, to improve system performance and efficiency, and thus bring new breakthroughs and progress in the field of cloud computing resource management.Rational allocation of resources plays a crucial role in cloud computing. In the resource allocation of cloud computing, the cloud computing center has limited cloud resources, and users arrive in sequence. Each user requests the cloud computing center to use a certain number of cloud resources at a specific time.",CS,AI_ML,0.85,Extracted from log - paper 378
Evolution of Cloud Storage as Cloud Computing Infrastructure Service,"Enterprises are driving towards less cost, more availability, agility, managed risk - all of which is accelerated towards Cloud Computing. Cloud is not a particular product, but a way of delivering IT services that are consumable on demand, elastic to scale up and down as needed, and follow a pay-for-usage model. Out of the three common types of cloud computing service models, Infrastructure as a Service (IaaS) is a service model that provides servers, computing power, network bandwidth and Storage capacity, as a service to their subscribers. Cloud can relate to many things but without the fundamental storage pieces, which is provided as a service namely Cloud Storage, none of the other applications is possible. This paper introduces Cloud Storage, which covers the key technologies in cloud computing and Cloud Storage, management insights about cloud computing, different types of cloud services, driving forces of cloud computing and cloud storage, advantages and challenges of cloud storage and concludes by pinpointing few challenges to be addressed by the cloud storage providers.",CS,AI_ML,0.85,Extracted from log - paper 379
A Survey on Cloud Security Issues and Techniques,"Today, cloud computing is an emerging way of computing in computer science. Cloud computing is a set of resources and services that are offered by the network or internet. Cloud computing extends various computing techniques like grid computing, distributed computing. Today cloud computing is used in both industrial field and academic field. Cloud facilitates its users by providing virtual resources via internet. As the field of cloud computing is spreading the new techniques are developing. This increase in cloud computing environment also increases security challenges for cloud developers. Users of cloud save their data in the cloud hence the lack of security in cloud can lose the users trust. In this paper we will discuss some of the cloud security issues in various aspects like multi-tenancy, elasticity, availability etc. The paper also discuss existing security techniques and approaches for a secure cloud. This paper will enable researchers and professionals to know about different security threats and models and tools proposed.",CS,AI_ML,0.85,Extracted from log - paper 380
Discussion of various models related to cloud performance,"This paper discusses the various models related to cloud computing. Knowing the metrics related to infrastructure is very critical to enhance the performance of cloud services. Various metrics related to clouds such as pageview response time, admission control and enforcing elasticity to cloud infrastructure are very crucial in analyzing the characteristics of the cloud to enhance the cloud performance.",CS,AI_ML,0.85,Extracted from log - paper 381
A Comparative Study of Load Balancing Algorithms in Cloud Computing Environment,Cloud Computing is a new trend emerging in IT environment with huge requirements of infrastructure and resources. Load Balancing is an important aspect of cloud computing environment. Efficient load balancing scheme ensures efficient resource utilization by provisioning of resources to cloud users on demand basis in pay as you say manner. Load Balancing may even support prioritizing users by applying appropriate scheduling criteria. This paper presents various load balancing schemes in different cloud environment based on requirements specified in Service Level Agreement (SLA).,CS,AI_ML,0.85,Extracted from log - paper 382
Resource Management in Cloud Computing: Classification and Taxonomy,Cloud Computing is a new era of remote computing / Internet based computing where one can access their personal resources easily from any computer through Internet. Cloud delivers computing as a utility as it is available to the cloud consumers on demand. It is a simple pay-per-use consumer-provider service model. It contains large number of shared resources. So Resource Management is always a major issue in cloud computing like any other computing paradigm. Due to the availability of finite resources it is very challenging for cloud providers to provide all the requested resources. From the cloud providers perspective cloud resources must be allocated in a fair and efficient manner. Research Survey is not available from the perspective of resource management as a process in cloud computing. So this research paper provides a detailed sequential view / steps on resource management in cloud computing. Firstly this research paper classifies various resources in cloud computing. It also gives taxonomy on resource management in cloud computing through which one can do further research. Lastly comparisons on various resource management algorithms has been presented.,CS,AI_ML,0.85,Extracted from log - paper 383
Securing the Data in Clouds with Hyperelliptic Curve Cryptography,"In todays world, Cloud computing has attracted research communities as it provides services in reduced cost due to virtualizing all the necessary resources. Even modern business architecture depends upon Cloud computing .As it is a internet based utility, which provides various services over a network, it is prone to network based attacks. Hence security in clouds is the most important in case of cloud computing. Cloud Security concerns the customer to fully rely on storing data on clouds. That is why Cloud security has attracted attention of the research community. This paper will discuss securing the data in clouds by implementing key agreement, encryption and signature verification/generation with hyperelliptic curve cryptography.",CS,AI_ML,0.85,Extracted from log - paper 384
A Survey on Cloud Computing Security,"Computation encounter the new approach of cloud computing which maybe keeps the world and possibly can prepare all the human's necessities. In other words, cloud computing is the subsequent regular step in the evolution of on-demand information technology services and products. The Cloud is a metaphor for the Internet and is a concept for the covered complicated infrastructure; it also depends on sketching in computer network diagrams. In this paper we will focus on concept of cloud computing, cloud deployment models, cloud security challenges encryption and data protection, privacy and security and data management and movement from grid to cloud.",CS,AI_ML,0.85,Extracted from log - paper 385
A Preliminary Study On Emerging Cloud Computing Security Challenges,"Cloud computing is the internet based provisioning of the computing resources, software, and information on demand. Cloud Computing is referred to as one of most recent emerging paradigms of computing utilities. Since Cloud computing is the dominant infrastructure of the shared services over the internet, it is important to be aware of the security risk and the challenges associated with this emerging computing paradigm. This survey provides a brief introduction to the cloud computing, its major characteristics, and service models. It also explores cloud security threats, lists a few security solutions , and proposes a promsing research direction to deal with the evolving security challenges in Cloud computing.",CS,AI_ML,0.85,Extracted from log - paper 386
Framework for cloud computing adoption: A road map for Smes to cloud migration,"Small and Medium size Enterprises (SME) are considered as a backbone of many developing and developed economies of the world; they are the driving force to any major economy across the globe. Through Cloud Computing firms outsource their entire information technology (IT) process while concentrating more on their core business. It allows businesses to cut down heavy cost incurred over IT infrastructure without losing focus on customer needs. However, Cloud industry to an extent has struggled to grow among SMEs due to the reluctance and concerns expressed by them. Throughout the course of this study several interviews were conducted and the literature was reviewed to understand how cloud providers offer services and what challenges SMEs are facing. The study identified issues like cloud knowledge, interoperability, security and contractual concerns to be hindering SMEs adoption of cloud services. From the interviews common practices followed by cloud vendors and what concerns SMEs have were identified as a basis for a cloud framework which will bridge gaps between cloud vendors and SMEs. A stepwise framework for cloud adoption is formulated which identifies and provides recommendation to four most predominant challenges which are hurting cloud industry and taking SMEs away from cloud computing, as well as guide SMEs aiding in successful cloud adoption. Moreover, this framework streamlines the cloud adoption process for SMEs by removing ambiguity in regards to fundamentals associated with their organisation and cloud adoption process.",CS,AI_ML,0.85,Extracted from log - paper 387
Usage of Cloud Computing Simulators and Future Systems For Computational Research,"Cloud Computing is an Internet based computing, whereby shared resources, software and information, are provided to computers and devices on demand, like the electricity grid. Currently, IaaS (Infrastructure as a Service), PaaS (Platform as a Service) and SaaS (Software as a Service) are used as a business model for Cloud Computing. Nowadays, the adoption and deployment of Cloud Computing is increasing in various domains, forcing researchers to conduct research in the area of Cloud Computing globally. Setting up the research environment is critical for the researchers in the developing countries to evaluate the research outputs. Currently, modeling, simulation technology and access of resources from various university data centers has become a useful and powerful tool in cloud computing research. Several cloud simulators have been specifically developed by various universities to carry out Cloud Computing research, including CloudSim, SPECI, Green Cloud and Future Systems (the Indiana University machines India, Bravo, Delta, Echo and Foxtrot) supports leading edge data science research and a broad range of computing-enabled education as well as integration of ideas from cloud and HPC systems. In this paper, the features, suitability, adaptability and the learning curve of the existing Cloud Computing simulators and Future Systems are reviewed and analyzed.",CS,AI_ML,0.85,Extracted from log - paper 388
Is Cloud Computing Steganography-proof?,The paper focuses on characterisation of information hiding possibilities in Cloud Computing. After general introduction to cloud computing and its security we move to brief description of steganography. In particular we introduce classification of steganographic communication scenarios in cloud computing which is based on location of the steganograms receiver. These scenarios as well as the threats that steganographic methods can cause must be taken into account when designing secure cloud computing services.,CS,AI_ML,0.85,Extracted from log - paper 389
Surrogate cloud fields with measured cloud properties,"This paper describes two new methods to generate 2D and 3D cloud fields based on 1D and 2D ground based profiler meas-urements. These cloud fields share desired statistical properties with real cloud fields. As they, however, are similar but not the same as real clouds, we call them surrogate clouds. One important advantage of the new methods is that the amplitude distribution of cloud liquid water is also exactly determined by the measurement: The surrogate clouds made with the classi-cal methods such as the Fourier method and the Bounded Cascade method are Gaussian and 'log-normal-like', respectively. Our first new method iteratively creates a time series with a measured amplitude distribution and power spectrum. Our sec-ond method uses an evolutionary search algorithm to generate cloud fields with practically arbitrary constraints. These clouds will be used to study the relation between radiation and cloud structure.",CS,AI_ML,0.85,Extracted from log - paper 390
SecLaaS: Secure Logging-as-a-Service for Cloud Forensics,"Cloud computing has emerged as a popular computing paradigm in recent years. However, today's cloud computing architectures often lack support for computer forensic investigations. Analyzing various logs (e.g., process logs, network logs) plays a vital role in computer forensics. Unfortunately, collecting logs from a cloud is very hard given the black-box nature of clouds and the multi-tenant cloud models, where many users share the same processing and network resources. Researchers have proposed using log API or cloud management console to mitigate the challenges of collecting logs from cloud infrastructure. However, there has been no concrete work, which shows how to provide cloud logs to investigator while preserving users' privacy and integrity of the logs. In this paper, we introduce Secure-Logging-as-a-Service (SecLaaS), which stores virtual machines' logs and provides access to forensic investigators ensuring the confidentiality of the cloud users. Additionally, SeclaaS preserves proofs of past log and thus protects the integrity of the logs from dishonest investigators or cloud providers. Finally, we evaluate the feasibility of the scheme by implementing SecLaaS for network access logs in OpenStack - a popular open source cloud platform.",CS,AI_ML,0.85,Extracted from log - paper 391
"soCloud: A service-oriented component-based PaaS for managing portability, provisioning, elasticity, and high availability across multiple clouds","Multi-cloud computing is a promising paradigm to support very large scale world wide distributed applications. Multi-cloud computing is the usage of multiple, independent cloud environments, which assumed no priori agreement between cloud providers or third party. However, multi-cloud computing has to face several key challenges such as portability, provisioning, elasticity, and high availability. Developers will not only have to deploy applications to a specific cloud, but will also have to consider application portability from one cloud to another, and to deploy distributed applications spanning multiple clouds. This article presents soCloud a service-oriented component-based Platform as a Service (PaaS) for managing portability, elasticity, provisioning, and high availability across multiple clouds. soCloud is based on the OASIS Service Component Architecture (SCA) standard in order to address portability. soCloud provides services for managing provisioning, elasticity, and high availability across multiple clouds. soCloud has been deployed and evaluated on top of ten existing cloud providers: Windows Azure, DELL KACE, Amazon EC2, CloudBees, OpenShift, dotCloud, Jelastic, Heroku, Appfog, and an Eucalyptus private cloud.",CS,AI_ML,0.85,Extracted from log - paper 392
Cloud Computing and Grid Computing 360-Degree Compared,"Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.",CS,AI_ML,0.85,Extracted from log - paper 393
Cloud Adoption A Modern Approach,"Todays Information Technology world is cloud-centric. Companies are intrigued to migrate their workload private cloud from on-premise Datacenter to Public cloud to take advantage of the latest innovations. It drives the business growth and competitiveness of the organization. At the same time, it is important for Enterprise Architects to understand the drawbacks and challenges to migrate the workload to Cloud. This paper aims to identify the key factors to migrate the workload to the cloud. It also helps an organization to identify the candidate for cloud migration. An impulsive decision to move to the Cloud may be detrimental to an organization. Also, I will discuss one case study to see the benefits and disadvantages of cloud migration. This will help the organization to maximize its ROI.",CS,AI_ML,0.85,Extracted from log - paper 394
Cloud Security and Security Challenges Revisited,"In recent years, Cloud Computing has transformed local businesses and created new business models on the Internet- and Cloud services are still flourishing. But after the emphatic hype in the early years, a more realistic perception of Cloud services has emerged. One reason for this surely is that today, Cloud Computing is considered as an established and well-accepted technology and no longer as a technical novelty. But the second reason for this assessment might also be numerous security issues that Cloud Computing in general or specific Cloud services have experienced since then. In this paper, we revisit attacks on Cloud services and Cloud-related attack vectors that have been published in recent years. We then consider successful or proposed solutions to cope with these challenges. Based on these findings, we apply a security metric in order to rank all these Cloud-related security challenges concerning their severity. This should assist security professionals to prioritize their efforts toward addressing these issues.",CS,AI_ML,0.85,Extracted from log - paper 395
"11-Year Warm Cloud Modification Experiment in Maharashtra State, India","A warm cloud modification experiment was carried out in an area of 4800 Sq.Km in the Pune region,India, during the 11-summer monsoon (June-September) seasons (1973-74, 1976, 1979-86). A double-area cross-over design with area randomization was adopted and an instrumented aircraft was used for seeding and cloud physical measurements. Finely pulverised salt (sodium chloride) particles were released into the monsoon clouds (cumulus and stratocumulus) during aircraft penetrations into the clouds at a height of 200-300 m above the cloud-base. The warm cloud responses to salt seeding are found to be critically dependent on the cloud physical characteristics e.g., vertical thickness and liquid water content. Clouds with vertical thickness greater than 1 km, LWC greater than 0.5 gm/cubic m when seeded with salt particles (modal diameter 10 micro m, concentration 1 per litre of cloud air) produced increase in rainfall of 24 per cent significant at 4 per cent level. Shallow clouds (vertical thickness less than 1 km, LWC less than 0.5 gm/cubic m) when seeded showed tendency for dissipation. The cloud physical observations made in not-seeded (control) and seeded (target) clouds have provided some useful evidence to test the applicability of the warm cloud modification hypothesis. The results of the cloud model computations suggested that moderate convergence at the cloud-base is essential for the cloud growth and development of precipitation in the real world. Hygroscopic particle seeding of warm clouds under favourable dynamical conditions (convergence at the cloud-base level) may result in the acceleration of the collision-coalescence process resulting in the enhancement of rainfall.",CS,AI_ML,0.85,Extracted from log - paper 396
Application of Ontologies in Cloud Computing: The State-Of-The-Art,"This paper presents a systematic survey on existing literature and seminal works relevant to the application of ontologies in different aspects of Cloud computing. Our hypothesis is that ontologies along with their reasoning capabilities can have significant impact on improving various aspects of the Cloud computing phenomena. Ontologies can promote intelligent decision support mechanisms for various Cloud based services. They can also provide effective interoperability among the Cloud based systems and resources. This survey can promote a comprehensive understanding on the roles and significance of ontologies within the overall domain of Cloud Computing. Also, this project can potentially form the basis of new research area and possibilities for both ontology and Cloud computing communities.",CS,AI_ML,0.85,Extracted from log - paper 397
A Slow Read attack Using Cloud,"Cloud computing relies on sharing computing resources rather than having local servers or personal devices to handle applications. Nowadays, cloud computing has become one of the fastest growing fields in information technology. However, several new security issues of cloud computing have emerged due to its service delivery models. In this paper, we discuss the case of distributed denial-of-service (DDoS) attack using Cloud resources. First, we show how such attack using a cloud platform could not be detected by previous techniques. Then we present a tricky solution based on the cloud as well.",CS,AI_ML,0.85,Extracted from log - paper 398
KCES: A Workflow Containerization Scheduling Scheme Under Cloud-Edge Collaboration Framework,"As more IoT applications gradually move towards the cloud-edge collaborative mode, the containerized scheduling of workflows extends from the cloud to the edge. However, given the high delay of the communication network, loose coupling of structure, and resource heterogeneity between cloud and edge, workflow containerization scheduling in the cloud-edge scenarios faces the difficulty of resource coordination and application collaboration management. To address these two issues, we propose a KubeEdge-Cloud-Edge-Scheduling scheme named KCES, a workflow containerization scheduling scheme for the KubeEdge cloud-edge framework. The KCES includes a cloud-edge workflow scheduling engine for KubeEdge and workflow scheduling strategies for task horizontal roaming and vertical offloading. Considering the scheduling optimization of cloud-edge workflows, this paper proposes a cloud-edge workflow scheduling model and cloud-edge node model and designs a cloud-edge workflow scheduling engine to maximize cloud-edge resource utilization under the constraint of workflow task delay. A cloud-edge resource hybrid management technology is used to design the cloud-edge resource evaluation and resource allocation algorithms to achieve cloud-edge resource collaboration. Based on the ideas of distributed functional roles and the hierarchical division of computing power, the horizontal roaming among the edges and vertical offloading strategies between the cloud and edges for workflow tasks are designed to realize the cloud-edge application collaboration. Through a customized IoT application workflow instance, experimental results show that KCES is superior to the baseline in total workflow duration, average workflow duration, and resource usage and has the capabilities of horizontal roaming and vertical offloading for workflow tasks.",CS,AI_ML,0.85,Extracted from log - paper 399
Research Challenges for Enterprise Cloud Computing,"Cloud computing represents a shift away from computing as a product that is purchased, to computing as a service that is delivered to consumers over the internet from large-scale data centers - or ""clouds"". This paper discusses some of the research challenges for cloud computing from an enterprise or organizational perspective, and puts them in context by reviewing the existing body of literature in cloud computing. Various research challenges relating to the following topics are discussed: the organizational changes brought about by cloud computing; the economic and organizational implications of its utility billing model; the security, legal and privacy issues that cloud computing raises. It is important to highlight these research challenges because cloud computing is not simply about a technological improvement of data centers but a fundamental change in how IT is provisioned and used. This type of research has the potential to influence wider adoption of cloud computing in enterprise, and in the consumer market too.",CS,AI_ML,0.85,Extracted from log - paper 400
I Have the Proof: Providing Proofs of Past Data Possession in Cloud Forensics,"Cloud computing has emerged as a popular computing paradigm in recent years. However, today's cloud computing architectures often lack support for computer forensic investigations. A key task of digital forensics is to prove the presence of a particular file in a given storage system. Unfortunately, it is very hard to do so in a cloud given the black-box nature of clouds and the multi-tenant cloud models. In clouds, analyzing the data from a virtual machine instance or data stored in a cloud storage only allows us to investigate the current content of the cloud storage, but not the previous contents. In this paper, we introduce the idea of building proofs of past data possession in the context of a cloud storage service. We present a scheme for creating such proofs and evaluate its performance in a real cloud provider. We also discuss how this proof of past data possession can be used effectively in cloud forensics.",CS,AI_ML,0.85,Extracted from log - paper 401
A Cloud Computing Survey: Developments and Future Trends in Infrastructure as a Service Computing,"Cloud computing is a recent paradigm based around the notion of delivery of resources via a service model over the Internet. Despite being a new paradigm of computation, cloud computing owes its origins to a number of previous paradigms. The term cloud computing is well defined and no longer merits rigorous taxonomies to furnish a definition. Instead this survey paper considers the past, present and future of cloud computing. As an evolution of previous paradigms, we consider the predecessors to cloud computing and what significance they still hold to cloud services. Additionally we examine the technologies which comprise cloud computing and how the challenges and future developments of these technologies will influence the field. Finally we examine the challenges that limit the growth, application and development of cloud computing and suggest directions required to overcome these challenges in order to further the success of cloud computing.",CS,AI_ML,0.85,Extracted from log - paper 402
Dynamic Resource Allocation for Virtual Machine Migration Optimization using Machine Learning,"The paragraph is grammatically correct and logically coherent. It discusses the importance of mobile terminal cloud computing migration technology in meeting the demands of evolving computer and cloud computing technologies. It emphasizes the need for efficient data access and storage, as well as the utilization of cloud computing migration technology to prevent additional time delays. The paragraph also highlights the contributions of cloud computing migration technology to expanding cloud computing services. Additionally, it acknowledges the role of virtualization as a fundamental capability of cloud computing while emphasizing that cloud computing and virtualization are not inherently interconnected. Finally, it introduces machine learning-based virtual machine migration optimization and dynamic resource allocation as a critical research direction in cloud computing, citing the limitations of static rules or manual settings in traditional cloud computing environments. Overall, the paragraph effectively communicates the importance of machine learning technology in addressing resource allocation and virtual machine migration challenges in cloud computing.",CS,AI_ML,0.85,Extracted from log - paper 403
Compute and Storage Clouds Using Wide Area High Performance Networks,"We describe a cloud based infrastructure that we have developed that is optimized for wide area, high performance networks and designed to support data mining applications. The infrastructure consists of a storage cloud called Sector and a compute cloud called Sphere. We describe two applications that we have built using the cloud and some experimental studies.",CS,AI_ML,0.85,Extracted from log - paper 404
An Automated Implementation of Hybrid Cloud for Performance Evaluation of Distributed Databases,"A Hybrid cloud is an integration of resources between private and public clouds. It enables users to horizontally scale their on-premises infrastructure up to public clouds in order to improve performance and cut up-front investment cost. This model of applications deployment is called cloud bursting that allows data-intensive applications especially distributed database systems to have the benefit of both private and public clouds. In this work, we present an automated implementation of a hybrid cloud using (i) a robust and zero-cost Linux-based VPN to make a secure connection between private and public clouds, and (ii) Terraform as a software tool to deploy infrastructure resources based on the requirements of hybrid cloud. We also explore performance evaluation of cloud bursting for six modern and distributed database systems on the hybrid cloud spanning over local OpenStack and Microsoft Azure. Our results reveal that MongoDB and MySQL Cluster work efficient in terms of throughput and operations latency if they burst into a public cloud to supply their resources. In contrast, the performance of Cassandra, Riak, Redis, and Couchdb reduces if they significantly leverage their required resources via cloud bursting.",CS,AI_ML,0.85,Extracted from log - paper 405
Characterizing User and Provider Reported Cloud Failures,"Cloud computing is the backbone of the digital society. Digital banking, media, communication, gaming, and many others depend on cloud services. Unfortunately, cloud services may fail, leading to damaged services, unhappy users, and perhaps millions of dollars lost for companies. Understanding a cloud service failure requires a detailed report on why and how the service failed. Previous work studies how cloud services fail using logs published by cloud operators. However, information is lacking on how users perceive and experience cloud failures. Therefore, we collect and characterize the data for user-reported cloud failures from Down Detector for three cloud service providers over three years. We count and analyze time patterns in the user reports, and derive failures from those user reports and characterize their duration and interarrival time. We characterize provider-reported cloud failures and compare the results with the characterization of user-reported failures. The comparison reveals the information of how users perceive failures and how much of the failures are reported by cloud service providers. Overall, this work provides a characterization of user- and provider-reported cloud failures and compares them with each other.",CS,AI_ML,0.85,Extracted from log - paper 406
Platforms for Building and Deploying Applications for Cloud Computing,"Cloud computing is rapidly emerging as a new paradigm for delivering IT services as utlity-oriented services on subscription-basis. The rapid development of applications and their deployment in Cloud computing environments in efficient manner is a complex task. In this article, we give a brief introduction to Cloud computing technology and Platform as a Service, we examine the offerings in this category, and provide the basis for helping readers to understand basic application platform opportunities in Cloud by technologies such as Microsoft Azure, Sales Force, Google App, and Aneka for Cloud computing. We demonstrate that Manjrasoft Aneka is a Cloud Application Platform (CAP) leveraging these concepts and allowing an easy development of Cloud ready applications on a Private/Public/Hybrid Cloud. Aneka CAP offers facilities for quickly developing Cloud applications and a modular platform where additional services can be easily integrated to extend the system capabilities, thus being at pace with the rapidly evolution of Cloud computing.",CS,AI_ML,0.85,Extracted from log - paper 407
Formal Specification Language Based IaaS Cloud Workload Regression Analysis,"Cloud Computing is an emerging area for accessing computing resources. In general, Cloud service providers offer services that can be clustered into three categories: SaaS, PaaS and IaaS. This paper discusses the Cloud workload analysis. The efficient Cloud workload resource mapping technique is proposed. This paper aims to provide a means of understanding and investigating IaaS Cloud workloads and the resources. In this paper, regression analysis is used to analyze the Cloud workloads and identifies the relationship between Cloud workloads and available resources. The effective organization of dynamic nature resources can be done with the help of Cloud workloads. Till Cloud workload is considered a vital talent, the Cloud resources cannot be consumed in an effective style. The proposed technique has been validated by Z Formal specification language. This approach is effective in minimizing the cost and submission burst time of Cloud workloads.",CS,AI_ML,0.85,Extracted from log - paper 408
An Analysis of the Cloud Computing Security Problem,"Cloud computing is a new computational paradigm that offers an innovative business model for organizations to adopt IT without upfront investment. Despite the potential gains achieved from the cloud computing, the model security is still questionable which impacts the cloud model adoption. The security problem becomes more complicated under the cloud model as new dimensions have entered into the problem scope related to the model architecture, multi-tenancy, elasticity, and layers dependency stack. In this paper we introduce a detailed analysis of the cloud security problem. We investigated the problem from the cloud architecture perspective, the cloud offered characteristics perspective, the cloud stakeholders' perspective, and the cloud service delivery models perspective. Based on this analysis we derive a detailed specification of the cloud security problem and key features that should be covered by any proposed security solution.",CS,AI_ML,0.85,Extracted from log - paper 409
Datacenter Changes vs. Employment Rates for Datacenter Managers In the Cloud Computing Era,"Due to the evolving Cloud Computing paradigm, there is a prevailing concern that in the near future data center managers may be in short supply. Cloud computing, as a whole, is becoming more prevalent into today s computing world. In fact, cloud computing has become so popular that some are now referring to data centers as cloud centers. How does this interest in cloud computing translate into employment rates for data center managers? The popularity of the public and private cloud models are the prevailing force behind answering this question. Therefore, the skill set of the datacenter manager has evolved to harness the on demand self-services, broad network access, resource pooling, rapid elasticity, measured service, and multi tenacity characteristics of cloud computing. Using diverse sources ranging from the Bureau of Labor and Statistics to trade articles, this manuscript takes an in-depth look at these employment rates related to the cloud and the determining factors behind them. Based on the information available, datacenter manager employment rates in the cloud computing era will continue to increase well into 2016.",CS,AI_ML,0.85,Extracted from log - paper 410
A Novel Application Licensing Framework for Mobile Cloud Environment,"Mobile cloud computing is a new technology that enhances smartphone applications capabilities in terms of performance, energy efficiency, and execution support. These features are achieved via computation offloading technique that is supported by specialized mobile cloud application development models. However, the cloud-enabled applications are prone to application piracy issue for which the traditional licensing frameworks are of no use. Therefore, a new licensing framework is required to control application piracy in mobile cloud environment. This paper presents a preliminary design of a novel application licensing framework for mobile cloud environment that restricts execution of applications on unauthenticated smartphones and cloud resources.",CS,AI_ML,0.85,Extracted from log - paper 411
Towards Constraint-based High Performance Cloud System in the Process of Cloud Computing Adoption in an Organization,"Cloud computing is penetrating into various domains and environments, from theoretical computer science to economy, from marketing hype to educational curriculum and from R&D lab to enterprise IT infrastructure. Yet, the currently developing state of cloud computing leaves several issues to address and also affects cloud computing adoption by organizations. In this paper, we explain how the transition into the cloud can occur in an organization and describe the mechanism for transforming legacy infrastructure into a virtual infrastructure-based cloud. We describe the state of the art of infrastructural cloud, which is essential in the decision making on cloud adoption, and highlight the challenges that can limit the scale and speed of the adoption. We then suggest a strategic framework for designing a high performance cloud system. This framework is applicable when transformation cloudbased deployment model collides with some constraints. We give an example of the implementation of the framework in a design of a budget-constrained high availability cloud system.",CS,AI_ML,0.85,Extracted from log - paper 412
Cloud Infrastructure Service Management - A Review,"The new era of computing called Cloud Computing allows the user to access the cloud services dynamically over the Internet wherever and whenever needed. Cloud consists of data and resources; and the cloud services include the delivery of software, infrastructure, applications, and storage over the Internet based on user demand through Internet. In short, cloud computing is a business and economic model allowing the users to utilize high-end computing and storage virtually with minimal infrastructure on their end. Cloud has three service models namely, Cloud Software-as-a-Service (SaaS), Cloud Platform-as-a-Service (PaaS), and Cloud Infrastructure-as-a-Service (IaaS). This paper talks in depth of cloud infrastructure service management.",CS,AI_ML,0.85,Extracted from log - paper 413
Towards a Taxonomy of Performance Evaluation of Commercial Cloud Services,"Cloud Computing, as one of the most promising computing paradigms, has become increasingly accepted in industry. Numerous commercial providers have started to supply public Cloud services, and corresponding performance evaluation is then inevitably required for Cloud provider selection or cost-benefit analysis. Unfortunately, inaccurate and confusing evaluation implementations can be often seen in the context of commercial Cloud Computing, which could severely interfere and spoil evaluation-related comprehension and communication. This paper introduces a taxonomy to help profile and standardize the details of performance evaluation of commercial Cloud services. Through a systematic literature review, we constructed the taxonomy along two dimensions by arranging the atomic elements of Cloud-related performance evaluation. As such, this proposed taxonomy can be employed both to analyze existing evaluation practices through decomposition into elements and to design new experiments through composing elements for evaluating performance of commercial Cloud services. Moreover, through smooth expansion, we can continually adapt this taxonomy to the more general area of evaluation of Cloud Computing.",CS,AI_ML,0.85,Extracted from log - paper 414
A Survey and Comparative Study on Multi-Cloud Architectures: Emerging Issues And Challenges For Cloud Federation,"Multi-cloud concept has broaden the world of cloud computing and has become a buzzword today. The word Multi-cloud envisions utilization of services from multiple heterogeneous cloud providers via a single architecture at customer premises. Though cloud computing has many issues and offers open research challenges, still the academics and industrial research has paved a pathway for multi-cloud environment. The concept of multi-cloud is in maturing phase, and many research projects are in progress to provide a multi-cloud architecture which is successfully enabled in all the respects like easy configuration, security, management etc. In this paper, concepts, challenges, requirement and future directions for multi-cloud environment are discussed. A survey of existing approaches and solutions provided by different multi-cloud architectures is entailed along with analysis of the pros and cons of different architectures while comparing the same.",CS,AI_ML,0.85,Extracted from log - paper 415
Molecular Dynamics Simulations on Cloud Computing and Machine Learning Platforms,"Scientific computing applications have benefited greatly from high performance computing infrastructure such as supercomputers. However, we are seeing a paradigm shift in the computational structure, design, and requirements of these applications. Increasingly, data-driven and machine learning approaches are being used to support, speed-up, and enhance scientific computing applications, especially molecular dynamics simulations. Concurrently, cloud computing platforms are increasingly appealing for scientific computing, providing ""infinite"" computing powers, easier programming and deployment models, and access to computing accelerators such as TPUs (Tensor Processing Units). This confluence of machine learning (ML) and cloud computing represents exciting opportunities for cloud and systems researchers. ML-assisted molecular dynamics simulations are a new class of workload, and exhibit unique computational patterns. These simulations present new challenges for low-cost and high-performance execution. We argue that transient cloud resources, such as low-cost preemptible cloud VMs, can be a viable platform for this new workload. Finally, we present some low-hanging fruits and long-term challenges in cloud resource management, and the integration of molecular dynamics simulations into ML platforms (such as TensorFlow).",CS,AI_ML,0.85,Extracted from log - paper 416
Model-Based Cloud Resource Management with TOSCA and OCCI,"With the advent of cloud computing, different cloud providers with heterogeneous cloud services (compute, storage, network, applications, etc.) and their related Application Programming Interfaces (APIs) have emerged. This heterogeneity complicates the implementation of an interoperable cloud system. Several standards have been proposed to address this challenge and provide a unified interface to cloud resources. The Open Cloud Computing Interface (OCCI) thereby focuses on the standardization of a common API for Infrastructure-as-a-Service (IaaS) providers while the Topology and Orchestration Specification for Cloud Applications (TOSCA) focuses on the standardization of a template language to enable the proper definition of the topology of cloud applications and their orchestrations on top of a cloud system. TOSCA thereby does not define how the application topologies are created on the cloud. Therefore, we analyse the conceptual similarities between the two approaches and we study how we can integrate them to obtain a complete standard-based approach to manage both cloud infrastructure and cloud application layers. We propose an automated extensive mapping between the concepts of the two standards and we provide TOSCA Studio, a model-driven tool chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically design cloud applications as well as to deploy and manage them at runtime using a fully model-driven cloud orchestrator based on the two standards. Our contribution is validated by successfully designing and deploying three cloud applications: WordPress, Node Cellar and Multi-Tier.",CS,AI_ML,0.85,Extracted from log - paper 417
Defining Cross-Cloud Systems,"Recent years have seen an increasing number of cross-cloud architectures, i.e. systems that span across cloud provisioning boundaries. However, the cloud computing world still lacks any standards in terms of programming interfaces, which has a knock-on effect on the costs associated with interoperability and severely limits the flexibility and portability of applications and virtual infrastructures. This paper outlines the different types of cross-cloud systems, and the associated design decisions.",CS,AI_ML,0.85,Extracted from log - paper 418
An Experimental Study of Load Balancing of OpenNebula Open-Source Cloud Computing Platform,"Cloud Computing is becoming a viable computing solution for services oriented computing. Several open-source cloud solutions are available to these supports. Open-source software stacks offer a huge amount of customizability without huge licensing fees. As a result, open source software are widely used for designing cloud, and private clouds are being built increasingly in the open source way. Numerous contributions have been made by the open-source community related to private-IaaS-cloud. OpenNebula - a cloud platform is one of the popular private cloud management software. However, little has been done to systematically investigate the performance evaluation of this open-source cloud solution in the existing literature. The performance evaluation aids new and existing research, industry and international projects when selecting OpenNebula software to their work. The objective of this paper is to evaluate the load-balancing performance of the OpenNebula cloud management software. For the performance evaluation, the OpenNebula cloud management software is installed and configured as a prototype implementation and tested on the DIU Cloud Lab. In this paper, two set of experiments are conducted to identify the load balancing performance of the OpenNebula cloud management platform- (1) Delete and Add Virtual Machine (VM) from OpenNebula cloud platform; (2) Mapping Physical Hosts to Virtual Machines (VMs) in the OpenNebula cloud platform.",CS,AI_ML,0.85,Extracted from log - paper 419
SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering,"Point clouds obtained from 3D sensors are usually sparse. Existing methods mainly focus on upsampling sparse point clouds in a supervised manner by using dense ground truth point clouds. In this paper, we propose a self-supervised point cloud upsampling network (SSPU-Net) to generate dense point clouds without using ground truth. To achieve this, we exploit the consistency between the input sparse point cloud and generated dense point cloud for the shapes and rendered images. Specifically, we first propose a neighbor expansion unit (NEU) to upsample the sparse point clouds, where the local geometric structures of the sparse point clouds are exploited to learn weights for point interpolation. Then, we develop a differentiable point cloud rendering unit (DRU) as an end-to-end module in our network to render the point cloud into multi-view images. Finally, we formulate a shape-consistent loss and an image-consistent loss to train the network so that the shapes of the sparse and dense point clouds are as consistent as possible. Extensive results on the CAD and scanned datasets demonstrate that our method can achieve impressive results in a self-supervised manner. Code is available at https://github.com/fpthink/SSPU-Net.",CS,AI_ML,0.85,Extracted from log - paper 420
A Hybrid Cloud ERP Framework For Processing Purchasing Data,"Cloud-based enterprise resource planning (cloud ERP) systems have existed in the business market for around ten years. Cloud ERP supports enterprises' daily activities by integrating organizational back-end systems in the cloud environment. One of the critical functions that cloud ERP offers is the purchasing application. The purchasing function of cloud ERP enables enterprises to streamline all the online purchasing transactions in real-time automatically. Even cloud ERP is deployed quite often these days, organizations somehow still lack the knowledge of it; to be specific, there are many issues attached to cloud ERP implementation yet to be solved. Hence, this paper compares four leading cloud ERP platforms in Australia and proposes a hybrid cloud ERP framework to process online purchasing transactions. By adopting a case study approach, a purchasing web-based application is designed and presented in this paper. In general, the proposed hybrid cloud ERP framework and the integrated web-based purchasing application allow user companies to process online purchasing transactions with short operation time and increased business efficiency; in the meantime, the proposed framework also reduces security risks attached to the public cloud.",CS,AI_ML,0.85,Extracted from log - paper 421
Configuration management in the distributed cloud,"Owing to their cost-effectiveness and flexibility, cloud services have been the default choice for the deployment of innumerable software systems over the years. However, novel paradigms are beginning to emerge, as the cloud can't meet the requirements of increasingly many latency- and privacy-sensitive applications. The distributed cloud model, being one of the attempts to overcome these challenges, places a distributed cloud layer between device and cloud layers, intending to bring resources closer to data sources. As application code should be kept separate from its configuration, especially in highly dynamic cloud environments, there is a need to incorporate configuration primitives in future distributed cloud platforms. In this paper, we present the design and implementation of a configuration management subsystem for an open-source distributed cloud platform. Our solution spreads across the cloud and distributed cloud layers and supports configuration versioning, selective dissemination to nodes in the distributed cloud layer, and logical isolation via namespaces. Our work serves as a demonstration of the feasibility and usability of the new cloud-extending models and provides valuable insight into one of the possible implementations.",CS,AI_ML,0.85,Extracted from log - paper 422
A Cost-Effective Strategy for Storing Scientific Datasets with Multiple Service Providers in the Cloud,"Cloud computing provides scientists a platform that can deploy computation and data intensive applications without infrastructure investment. With excessive cloud resources and a decision support system, large generated data sets can be flexibly 1 stored locally in the current cloud, 2 deleted and regenerated whenever reused or 3 transferred to cheaper cloud service for storage. However, due to the pay for use model, the total application cost largely depends on the usage of computation, storage and bandwidth resources, hence cutting the cost of cloud based data storage becomes a big concern for deploying scientific applications in the cloud. In this paper, we propose a novel strategy that can cost effectively store large generated data sets with multiple cloud service providers. The strategy is based on a novel algorithm that finds the trade off among computation, storage and bandwidth costs in the cloud, which are three key factors for the cost of data storage. Both general (random) simulations conducted with popular cloud service providers pricing models and three specific case studies on real world scientific applications show that the proposed storage strategy is highly cost effective and practical for run time utilization in the cloud.",CS,AI_ML,0.85,Extracted from log - paper 423
A Context Aware and Self Adaptation Strategy for Cloud Service Selection and Configuration in Run Time,"Day after day, the number of mobile applications deployed on cloud computing continues in increasing because o f smartphone capabilities improvement. Cloud computing has already succeeded in the web based application, for that reason, the demand for context aware services provided by cloud computing increases. To customize a cloud service that takes into account th e consumer requirements, which depend on information change, it brings to light many recent challenges to cloud computing about environment aware, location aware, time aware. The cloud provider, moreover, has to manage personalized applications and the con straints of mobile devices in matters of interaction abilities and communication restrictions. This paper proposes a strategy for selecting automatically an appropriate cloud environment that runs out whole requirements, defines a configuration for the ass ociated cloud environment and able to easily adapt to the change of the environment on either the user or the cloud side or both. This process builds on the principles of dynamic software product lines, Agent oriented software engineering, and the MAPE k m odel to select and configure cloud environments according to the consumer needs and the context change.",CS,AI_ML,0.85,Extracted from log - paper 424
What is the next innovation after the internet of things?,"The world had witnessed several generations of the Internet. Starting with the Fixed Internet, then the Mobile Internet, scientists now focus on many types of research related to the ""Thing"" Internet (or Internet of Things). The question is ""what is the next Internet generation after the Thing Internet?"" This paper envisions about the Tactile Internet which could be the next Internet generation in the near future. The paper will introduce what is the tactile internet, why it could be the next future Internet, as well as the impact and its application in the future society. Furthermore, some challenges and the requirements are presented to guide further research in this near future field.",CS,AI_ML,0.85,Extracted from log - paper 425
In Things We Trust? Towards trustability in the Internet of Things,"This essay discusses the main privacy, security and trustability issues with the Internet of Things.",CS,AI_ML,0.85,Extracted from log - paper 426
Privacy in the Internet of Things: Threats and Challenges,"The Internet of Things paradigm envisions the pervasive interconnection and cooperation of smart things over the current and future Internet infrastructure. The Internet of Things is, thus, the evolution of the Internet to cover the real-world, enabling many new services that will improve people's everyday lives, spawn new businesses and make buildings, cities and transport smarter. Smart things allow indeed for ubiquitous data collection or tracking, but these useful features are also examples of privacy threats that are already now limiting the success of the Internet of Things vision when not implemented correctly. These threats involve new challenges such as the pervasive privacy-aware management of personal data or methods to control or avoid ubiquitous tracking and profiling. This paper analyzes the privacy issues in the Internet of Things in detail. To this end, we first discuss the evolving features and trends in the Internet of Things with the goal of scrutinizing their privacy implications. Second, we classify and examine privacy threats in this new setting, pointing out the challenges that need to be overcome to ensure that the Internet of Things becomes a reality.",CS,AI_ML,0.85,Extracted from log - paper 427
Sensing as a Service (S2aaS): Buying and Selling IoT Data,"The Internet of Things (IoT) [1] envisions the creation of an environment where everyday objects (e.g. microwaves, fridges, cars, coffee machines, etc.) are connected to the internet and make users' lives more convenient. It will also lead users to consume resources more efficiently.",CS,AI_ML,0.85,Extracted from log - paper 428
Rentable Internet of Things Infrastructure for Sensing as a Service (S2aaS),Sensing as a Service (S2aaS) model [1] [2] is inspired by the traditional Everything as a service (XaaS) approaches [3]. It aims to better utilize the existing Internet of Things (IoT) infrastructure. S2aaS vision aims to create 'rentable infrastructure' where interested parties can gather IoT data by paying a fee for the infrastructure owners.,CS,AI_ML,0.85,Extracted from log - paper 429
User Empowerment in the Internet of Things,This paper focuses on the characteristics of two big triggers that facilitated wide user adoption of the Internet: Web 2.0 and online social networks. We detect brakes for reproduction of these events in Internet of things. To support our hypothesis we first compare the difference between the ways of use of the Internet with the future scenarios of Internet of things. We detect barriers that could slow down apparition of this kind of social events during user adoption of Internet of Things and we propose a conceptual framework to solve these problems.,CS,AI_ML,0.85,Extracted from log - paper 430
Review of internet of things of security threats and Challenges,"The Internet of Things has received a lot of research attention. It is considered part of the Internet of the future and is made up of billions of intelligent communication. The future of the Internet will consist of heterogeneously connected devices that expand the world boundaries with physical entities and virtual components. It provides new functionality for related things. This study systematically examines the definition, architecture, essential technologies, and applications of the Internet of Things. We will introduce various definitions of the Internet of Things. Then, it will be discussed new techniques for implementing the Internet of Things and several open issues related to the Internet of Things applications will be investigated. Finally, the key challenges that need to be addressed by the research community and possible solutions to address them are investigated.",CS,AI_ML,0.85,Extracted from log - paper 431
Challenges and Opportunities in Securing the Industrial Internet of Things,"Given the tremendous success of the Internet of Things in interconnecting consumer devices, we observe a natural trend to likewise interconnect devices in industrial settings, referred to as Industrial Internet of Things or Industry 4.0. While this coupling of industrial components provides many benefits, it also introduces serious security challenges. Although sharing many similarities with the consumer Internet of Things, securing the Industrial Internet of Things introduces its own challenges but also opportunities, mainly resulting from a longer lifetime of components and a larger scale of networks. In this paper, we identify the unique security goals and challenges of the Industrial Internet of Things, which, unlike consumer deployments, mainly follow from safety and productivity requirements. To address these security goals and challenges, we provide a comprehensive survey of research efforts to secure the Industrial Internet of Things, discuss their applicability, and analyze their security benefits.",CS,AI_ML,0.85,Extracted from log - paper 432
A Study on Internet of Things based Applications,"This paper gives a detail analysis of various applications based on Internet of Thing (IoT)s. This explains about how internet of things evolved from mobile computing and ubiquitous computing. It emphasises the fact that objects are connected over the internet rather than people. The properties of Internet of Things (IOT) are product information, electronic tag, standard expressed and uploading information. It utilises the Radio Frequency Identification (RFID) technology and wireless sensor networks (WSN). IOT applications are used in domains such as healthcare, supply chain management, defence and agriculture. Lastly the paper focuses on issues involved in IOT. Though it is a boon, IOT faces certain crucial issues like privacy and security.",CS,AI_ML,0.85,Extracted from log - paper 433
Human Resource Development and the Internet of Things,"The Internet of Things (IoT) is affecting national innovation ecosystems and the approach of organizations to innovation and how they create and capture value in everyday business activities. The Internet of Things (IoT), is disruptive, and it will change the manner in which human resources are developed and managed, calling for a new and adaptive human resource development approach. The Classical Internet communication form is human-human. The prospect of IoT is that every object will have a unique way of identification and can be addressed so that every object can be connected. The communication forms will expand from human-human to human-human, human-thing, and thing-thing. This will bring a new challenge to how Human Resource Development (HRD) is practiced. This paper provides an overview of the Internet of Things and conceptualizes the role of HRD in the age of the Internet of Things. Keywords:",CS,AI_ML,0.85,Extracted from log - paper 434
6G Internet of Things: A Comprehensive Survey,"The sixth generation (6G) wireless communication networks are envisioned to revolutionize customer services and applications via the Internet of Things (IoT) towards a future of fully intelligent and autonomous systems. In this article, we explore the emerging opportunities brought by 6G technologies in IoT networks and applications, by conducting a holistic survey on the convergence of 6G and IoT. We first shed light on some of the most fundamental 6G technologies that are expected to empower future IoT networks, including edge intelligence, reconfigurable intelligent surfaces, space-air-ground-underwater communications, Terahertz communications, massive ultra-reliable and low-latency communications, and blockchain. Particularly, compared to the other related survey papers, we provide an in-depth discussion of the roles of 6G in a wide range of prospective IoT applications via five key domains, namely Healthcare Internet of Things, Vehicular Internet of Things and Autonomous Driving, Unmanned Aerial Vehicles, Satellite Internet of Things, and Industrial Internet of Things. Finally, we highlight interesting research challenges and point out potential directions to spur further research in this promising area.",CS,AI_ML,0.85,Extracted from log - paper 435
On Web-based Domain-Specific Language for Internet of Things,"This paper discusses the challenges of the Internet of Things programming. Sensing and data gathering from the various sources are often the key elements of applications for Smart Cities. So, the effective programming models for them are very important. In this article, we discuss system software models and solutions, rather than network related aspects. In our paper, we present the web-based domain-specific language for Internet of Things applications. Our goal is to present the modern models for data processing in Internet of Things and Smart Cities applications. In our view, the use of this kind of tools should seriously reduce the time to develop new applications.",CS,AI_ML,0.85,Extracted from log - paper 436
"Internet of Nano, Bio-Nano, Biodegradable and Ingestible Things: A Survey","In recent years, advances in biotechnology, nanotechnology and materials science have led to development of revolutionizing applications in Internet of Things (IoT). In particular, the interconnection of nanomaterials, nanoimplants and nanobiosensors with existing IoT networks have inspired the concepts of Internet of Nano Things (IoNT), Internet of Bio-Nano Things (IoBNT), Internet of Biodegradable Things (IoBDT) and Internet of Ingestible Things (IoIT). To date, although there are several survey papers that addressed these concepts separately, there is no current survey covering all studies in IoNT, IoBNT, IoBDT and IoIT. Therefore, in this paper, we provide a complete overview of all recent work in these four areas. Furthermore, we emphasize the research challenges, potential applications, and open research areas.",CS,AI_ML,0.85,Extracted from log - paper 437
The Internet of Things: Perspectives on Security from RFID and WSN,"A massive current research effort focuses on combining pre-existing 'Intranets' of Things into one Internet of Things. However, this unification is not a panacea; it will expose new attack surfaces and vectors, just as it enables new applications. We therefore urgently need a model of security in the Internet of Things. In this regard, we note that IoT descends directly from pre-existing research (in embedded Internet and pervasive intelligence), so there exist several bodies of related work: security in RFID, sensor networks, cyber-physical systems, and so on. In this paper, we survey the existing literature on RFID and WSN security, as a step to compiling all known attacks and defenses relevant to the Internet of Things.",CS,AI_ML,0.85,Extracted from log - paper 438
Turing Test for the Internet of Things,"How smart is your kettle? How smart are things in your kitchen, your house, your neighborhood, on the internet? With the advent of Internet of Things, and the move of making devices `smart' by utilizing AI, a natural question arrises, how can we evaluate the progress. The standard way of evaluating AI is through the Turing Test. While Turing Test was designed for AI; the device that it was tailored to was a computer. Applying the test to variety of devices that constitute Internet of Things poses a number of challenges which could be addressed through a number of adaptations.",CS,AI_ML,0.85,Extracted from log - paper 439
Privacy Preservation Technologies in Internet of Things,"Since the beginning of the Internet thirty years ago, we have witnessed a number of changes in the application of communication technologies. Today, the Internet can be described to a large extent as a ubiquitous infrastructure that is always accessible. After the era of connecting places and connecting people, the Internet of the future will also connect things. The idea behind the resulting Internet of Things is to seamlessly gather and use information about objects of the real world during their entire lifecycle. In this paper, we consider different approaches to technological protection of user data privacy in the world of Internet of Things. In particular,we consider what kind of security problems are being faced and what level of protection can be provided by applying approaches based on secure multi-party computations.",CS,AI_ML,0.85,Extracted from log - paper 440
User-driven Privacy Enforcement for Cloud-based Services in the Internet of Things,"Internet of Things devices are envisioned to penetrate essentially all aspects of life, including homes and urbanspaces, in use cases such as health care, assisted living, and smart cities. One often proposed solution for dealing with the massive amount of data collected by these devices and offering services on top of them is the federation of the Internet of Things and cloud computing. However, user acceptance of such systems is a critical factor that hinders the adoption of this promising approach due to severe privacy concerns. We present UPECSI, an approach for user-driven privacy enforcement for cloud-based services in the Internet of Things to address this critical factor. UPECSI enables enforcement of all privacy requirements of the user once her sensitive data leaves the border of her network, provides a novel approach for the integration of privacy functionality into the development process of cloud-based services, and offers the user an adaptable and transparent configuration of her privacy requirements. Hence, UPECSI demonstrates an approach for realizing user-accepted cloud services in the Internet of Things.",CS,AI_ML,0.85,Extracted from log - paper 441
Low power communication signal enhancement method of Internet of things based on nonlocal mean denoising,"In order to improve the transmission effect of low-power communication signal of Internet of things and compress the enhancement time of low-power communication signal, this paper designs a low-power communication signal enhancement method of Internet of things based on nonlocal mean denoising. Firstly, the residual of one-dimensional communication layer is pre processed by convolution core to obtain the residual of one-dimensional communication layer; Then, according to the two classification recognition method, the noise reduction signal feature recognition of the low-power communication signal of the Internet of things is realized, the non local mean noise reduction algorithm is used to remove the low-power communication signal of the Internet of things, and the weight value between similar blocks is calculated according to the European distance method. Finally, the low-power communication signal enhancement of the Internet of things is realized by the non local mean value denoising method. The experimental results show that the communication signal enhancement time overhead of this method is low, which is always less than 2.6s. The lowest bit error rate after signal enhancement is about 1%, and the signal-to-noise ratio is up to 18 dB, which shows that this method can achieve signal enhancement.",CS,AI_ML,0.85,Extracted from log - paper 442
Cells in the Internet of Things,"The Internet of Things combines various earlier areas of research. As a result, research on the subject is still organized around these pre-existing areas: distributed computing with services and objects, networks (usually combining 6lowpan with Zigbee etc. for the last-hop), artificial intelligence and semantic web, and human-computer interaction. We are yet to create a unified model that covers all these perspectives - domain, device, service, agent, etc. In this paper, we propose the concept of cells as units of structure and context in the Internet of things. This allows us to have a unified vocabulary to refer to single entities (whether dumb motes, intelligent spimes, or virtual services), intranets of things, and finally the complete Internet of things. The question that naturally follows, is what criteria we choose to demarcate boundaries; we suggest various possible answers to this question. We also mention how this concept ties into the existing visions and protocols, and suggest how it may be used as the foundation of a formal model.",CS,AI_ML,0.85,Extracted from log - paper 443
Economic viability and Future Impact of Internet of Things in India: An Inevitable wave,"The Internet of things , sometimes referred as Internet of objects can be stated as an environment in which any physical things or objects are assiThis paper studies the evolution of internet usage and classifies the impact areas where internet will go beyond personal communication or knowledge interface but it will provide communication and knowledge base support to numerous gadgets and systems around us",CS,AI_ML,0.85,Extracted from log - paper 444
Wireless Sensors Networks for Internet of Things,"The Internet is smoothly migrating from an Internet of people towards an Internet of Things (IoT). By 2020, it is expected to have 50 billion things connected to the Internet. However, such a migration induces a strong level of complexity when handling interoperability between the heterogeneous Inter- net things, e.g., RFIDs (Radio Frequency Identification), mobile handheld devices, and wireless sensors. In this context, a couple of standards have been already set, e.g., IPv6, 6LoWPAN (IPv6 over Low power Wireless Personal Area Networks), and M2M (Machine to Machine communications). In this paper, we focus on the integration of wireless sensor networks into IoT, and shed further light on the subtleties of such integration. We present a real-world test bed deployment where wireless sensors are used to control electrical appliances in a smart building. Encountered problems are highlighted and suitable solutions are presented.",CS,AI_ML,0.85,Extracted from log - paper 445
A Middleware for the Internet of Things,"The Internet of Things (IoT) connects everyday objects including a vast array of sensors, actuators, and smart devices, referred to as things to the Internet, in an intelligent and pervasive fashion. This connectivity gives rise to the possibility of using the tracking capabilities of things to impinge on the location privacy of users. Most of the existing management and location privacy protection solutions do not consider the low-cost and low-power requirements of things, or, they do not account for the heterogeneity, scalability, or autonomy of communications supported in the IoT. Moreover, these traditional solutions do not consider the case where a user wishes to control the granularity of the disclosed information based on the context of their use (e.g. based on the time or the current location of the user). To fill this gap, a middleware, referred to as the Internet of Things Management Platform (IoT-MP) is proposed in this paper.",CS,AI_ML,0.85,Extracted from log - paper 446
A Review on Security and Privacy of Internet of Medical Things,"The Internet of Medical Things (IoMT) are increasing the accuracy, reliability, and the production capability of electronic devices by playing a very important part in the industry of healthcare. The available medical resources and services related to healthcare are working to get an interconnection with each other by the digital healthcare system by the contribution of the researchers. Sensors, wearable devices, medical devices, and clinical devices are all connected to form an ecosystem of the Internet of Medical Things. The different applications of healthcare are enabled by the Internet of Medical Things to reduce the healthcare costs, to attend the medical responses on time and it also helps in increasing the quality of the medical treatment. The healthcare industry is transformed by the Internet of Medical Things as it delivers targeted and personalized medical care and it also seamlessly enables the communication of medical data. Devices used in the medical field and their application are connected to the system of healthcare of Information technology with the help of the digital world.",CS,AI_ML,0.85,Extracted from log - paper 447
Social Internet of Things: Architectural Approaches and Challenges,"Social Internet of Things (SIoT) takes a step forward over the traditional Internet of Things (IoT), introducing a new paradigm that combines the concepts of social networks with the IoT, to obtain the benefits of both worlds, as in the case of the Social Internet of Vehicles. With the emergence of the Social Internet of Things, new challenges also arise that need to be analyzed in depth. In this article, the key challenges around the software architecture of the various SIoT system described in the literature are analyzed. One of the conclusions is that SIoT is still at an early stage of development, and therefore, SIoT systems architecture will be concerned by this fact. Challenging quality attributes specific for SIoT include scalability, navigability and trust",CS,AI_ML,0.85,Extracted from log - paper 448
Can Blockchain Protect Internet-of-Things?,"In the Internet-of-Things, the number of connected devices is expected to be extremely huge, i.e., more than a couple of ten billion. It is however well-known that the security for the Internet-of-Things is still open problem. In particular, it is difficult to certify the identification of connected devices and to prevent the illegal spoofing. It is because the conventional security technologies have advanced for mainly protecting logical network and not for physical network like the Internet-of-Things. In order to protect the Internet-of-Things with advanced security technologies, we propose a new concept (datachain layer) which is a well-designed combination of physical chip identification and blockchain. With a proposed solution of the physical chip identification, the physical addresses of connected devices are uniquely connected to the logical addresses to be protected by blockchain.",CS,AI_ML,0.85,Extracted from log - paper 449
From Internet of Things to Internet of Data Apps,"We introduce the Internet of Data Apps (IoDA), representing the next natural progression of the Internet, Big Data, AI, and the Internet of Things. Despite advancements in these fields, the full potential of universal data access - the capability to seamlessly consume and contribute data via data applications - remains stifled by organizational and technological silos. To address these constraints, we propose the designs of an IoDA layer borrowing inspirations from the standard Internet protocols. This layer facilitates the interconnection of data applications across different devices and domains. This short paper serves as an invitation to dialogue over this proposal.",CS,AI_ML,0.85,Extracted from log - paper 450
When Distributed Ledger Technology meets Internet of Things -- Benefits and Challenges,"There is a growing interest from both the academia and industry to employ distributed ledger technology in the Internet-of-Things domain for addressing security-related and performance challenges. Distributed ledger technology enables non-trusted entities to communicate and reach consensus in a fully distributed manner through a cryptographically secure and immutable ledger. However, significant challenges arise mainly related to transaction processing speed and user privacy. This work explores the interplay between Internet-of-Things and distributed ledger technology, analysing the fundamental characteristics of this technology and discussing the related benefits and challenges.",CS,AI_ML,0.85,Extracted from log - paper 451
Wearable Internet of Things for Personalized Healthcare Study of Trends and Latent Research,"In this age of heterogeneous systems, diverse technologies are integrated to create application-specific solutions. The recent upsurge in acceptance of technologies such as cloud computing and ubiquitous Internet has cleared the path for Internet of Things (IoT). Moreover, the increasing Internet penetration with the rising use of mobile devices has inspired an era of technology that allows interfacing of physical objects and connecting them to Internet for developing applications serving a wide range of purposes. Recent developments in the area of wearable devices has led to the creation of another segment in IoT, which can be conveniently referred to as Wearable Internet of Things (WIoT). Research in this area promises to personalize healthcare in previously unimaginable ways by allowing individual tracking of wellness and health information. This chapter shall cover the different facets of Wearable Internet of Things (WIoT) and ways in which it is a key driving technology behind the concept of personalized healthcare. It shall discuss the theoretical aspects of WIoT, focusing on functionality, design and applicability. Moreover, it shall also elaborate on the role of wearable sensors, big data and cloud computing as enabling technologies for WIoT.",CS,AI_ML,0.85,Extracted from log - paper 452
Context-aware Dynamic Discovery and Configuration of 'Things' in Smart Environments,"The Internet of Things (IoT) is a dynamic global information network consisting of Internet-connected objects, such as RFIDs, sensors, actuators, as well as other instruments and smart appliances that are becoming an integral component of the future Internet. Currently, such Internet-connected objects or `things' outnumber both people and computers connected to the Internet and their population is expected to grow to 50 billion in the next 5 to 10 years. To be able to develop IoT applications, such `things' must become dynamically integrated into emerging information networks supported by architecturally scalable and economically feasible Internet service delivery models, such as cloud computing. Achieving such integration through discovery and configuration of `things' is a challenging task. Towards this end, we propose a Context-Aware Dynamic Discovery of {Things} (CADDOT) model. We have developed a tool SmartLink, that is capable of discovering sensors deployed in a particular location despite their heterogeneity. SmartLink helps to establish the direct communication between sensor hardware and cloud-based IoT middleware platforms. We address the challenge of heterogeneity using a plug in architecture. Our prototype tool is developed on an Android platform. Further, we employ the Global Sensor Network (GSN) as the IoT middleware for the proof of concept validation. The significance of the proposed solution is validated using a test-bed that comprises 52 Arduino-based Libelium sensors.",CS,AI_ML,0.85,Extracted from log - paper 453
Achieving Ethical Algorithmic Behaviour in the Internet-of-Things: a Review,"The Internet-of-Things is emerging as a vast inter-connected space of devices and things surrounding people, many of which are increasingly capable of autonomous action, from automatically sending data to cloud servers for analysis, changing the behaviour of smart objects, to changing the physical environment. A wide range of ethical concerns has arisen in their usage and development in recent years. Such concerns are exacerbated by the increasing autonomy given to connected things. This paper reviews, via examples, the landscape of ethical issues, and some recent approaches to address these issues, concerning connected things behaving autonomously, as part of the Internet-of-Things. We consider ethical issues in relation to device operations and accompanying algorithms. Examples of concerns include unsecured consumer devices, data collection with health related Internet-of-Things, hackable vehicles and behaviour of autonomous vehicles in dilemma situations, accountability with Internet-of-Things systems, algorithmic bias, uncontrolled cooperation among things, and automation affecting user choice and control. Current ideas towards addressing a range of ethical concerns are reviewed and compared, including programming ethical behaviour, whitebox algorithms, blackbox validation, algorithmic social contracts, enveloping IoT systems, and guidelines and code of ethics for IoT developers - a suggestion from the analysis is that a multi-pronged approach could be useful, based on the context of operation and deployment.",CS,AI_ML,0.85,Extracted from log - paper 454
"A Review on Internet of Things (IoT), Internet of Everything (IoE) and Internet of Nano Things (IoNT)","The current prominence and future promises of the Internet of Things (IoT), Internet of Everything (IoE) and Internet of Nano Things (IoNT) are extensively reviewed and a summary survey report is presented. The analysis clearly distinguishes between IoT and IoE which are wrongly considered to be the same by many people. Upon examining the current advancement in the fields of IoT, IoE and IoNT, the paper presents scenarios for the possible future expansion of their applications.",CS,AI_ML,0.85,Extracted from log - paper 455
IoT Applications in Urban Sustainability,"Internet of Things is one of the driving technologies behind the concept of Smart Cities and is capable of playing a significant role in facilitating urban sustainable development. This chapter explores the relationship between three core concepts namely Smart Cities, Internet of Things and Sustainability; thereby identifying the challenges and opportunities that exist in the synergistic use of Internet of Things for sustainability, in the Smart Cities context. Moreover, this chapter also presents some of the existing use cases that apply Internet of Things for urban sustainable development, also presenting the vision for these applications as they continue to evolve in and adapt to the real world scenario. It is because of the interdisciplinary nature of these applications that a clear comprehension of the associated challenges becomes quintessential. Study of challenges and opportunities in this area shall facilitate collaboration between different sectors of urban planning and optimize the utilization of Internet of Things for sustainability.",CS,AI_ML,0.85,Extracted from log - paper 456
Comment on Chen et al.'s Authentication Protocol for Internet of Health Things,"The Internet of Medical Things has revolutionized the healthcare industry, enabling the seamless integration of connected medical devices and wearable sensors to enhance patient care and optimize healthcare services. However, the rapid adoption of the Internet of Medical Things also introduces significant security challenges that must be effectively addressed to preserve patient privacy, protect sensitive medical data, and ensure the overall reliability and safety of Internet of Medical Things systems. In this context, a key agreement protocol is used to securely establish shared cryptographic keys between interconnected medical devices and the central system, ensuring confidential and authenticated communication. Recently Chen et al. proposed a lightweight authentication and key agreement protocol for the Internet of health things. In this article, we provide a descriptive analysis of their proposed scheme and prove that Chen et al.'s scheme is vulnerable to Known session-specific temporary information attacks and stolen verifier attacks.",CS,AI_ML,0.85,Extracted from log - paper 457
Addressing Security and Privacy Challenges in Internet of Things,"Internet of Things (IoT), also referred to as the Internet of Objects, is envisioned as a holistic and transformative approach for providing numerous services. The rapid development of various communication protocols and miniaturization of transceivers along with recent advances in sensing technologies offer the opportunity to transform isolated devices into communicating smart things. Smart things, that can sense, store, and even process electrical, thermal, optical, chemical, and other signals to extract user-/environment-related information, have enabled services only limited by human imagination. Despite picturesque promises of IoT-enabled systems, the integration of smart things into the standard Internet introduces several security challenges because the majority of Internet technologies, communication protocols, and sensors were not designed to support IoT. Several recent research studies have demonstrated that launching security/privacy attacks against IoT-enabled systems, in particular wearable medical sensor (WMS)-based systems, may lead to catastrophic situations and life-threatening conditions. Therefore, security threats and privacy concerns in the IoT domain need to be proactively studied and aggressively addressed. In this thesis, we tackle several domain-specific security/privacy challenges associated with IoT-enabled systems.",CS,AI_ML,0.85,Extracted from log - paper 458
Sense-Deliberate-Act Cognitive Agents for Sense-Compute-Control Applications in the Internet of Things & Services,"In this paper, we advocate Agent-Oriented Software Engi-neering (AOSE) through employing Belief-Desire-Intention (BDI) intel-ligent agents for developing Sense-Compute-Control (SCC) applications in the Internet of Things and Services (IoTS). We argue that not only the agent paradigm, in general, but also cognitive BDI agents with sense-deliberate-act cycle, in particular, fit very well to the nature of SCC applications in the IoTS. However, considering the highly constrained heterogeneous devices that are prevalent in the IoTS, existing BDI agent frameworks, even those especially created for Wireless Sensor Networks (WSNs), do not work. We elaborate on the challenges and propose pos-sible approaches to address them.",CS,AI_ML,0.85,Extracted from log - paper 459
A Conceptual Paper on SERVQUAL-Framework for Assessing Quality of Internet of Things (IoT) Services,"Service quality possesses the vital prominence in usability of innovative products and services. As technological innovation has made the life synchronized and effective, Internet of Things (IoT) is matter of discussion everywhere. From users' perspective, IoT services are always embraced by various system characteristics of security and performance. A service quality model can better present the preference of such technology customers. the study intends to project theoretical model of service quality for internet of things (IoT). Based on the existing models of service quality and the literature in internet of things, a framework is proposed to conceptualize and measure service quality for internet of things.This study established the IoT-Servqual model with four dimensions (i.e., Privacy, Functionality, Efficiency, and Tangibility) of multiple service quality models. These dimensions are essential and inclined towards the users' leaning of IoT Services. This paper contributes to research on internet of things services by development of a comprehensive framework for customers' quality apprehension. This model will previse the expression of information secrecy of users related with internet of things (IoT). This research will advance understanding of service quality in modern day technology and assist firms to devise the fruitful service structure.",CS,AI_ML,0.85,Extracted from log - paper 460
Semantic Reasoning for Context-aware Internet of Things Applications,"Advances in ICT are bringing into reality the vision of a large number of uniquely identifiable, interconnected objects and things that gather information from diverse physical environments and deliver the information to a variety of innovative applications and services. These sensing objects and things form the Internet of Things (IoT) that can improve energy and cost efficiency and automation in many different industry fields such as transportation and logistics, health care and manufacturing, and facilitate our everyday lives as well. IoT applications rely on real-time context data and allow sending information for driving the behaviors of users in intelligent environments.",CS,AI_ML,0.85,Extracted from log - paper 461
HMIoT: A New Healthcare Model Based on Internet of Things,"In recent century, with developing of equipment, using of the internet and things connected to the internet is growing. Therefore, the need for informing in the process of expanding the scope of its application is very necessary and important. These days, using intelligent and autonomous devices in our daily lives has become commonplace and the Internet is the most important part of the relationship between these tools and even at close distances also. Things connected to the Internet that are currently in use and can be inclusive of all the sciences as a step to develop and coordinate of them. In this paper we investigate application and using of Internet of things from the perspective of various sciences. We show that how this phenomenon can influence on future health of people.",CS,AI_ML,0.85,Extracted from log - paper 462
"Internet of Nano-Things, Things and Everything: Future Growth Trends","The current statuses and future promises of the Internet of Things (IoT), Internet of Everything (IoE) and Internet of Nano-Things (IoNT) are extensively reviewed and a summarized survey is presented. The analysis clearly distinguishes between IoT and IoE, which are wrongly considered to be the same by many commentators. After evaluating the current trends of advancement in the fields of IoT, IoE and IoNT, this paper identifies the 21 most significant current and future challenges as well as scenarios for the possible future expansion of their applications. Despite possible negative aspects of these developments, there are grounds for general optimism about the coming technologies. Certainly, many tedious tasks can be taken over by IoT devices. However, the dangers of criminal and other nefarious activities, plus those of hardware and software errors, pose major challenges that are a priority for further research. Major specific priority issues for research are identified.",CS,AI_ML,0.85,Extracted from log - paper 463
Survey of Security and Privacy Issues of Internet of Things,This paper is a general survey of all the security issues existing in the Internet of Things (IoT) along with an analysis of the privacy issues that an end-user may face as a consequence of the spread of IoT. The majority of the survey is focused on the security loopholes arising out of the information exchange technologies used in Internet of Things. No countermeasure to the security drawbacks has been analyzed in the paper.,CS,AI_ML,0.85,Extracted from log - paper 464
"Internet of Things: Concept, Building blocks, Applications and Challenges","Internet of things (IoT) constitutes one of the most important technology that has the potential to affect deeply our way of life, after mobile phones and Internet. The basic idea is that every objet that is around us will be part of the network (Internet), interacting to reach a common goal. In another word, the Internet of Things concept aims to link the physical world to the digital one. Technology advances along with popular demand will foster the wide spread deployement of IoT's services, it would radically transform our corporations, communities, and personal spheres. In this survey, we aim to provide the reader with a broad overview of the Internet of things concept, its building blocks, its applications along with its challenges.",CS,AI_ML,0.85,Extracted from log - paper 465
"Development of Internet of Things, Augmented Reality and 5G technologies (review)","Just as the emergence of personal computers and smartphones has changed the life of modern society, the Internet of Things, augmented reality and ultra-fast and reliable telecommunications networks of the new generation, by combining the physical objects of the real world with the ever-increasing computing power and intelligence of cyberspace, will make the next big revolution in all spheres of human activity. Keywords: Internet of Things, 5G, augmented reality.",CS,AI_ML,0.85,Extracted from log - paper 466
Improving the quality of healthcare through Internet of Things,"This paper attempts to outline how the adoption of Internet of Things (IoT) in healthcare can create real economic value and improve the patient experience. Thus, getting the maximum benefits requires understanding both the IoT paradigm and the enabling technologies, and how IoT can be applied in the field of healthcare. We will mention some open challenging issues to be addressed by the research community, and not only. Besides the real barriers in adopting the Internet of Things, there are some advantages regard collecting and processing patient data, and monitoring the daily health states of individuals, just to name a few. These aspects could revolutionize the healthcare industry.",CS,AI_ML,0.85,Extracted from log - paper 467
A Novel Method for Developing Robotics via Artificial Intelligence and Internet of Things,"This paper describe about a new methodology for developing and improving the robotics field via artificial intelligence and internet of things. Now a day, we can say Artificial Intelligence take the world into robotics. Almost all industries use robots for lot of works. They are use co-operative robots to make different kind of works. But there was some problem to make robot for multi tasks. So there was a necessary new methodology to made multi tasking robots. It will be done only by artificial intelligence and internet of things.",CS,AI_ML,0.85,Extracted from log - paper 468
Towards a Practical Architecture for India Centric Internet of Things,"An effective architecture for the Internet of Things (IoT), particularly for an emerging nation like India with limited technology penetration at the national scale, should be based on tangible technology advances in the present, practical application scenarios of social and entrepreneurial value, and ubiquitous capabilities that make the realization of IoT affordable and sustainable. Humans, data, communication and devices play key roles in the IoT ecosystem that we perceive. In a push towards this sustainable and practical IoT Architecture for India, we synthesize ten design paradigms to consider.",CS,AI_ML,0.85,Extracted from log - paper 469
Unveiling Contextual Similarity of Things via Mining Human-Thing Interactions in the Internet of Things,"With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. Finding correlations of ubiquitous things is a crucial prerequisite for many important applications such as things search, discovery, classification, recommendation, and composition. This article presents DisCor-T, a novel graph-based method for discovering underlying connections of things via mining the rich content embodied in human-thing interactions in terms of user, temporal and spatial information. We model these various information using two graphs, namely spatio-temporal graph and social graph. Then, random walk with restart (RWR) is applied to find proximities among things, and a relational graph of things (RGT) indicating implicit correlations of things is learned. The correlation analysis lays a solid foundation contributing to improved effectiveness in things management. To demonstrate the utility, we develop a flexible feature-based classification framework on top of RGT and perform a systematic case study. Our evaluation exhibits the strength and feasibility of the proposed approach.",CS,AI_ML,0.85,Extracted from log - paper 470
KubeAdaptor: A Docking Framework for Workflow Containerization on Kubernetes,"As Kubernetes becomes the infrastructure of the cloud-native era, the integration of workflow systems with Kubernetes is gaining more and more popularity. To our knowledge, workflow systems employ scheduling algorithms that optimize task execution order of workflow to improve performance and execution efficiency. However, due to its inherent scheduling mechanism, Kubernetes does not execute containerized scheduling following the optimized task execution order of workflow amid migrating workflow systems to the Kubernetes platform. This inconsistency in task scheduling order seriously degrades the efficiency of workflow execution and brings numerous challenges to the containerized process of workflow systems on Kubernetes. In this paper, we propose a cloud-native workflow engine, also known as KubeAdaptor, a docking framework able to implement workflow containerization on Kubernetes, integrate workflow systems with Kubernetes, ensuring the consistency of task scheduling order. We introduce the design and architecture of the KubeAdaptor, elaborate on the functionality implementation and the event-trigger mechanism within the KubeAdaptor. Experimental results about four real-world workflows show that the KubeAdaptor ensures the consistency of the workflow systems and Kubernetes in the task scheduling order. Compared with the baseline Argo workflow engine, the KubeAdaptor achieves better performance in terms of the average execution time of task pod, average workflow lifecycle, and resource usage rate.",CS,AI_ML,0.85,Extracted from log - paper 471
Multi-objective Optimization of Clustering-based Scheduling for Multi-workflow On Clouds Considering Fairness,"Distributed computing, such as cloud computing, provides promising platforms to execute multiple workflows. Workflow scheduling plays an important role in multi-workflow execution with multi-objective requirements. Although there exist many multi-objective scheduling algorithms, they focus mainly on optimizing makespan and cost for a single workflow. There is a limited research on multi-objective optimization for multi-workflow scheduling. Considering multi-workflow scheduling, there is an additional key objective to maintain the fairness of workflows using the resources. To address such issues, this paper first defines a new multi-objective optimization model based on makespan, cost, and fairness, and then proposes a global clustering-based multi-workflow scheduling strategy for resource allocation. Experimental results show that the proposed approach performs better than the compared algorithms without significant compromise of the overall makespan and cost as well as individual fairness, which can guide the simulation workflow scheduling on clouds.",CS,AI_ML,0.85,Extracted from log - paper 472
Data-Aware Approximate Workflow Scheduling,"Optimization of data placement in complex scientific workflows has become very crucial since the large amounts of data generated by these workflows significantly increases the turnaround time of the end-to-end application. It is almost impossible to make an optimal scheduling for the end-to-end workflow without considering the intermediate data movement. In order to reduce the complexity of the workflow-scheduling problem, most of the existing work constrains the problem space by some unrealistic assumptions, which result in non-optimal scheduling in practice. In this study, we propose a genetic data-aware algorithm for the end-to-end workflow scheduling problem. Distinct from the past research, we develop a novel data-aware evaluation function for each chromosome, a common augmenting crossover operator and a simple but effective mutation operator. Our experiments on different workflow structures show that the proposed GA based approach gives a scheduling close to the optimal one.",CS,AI_ML,0.85,Extracted from log - paper 473
Energy-efficient workflow scheduling based on workflow structures under deadline and budget constraints in the cloud,"The utilization of cloud environments to deploy scientific workflow applications is an emerging trend in scientific community. In this area, the main issue is the scheduling of workflows, which is known as an NP-complete problem. Apart from respecting user-defined deadline and budget, energy consumption is a major concern for cloud providers in implementing the scheduling strategy. The types and the number of virtual machines (VMs) used are determinant to handle those issues, and their determination is highly influenced by the structure of the workflow. In this paper, we propose two workflow scheduling algorithms that take advantage of the structural properties of the workflows. The first algorithm is called Structure-based Multi-objective Workflow Scheduling with an Optimal instance type (SMWSO). It introduces a new approach to determine the optimal instance type along with the optimal number of VMs to be provisioned. We also consider the use of heterogeneous VMs in the Structure-based Multi-objective Workflow Scheduling with Heterogeneous instance types (SMWSH), to highlight the algorithm's strength within the heterogeneous environment. The simulation results show that our proposal produces better energy-efficiency in 80% of workflow/workload scenarios, and save more than 50% overall energy compared to a recent state-of-the-art algorithm.",CS,AI_ML,0.85,Extracted from log - paper 474
WOW: Workflow-Aware Data Movement and Task Scheduling for Dynamic Scientific Workflows,"Scientific workflows process extensive data sets over clusters of independent nodes, which requires a complex stack of infrastructure components, especially a resource manager (RM) for task-to-node assignment, a distributed file system (DFS) for data exchange between tasks, and a workflow engine to control task dependencies. To enable a decoupled development and installation of these components, current architectures place intermediate data files during workflow execution independently of the future workload. In data-intensive applications, this separation results in suboptimal schedules, as tasks are often assigned to nodes lacking input data, causing network traffic and bottlenecks. This paper presents WOW, a new scheduling approach for dynamic scientific workflow systems that steers both data movement and task scheduling to reduce network congestion and overall runtime. For this, WOW creates speculative copies of intermediate files to prepare the execution of subsequently scheduled tasks. WOW supports modern workflow systems that gain flexibility through the dynamic construction of execution plans. We prototypically implemented WOW for the popular workflow engine Nextflow using Kubernetes as a resource manager. In experiments with 16 synthetic and real workflows, WOW reduced makespan in all cases, with improvement of up to 94.5% for workflow patterns and up to 53.2% for real workflows, at a moderate increase of temporary storage space. It also has favorable effects on CPU allocation and scales well with increasing cluster size.",CS,AI_ML,0.85,Extracted from log - paper 475
A Cost-Driven Fuzzy Scheduling Strategy for Intelligent Workflow Decision Making Systems in Uncertain Edge-Cloud Environments,"Workflow decision making is critical to performing many practical workflow applications. Scheduling in edge-cloud environments can address the high complexity problem of workflow applications, while decreasing the data transmission delay between the cloud and end devices. However, because of the heterogeneous resources in edge-cloud environments and the complicated data dependencies among the tasks in a workflow, significant challenges for workflow scheduling remain, including the selection of an optimal tasks-servers solution from the possible numerous combinations. The existing studies have been mainly done subject to rigorous conditions without fluctuations, ignoring the fact that workflow scheduling is typically present in uncertain environments. In this study, we focus on reducing the execution cost of workflow applications mainly caused by task computation and data transmission, while satisfying the workflow deadline in uncertain edge-cloud environments. The Triangular Fuzzy Numbers (TFNs) are adopted to represent the task processing time and data transferring time. A cost-driven fuzzy scheduling strategy based on an Adaptive Discrete Particle Swarm Optimization (ADPSO) algorithm is proposed, which employs the operators of Genetic Algorithm (GA). This strategy introduces the randomly two-point crossover operator, neighborhood mutation operator, and adaptive multipoint mutation operator of GA to effectively avoid converging on local optima. The experimental results show that our strategy can effectively reduce the workflow execution cost in uncertain edge-cloud environments, compared with other benchmark solutions.",CS,AI_ML,0.85,Extracted from log - paper 476
Analysis of Workflow Schedulers in Simulated Distributed Environments,"Task graphs provide a simple way to describe scientific workflows (sets of tasks with dependencies) that can be executed on both HPC clusters and in the cloud. An important aspect of executing such graphs is the used scheduling algorithm. Many scheduling heuristics have been proposed in existing works; nevertheless, they are often tested in oversimplified environments. We provide an extensible simulation environment designed for prototyping and benchmarking task schedulers, which contains implementations of various scheduling algorithms and is open-sourced, in order to be fully reproducible. We use this environment to perform a comprehensive analysis of workflow scheduling algorithms with a focus on quantifying the effect of scheduling challenges that have so far been mostly neglected, such as delays between scheduler invocations or partially unknown task durations. Our results indicate that network models used by many previous works might produce results that are off by an order of magnitude in comparison to a more realistic model. Additionally, we show that certain implementation details of scheduling algorithms which are often neglected can have a large effect on the scheduler's performance, and they should thus be described in great detail to enable proper evaluation.",CS,AI_ML,0.85,Extracted from log - paper 477
Efficient Orchestrated AI Workflows Execution on Scale-out Spatial Architecture,"Given the increasing complexity of AI applications, traditional spatial architectures frequently fall short. Our analysis identifies a pattern of interconnected, multi-faceted tasks encompassing both AI and general computational processes. In response, we have conceptualized ""Orchestrated AI Workflows,"" an approach that integrates various tasks with logic-driven decisions into dynamic, sophisticated workflows. Specifically, we find that the intrinsic Dual Dynamicity of Orchestrated AI Workflows, namely dynamic execution times and frequencies of Task Blocks, can be effectively represented using the Orchestrated Workflow Graph. Furthermore, the intrinsic Dual Dynamicity poses challenges to existing spatial architecture, namely Indiscriminate Resource Allocation, Reactive Load Rebalancing, and Contagious PEA Idleness. To overcome these challenges, we present Octopus, a scale-out spatial architecture and a suite of advanced scheduling strategies optimized for executing Orchestrated AI Workflows, such as the Discriminate Dual-Scheduling Mechanism, Adaptive TBU Scheduling Strategy, and Proactive Cluster Scheduling Strategy. Our evaluations demonstrate that Octopus significantly outperforms traditional architectures in handling the dynamic demands of Orchestrated AI Workflows, and possesses robust scalability in large scale hardware such as wafer-scale chip.",CS,AI_ML,0.85,Extracted from log - paper 478
Resource Provisioning and Scheduling Algorithm for Meeting Cost and Deadline-Constraints of Scientific Workflows in IaaS Clouds,"Infrastructure as a Service model of cloud computing is a desirable platform for the execution of cost and deadline constrained workflow applications as the elasticity of cloud computing allows large-scale complex scientific workflow applications to scale dynamically according to their deadline requirements. However, scheduling of these multitask workflow jobs in a distributed computing environment is a computationally hard multi-objective combinatorial optimization problem. The critical challenge is to schedule the workflow tasks whilst meeting user quality of service (QoS) requirements and the application's deadline. The existing research work not only fails to address this challenge but also do not incorporate the basic principles of elasticity and heterogeneity of computing resources in cloud environment. In this paper, we propose a resource provisioning and scheduling algorithm to schedule the workflow applications on IaaS clouds to meet application deadline constraints while optimizing the execution cost. The proposed algorithm is based on the nature-inspired population based Intelligent Water Drop (IWD) optimization algorithm. The experimental results in the simulated environment of CloudSim with four real-world workflow applications demonstrates that IWD algorithm schedules workflow tasks with optimized cost within the specified deadlines. Moreover, the IWD algorithm converges fast to near optimal solution.",CS,AI_ML,0.85,Extracted from log - paper 479
An Efficient Fault Tolerant Workflow Scheduling Approach using Replication Heuristics and Checkpointing in the Cloud,"Scientific workflows have been predominantly used for complex and large scale data analysis and scientific computation/automation and the need for robust workflow scheduling techniques has grown considerably. But, most of the existing workflow scheduling algorithms do not provide the required reliability and robustness. In this paper, a new fault tolerant workflow scheduling algorithm that learns replication heuristics in an unsupervised manner has been proposed. Furthermore, the use of light weight synchronized checkpointing enables efficient resubmission of failed tasks and ensures workflow completion even in precarious environments. The proposed technique improves upon metrics like Resource Wastage and Resource Usage in comparison to the Replicate-All algorithm, while maintaining an acceptable increase in Makespan as compared to the vanilla Heterogeneous Earliest Finish Time (HEFT).",CS,AI_ML,0.85,Extracted from log - paper 480
Reinforcement Learning-driven Data-intensive Workflow Scheduling for Volunteer Edge-Cloud,"In recent times, Volunteer Edge-Cloud (VEC) has gained traction as a cost-effective, community computing paradigm to support data-intensive scientific workflows. However, due to the highly distributed and heterogeneous nature of VEC resources, centralized workflow task scheduling remains a challenge. In this paper, we propose a Reinforcement Learning (RL)-driven data-intensive scientific workflow scheduling approach that takes into consideration: i) workflow requirements, ii) VEC resources' preference on workflows, and iii) diverse VEC resource policies, to ensure robust resource allocation. We formulate the long-term average performance optimization problem as a Markov Decision Process, which is solved using an event-based Asynchronous Advantage Actor-Critic RL approach. Our extensive simulations and testbed implementations demonstrate our approach's benefits over popular baseline strategies in terms of workflow requirement satisfaction, VEC preference satisfaction, and available VEC resource utilization.",CS,AI_ML,0.85,Extracted from log - paper 481
Workflow-as-a-Service Cloud Platform and Deployment of Bioinformatics Workflow Applications,"Workflow management systems (WMS) support the composition and deployment of workflow-oriented applications in distributed computing environments. They hide the complexity of managing large-scale applications, which includes the controlling data pipelining between tasks, ensuring the application's execution, and orchestrating the distributed computational resources to get a reasonable processing time. With the increasing trends of scientific workflow adoption, the demand to deploy them using a third-party service begins to increase. Workflow-as-a-service (WaaS) is a term representing the platform that serves the users who require to deploy their workflow applications on third-party cloud-managed services. This concept drives the existing WMS technology to evolve towards the development of the WaaS cloud platform. Based on this requirement, we extend CloudBus WMS functionality to handle the workload of multiple workflows and develop the WaaS cloud platform prototype. We implemented the Elastic Budget-constrained resource Provisioning and Scheduling algorithm for Multiple workflows (EBPSM) algorithm that is capable of scheduling multiple workflows and evaluated the platform using two bioinformatics workflows. Our experimental results show that the platform is capable of efficiently handling multiple workflows execution and gaining its purpose to minimize the makespan while meeting the budget.",CS,AI_ML,0.85,Extracted from log - paper 482
Adaptive Scheduling for Efficient Execution of Dynamic Stream Workflows,"Stream workflow application such as online anomaly detection or online traffic monitoring, integrates multiple streaming big data applications into data analysis pipeline. This application can be highly dynamic in nature, where the data velocity may change at runtime and therefore the resources should be managed overtime. To manage these changes, the orchestration of this application requires a dynamic execution environment and dynamic scheduling technique. For the former requirement, Multicloud environment is a visible solution to cope with the dynamic aspects of this workflow application. While for the latter requirement, dynamic scheduling technique not only need to adhere to end user's requirements in terms of data processing and deadline for decision making, and data stream sources location constraints, but also adjust provisioning and scheduling plan at runtime to cope with dynamic variations of stream data rates. Therefore, we propose a two-phase adaptive scheduling technique to efficiently schedule dynamic workflow application in Multicloud environment that can respond to changes in the velocity of data at runtime. The experimental results showed that the proposed technique is close to the lower bound and effective for different experiment scenarios.",CS,AI_ML,0.85,Extracted from log - paper 483
BeeFlow: Behavior Tree-based Serverless Workflow Modeling and Scheduling for Resource-Constrained Edge Clusters,"Serverless computing has gained popularity in edge computing due to its flexible features, including the pay-per-use pricing model, auto-scaling capabilities, and multi-tenancy support. Complex Serverless-based applications typically rely on Serverless workflows (also known as Serverless function orchestration) to express task execution logic, and numerous application- and system-level optimization techniques have been developed for Serverless workflow scheduling. However, there has been limited exploration of optimizing Serverless workflow scheduling in edge computing systems, particularly in high-density, resource-constrained environments such as system-on-chip clusters and single-board-computer clusters. In this work, we discover that existing Serverless workflow scheduling techniques typically assume models with limited expressiveness and cause significant resource contention. To address these issues, we propose modeling Serverless workflows using behavior trees, a novel and fundamentally different approach from existing directed-acyclic-graph- and state machine-based models. Behavior tree-based modeling allows for easy analysis without compromising workflow expressiveness. We further present observations derived from the inherent tree structure of behavior trees for contention-free function collections and awareness of exact and empirical concurrent function invocations. Based on these observations, we introduce BeeFlow, a behavior tree-based Serverless workflow system tailored for resource-constrained edge clusters. Experimental results demonstrate that BeeFlow achieves up to 3.2X speedup in a high-density, resource-constrained edge testbed and 2.5X speedup in a high-profile cloud testbed, compared with the state-of-the-art.",CS,AI_ML,0.85,Extracted from log - paper 484
RIOT: a Stochastic-based Method for Workflow Scheduling in the Cloud,"Cloud computing provides engineers or scientists a place to run complex computing tasks. Finding a workflow's deployment configuration in a cloud environment is not easy. Traditional workflow scheduling algorithms were based on some heuristics, e.g. reliability greedy, cost greedy, cost-time balancing, etc., or more recently, the meta-heuristic methods, such as genetic algorithms. These methods are very slow and not suitable for rescheduling in the dynamic cloud environment. This paper introduces RIOT (Randomized Instance Order Types), a stochastic based method for workflow scheduling. RIOT groups the tasks in the workflow into virtual machines via a probability model and then uses an effective surrogate-based method to assess a large amount of potential scheduling. Experiments in dozens of study cases showed that RIOT executes tens of times faster than traditional methods while generating comparable results to other methods.",CS,AI_ML,0.85,Extracted from log - paper 485
Scheduling Algorithms for Efficient Execution of Stream Workflow Applications in Multicloud Environments,"Big data processing applications are becoming more and more complex. They are no more monolithic in nature but instead they are composed of decoupled analytical processes in the form of a workflow. One type of such workflow applications is stream workflow application, which integrates multiple streaming big data applications to support decision making. Each analytical component of these applications runs continuously and processes data streams whose velocity will depend on several factors such as network bandwidth and processing rate of parent analytical component. As a consequence, the execution of these applications on cloud environments requires advanced scheduling techniques that adhere to end user's requirements in terms of data processing and deadline for decision making. In this paper, we propose two Multicloud scheduling and resource allocation techniques for efficient execution of stream workflow applications on Multicloud environments while adhering to workflow application and user performance requirements and reducing execution cost. Results showed that the proposed genetic algorithm is an adequate and effective for all experiments.",CS,AI_ML,0.85,Extracted from log - paper 486
Efficient Probabilistic Workflow Scheduling for IaaS Clouds,"The flexibility and the variety of computing resources offered by the cloud make it particularly attractive for executing user workloads. However, IaaS cloud environments pose non-trivial challenges in the case of workflow scheduling under deadlines and monetary cost constraints. Indeed, given the typical uncertain performance behavior of cloud resources, scheduling algorithms that assume deterministic execution times may fail, thus requiring probabilistic approaches. However, existing probabilistic algorithms are computationally expensive, mainly due to the greater complexity of the workflow scheduling problem in its probabilistic form, and they hardily scale with the size of the problem instance. In this article, we propose EPOSS, a novel workflow scheduling algorithm for IaaS cloud environments based on a probabilistic formulation. Our solution blends together the low execution latency of state-of-the-art scheduling algorithms designed for the case of deterministic execution times and the capability to enforce probabilistic constraints.Designed with computational efficiency in mind, EPOSS achieves one to two orders lower execution times in comparison with existing probabilistic schedulers. Furthermore, it ensures good scaling with respect to workflow size and number of heterogeneous virtual machine types offered by the IaaS cloud environment. We evaluated the benefits of our algorithm via an experimental comparison over a variety of workloads and characteristics of IaaS cloud environments.",CS,AI_ML,0.85,Extracted from log - paper 487
Solving workflow scheduling problems with QUBO modeling,"In this paper we investigate the workflow scheduling problem, a known NP-hard class of scheduling problems. We derive problem instances from an industrial use case and compare against several quantum, classical, and hybrid quantum-classical algorithms. We develop a novel QUBO to represent our scheduling problem and show how the QUBO complexity depends on the input problem. We derive and present a decomposition method for this specific application to mitigate this complexity and demonstrate the effectiveness of the approach.",CS,AI_ML,0.85,Extracted from log - paper 488
Resource-sharing Policy in Multi-tenant Scientific Workflow-as-a-Service Cloud Platform,"Increased adoption of scientific workflows in the community has urged for the development of multi-tenant platforms that provide these workflow executions as a service. As a result, Workflow-as-a-Service (WaaS) concept has been created by researchers to address the future design of Workflow Management Systems (WMS) that can serve a large number of users from a single point of service. These platforms differ from traditional WMS in that they handle a workload of workflows at runtime. A traditional WMS is usually designed to execute a single workflow in a dedicated process while WaaS cloud platforms enhance the process by exploiting multiple workflows execution in a multi-tenant environment model. In this paper, we explore a novel resource-sharing policy to improve system utilization and to fulfil various Quality of Service (QoS) requirements from multiple users in WaaS cloud platforms. We propose an Elastic Budget-constrained resource Provisioning and Scheduling algorithm for Multiple workflows that can reduce the computational overhead by encouraging resource sharing to minimize workflows' makespan while meeting a user-defined budget. Our experiments show that the EBPSM algorithm can utilize the resource-sharing policy to achieve higher performance in terms of minimizing the makespan compared to the state-of-the-art budget-constraint scheduling algorithm.",CS,AI_ML,0.85,Extracted from log - paper 489
Workflow Scheduling in the Cloud with Weighted Upward-rank Priority Scheme Using Random Walk and Uniform Spare Budget Splitting,"We study a difficult problem of how to schedule complex workflows with precedence constraints under a limited budget in the cloud environment. We first formulate the scheduling problem as an integer programming problem, which can be optimized and used as the baseline of performance. We then consider the traditional approach of scheduling jobs in a prioritized order based on the upward-rank of each job. For those jobs with no precedence constraints among themselves, the plain upward-rank priority scheme assigns priorities in an arbitrary way. We propose a job prioritization scheme that uses Markovian chain stationary probabilities as a measure of importance of jobs. The scheme keeps the precedence order for the jobs that have precedence constraints between each other, and assigns priorities according to the jobs' importance for the jobs without precedence constraints. We finally design a uniform spare budget splitting strategy, which splits the spare budget uniformly across all the jobs. We test our algorithms on a variety of workflows, including FFT, Gaussian elimination, typical scientific workflows, randomly generated workflows and workflows from an in-production cluster of an online streaming service company. We compare our algorithms with the-state-of-art algorithms. The empirical results show that the uniform spare budget splitting scheme outperforms the splitting scheme in proportion to extra demand in average for most cases, and the Markovian based prioritization further improves the workflow makespan.",CS,AI_ML,0.85,Extracted from log - paper 490
Minimizing Energy in Reliability and Deadline-Ensured Workflow Scheduling in Cloud,"With the increasing prevalence of computationally intensive workflows in cloud environments, it has become crucial for cloud platforms to optimize energy consumption while ensuring the feasibility of user workflow schedules with respect to strict deadlines and reliability constraints. The key challenges faced when cloud systems provide virtual machines of varying levels of reliability, energy consumption, processing frequencies, and computing capabilities to execute tasks of these workflows. To address these issues, we propose an adaptive strategy based on maximum fan-out ratio considering the slack of tasks and deadline distribution for scheduling workflows in a single cloud platform, intending to minimise energy consumption while ensuring strict reliability and deadline constraints. We also propose an approach for dynamic scheduling of workflow using the rolling horizon concept to consider the dynamic execution time of tasks of the workflow where the actual task execution time at run time is shorter than worst-case execution time in most of the cases. Our proposed static approach outperforms the state-of-the-art (SOTA) by up to 70% on average in scenarios without deadline constraints, and achieves an improvement of approximately 2% in deadline-constrained cases. The dynamic variant of our approach demonstrates even stronger performance, surpassing SOTA by 82% in non-deadline scenarios and by up to 27% on average when deadline constraints are enforced. Furthermore, in comparison with the static optimal solution, our static approach yields results within a factor of 1.1, while the dynamic approach surpasses the optimal baseline by an average of 25%.",CS,AI_ML,0.85,Extracted from log - paper 491
A Deep Reinforcement Learning Approach for Cost Optimized Workflow Scheduling in Cloud Computing Environments,"Cost optimization is a common goal of workflow schedulers operating in cloud computing environments. The use of spot instances is a potential means of achieving this goal, as they are offered by cloud providers at discounted prices compared to their on-demand counterparts in exchange for reduced reliability. This is due to the fact that spot instances are subjected to interruptions when spare computing capacity used for provisioning them is needed back owing to demand variations. Also, the prices of spot instances are not fixed as pricing is dependent on long term supply and demand. The possibility of interruptions and pricing variations associated with spot instances adds a layer of uncertainty to the general problem of workflow scheduling across cloud computing environments. These challenges need to be efficiently addressed for enjoying the cost savings achievable with the use of spot instances without compromising the underlying business requirements. To this end, in this paper we use Deep Reinforcement Learning for developing an autonomous agent capable of scheduling workflows in a cost efficient manner by using an intelligent mix of spot and on-demand instances. The proposed solution is implemented in the open source container native Argo workflow engine that is widely used for executing industrial workflows. The results of the experiments demonstrate that the proposed scheduling method is capable of outperforming the current benchmarks.",CS,AI_ML,0.85,Extracted from log - paper 492
A Comparative Evaluation of Population-based Optimization Algorithms for Workflow Scheduling in Cloud-Fog Environments,"This work presents a comparative evaluation of four population-based optimization algorithms for workflow scheduling in cloud-fog environments. These algorithms are as follows: Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Differential Evolution (DE) and GA-PSO. This work also provides the motivational groundwork for the weighted sum objective function for the workflow scheduling problem and develops this function based on three objectives: makespan, cost and energy. The recently proposed FogWorkflowSim is used as the simulation environment with the aforementioned objectives serving performance metrics. Results show that hybrid combination of the GA-PSO algorithm exhibits slightly better than the standard algorithms. Future work will include expansion of the workflows used by increasing the number of tasks as well as adding some more workflows. The addition of some more objectives to the weighted objective function will also be pursued",CS,AI_ML,0.85,Extracted from log - paper 493
MCDS: AI Augmented Workflow Scheduling in Mobile Edge Cloud Computing Systems,"Workflow scheduling is a long-studied problem in parallel and distributed computing (PDC), aiming to efficiently utilize compute resources to meet user's service requirements. Recently proposed scheduling methods leverage the low response times of edge computing platforms to optimize application Quality of Service (QoS). However, scheduling workflow applications in mobile edge-cloud systems is challenging due to computational heterogeneity, changing latencies of mobile devices and the volatile nature of workload resource requirements. To overcome these difficulties, it is essential, but at the same time challenging, to develop a long-sighted optimization scheme that efficiently models the QoS objectives. In this work, we propose MCDS: Monte Carlo Learning using Deep Surrogate Models to efficiently schedule workflow applications in mobile edge-cloud computing systems. MCDS is an Artificial Intelligence (AI) based scheduling approach that uses a tree-based search strategy and a deep neural network-based surrogate model to estimate the long-term QoS impact of immediate actions for robust optimization of scheduling decisions. Experiments on physical and simulated edge-cloud testbeds show that MCDS can improve over the state-of-the-art methods in terms of energy consumption, response time, SLA violations and cost by at least 6.13, 4.56, 45.09 and 30.71 percent respectively.",CS,AI_ML,0.85,Extracted from log - paper 494
A Survey on Deadline Constrained Workflow Scheduling Algorithms in Cloud Environment,"Cloud Computing is the latest blooming technology in the era of Computer Science and Information Technology domain. There is an enormous pool of data centres, which are termed as Clouds where the services and associated data are being deployed and users need a constant Internet connection to access them. One of the highlights in Cloud is the delivering of applications or services in an on-demand environment. One of the most promising areas in Cloud scheduling is Scheduling of workflows which is intended to match the request of the user to the appropriate resources. There are several algorithms to automate the workflows in a way to satisfy the Quality of service (QoS) of the user among which deadline is considered as a major criterion, i.e. Satisfying the needs of the user with minimized cost and within the minimum stipulated time. This paper surveys various workflow scheduling algorithms that have a deadline as its criterion.",CS,AI_ML,0.85,Extracted from log - paper 495
"Reinforcement Learning based Workflow Scheduling in Cloud and Edge Computing Environments: A Taxonomy, Review and Future Directions","Deep Reinforcement Learning (DRL) techniques have been successfully applied for solving complex decision-making and control tasks in multiple fields including robotics, autonomous driving, healthcare and natural language processing. The ability of DRL agents to learn from experience and utilize real-time data for making decisions makes it an ideal candidate for dealing with the complexities associated with the problem of workflow scheduling in highly dynamic cloud and edge computing environments. Despite the benefits of DRL, there are multiple challenges associated with the application of DRL techniques including multi-objectivity, curse of dimensionality, partial observability and multi-agent coordination. In this paper, we comprehensively analyze the challenges and opportunities associated with the design and implementation of DRL oriented solutions for workflow scheduling in cloud and edge computing environments. Based on the identified characteristics, we propose a taxonomy of workflow scheduling with DRL. We map reviewed works with respect to the taxonomy to identify their strengths and weaknesses. Based on taxonomy driven analysis, we propose novel future research directions for the field.",CS,AI_ML,0.85,Extracted from log - paper 496
Portability of Scientific Workflows in NGS Data Analysis: A Case Study,"The analysis of next-generation sequencing (NGS) data requires complex computational workflows consisting of dozens of autonomously developed yet interdependent processing steps. Whenever large amounts of data need to be processed, these workflows must be executed on a parallel and/or distributed systems to ensure reasonable runtime. Porting a workflow developed for a particular system on a particular hardware infrastructure to another system or to another infrastructure is non-trivial, which poses a major impediment to the scientific necessities of workflow reproducibility and workflow reusability. In this work, we describe our efforts to port a state-of-the-art workflow for the detection of specific variants in whole-exome sequencing of mice. The workflow originally was developed in the scientific workflow system snakemake for execution on a high-performance cluster controlled by Sun Grid Engine. In the project, we ported it to the scientific workflow system SaasFee that can execute workflows on (multi-core) stand-alone servers or on clusters of arbitrary sizes using the Hadoop. The purpose of this port was that also owners of low-cost hardware infrastructures, for which Hadoop was made for, become able to use the workflow. Although both the source and the target system are called scientific workflow systems, they differ in numerous aspects, ranging from the workflow languages to the scheduling mechanisms and the file access interfaces. These differences resulted in various problems, some expected and more unexpected, that had to be resolved before the workflow could be run with equal semantics. As a side-effect, we also report cost/runtime ratios for a state-of-the-art NGS workflow on very different hardware platforms: A comparably cheap stand-alone server (80 threads), a mid-cost, mid-sized cluster (552 threads), and a high-end HPC system (3784 threads).",CS,AI_ML,0.85,Extracted from log - paper 497
Lotaru: Locally Estimating Runtimes of Scientific Workflow Tasks in Heterogeneous Clusters,"Many scientific workflow scheduling algorithms need to be informed about task runtimes a-priori to conduct efficient scheduling. In heterogeneous cluster infrastructures, this problem becomes aggravated because these runtimes are required for each task-node pair. Using historical data is often not feasible as logs are typically not retained indefinitely and workloads as well as infrastructure changes. In contrast, online methods, which predict task runtimes on specific nodes while the workflow is running, have to cope with the lack of example runs, especially during the start-up. In this paper, we present Lotaru, a novel online method for locally estimating task runtimes in scientific workflows on heterogeneous clusters. Lotaru first profiles all nodes of a cluster with a set of short-running and uniform microbenchmarks. Next, it runs the workflow to be scheduled on the user's local machine with drastically reduced data to determine important task characteristics. Based on these measurements, Lotaru learns a Bayesian linear regression model to predict a task's runtime given the input size and finally adjusts the predicted runtime specifically for each task-node pair in the cluster based on the micro-benchmark results. Due to its Bayesian approach, Lotaru can also compute robust uncertainty estimates and provides them as an input for advanced scheduling methods. Our evaluation with five real-world scientific workflows and different datasets shows that Lotaru significantly outperforms the baselines in terms of prediction errors for homogeneous and heterogeneous clusters.",CS,AI_ML,0.85,Extracted from log - paper 498
A Makespan and Energy-Aware Scheduling Algorithm for Workflows under Reliability Constraint on a Multiprocessor Platform,"Many scientific workflows can be modeled as a Directed Acyclic Graph (henceforth mentioned as DAG) where the nodes represent individual tasks, and the directed edges represent data and control flow dependency between two tasks. Due to the large volume of data, multiprocessor systems are often used to execute these workflows. Hence, scheduling the tasks of a workflow to achieve certain goals (such as minimizing the makespan, energy, or maximizing reliability, processor utilization, etc.) remains an active area of research in embedded systems. In this paper, we propose a workflow scheduling algorithm to minimize the makespan and energy for a given reliability constraint. If the reliability constraint is higher, we further propose Energy Aware Fault Tolerant Scheduling (henceforth mentioned as EAFTS) based on active replication. Additionally, given that the allocation of task nodes to processors is known, we develop a frequency allocation algorithm that assigns frequencies to the processors. Mathematically we show that our algorithms can work for any satisfiable reliability constraint. We analyze the proposed solution approaches to understand their time requirements. Experiments with real-world Workflows show that our algorithms, MERT and EAFTS, outperform the state-of-art approaches. In particular, we observe that MERT gives 3.12% lesser energy consumption and 14.14% lesser makespan on average. In the fault-tolerant setting, our method EAFTS gives 11.11% lesser energy consumption on average when compared with the state-of-art approaches.",CS,AI_ML,0.85,Extracted from log - paper 499
Cost-Aware Dynamic Cloud Workflow Scheduling using Self-Attention and Evolutionary Reinforcement Learning,"The Cost-aware Dynamic Multi-Workflow Scheduling (CDMWS) in the cloud is a kind of cloud workflow management problem, which aims to assign virtual machine (VM) instances to execute tasks in workflows so as to minimize the total costs, including both the penalties for violating Service Level Agreement (SLA) and the VM rental fees. Powered by deep neural networks, Reinforcement Learning (RL) methods can construct effective scheduling policies for solving CDMWS problems. Traditional policy networks in RL often use basic feedforward architectures to separately determine the suitability of assigning any VM instances, without considering all VMs simultaneously to learn their global information. This paper proposes a novel self-attention policy network for cloud workflow scheduling (SPN-CWS) that captures global information from all VMs. We also develop an Evolution Strategy-based RL (ERL) system to train SPN-CWS reliably and effectively. The trained SPN-CWS can effectively process all candidate VM instances simultaneously to identify the most suitable VM instance to execute every workflow task. Comprehensive experiments show that our method can noticeably outperform several state-of-the-art algorithms on multiple benchmark CDMWS problems.",CS,AI_ML,0.85,Extracted from log - paper 500
A Reference Architecture for Datacenter Scheduling: Extended Technical Report,"Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.",CS,AI_ML,0.85,Extracted from log - paper 501
"A Survey and Annotated Bibliography of Workflow Scheduling in Computing Infrastructures: Community, Keyword, and Article Reviews -- Extended Technical Report","Workflows are prevalent in today's computing infrastructures. The workflow model support various different domains, from machine learning to finance and from astronomy to chemistry. Different Quality-of-Service (QoS) requirements and other desires of both users and providers makes workflow scheduling a tough problem, especially since resource providers need to be as efficient as possible with their resources to be competitive. To a newcomer or even an experienced researcher, sifting through the vast amount of articles can be a daunting task. Questions regarding the difference techniques, policies, emerging areas, and opportunities arise. Surveys are an excellent way to cover these questions, yet surveys rarely publish their tools and data on which it is based. Moreover, the communities that are behind these articles are rarely studied. We attempt to address these shortcomings in this work. We focus on four areas within workflow scheduling: 1) the workflow formalism, 2) workflow allocation, 3) resource provisioning, and 4) applications and services. Each part features one or more taxonomies, a view of the community, important and emerging keywords, and directions for future work. We introduce and make open-source an instrument we used to combine and store article meta-data. Using this meta-data, we 1) obtain important keywords overall and per year, per community, 2) identify keywords growing in importance, 3) get insight into the structure and relations within each community, and 4) perform a systematic literature survey per part to validate and complement our taxonomies.",CS,AI_ML,0.85,Extracted from log - paper 502
A Cost Effective Reliability Aware Scheduler for Task Graphs in Multi-Cloud System,"Many scientific workflows can be represented by a Directed Acyclic Graph (DAG) where each node represents a task, and there will be a directed edge between two tasks if and only if there is a dependency relationship between the two i.e. the second one can not be started unless the first one is finished. Due to the increasing computational requirements of these workflows, they are deployed on cloud computing systems. Scheduling of workflows on such systems to achieve certain goals(e.g. minimization of makespan, cost, or maximization of reliability, etc.) remains an active area of research. In this paper, we propose a scheduling algorithm for allocating the nodes of our task graph in a heterogeneous multi-cloud system. The proposed scheduler considers many practical concerns such as pricing mechanisms, discounting schemes, and reliability analysis for task execution. This is a list-based heuristic that allocates tasks based on the expected times for which VMs need to be rented for them. We have analyzed the proposed approach to understand its time requirement. We perform a large number of experiments with real-world workflows: FFT, Ligo, Epigenomics, and Random workflows and observe that the proposed scheduler outperforms the state-of-art approaches up to 12%, 11%, and 1.1% in terms of cost, makespan, and reliability, respectively.",CS,AI_ML,0.85,Extracted from log - paper 503
MARS: Malleable Actor-Critic Reinforcement Learning Scheduler,"In this paper, we introduce MARS, a new scheduling system for HPC-cloud infrastructures based on a cost-aware, flexible reinforcement learning approach, which serves as an intermediate layer for next generation HPC-cloud resource manager. MARS ensembles the pre-trained models from heuristic workloads and decides on the most cost-effective strategy for optimization. A whole workflow application would be split into several optimizable dependent sub-tasks, then based on the pre-defined resource management plan, a reward will be generated after executing a scheduled task. Lastly, MARS updates the Deep Neural Network (DNN) model based on the reward. MARS is designed to optimize the existing models through reinforcement mechanisms. MARS adapts to the dynamics of workflow applications, selects the most cost-effective scheduling solution among pre-built scheduling strategies (backfilling, SJF, etc.) and self-learning deep neural network model at run-time. We evaluate MARS with different real-world workflow traces. MARS can achieve 5%-60% increased performance compared to the state-of-the-art approaches.",CS,AI_ML,0.85,Extracted from log - paper 504
Cybersecurity Dynamics,"We explore the emerging field of {\em Cybersecurity Dynamics}, a candidate foundation for the Science of Cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 505
Novel Approach for Cybersecurity Workforce Development: A Course in Secure Design,"Training the future cybersecurity workforce to respond to emerging threats requires introduction of novel educational interventions into the cybersecurity curriculum. To be effective, these interventions have to incorporate trending knowledge from cybersecurity and other related domains while allowing for experiential learning through hands-on experimentation. To date, the traditional interdisciplinary approach for cybersecurity training has infused political science, law, economics or linguistics knowledge into the cybersecurity curriculum, allowing for limited experimentation. Cybersecurity students were left with little opportunity to acquire knowledge, skills, and abilities in domains outside of these. Also, students in outside majors had no options to get into cybersecurity. With this in mind, we developed an interdisciplinary course for experiential learning in the fields of cybersecurity and interaction design. The inaugural course teaches students from cybersecurity, user interaction design, and visual design the principles of designing for secure use - or secure design - and allows them to apply them for prototyping of Internet-of-Things (IoT) products for smart homes. This paper elaborates on the concepts of secure design and how our approach enhances the training of the future cybersecurity workforce.",CS,AI_ML,0.85,Extracted from log - paper 506
"Building a Resilient Cybersecurity Posture: A Framework for Leveraging Prevent, Detect and Respond Functions and Law Enforcement Collaboration","This research paper proposes a framework for building a resilient cybersecurity posture that leverages prevent, detect, and respond functions and law enforcement collaboration. The Cybersecurity Resilience and Law Enforcement Collaboration (CyRLEC) Framework is designed to provide a comprehensive and integrated approach to cybersecurity that emphasizes collaboration with law enforcement agencies to mitigate cyber threats. The paper compares and contrasts the CyRLEC Framework with the NIST Cybersecurity Framework and highlights the critical differences between the two frameworks. While the NIST framework focuses on managing cybersecurity risk, the CyRLEC Framework takes a broader view of cybersecurity, including proactive prevention, early detection, rapid response to cyber-attacks, and close collaboration with law enforcement agencies to investigate and prosecute cybercriminals. The paper also provides a case study of a simulated real-world implementation of the CyRLEC Framework and evaluates its effectiveness in improving an organization's cybersecurity posture. The research findings demonstrate the value of the CyRLEC Framework in enhancing cybersecurity resilience and promoting effective collaboration with law enforcement agencies. Overall, this research paper contributes to the growing knowledge of cybersecurity frameworks and provides practical insights for organizations seeking to improve their cybersecurity posture.",CS,AI_ML,0.85,Extracted from log - paper 507
Cybersecurity Dynamics: A Foundation for the Science of Cybersecurity,"Cybersecurity Dynamics is new concept that aims to achieve the modeling, analysis, quantification, and management of cybersecurity from a holistic perspective, rather than from a building-blocks perspective. It is centered at modeling and analyzing the attack-defense interactions in cyberspace, which cause a ``natural'' phenomenon -- the evolution of the global cybersecurity state. In this Chapter, we systematically introduce and review the Cybersecurity Dynamics foundation for the Science of Cybersecurity. We review the core concepts, technical approaches, research axes, and results that have been obtained in this endeavor. We outline a research roadmap towards the ultimate research goal, including a systematic set of technical barriers.",CS,AI_ML,0.85,Extracted from log - paper 508
Adopting the Cybersecurity Concepts into Curriculum The Potential Effects on Students Cybersecurity Knowledge,"This study examines the effect of adopting cybersecurity concepts on the IT curriculum and determines the potential effect on students' knowledge of cybersecurity practices and level of awareness. To this end, a pilot study was first conducted to measure the current level of cybersecurity awareness. The results revealed that students do not have much knowledge of Cybersecurity. Thus, a four-step approach was proposed to infuse the relevant cybersecurity topics in five matched courses based on the latest Cybersecurity curricular guidelines (CSEC2017). A sample of 42 students was selected purposively without prior knowledge of Cybersecurity and divided identically into experimental and control groups. Students in the experimental group were asked to take five consecutive courses over five semesters. In each course, groups went through a pre-test for the infused topics. Then, the experimental group taught the corresponding infused topics. A post-test was administered to both groups at the end of each course, and the t-test was conducted. The results found significant differences between marks of prior and post-tests for 11 out of 14 infused topics. These satisfactory results would encourage universities to infuse cybersecurity concepts into their curriculum",CS,AI_ML,0.85,Extracted from log - paper 509
Practical Cybersecurity Ethics: Mapping CyBOK to Ethical Concerns,"Research into the ethics of cybersecurity is an established and growing topic of investigation, however the translation of this research into practice is lacking: there exists a small number of professional codes of ethics or codes of practice in cybersecurity, however these are very broad and do not offer much insight into the ethical dilemmas that can be faced while performing specific cybersecurity activities. In order to address this gap, we leverage ongoing work on the Cyber Security Body of Knowledge (CyBOK) to help elicit and document the responsibilities and ethics of the profession. Based on a literature review of the ethics of cybersecurity, we use CyBOK to frame the exploration of ethical challenges in the cybersecurity profession through a series of 15 interviews with cybersecurity experts. Our approach is qualitative and exploratory, aiming to answer the research question ""What ethical challenges, insights, and solutions arise in different areas of cybersecurity?"". Our findings indicate that there are broad ethical challenges across the whole of cybersecurity, but also that different areas of cybersecurity can face specific ethical considerations for which more detailed guidance can help professionals in those areas. In particular, our findings indicate that security decision-making is expected of all security professionals, but that this requires them to balance a complex mix of technical, objective and subjective points of view, and that resolving conflicts raises challenging ethical dilemmas. We conclude that more work is needed to explore, map, and integrate ethical considerations into cybersecurity practice; the urgent need to conduct further research into the ethics of cybersecurity AI; and highlight the importance of this work for individuals and professional bodies who seek to develop and mature the cybersecurity profession in a responsible manner.",CS,AI_ML,0.85,Extracted from log - paper 510
Toward a Blockchain-based Platform to Manage Cybersecurity Certification of IoT devices,"The goal of this paper is to propose a blockchain-based platform to enhance transparency and traceability of cybersecurity certification information motivated by the recently adopted EU Cybersecurity Act. The proposed platform is generic and intended to support the trusted exchange of cybersecurity certification information for any electronic product, service, or process. However, for the purposes of this paper, we focus on the case study of the cybersecurity certification of IoT devices, which are explicitly referenced in the recently adopted Cybersecurity Act as one of the main domains where it is highlighted the need for an increased level of trust.",CS,AI_ML,0.85,Extracted from log - paper 511
Assessing and Improving Cybersecurity Maturity for SMEs: Standardization aspects,"SMEs constitute a very large part of the economy in every country and they play an important role in economic growth and social development. SMEs are frequent targets of cybersecurity attacks similar to large enterprises. However, unlike large enterprises, SMEs mostly have limited capabilities regarding cybersecurity practices. Given the increasing cybersecurity risks and the large impact that the risks may bring to the SMEs, assessing and improving the cybersecurity capabilities is crucial for SMEs for sustainability. This research aims to provide an approach for SMEs for assessing and improving their cybersecurity capabilities by integrating key elements from existing industry standards.",CS,AI_ML,0.85,Extracted from log - paper 512
"ICAR, a categorical framework to connect vulnerability, threat and asset managements","We present ICAR, a mathematical framework derived from category theory for representing cybersecurity NIST and MITRE's ontologies. Designed for cybersecurity, ICAR is a category whose objects are cybersecurity knowledge (weakness, vulnerability, impacted product, attack technique, etc.) and whose morphisms are relations between this knowledge, that make sense for cybersecurity. Within this rigorous and unified framework, we obtain a knowledge graph capable of identifying the attack and weakness structures of an IS, at the interface between description logics, database theory and cybersecurity. We then define ten cybersecurity queries to help understand the risks incurred by IS and organise their defence.",CS,AI_ML,0.85,Extracted from log - paper 513
Collaborative Approaches to Enhancing Smart Vehicle Cybersecurity by AI-Driven Threat Detection,"The introduction sets the stage for exploring collaborative approaches to bolstering smart vehicle cybersecurity through AI-driven threat detection. As the automotive industry increasingly adopts connected and automated vehicles (CAVs), the need for robust cybersecurity measures becomes paramount. With the emergence of new vulnerabilities and security requirements, the integration of advanced technologies such as 5G networks, blockchain, and quantum computing presents promising avenues for enhancing CAV cybersecurity . Additionally, the roadmap for cybersecurity in autonomous vehicles emphasizes the importance of efficient intrusion detection systems and AI-based techniques, along with the integration of secure hardware, software stacks, and advanced threat intelligence to address cybersecurity challenges in future autonomous vehicles.",CS,AI_ML,0.85,Extracted from log - paper 514
Cybersecurity in the AWS Cloud,"This paper re-examines the content of a standard advanced course in Cybersecurity from the perspective of Cloud Computing. More precisely, we review the core concepts of Cybersecurity, as presented in a senior undergraduate or graduate class, in light of the Amazon Web Services (AWS) cloud.",CS,AI_ML,0.85,Extracted from log - paper 515
Ten AI Stepping Stones for Cybersecurity,"With the turmoil in cybersecurity and the mind-blowing advances in AI, it is only natural that cybersecurity practitioners consider further employing learning techniques to help secure their organizations and improve the efficiency of their security operation centers. But with great fears come great opportunities for both the good and the evil, and a myriad of bad deals. This paper discusses ten issues in cybersecurity that hopefully will make it easier for practitioners to ask detailed questions about what they want from an AI system in their cybersecurity operations. We draw on the state of the art to provide factual arguments for a discussion on well-established AI in cybersecurity issues, including the current scope of AI and its application to cybersecurity, the impact of privacy concerns on the cybersecurity data that can be collected and shared externally to the organization, how an AI decision can be explained to the person running the operations center, and the implications of the adversarial nature of cybersecurity in the learning techniques. We then discuss the use of AI by attackers on a level playing field including several issues in an AI battlefield, and an AI perspective on the old cat-and-mouse game including how the adversary may assess your AI power.",CS,AI_ML,0.85,Extracted from log - paper 516
A Systems Thinking for Cybersecurity Modeling,"Solving cybersecurity issues requires a holistic understanding of components, factors, structures and their interactions in cyberspace, but conventional modeling approaches view the field of cybersecurity by their boundaries so that we are still not clear to cybersecurity and its changes. In this paper, we attempt to discuss the application of systems thinking approaches to cybersecurity modeling. This paper reviews the systems thinking approaches and provides the systems theories and methods for tackling cybersecurity challenges, regarding relevant fields, associated impact factors and their interactions. Moreover, an illustrative example of systems thinking frameworks for cybersecurity modeling is developed to help broaden the mind in methodology, theory, technology and practice. This article concludes that systems thinking can be considered as one of the powerful tools of cybersecurity modeling to find, characterize, understand, evaluate and predict cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 517
Classifying SMEs for Approaching Cybersecurity Competence and Awareness,"Cybersecurity is increasingly a concern for small and medium-sized enterprises (SMEs), and there exist many awareness training programs and tools for them. The literature mainly studies SMEs as a unitary type of company and provides one-size-fits-all recommendations and solutions. However, SMEs are not homogeneous. They are diverse with different vulnerabilities, cybersecurity needs, and competencies. Few studies considered such differences in standards and certificates for security tools adoption and cybersecurity tailoring for these SMEs. This study proposes a classification framework with an outline of cybersecurity improvement needs for each class. The framework suggests five SME types based on their characteristics and specific security needs: cybersecurity abandoned SME, unskilled SME, expert-connected SME, capable SME, and cybersecurity provider SME. In addition to describing the five classes, the study explains the framework's usage in sampled SMEs. The framework proposes solutions for each class to approach cybersecurity awareness and competence more consistent with SME needs. The final publication is available at ACM Digital Library via this https URL https://doi.org/10.1145/3465481.3469200",CS,AI_ML,0.85,Extracted from log - paper 518
"A Review of Quantum Cybersecurity: Threats, Risks and Opportunities","The promise of quantum computing is not speeding up conventional computing rather delivering an exponential advantage for certain classes of problems, with profound implications for cybersecurity for instance. With the advent and development of quantum computers, cyberspace security can surely become the most critical problem for the Internet in near future. On contrary, prosaic quantum technology can be promising to transform cybersecurity. This research aims to synthesize basic and fundamental studies concerning quantum cybersecurity that can be emerged both as a threat and solution to critical cybersecurity issues based on a systematic study. We provide a comprehensive, illustrative description of the current state-of-the-art quantum computing and cybersecurity and present the proposed approaches to date. Findings in quantum computing cybersecurity suggest that quantum computing can be adopted for the betterment of cybersecurity threats while it poses the most unexpected threats to cybersecurity. The focus and depth of this systematic survey not only provide quantum and cybersecurity practitioners and researchers with a consolidated body of knowledge about current trends in this area but also underpins a starting point for further research in this field.",CS,AI_ML,0.85,Extracted from log - paper 519
Ontological Approach toward Cybersecurity in Cloud Computing,"Widespread deployment of the Internet enabled building of an emerging IT delivery model, i.e., cloud computing. Albeit cloud computing-based services have rapidly developed, their security aspects are still at the initial stage of development. In order to preserve cybersecurity in cloud computing, cybersecurity information that will be exchanged within it needs to be identified and discussed. For this purpose, we propose an ontological approach to cybersecurity in cloud computing. We build an ontology for cybersecurity operational information based on actual cybersecurity operations mainly focused on non-cloud computing. In order to discuss necessary cybersecurity information in cloud computing, we apply the ontology to cloud computing. Through the discussion, we identify essential changes in cloud computing such as data-asset decoupling and clarify the cybersecurity information required by the changes such as data provenance and resource dependency information.",CS,AI_ML,0.85,Extracted from log - paper 520
Emergent Behavior in Cybersecurity,We argue that emergent behavior is inherent to cybersecurity.,CS,AI_ML,0.85,Extracted from log - paper 521
Cybersecurity Cost of Quality: Managing the Costs of Cybersecurity Risk Management,"There is no standard yet for measuring and controlling the costs associated with implementing cybersecurity programs. To advance research and practice towards this end, we develop a mapping using the well-known concept of quality costs and the Framework Core within the Cybersecurity Framework produced by the National Institute of Standards and Technology (NIST) in response to the Cybersecurity Enhancement Act of 2014. This mapping can be easily adopted by organizations that are already using the NIST CSF for cybersecurity risk management to plan, manage, and continually improve cybersecurity operations. If an organization is not using the NIST CSF, this mapping may still be useful for linking elements in accounting systems that are associated with cybersecurity operations and risk management to a quality cost model.",CS,AI_ML,0.85,Extracted from log - paper 522
Explaining Cybersecurity with Films and the Arts (Extended Abstract),Explaining Cybersecurity with Films and the Arts,CS,AI_ML,0.85,Extracted from log - paper 523
A Value Driven Framework for Cybersecurity Innovation in Transportation & Infrastructure,"This paper introduces a value-driven cybersecurity innovation framework for the transportation and infrastructure sectors, as opposed to the traditional market-centric approaches that have dominated the field. Recontextualizing innovation categories into sustaining, incremental, disruptive, and transformative, we aim to foster a culture of self-innovation within organizations, enabling a strategic focus on cybersecurity measures that directly contribute to business value and strategic goals. This approach enhances operational effectiveness and efficiency of cyber defences primarily, while also aligns cybersecurity initiatives with mission-critical objectives. We detail a practical method for evaluating the business value of cybersecurity innovations and present a pragmatic approach for organizations to funnel innovative ideas in a structured and repeatable manner. The framework is designed to reinforce cybersecurity capabilities against an evolving cyber threat landscape while maintaining infrastructural integrity. Shifting the focus from general market appeal to sector-specific needs, our framework provides cybersecurity leaders with the strategic cyber-foresight necessary for prioritizing impactful initiatives, thereby making cybersecurity a core business enabler rather than a burden.",CS,AI_ML,0.85,Extracted from log - paper 524
Exploring the Cybersecurity-Resilience Gap: An Analysis of Student Attitudes and Behaviors in Higher Education,"Cyberattacks frequently target higher educational institutions, making cybersecurity awareness and resilience critical for students. However, limited research exists on cybersecurity awareness, attitudes, and resilience among students in higher education. This study addresses this gap using the Theory of Planned Behavior as a theoretical framework. A modified Human Aspects of Information Security Questionnaire was employed to gather 266 valid responses from undergraduate and postgraduate students at a South African higher education institution. Key dimensions of cybersecurity awareness and behavior, including password management, email usage, social media practices, and mobile device security, were assessed. A significant disparity in cybersecurity awareness and practices, with postgraduate students demonstrating superior performance across several dimensions was noted. This research postulates the existence of a Cybersecurity-Education Inflection Point during the transition to postgraduate studies, coined as the Cybersecurity-Resilience Gap. These concepts provide a foundation for developing targeted cybersecurity education initiatives in higher education, particularly highlighting the need for earlier intervention at the undergraduate level.",CS,AI_ML,0.85,Extracted from log - paper 525
A Review of Topological Data Analysis for Cybersecurity,"In cybersecurity it is often the case that malicious or anomalous activity can only be detected by combining many weak indicators of compromise, any one of which may not raise suspicion when taken alone. The path that such indicators take can also be critical. This makes the problem of analysing cybersecurity data particularly well suited to Topological Data Analysis (TDA), a field that studies the high level structure of data using techniques from algebraic topology, both for exploratory analysis and as part of a machine learning workflow. By introducing TDA and reviewing the work done on its application to cybersecurity, we hope to highlight to researchers a promising new area with strong potential to improve cybersecurity data science.",CS,AI_ML,0.85,Extracted from log - paper 526
Cybersecurity Entity Alignment via Masked Graph Attention Networks,"Cybersecurity vulnerability information is often recorded by multiple channels, including government vulnerability repositories, individual-maintained vulnerability-gathering platforms, or vulnerability-disclosure email lists and forums. Integrating vulnerability information from different channels enables comprehensive threat assessment and quick deployment to various security mechanisms. Efforts to automatically gather such information, however, are impeded by the limitations of today's entity alignment techniques. In our study, we annotate the first cybersecurity-domain entity alignment dataset and reveal the unique characteristics of security entities. Based on these observations, we propose the first cybersecurity entity alignment model, CEAM, which equips GNN-based entity alignment with two mechanisms: asymmetric masked aggregation and partitioned attention. Experimental results on cybersecurity-domain entity alignment datasets demonstrate that CEAM significantly outperforms state-of-the-art entity alignment methods.",CS,AI_ML,0.85,Extracted from log - paper 527
Elicitation of SME Requirements for Cybersecurity Solutions by Studying Adherence to Recommendations,"Small and medium-sized enterprises (SME) have become the weak spot of our economy for cyber attacks. These companies are large in number and often do not have the controls in place to prevent successful attacks, respectively are not prepared to systematically manage their cybersecurity capabilities. One of the reasons for why many SME do not adopt cybersecurity is that developers of cybersecurity solutions understand little the SME context and the requirements for successful use of these solutions. We elicit requirements by studying how cybersecurity experts provide advice to SME. The experts recommendations offer insights into what important capabilities of the solution are and how these capabilities ought to be used for mitigating cybersecurity threats. The adoption of a recommendation hints at a correct match of the solution, hence successful consideration of requirements. Abandoned recommendations point to a misalignment that can be used as a source to inquire missed requirements. Re-occurrence of adoption or abandonment decisions corroborate the presence of requirements. This poster describes the challenges of SME regarding cybersecurity and introduces our proposed approach to elicit requirements for cybersecurity solutions. The poster describes CYSEC, our tool used to capture cybersecurity advice and help to scale cybersecurity requirements elicitation to a large number of participating SME. We conclude by outlining the planned research to develop and validate CYSEC.",CS,AI_ML,0.85,Extracted from log - paper 528
An Assessment Methodology and Instrument for Cybersecurity: The Ireland Use Case,"Governments around the world are required to strengthen their national cybersecurity capabilities to respond effectively to the growing, changing, and sophisticated cyber threats and attacks, thus protecting society and the way of life as a whole. Responsible government institutions need to revise, evaluate, and bolster their national cybersecurity capabilities to fulfill the new requirements, for example regarding new trends affecting cybersecurity, key supporting laws and regulations, and implementations risk and challenges. This report presents a comprehensive assessment instrument for cybersecurity at the national level in order to help countries to ensure optimum response capability and more effective use of critical resources of each state. More precisely, the report - builds a common understanding of the critical cybersecurity capabilities and competence to be assessed at the national level, - adds value to national strategic planning and implementation which impact the development and adaptation of national cybersecurity strategies, - provides an overview of the assessment approaches at the national level, including capabilities, frameworks, and controls, - introduces a comprehensive cybersecurity instrument for countries to determine areas of improvement and develop enduring national capabilities, - describes how to apply the proposed national cybersecurity assessment framework in a real-world case, and - presents the results and lessons learned of the application of the assessment framework at the national level to assist governments in further building cybersecurity capabilities.",CS,AI_ML,0.85,Extracted from log - paper 529
Towards a Systematic View on Cybersecurity Ecology,"Current network security systems are progressively showing their limitations. One credible estimate is that only about 45% of new threats are detected. Therefore it is vital to find a new direction that cybersecurity development should follow. We argue that the next generation of cybersecurity systems should seek inspiration in nature. This approach has been used before in the first generation of cybersecurity systems; however, since then cyber threats and environment have evolved significantly, and accordingly the first-generation systems have lost their effectiveness. A next generation of bio-inspired cybersecurity research is emerging, but progress is hindered by the lack of a framework for mapping biological security systems to their cyber analogies. In this paper, using terminology and concepts from biology, we describe a cybersecurity ecology and a framework that may be used to systematically research and develop bio-inspired cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 530
Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling,"Indicators of Compromise (IOCs) are artifacts observed on a network or in an operating system that can be utilized to indicate a computer intrusion and detect cyber-attacks in an early stage. Thus, they exert an important role in the field of cybersecurity. However, state-of-the-art IOCs detection systems rely heavily on hand-crafted features with expert knowledge of cybersecurity, and require large-scale manually annotated corpora to train an IOC classifier. In this paper, we propose using an end-to-end neural-based sequence labelling model to identify IOCs automatically from cybersecurity articles without expert knowledge of cybersecurity. By using a multi-head self-attention module and contextual features, we find that the proposed model is capable of gathering contextual information from texts of cybersecurity articles and performs better in the task of IOC identification. Experiments show that the proposed model outperforms other sequence labelling models, achieving the average F1-score of 89.0% on English cybersecurity article test set, and approximately the average F1-score of 81.8% on Chinese test set.",CS,AI_ML,0.85,Extracted from log - paper 531
Cybersecurity and Sustainable Development,"Growing interdependencies between organizations lead them towards the creation of inter-organizational networks where cybersecurity and sustainable development have become one of the most important issues. The Environmental Goods and Services Sector (EGSS) is one of the fastest developing sectors of the economy fueled by the growing relationships between network entities based on ICT usage. In this sector, Green Cybersecurity is an emerging issue because it secures processes related directly and indirectly to environmental management and protection. In the future, the multidimensional development of the EGSS can help European Union to overcome the upcoming crises. At the same time, computer technologies and cybersecurity can contribute to the implementation of the concept of sustainable development. The development of environmental technologies along with their cybersecurity is one of the aims of the realization of sustainable production and domestic security concepts among the EU countries. Hence, the aim of this article is a theoretical discussion and research on the relationships between cybersecurity and sustainable development in inter-organizational networks. Therefore, the article is an attempt to give an answer to the question about the current state of the implementation of cybersecurity in relation to the EGSS part of the economy in different EU countries.",CS,AI_ML,0.85,Extracted from log - paper 532
A Model-Driven Methodology for Automotive Cybersecurity Test Case Generation,"Through international regulations (most prominently the latest UNECE regulation) and standards, the already widely perceived higher need for cybersecurity in automotive systems has been recognized and will mandate higher efforts for cybersecurity engineering. T he UNECE also demands the effectiveness of these engineering to be verified and validated through testing. T his requires both a significantly higher rate and more comprehensiveness of cybersecurity testing that is not effectively to cope with using current, predominantly manual, automotive cybersecurity testing techniques. To allow for comprehensive and efficient testing at all stages of the automotive life cycle, including supply chain parts not at band, and to facilitate efficient third party testing, as well as to test under real-world conditions, also methodologies for testing the cybersecurity of vehicular systems as a black box are necessary. T his paper therefore presents a model and attack tree-based approach to (semi-)automate automotive cybersecurity testing, as well as considerations for automatically black box-deriving models for the use in attack modeling.",CS,AI_ML,0.85,Extracted from log - paper 533
Understanding parents' perceptions of children's cybersecurity awareness in Norway,"Children are increasingly using the internet nowadays. While internet use exposes children to various privacy and security risks, few studies have examined how parents perceive and address their children's cybersecurity risks. To address this gap, we conducted a qualitative study with 25 parents living in Norway with children aged between 10 to 15. We conducted semi-structured interviews with the parents and performed a thematic analysis of the interview data. The results of this paper include a list of cybersecurity awareness needs for children from a parental perspective, a list of learning resources for children, and a list of challenges for parents to ensure cybersecurity at home. Our results are useful for developers and educators in developing cybersecurity solutions for children. Future research should focus on defining cybersecurity theories and practices that contribute to children's and parents' awareness about cybersecurity risks, needs, and solutions.",CS,AI_ML,0.85,Extracted from log - paper 534
Multidimensional Cybersecurity Framework for Strategic Foresight,"Cybersecurity is now at the forefront of most organisational digital transformative agendas and National economic, social and political programmes. Hence its impact to society can no longer be seen to be one dimensional. The rise in National cybersecurity laws and regulations is a good indicator of its perceived importance to nations. And the recent awakening for social and ethical transparency in society and coupled with sustainability issues demonstrate the need for a paradigm shift in how cybersecurity discourses can now happen. In response to this shift, a multidimensional cybersecurity framework for strategic foresight underpinned on situational awareness is proposed. The conceptual cybersecurity framework comprising six domains such as Physical, Cultural, Economic, Social, Political and Cyber, is discussed. The guiding principles underpinning the framework are outlined, followed by in-depth reflection on the Business, Operational, Technological and Human (BOTH) factors and their implications for strategic foresight for cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 535
The Future of Cybersecurity in Southeast Asia along the Maritime Silk Road,"This paper proposes an analysis of the prospects of the cyber security industry and educational ecosystems in four Southeast Asian countries, namely Vietnam, Singapore, Malaysia, and Indonesia, which are along the Maritime Silk Road, by using two novel metrics: the ""Cybersecurity Education Prospects Index"" (CEPI) and the ""Cybersecurity Industry Prospects Index"" (CIPI). The CEPI evaluates the state of cybersecurity education by assessing the availability and quality of cybersecurity degrees together with their ability to attract new students. On the other hand, the CIPI measures the potential for the cybersecurity industry's growth and development by assessing the talent pool needed to build and sustain its growth. Ultimately, this study emphasizes the vital importance of a healthy cybersecurity ecosystem where education is responsible for supporting the industry to ensure the security and reliability of commercial operations in these countries against a complex and evolving cyber threat landscape.",CS,AI_ML,0.85,Extracted from log - paper 536
"Unaware, Unfunded and Uneducated: A Systematic Review of SME Cybersecurity","Small and Medium Enterprises (SMEs) are pivotal in the global economy, accounting for over 90% of businesses and 60% of employment worldwide. Despite their significance, SMEs have been disregarded from cybersecurity initiatives, rendering them ill-equipped to deal with the growing frequency, sophistication, and destructiveness of cyber-attacks. We systematically reviewed the cybersecurity literature on SMEs published between 2017 and 2023. We focus on research discussing cyber threats, adopted controls, challenges, and constraints SMEs face in pursuing cybersecurity resilience. Our search yielded 916 studies that we narrowed to 77 relevant papers. We identified 44 unique themes and categorised them as novel findings or established knowledge. This distinction revealed that research on SMEs is shallow and has made little progress in understanding SMEs' roles, threats, and needs. Studies often repeated early discoveries without replicating or offering new insights. The existing research indicates that the main challenges to attaining cybersecurity resilience of SMEs are a lack of awareness of the cybersecurity risks, limited cybersecurity literacy and constrained financial resources. However, resource availability varied between developed and developing countries. Our analysis indicated a relationship among these themes, suggesting that limited literacy is the root cause of awareness and resource constraint issues.",CS,AI_ML,0.85,Extracted from log - paper 537
Data Driven Approaches to Cybersecurity Governance for Board Decision-Making -- A Systematic Review,"Cybersecurity governance influences the quality of strategic decision-making to ensure cyber risks are managed effectively. Board of Directors are the decisions-makers held accountable for managing this risk; however, they lack adequate and efficient information necessary for making such decisions. In addition to the myriad of challenges they face, they are often insufficiently versed in the technology or cybersecurity terminology or not provided with the correct tools to support them to make sound decisions to govern cybersecurity effectively. A different approach is needed to ensure BoDs are clear on the approach the business is taking to build a cyber resilient organization. This systematic literature review investigates the existing risk measurement instruments, cybersecurity metrics, and associated models for supporting BoDs. We identified seven conceptual themes through literature analysis that form the basis of this study's main contribution. The findings showed that, although sophisticated cybersecurity tools exist and are developing, there is limited information for Board of Directors to support them in terms of metrics and models to govern cybersecurity in a language they understand. The review also provides some recommendations on theories and models that can be further investigated to provide support to Board of Directors.",CS,AI_ML,0.85,Extracted from log - paper 538
Designing Cybersecurity Awareness Solutions for the Young People in Rural Developing Countries: The Need for Diversity and Inclusion,"Cybersecurity challenges and the need for awareness are well-recognized in developed countries, but this still needs attention in less-developed countries. With the expansion of technology, security concerns are also becoming more prevalent worldwide. This paper presents a design and creation research study exploring which factors we should consider when designing cybersecurity awareness solutions for young people in developing countries. We have developed prototypes of mini-cybersecurity awareness applications and conducted a pilot study with eight participants (aged 16-30) from Gambia, Eritrea, and Syria. Our findings show that factors like the influence of culture and social constructs, literacy, and language competence, the way of introducing cybersecurity terms and concepts, and the need for reflection are essential to consider when designing and developing cybersecurity awareness solutions for target users in developing countries. The findings of this study will guide future researchers to design more inclusive cybersecurity awareness solutions for users in developing countries.",CS,AI_ML,0.85,Extracted from log - paper 539
Toward a Quantum Information System Cybersecurity Taxonomy and Testbed: Exploiting a Unique Opportunity for Early Impact,"Any human-designed system can potentially be exploited in ways that its designers did not envision, and information systems or networks using quantum components do not escape this reality. We are presented with a unique but quickly waning opportunity to bring cybersecurity concerns to the forefront for quantum information systems before they become widely deployed. The resources and knowledge required to do so, however, may not be common in the cybersecurity community. Yet, a nexus exist. Cybersecurity starts with risk, and there are good taxonomies for security vulnerabilities and impacts in classical systems. In this paper, we propose a preliminary taxonomy for quantum cybersecurity vulnerabilities that accounts for the latest advances in quantum information systems, and must evolve to incorporate well-established cybersecurity principles and methodologies. We envision a testbed environment designed and instrumented with the specific purpose of enabling a broad collaborative community of cybersecurity and quantum information system experts to conduct experimental evaluation of software and hardware security including both physical and virtual quantum components. Furthermore, we envision that such a resource may be available as a user facility to the open science research community.",CS,AI_ML,0.85,Extracted from log - paper 540
When LLMs Meet Cybersecurity: A Systematic Literature Review,"The rapid development of large language models (LLMs) has opened new avenues across various fields, including cybersecurity, which faces an evolving threat landscape and demand for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper addresses this gap by providing a systematic literature review, covering the analysis of over 300 works, encompassing 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three key research questions: the construction of cybersecurity-oriented LLMs, the application of LLMs to various cybersecurity tasks, the challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices and serve as a valuable resource for applying LLMs in this field. We also maintain and regularly update a list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 541
Gender of Recruiter Makes a Difference: A study into Cybersecurity Graduate Recruitment,"An ever-widening workforce gap exists in the global cybersecurity industry but diverse talent is underutilized. The global cybersecurity workforce is only 25% female. Much research exists on the effect of gender bias on the hiring of women into the technical workforce, but little on how the gender of the recruiter (gender difference) affects recruitment decisions. This research reveals differences between the non-technical skills sought by female vs non-female cybersecurity recruiters. The former look for recruits with people-focused skills while the latter look for task-focused skills, highlighting the need for gender diversity in recruitment panels. Recruiters are increasingly seeking non-technical (soft) skills in technical graduate recruits. This requires STEM curriculum in Universities to adapt to match. Designing an industry-ready cybersecurity curriculum requires knowledge of these non-technical skills. An online survey of cybersecurity professionals was used to determine the most sought after non-technical skills in the field. Analysis of the data reveals distinct gender differences in the non-technical skills most valued in a recruit, based on the gender of the recruiter (not the recruited). The gender differences discovered do not correspond to the higher proportion of women employed in non-technical cybersecurity roles.",CS,AI_ML,0.85,Extracted from log - paper 542
SoK: Identifying Limitations and Bridging Gaps of Cybersecurity Capability Maturity Models (CCMMs),"In the rapidly evolving digital landscape, where organisations are increasingly vulnerable to cybersecurity threats, Cybersecurity Capability Maturity Models (CCMMs) emerge as pivotal tools in enhancing organisational cybersecurity posture. CCMMs provide a structured framework to guide organisations in assessing their current cybersecurity capabilities, identifying critical gaps, and prioritising improvements. However, the full potential of CCMMs is often not realised due to inherent limitations within the models and challenges encountered during their implementation and adoption processes. These limitations and challenges can significantly hamper the efficacy of CCMMs in improving cybersecurity. As a result, organisations remain vulnerable to cyber threats as they may fail to identify and address critical security gaps, implement necessary improvements or allocate resources effectively. To address these limitations and challenges, conducting a thorough investigation into existing models is essential. Therefore, we conducted a Systematic Literature Review (SLR) analysing 43 publications to identify existing CCMMs, their limitations, and the challenges organisations face when implementing and adopting them. By understanding these barriers, we aim to explore avenues for enhancing the efficacy of CCMMs, ensuring they more effectively meet the cybersecurity needs of organisational entities.",CS,AI_ML,0.85,Extracted from log - paper 543
A Survey-Based Quantitative Analysis of Stress Factors and Their Impacts Among Cybersecurity Professionals,"This study investigates the prevalence and underlying causes of work-related stress and burnout among cybersecurity professionals using a quantitative survey approach guided by the Job Demands-Resources model. Analysis of responses from 50 cybersecurity practitioners reveals an alarming reality: 44% report experiencing severe work-related stress and burnout, while an additional 28% are uncertain about their condition. The demanding nature of cybersecurity roles, unrealistic expectations, and unsupportive organizational cultures emerge as primary factors fueling this crisis. Notably, 66% of respondents perceive cybersecurity jobs as more stressful than other IT positions, with 84% facing additional challenges due to the pandemic and recent high-profile breaches. The study finds that most cybersecurity experts are reluctant to report their struggles to management, perpetuating a cycle of silence and neglect. To address this critical issue, the paper recommends that organizations foster supportive work environments, implement mindfulness programs, and address systemic challenges. By prioritizing the mental health of cybersecurity professionals, organizations can cultivate a more resilient and effective workforce to protect against an ever-evolving threat landscape.",CS,AI_ML,0.85,Extracted from log - paper 544
Cybersecurity Study Programs: What's in a Name?,"Improving cybersecurity education has become a priority for many countries and organizations worldwide. Computing societies and professional associations have recognized cybersecurity as a distinctive computing discipline and created specialized cybersecurity curricular guidelines. Higher education institutions are introducing new cybersecurity programs, attracting students to this expanding field. In this paper, we examined 101 study programs across 24 countries. Based on their analysis, we argue that top-ranked universities have not yet fully implemented the guidelines and offer programs that have ""cyber"" in their name but lack some essential elements of a cybersecurity program. In particular, most programs do not sufficiently cover non-technical components, such as law, policies, or risk management. Also, most programs teach knowledge and skills but do not expose students to experiential learning outside the traditional classroom (such as internships) to develop their competencies. As a result, graduates of these programs may not meet employer expectations and may require additional training. To help program directors and educators improve their programs and courses, this paper offers examples of effective practices from cybersecurity programs around the world and our teaching practice.",CS,AI_ML,0.85,Extracted from log - paper 545
"Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice","Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",CS,AI_ML,0.85,Extracted from log - paper 546
Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report,"As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.",CS,AI_ML,0.85,Extracted from log - paper 547
SecureBERT: A Domain-Specific Language Model for Cybersecurity,"Natural Language Processing (NLP) has recently gained wide attention in cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber automation. Increased connection and automation have revolutionized the world's economic and cultural infrastructures, while they have introduced risks in terms of cyber attacks. CTI is information that helps cybersecurity analysts make intelligent security decisions, that is often delivered in the form of natural language text, which must be transformed to machine readable format through an automated procedure before it can be used for automated security measures. This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text (e.g., CTI) and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual efforts. SecureBERT has been trained using a large corpus of cybersecurity text.To make SecureBERT effective not just in retaining general English understanding, but also when applied to text with cybersecurity implications, we developed a customized tokenizer as well as a method to alter pre-trained weights. The SecureBERT is evaluated using the standard Masked Language Model (MLM) test as well as two additional standard NLP tasks. Our evaluation studies show that SecureBERT\footnote{\url{https://github.com/ehsanaghaei/SecureBERT}} outperforms existing similar models, confirming its capability for solving crucial NLP tasks in cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 548
"Systemization of Knowledge (SoK)- Cross Impact of Transfer Learning in Cybersecurity: Offensive, Defensive and Threat Intelligence Perspectives","Recent literature highlights a significant cross-impact between transfer learning and cybersecurity. Many studies have been conducted on using transfer learning to enhance security, leading to various applications in different cybersecurity tasks. However, previous research is focused on specific areas of cybersecurity. This paper presents a comprehensive survey of transfer learning applications in cybersecurity by covering a wide range of domains, identifying current trends, and shedding light on under-explored areas. The survey highlights the significance of transfer learning in addressing critical issues in cybersecurity, such as improving detection accuracy, reducing training time, handling data imbalance, and enhancing privacy preservation. Additional insights are provided on the common problems solved using transfer learning, such as the lack of labeled data, different data distributions, and privacy concerns. The paper identifies future research directions and challenges that require community attention, including the need for privacy-preserving models, automatic tools for knowledge transfer, metrics for measuring domain relatedness, and enhanced privacy preservation mechanisms. The insights and roadmap presented in this paper will guide researchers in further advancing transfer learning in cybersecurity, fostering the development of robust and efficient cybersecurity systems to counter emerging threats and protect sensitive information. To the best of our knowledge, this paper is the first of its kind to present a comprehensive taxonomy of all areas of cybersecurity that benefited from transfer learning and propose a detailed future roadmap to shape the possible research direction in this area.",CS,AI_ML,0.85,Extracted from log - paper 549
A Transdisciplinary Approach to Cybersecurity: A Framework for Encouraging Transdisciplinary Thinking,"Classical cybersecurity is often perceived as a rigid science discipline filled with computer scientists and mathematicians. However, due to the rapid pace of technology development and integration, new criminal enterprises, new defense tactics, and the understanding of the human element, cybersecurity is quickly beginning to encompass more than just computers. Cybersecurity experts must broaden their perspectives beyond traditional disciplinary boundaries to provide the best protection possible. They must start to practice transdisciplinary cybersecurity. Taking influence from the Stakeholder Theory in business ethics, this paper presents a framework to encourage transdisciplinary thinking and assist experts in tackling the new challenges of the modern day. The framework uses the simple Think, Plan, Do approach to enable experts to develop their transdisciplinary thinking. The framework is intended to be used as an evaluation tool for existing cybersecurity practices or postures, as a development tool to engage with other disciplines to foster learning and create new methods, and as a guidance tool to encourage new ways of thinking about, perceiving, and executing cybersecurity practices. For each of those intended uses, a use case is presented as an example to showcase how the framework might be used. The ultimate goal of this paper is not the framework but transdisciplinary thinking. By using the tool presented here and developing their own transdisciplinary thinking, cybersecurity experts can be better prepared to face cybersecurity's unique and complex challenges.",CS,AI_ML,0.85,Extracted from log - paper 550
Navigating the road to automotive cybersecurity compliance,"The automotive industry has evolved significantly since the introduction of the Ford Model T in 1908. Today's vehicles are not merely mechanical constructs; they are integral components of a complex digital ecosystem, equipped with advanced connectivity features powered by Artificial Intelligence and cloud computing technologies. This evolution has enhanced vehicle safety, efficiency, and the overall driving experience. However, it also introduces new challenges, notably in cybersecurity. With the increasing integration of digital technologies, vehicles have become more susceptible to cyber-attacks, prompting significant cybersecurity concerns. These concerns include securing sensitive data, protecting vehicles from unauthorized access, and ensuring user privacy. In response, the automotive industry is compelled to adopt robust cybersecurity measures to safeguard both vehicles and data against potential threats. Legislative frameworks such as UNR155 and UNR156 by the United Nations, along with other international regulations, aim to establish stringent cybersecurity mandates. These regulations require compliance with comprehensive cybersecurity management systems and necessitate regular updates and testing to cope with the evolving nature of cyber threats. The introduction of such regulations highlights the growing recognition of cybersecurity as a critical component of automotive safety and functionality. The future of automotive cybersecurity lies in the continuous development of advanced protective measures and collaborative efforts among all stakeholders, including manufacturers, policymakers, and cybersecurity professionals. Only through such concerted efforts can the industry hope to address the dual goals of innovation in vehicle functionality and stringent security measures against the backdrop of an increasingly interconnected digital landscape.",CS,AI_ML,0.85,Extracted from log - paper 551
Assessing the Maturity of Cybersecurity Education in Virginia and the Impact of State Level Investment,"With a global shortage of cybersecurity students with the education and experience necessary to fill more than 3 million jobs, cybersecurity education is an international problem. Significant research within this field has explored this problem in depth, identifying a variety of shortcomings in the cybersecurity educational pipeline including lack of certifications, security clearances, and appropriate educational opportunities within institutions of higher education. Additional research has built on this, exploring specific gaps within what cybersecurity opportunities are provided within institutions of higher education. We build an ordinal scale for assessing this, the cybersecurity education maturity model scale (CEMMs), and provide evidence of reliability and validity. We then calculate the CEMMs score for all public four-year universities in the state of Virginia between 2017 and 2025, with 2017 marking a year in which the state started the Commonwealth Cyber Initiative (CCI). We find that the scale proposed provides a consistent and reliable way to compare the cybersecurity offerings available between universities. When comparing year to year average CEMMs score, we find that public four year universities in Virginia are increasing their program offerings in the area of cybersecurity, with potential to make an impact on the cybersecurity jobs gap.",CS,AI_ML,0.85,Extracted from log - paper 552
Cybersecurity Awareness,"Cybersecurity awareness can be viewed as the level of appreciation, understanding or knowledge of cybersecurity or information security aspects. Such aspects include cognizance of cyber risks and threats, but also appropriate protection measures.",CS,AI_ML,0.85,Extracted from log - paper 553
A systematic literature review on Ransomware attacks,"In the area of information technology, cybersecurity is critical. Information security is one of todays highest priorities. Cyber attacks, which are on the rise and include Ransomware, are the first thing that springs to mind when we think about cybersecurity. To counteract cybercrime, several governments and companies employ a range of strategies. Despite several cybersecurity measures, ransomware continues to terrify people.",CS,AI_ML,0.85,Extracted from log - paper 554
The NIST Definition of Cloud Computing,"Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model promotes availability and is composed of five essential characteristics, three service models, and four deployment models.",CS,AI_ML,0.85,Extracted from log - paper 555
Cloud computing: state-of-the-art and research challenges,"Cloud computing has recently emerged as a new paradigm for hosting and delivering services over the Internet. Cloud computing is attractive to business owners as it eliminates the requirement for users to plan ahead for provisioning, and allows enterprises to start from the small and increase resources only when there is a rise in service demand. However, despite the fact that cloud computing offers huge opportunities to the IT industry, the development of cloud computing technology is currently at its infancy, with many issues still to be addressed. In this paper, we present a survey of cloud computing, highlighting its key concepts, architectural principles, state-of-the-art implementation as well as research challenges. The aim of this paper is to provide a better understanding of the design challenges of cloud computing and identify important research directions in this increasingly important area.",CS,AI_ML,0.85,Extracted from log - paper 556
Energy efficiency in cloud computing data centers: a survey on software technologies,"Cloud computing is a commercial and economic paradigm that has gained traction since 2006 and is presently the most significant technology in IT sector. From the notion of cloud computing to its energy efficiency, cloud has been the subject of much discussion. The energy consumption of data centres alone will rise from 200 TWh in 2016 to 2967 TWh in 2030. The data centres require a lot of power to provide services, which increases CO2 emissions. In this survey paper, software-based technologies that can be used for building green data centers and include power management at individual software level has been discussed. The paper discusses the energy efficiency in containers and problem-solving approaches used for reducing power consumption in data centers. Further, the paper also gives details about the impact of data centers on environment that includes the e-waste and the various standards opted by different countries for giving rating to the data centers. This article goes beyond just demonstrating new green cloud computing possibilities. Instead, it focuses the attention and resources of academia and society on a critical issue: long-term technological advancement. The article covers the new technologies that can be applied at the individual software level that includes techniques applied at virtualization level, operating system level and application level. It clearly defines different measures at each level to reduce the energy consumption that clearly adds value to the current environmental problem of pollution reduction. This article also addresses the difficulties, concerns, and needs that cloud data centres and cloud organisations must grasp, as well as some of the factors and case studies that influence green cloud usage.",CS,AI_ML,0.85,Extracted from log - paper 557
Efficient Multi-User Computation Offloading for Mobile-Edge Cloud Computing,"Mobile-edge cloud computing is a new paradigm to provide cloud computing capabilities at the edge of pervasive radio access networks in close proximity to mobile users. In this paper, we first study the multi-user computation offloading problem for mobile-edge cloud computing in a multi-channel wireless interference environment. We show that it is NP-hard to compute a centralized optimal solution, and hence adopt a game theoretic approach for achieving efficient computation offloading in a distributed manner. We formulate the distributed computation offloading decision making problem among mobile device users as a multi-user computation offloading game. We analyze the structural property of the game and show that the game admits a Nash equilibrium and possesses the finite improvement property. We then design a distributed computation offloading algorithm that can achieve a Nash equilibrium, derive the upper bound of the convergence time, and quantify its efficiency ratio over the centralized optimal solutions in terms of two important performance metrics. We further extend our study to the scenario of multi-user computation offloading in the multi-channel wireless contention environment. Numerical results corroborate that the proposed algorithm can achieve superior computation offloading performance and scale well as the user size increases.",CS,AI_ML,0.85,Extracted from log - paper 558
"IoT and Cloud Computing Issues, Challenges and Opportunities: A Review","With the exponential growth of the Industrial Internet of Things (IIoT), multiple outlets are constantly producing a vast volume of data. It is unwise to locally store all the raw data in the IIoT devices since the energy and storage spaces of the end devices are strictly constrained. self-organization and short-range Internet of Things (IoT) networking also support outsourced data and cloud computing, independent of the distinctive resource constraint properties. For the remainder of the findings, there is a sequence of unfamiliar safeguards for IoT and cloud integration problems. The delivery of cloud computing is highly efficient, storage is becoming more and more current, and some groups are now altering their data from in house records Cloud Computing Vendors' hubs. Intensive IoT applications for workloads and data are subject to challenges while utilizing cloud computing tools. In this report, we research IoT and cloud computing and address cloud-compatible problems and computing techniques to promote the stable transition of IoT programs to the cloud.",CS,AI_ML,0.85,Extracted from log - paper 559
"Edge Computing and Sensor-Cloud: Overview, Solutions, and Directions","Sensor-cloud originates from extensive recent applications of wireless sensor networks and cloud computing. To draw a roadmap of the current research activities of the sensor-cloud community, we first investigate the state-of-the-art sensor-cloud literature reviews published since the late 2010s and discovered that these surveys have primarily studied the sensor-cloud in specific aspects, security-enabled solutions, efficient management mechanisms, and architectural challenges. While the existing surveys have reviewed the sensor-cloud from various perspectives, they are inadequate for the three key issues that require urgent attention in the sensor-cloud: reliability, energy, and heterogeneity. To fill this gap, we perform a thorough survey by examining the origins of the sensor-cloud and providing an in-depth and comprehensive discussion of these three key challenges. We summarize initial designs of the new edge-based schemes to address these challenges and identify several open issues and promising future research directions.",CS,AI_ML,0.85,Extracted from log - paper 560
CLOUD COMPUTING,"This journal review provides an overview of the current state of cloud computing, including its definition, benefits, and challenges. It examines the various cloud computing models and architectures, and discusses the security and privacy issues associated with the cloud. It also looks at the potential of cloud computing to transform the IT industry and provide new opportunities for businesses. Finally, the review looks at the future of cloud computing, including potential use cases and applications Cloud computing is a paradigm that has revolutionized the way we store, process and access data and applications. At its core, it involves delivering computing resources such as servers, storage, and applications over the internet, allowing users to access them on-demand from anywhere and at any time. This technology has numerous benefits, including cost-effectiveness, scalability, flexibility, and ease of maintenance. It has become a crucial enabler for many businesses and organizations, offering them the ability to streamline their operations and remain competitive in a constantly changing market. In this abstract, we explore the fundamentals of cloud computing, its key features, and its potential impact on the future of technology.. KEYWORDS Cloud computing, technology, advantages, disadvantages, security, challenges, opportunities.",CS,AI_ML,0.85,Extracted from log - paper 561
"A survey of mobile cloud computing: architecture, applications, and approaches","Together with an explosive growth of the mobile applications and emerging of cloud computing concept, mobile cloud computing (MCC) has been introduced to be a potential technology for mobile services. MCC integrates the cloud computing into the mobile environment and overcomes obstacles related to the performance (e.g., battery life, storage, and bandwidth), environment (e.g., heterogeneity, scalability, and availability), and security (e.g., reliability and privacy) discussed in mobile computing. This paper gives a survey of MCC, which helps general readers have an overview of the MCC including the definition, architecture, and applications. The issues, existing solutions, and approaches are presented. In addition, the future research directions of MCC are discussed. Copyright © 2011 John Wiley & Sons, Ltd.",CS,AI_ML,0.85,Extracted from log - paper 562
A Systematic Literature Review on Cloud Computing Security: Threats and Mitigation Strategies,"Cloud computing has become a widely exploited research area in academia and industry. Cloud computing benefits both cloud services providers (CSPs) and consumers. The security challenges associated with cloud computing have been widely studied in the literature. This systematic literature review (SLR) is aimed to review the existing research studies on cloud computing security, threats, and challenges. This SLR examined the research studies published between 2010 and 2020 within the popular digital libraries. We selected 80 papers after a meticulous screening of published works to answer the proposed research questions. The outcomes of this SLR reported seven major security threats to cloud computing services. The results showed that data tampering and leakage were among the highly discussed topics in the chosen literature. Other identified security risks were associated with the data intrusion and data storage in the cloud computing environment. This SLR’s results also indicated that consumers’ data outsourcing remains a challenge for both CSPs and cloud users. Our survey paper identified the blockchain as a partnering technology to alleviate security concerns. The SLR findings reveal some suggestions to be carried out in future works to bring data confidentiality, data integrity, and availability.",CS,AI_ML,0.85,Extracted from log - paper 563
Resource Allocation in 5G IoV Architecture Based on SDN and Fog-Cloud Computing,"In the traditional cloud-based Internet of Vehicles (IoV) architecture, it is difficult to guarantee the low latency requirements of the current intelligent transportation system (ITS). As a supplement to cloud computing, fog computing can effectively alleviate the bottlenecks of cloud computing bandwidth and computing resources and improve the quality of service (QoS) of the IoV. However, as a distributed system that operates near users, fog computing has a complicated network structure. In the complex and dynamic IoV environment, to effectively manage these computing resources with different attributes and provide high-quality services, it is necessary to design an efficient architecture and a resource allocation algorithm. Therefore, on the basis of fog-cloud computing and software-defined networking (SDN), a novel 5G IoV architecture is designed. In addition, after fully considering the service requirements of the IoV, a model of four objectives is constructed, and a many-objective optimization algorithm is proposed. The experiment results show that the proposed algorithm outperforms the other state-of-the-art algorithms.",CS,AI_ML,0.85,Extracted from log - paper 564
Cyber Security in IoT-Based Cloud Computing: A Comprehensive Survey,"Cloud computing provides the flexible architecture where data and resources are dispersed at various locations and are accessible from various industrial environments. Cloud computing has changed the using, storing, and sharing of resources such as data, services, and applications for industrial applications. During the last decade, industries have rapidly switched to cloud computing for having more comprehensive access, reduced cost, and increased performance. In addition, significant improvement has been observed in the internet of things (IoT) with the integration of cloud computing. However, this rapid transition into the cloud raised various security issues and concerns. Traditional security solutions are not directly applicable and sometimes ineffective for cloud-based systems. Cloud platforms’ challenges and security concerns have been addressed during the last three years, despite the successive use and proliferation of multifaceted cyber weapons. The rapid evolution of deep learning (DL) in the artificial intelligence (AI) domain has brought many benefits that can be utilized to address industrial security issues in the cloud. The findings of the proposed research include the following: we present a comprehensive survey of enabling cloud-based IoT architecture, services, configurations, and security models; the classification of cloud security concerns in IoT into four major categories (data, network and service, applications, and people-related security issues), which are discussed in detail; we identify and inspect the latest advancements in cloud-based IoT attacks; we identify, discuss, and analyze significant security issues in each category and present the limitations from a general, artificial intelligence and deep learning perspective; we provide the technological challenges identified in the literature and then identify significant research gaps in the IoT-based cloud infrastructure to highlight future research directions to blend cybersecurity in cloud.",CS,AI_ML,0.85,Extracted from log - paper 565
Blockchain Meets Cloud Computing: A Survey,"Blockchain technology has been deemed to be an ideal choice for strengthening existing computing systems in varied manners. As one of the network-enabled technologies, cloud computing has been broadly adopted in the industry through numerous cloud service models. Fusing blockchain technology with existing cloud systems has a great potential in both functionality/performance enhancement and security/privacy improvement. The question remains on how blockchain technology inserts into current deployed cloud solutions and enables the reengineering of cloud datacenter. This survey addresses this issue and investigates recent efforts in the technical fusion of blockchain and clouds. Three technical dimensions roughly are covered in this work. First, we concern the service model and review an emerging cloud-relevant blockchain service model, Blockchain-as-a-Service (BaaS); second, security is considered a key technical dimension in this work and both access control and searchable encryption schemes are assessed; finally, we examine the performance of cloud datacenter with supports/participance of blockchain from hardware and software perspectives. Main findings of this survey will be theoretical supports for future reference of blockchain-enabled reengineering of cloud datacenter.",CS,AI_ML,0.85,Extracted from log - paper 566
Blockchain-based Database to Ensure Data Integrity in Cloud Computing Environments,"Data is nowadays an invaluable resource; indeed, it guides all business decisions in most of the computer-aided human activities. Threats to data integrity are thus of paramount relevance, as tampering with data may maliciously affect crucial business decisions. This issue is especially true in cloud computing environments, where data owners cannot control fundamental data aspects, like the physical storage of data and the control of its accesses. Blockchain has recently emerged as a fascinating technology which, among others, provides compelling properties about data integrity. Using the Blockchain to face data integrity threats seems to be a natural choice, but its current limitations of low throughput, high latency, and weak stability hinder the practical feasibility of any Blockchain-based solutions. In this paper, by focusing on a case study from the European SUNFISH project, which concerns the design of a secure by-design cloud federation platform for the public sector, we precisely delineate the actual data integrity needs of cloud computing environments and the research questions to be tackled to adopt Blockchain-based databases. First, we detail the open research questions and the difficulties inherent in addressing them. Then, we outline a preliminary design of an effective Blockchain-based database for cloud computing environments.",CS,AI_ML,0.85,Extracted from log - paper 567
Collaborate Edge and Cloud Computing With Distributed Deep Learning for Smart City Internet of Things,"City Internet-of-Things (IoT) applications are becoming increasingly complicated and thus require large amounts of computational resources and strict latency requirements. Mobile cloud computing (MCC) is an effective way to alleviate the limitation of computation capacity by offloading complex tasks from mobile devices (MDs) to central clouds. Besides, mobile-edge computing (MEC) is a promising technology to reduce latency during data transmission and save energy by providing services in a timely manner. However, it is still difficult to solve the task offloading challenges in heterogeneous cloud computing environments, where edge clouds and central clouds work collaboratively to satisfy the requirements of city IoT applications. In this article, we consider the heterogeneity of edge and central cloud servers in the offloading destination selection. To jointly optimize the system utility and the bandwidth allocation for each MD, we establish a hybrid offloading model, including the collaboration of MCC and MEC. A distributed deep learning-driven task offloading (DDTO) algorithm is proposed to generate near-optimal offloading decisions over the MDs, edge cloud server, and central cloud server. Experimental results demonstrate the accuracy of the DDTO algorithm, which can effectively and efficiently generate near-optimal offloading decisions in the edge and cloud computing environments. Furthermore, it achieves high performance and greatly reduces the computational complexity when compared with other offloading schemes that neglect the collaboration of heterogeneous clouds. More precisely, the DDTO scheme can improve computational performance by 63%, compared with the local-only scheme.",CS,AI_ML,0.85,Extracted from log - paper 568
"Blockchain-based trust management in cloud computing systems: a taxonomy, review and future directions","Through virtualization and resource integration, cloud computing has expanded its service area and offers a better user experience than the traditional platforms, along with its business operation model bringing huge economic and social benefits. However, a large amount of evidence shows that cloud computing is facing with serious security and trust crisis, and building a trust-enabled transaction environment has become its key factor. The traditional cloud trust model usually adopts a centralized architecture, which causes large management overhead, network congestion and even single point of failure. Furthermore, due to a lack of transparency and traceability, trust evaluation results cannot be fully recognized by all participants. Blockchain is a new and promising decentralized framework and distributed computing paradigm. Its unique features in operating rules and traceability of records ensure the integrity, undeniability and security of the transaction data. Therefore, blockchain is very suitable for constructing a distributed and decentralized trust architecture. This paper carries out a comprehensive survey on blockchain-based trust approaches in cloud computing systems. Based on a novel cloud-edge trust management framework and a double-blockchain structure based cloud transaction model, it identifies the open challenges and gives directions for future research in this field.",CS,AI_ML,0.85,Extracted from log - paper 569
A Survey on Internet of Things and Cloud Computing for Healthcare,"The fast development of the Internet of Things (IoT) technology in recent years has supported connections of numerous smart things along with sensors and established seamless data exchange between them, so it leads to a stringy requirement for data analysis and data storage platform such as cloud computing and fog computing. Healthcare is one of the application domains in IoT that draws enormous interest from industry, the research community, and the public sector. The development of IoT and cloud computing is improving patient safety, staff satisfaction, and operational efficiency in the medical industry. This survey is conducted to analyze the latest IoT components, applications, and market trends of IoT in healthcare, as well as study current development in IoT and cloud computing-based healthcare applications since 2015. We also consider how promising technologies such as cloud computing, ambient assisted living, big data, and wearables are being applied in the healthcare industry and discover various IoT, e-health regulations and policies worldwide to determine how they assist the sustainable development of IoT and cloud computing in the healthcare industry. Moreover, an in-depth review of IoT privacy and security issues, including potential threats, attack types, and security setups from a healthcare viewpoint is conducted. Finally, this paper analyzes previous well-known security models to deal with security risks and provides trends, highlighted opportunities, and challenges for the IoT-based healthcare future development.",CS,AI_ML,0.85,Extracted from log - paper 570
A Study of Moving from Cloud Computing to Fog Computing,"The exponential growth of the Internet of Things (IoT) technology poses various challenges to the classic centralized cloud computing paradigm, including high latency, limited capacity, and network failure. Cloud computing and Fog computing carry the cloud closer to IoT computers in order to overcome these problems. Cloud and Fog provide IoT processing and storage of IoT items locally instead of sending them to the cloud. Cloud and Fog provide quicker reactions and better efficiency in conjunction with the cloud. Cloud and fog computing should also be viewed as the safest approach to ensure that IoT delivers reliable and stable resources to multiple IoT customers. This article discusses the latest in cloud and Fog computing and their convergence with IoT by stressing deployment's advantages and complexities. It also concentrates on cloud and Fog design and new IoT technologies, enhanced by utilizing the cloud and Fog model. Finally, transparent topics are addressed, along with potential testing recommendations for cloud storage and Fog computing, and IoT.",CS,AI_ML,0.85,Extracted from log - paper 571
Cloud Computing and Its Role in the Information Technology,"The concept of Cloud Computing has been distinguished as one of the major computing models in recent years. Cloud computing has become a great innovation that has important consequences not just for services on the internet but also for the entire Information technology (IT) market. Its emergence aims to optimize on-demand technology, hardware and information provisioning as a service, reaching the economy of scale in the distribution and operation of IT strategies. A great deal of cloud computing research has been concerned over some obstacles and challenges that rely upon behind the lure of cloud computing. Security has been always raised as one of the most critical issues of cloud computing where resolving such an issue would result in constant growth in the use and popularity of the cloud. Security requirements represent a major issue that has to be met in order of easing some of these obstacles. This article presents the role of cloud computing in the IT sectors.",CS,AI_ML,0.85,Extracted from log - paper 572
Efficient Computing Resource Sharing for Mobile Edge-Cloud Computing Networks,"Both the edge and the cloud can provide computing services for mobile devices to enhance their performance. The edge can reduce the conveying delay by providing local computing services while the cloud can support enormous computing requirements. Their cooperation can improve the utilization of computing resources and ensure the QoS, and thus is critical to edge-cloud computing business models. This paper proposes an efficient framework for mobile edge-cloud computing networks, which enables the edge and the cloud to share their computing resources in the form of wholesale and buyback. To optimize the computing resource sharing process, we formulate the computing resource management problems for the edge servers to manage their wholesale and buyback scheme and the cloud to determine the wholesale price and its local computing resources. Then, we solve these problems from two perspectives: i) social welfare maximization and ii) profit maximization for the edge and the cloud. For i), we have proved the concavity of the social welfare and proposed an optimal cloud computing resource management to maximize the social welfare. For ii), since it is difficult to directly prove the convexity of the primal problem, we first proved the concavity of the wholesaled computing resources with respect to the wholesale price and designed an optimal pricing and cloud computing resource management to maximize their profits. Numerical evaluations show that the total profit can be maximized by social welfare maximization while the respective profits can be maximized by the optimal pricing and cloud computing resource management.",CS,AI_ML,0.85,Extracted from log - paper 573
What Is Cloud Computing?,"Cloud computing is a distributed environment for multiple organizations to use remotely and get high scalability, reliability on anytime, anywhere, and pay-as-you-go concepts. An organization has to create data centres to store, manage, and process the information to achieve benefits from data and make decisions. Cloud gives organizations a successful approach that leads to profit without maintaining the cost of data centres and technical staff to manage the services. Cloud has different types of architectures, types of clouds, and cost packages for using the cloud. These services can be scaled up or down when required by an organization. Cloud has unbeatable future because IT world is acquiring it and giving a boost to their businesses. Many cloud providers are using it and the remaining are moving to cloud. Cloud computing also gives birth to edge computing, fog computing, and many more zero downtime solutions.",CS,AI_ML,0.85,Extracted from log - paper 574
Cost-Aware Multimedia Data Allocation for Heterogeneous Memory Using Genetic Algorithm in Cloud Computing,"Recent expansions of Internet-of-Things (IoT) applying cloud computing have been growing at a phenomenal rate. As one of the developments, heterogeneous cloud computing has enabled a variety of cloud-based infrastructure solutions, such as multimedia big data. Numerous prior researches have explored the optimizations of on-premise heterogeneous memories. However, the heterogeneous cloud memories are facing constraints due to the performance limitations and cost concerns caused by the hardware distributions and manipulative mechanisms. Assigning data tasks to distributed memories with various capacities is a combinatorial NP-hard problem. This paper focuses on this issue and proposes a novel approach, Cost-Aware Heterogeneous Cloud Memory Model (CAHCM), aiming to provision a high performance cloud-based heterogeneous memory service offerings. The main algorithm supporting CAHCM is Dynamic Data Allocation Advance (2DA) Algorithm that uses genetic programming to determine the data allocations on the cloud-based memories. In our proposed approach, we consider a set of crucial factors impacting the performance of the cloud memories, such as communication costs, data move operating costs, energy performance, and time constraints. Finally, we implement experimental evaluations to examine our proposed model. The experimental results have shown that our approach is adoptable and feasible for being a cost-aware cloud-based solution.",CS,AI_ML,0.85,Extracted from log - paper 575
Dynamic Computation Offloading for Mobile Cloud Computing: A Stochastic Game-Theoretic Approach,"Driven by the growing popularity of mobile applications, mobile cloud computing has been envisioned as a promising approach to enhance computation capability of mobile devices and reduce the energy consumptions. In this paper, we investigate the problem of multi-user computation offloading for mobile cloud computing under dynamic environment, wherein mobile users become active or inactive dynamically, and the wireless channels for mobile users to offload computation vary randomly. As mobile users are self-interested and selfish in offloading computation tasks to the mobile cloud, we formulate the mobile users’ offloading decision process under dynamic environment as a stochastic game. We prove that the formulated stochastic game is equivalent to a weighted potential game which has at least one Nash Equilibrium (NE). We quantify the efficiency of the NE, and further propose a multi-agent stochastic learning algorithm to reach the NE with a guaranteed convergence rate (which is also analytically derived). Finally, we conduct simulations to validate the effectiveness of the proposed algorithm and evaluate its performance under dynamic environment.",CS,AI_ML,0.85,Extracted from log - paper 576
Energy-Efficient Dynamic Computation Offloading and Cooperative Task Scheduling in Mobile Cloud Computing,"Mobile cloud computing (MCC) as an emerging and prospective computing paradigm, can significantly enhance computation capability and save energy for smart mobile devices (SMDs) by offloading computation-intensive tasks from resource-constrained SMDs onto resource-rich cloud. However, how to achieve energy-efficient computation offloading under hard constraint for application completion time remains a challenge. To address such a challenge, in this paper, we provide an energy-efficient dynamic offloading and resource scheduling (eDors) policy to reduce energy consumption and shorten application completion time. We first formulate the eDors problem into an energy-efficiency cost (EEC) minimization problem while satisfying task-dependency requirement and completion time deadline constraint. We then propose a distributed eDors algorithm consisting of three subalgorithms of computation offloading selection, clock frequency control, and transmission power allocation. Next, we show that computation offloading selection depends on not only the computing workload of a task, but also the maximum completion time of its immediate predecessors and the clock frequency and transmission power of the mobile device. Finally, we provide experimental results in a real testbed and demonstrate that the eDors algorithm can effectively reduce EEC by optimally adjusting CPU clock frequency of SMDs in local computing, and adapting the transmission power for wireless channel conditions in cloud computing.",CS,AI_ML,0.85,Extracted from log - paper 577
"A Smart Manufacturing Service System Based on Edge Computing, Fog Computing, and Cloud Computing","The state-of-the-art technologies in new generation information technologies (New IT) greatly stimulate the development of smart manufacturing. In a smart manufacturing environment, more and more devices would be connected to the Internet so that a large volume of data can be obtained during all phases of the product lifecycle. Cloud-based smart manufacturing paradigm facilitates a new variety of applications and services to analyze a large volume of data and enable large-scale manufacturing collaboration. However, different factors, such as the network unavailability, overfull bandwidth, and latency time, restrict its availability for high-speed and low-latency real-time applications. Fog computing and edge computing extended the compute, storage, and networking capabilities of the cloud to the edge, which will respond to the above-mentioned issues. Based on cloud computing, fog computing, and edge computing, in this paper, a hierarchy reference architecture is introduced for smart manufacturing. The architecture is expected to be applied in the digital twin shop floor, which opens a bright perspective of new applications within the field of manufacturing.",CS,AI_ML,0.85,Extracted from log - paper 578
A Survey of Communication Protocols for Internet of Things and Related Challenges of Fog and Cloud Computing Integration,"The fast increment in the number of IoT (Internet of Things) devices is accelerating the research on new solutions to make cloud services scalable. In this context, the novel concept of fog computing as well as the combined fog-to-cloud computing paradigm is becoming essential to decentralize the cloud, while bringing the services closer to the end-system. This article surveys e application layer communication protocols to fulfill the IoT communication requirements, and their potential for implementation in fog- and cloud-based IoT systems. To this end, the article first briefly presents potential protocol candidates, including request-reply and publish-subscribe protocols. After that, the article surveys these protocols based on their main characteristics, as well as the main performance issues, including latency, energy consumption, and network throughput. These findings are thereafter used to place the protocols in each segment of the system (IoT, fog, cloud), and thus opens up the discussion on their choice, interoperability, and wider system integration. The survey is expected to be useful to system architects and protocol designers when choosing the communication protocols in an integrated IoT-to-fog-to-cloud system architecture.",CS,AI_ML,0.85,Extracted from log - paper 579
Multi-User Multi-Task Computation Offloading in Green Mobile Edge Cloud Computing,"Mobile Edge Cloud Computing (MECC) has becoming an attractive solution for augmenting the computing and storage capacity of Mobile Devices (MDs) by exploiting the available resources at the network edge. In this work, we consider computation offloading at the mobile edge cloud that is composed of a set of Wireless Devices (WDs), and each WD has an energy harvesting equipment to collect renewable energy from the environment. Moreover, multiple MDs intend to offload their tasks to the mobile edge cloud simultaneously. We first formulate the multi-user multi-task computation offloading problem for green MECC, and use Lyaponuv Optimization Approach to determine the energy harvesting policy: how much energy to be harvested at each WD; and the task offloading schedule: the set of computation offloading requests to be admitted into the mobile edge cloud, the set of WDs assigned to each admitted offloading request, and how much workload to be processed at the assigned WDs. We then prove that the task offloading scheduling problem is NP-hard, and introduce centralized and distributed Greedy Maximal Scheduling algorithms to resolve the problem efficiently. Performance bounds of the proposed schemes are also discussed. Extensive evaluations are conducted to test the performance of the proposed algorithms.",CS,AI_ML,0.85,Extracted from log - paper 580
Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",CS,AI_ML,0.85,Extracted from log - paper 581
A Survey on Distributed Denial of Service (DDoS) Attacks in SDN and Cloud Computing Environments,"Recently, software defined networks (SDNs) and cloud computing have been widely adopted by researchers and industry. However, widespread acceptance of these novel networking paradigms has been hampered by the security threats. Advances in the processing technologies have helped attackers in increasing the attacks too, for instance, the development of Denial of Service (DoS) attacks to distributed DoS (DDoS) attacks which are seldom identified by conventional firewalls. In this paper, we present the state of art of the DDoS attacks in SDN and cloud computing scenarios. Especially, we focus on the analysis of SDN and cloud computing architecture. Besides, we also overview the research works and open problems in identifying and tackling the DDoS attacks.",CS,AI_ML,0.85,Extracted from log - paper 582
Load balancing in cloud computing – A hierarchical taxonomical classification,"Load unbalancing problem is a multi-variant, multi-constraint problem that degrades performance and efficiency of computing resources. Load balancing techniques cater the solution for load unbalancing situation for two undesirable facets- overloading and under-loading. In contempt of the importance of load balancing techniques to the best of our knowledge, there is no comprehensive, extensive, systematic and hierarchical classification about the existing load balancing techniques. Further, the factors that cause load unbalancing problem are neither studied nor considered in the literature. This paper presents a detailed encyclopedic review about the load balancing techniques. The advantages and limitations of existing methods are highlighted with crucial challenges being addressed so as to develop efficient load balancing algorithms in future. The paper also suggests new insights towards load balancing in cloud computing.",CS,AI_ML,0.85,Extracted from log - paper 583
Workflow Scheduling Using Hybrid GA-PSO Algorithm in Cloud Computing,"Cloud computing environment provides several on-demand services and resource sharing for clients. Business processes are managed using the workflow technology over the cloud, which represents one of the challenges in using the resources in an efficient manner due to the dependencies between the tasks. In this paper, a Hybrid GA-PSO algorithm is proposed to allocate tasks to the resources efficiently. The Hybrid GA-PSO algorithm aims to reduce the makespan and the cost and balance the load of the dependent tasks over the heterogonous resources in cloud computing environments. The experiment results show that the GA-PSO algorithm decreases the total execution time of the workflow tasks, in comparison with GA, PSO, HSGA, WSGA, and MTCT algorithms. Furthermore, it reduces the execution cost. In addition, it improves the load balancing of the workflow application over the available resources. Finally, the obtained results also proved that the proposed algorithm converges to optimal solutions faster and with higher quality compared to other algorithms.",CS,AI_ML,0.85,Extracted from log - paper 584
Issues and Challenges of Load Balancing Techniques in Cloud Computing,"With the growth in computing technologies, cloud computing has added a new paradigm to user services that allows accessing Information Technology services on the basis of pay-per-use at any time and any location. Owing to flexibility in cloud services, numerous organizations are shifting their business to the cloud and service providers are establishing more data centers to provide services to users. However, it is essential to provide cost-effective execution of tasks and proper utilization of resources. Several techniques have been reported in the literature to improve performance and resource use based on load balancing, task scheduling, resource management, quality of service, and workload management. Load balancing in the cloud allows data centers to avoid overloading/underloading in virtual machines, which itself is a challenge in the field of cloud computing. Therefore, it becomes a necessity for developers and researchers to design and implement a suitable load balancer for parallel and distributed cloud environments. This survey presents a state-of-the-art review of issues and challenges associated with existing load-balancing techniques for researchers to develop more effective algorithms.",CS,AI_ML,0.85,Extracted from log - paper 585
Block Design-Based Key Agreement for Group Data Sharing in Cloud Computing,"Data sharing in cloud computing enables multiple participants to freely share the group data, which improves the efficiency of work in cooperative environments and has widespread potential applications. However, how to ensure the security of data sharing within a group and how to efficiently share the outsourced data in a group manner are formidable challenges. Note that key agreement protocols have played a very important role in secure and efficient group data sharing in cloud computing. In this paper, by taking advantage of the symmetric balanced incomplete block design (SBIBD), we present a novel block design-based key agreement protocol that supports multiple participants, which can flexibly extend the number of participants in a cloud environment according to the structure of the block design. Based on the proposed group data sharing model, we present general formulas for generating the common conference key <inline-formula><tex-math notation=""LaTeX"">$\mathcal {K}$</tex-math><alternatives><mml:math><mml:mi mathvariant=""script"">K</mml:mi></mml:math><inline-graphic xlink:href=""xiang-ieq1-2725953.gif""/></alternatives></inline-formula> for multiple participants. Note that by benefiting from the <inline-formula><tex-math notation=""LaTeX"">$(v,k + 1,1)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""xiang-ieq2-2725953.gif""/></alternatives></inline-formula>-block design, the computational complexity of the proposed protocol linearly increases with the number of participants and the communication complexity is greatly reduced. In addition, the fault tolerance property of our protocol enables the group data sharing in cloud computing to withstand different key attacks, which is similar to Yi's protocol.",CS,AI_ML,0.85,Extracted from log - paper 586
Secure Data Storage and Searching for Industrial IoT by Integrating Fog Computing and Cloud Computing,"With the fast development of industrial Internet of things (IIoT), a large amount of data is being generated continuously by different sources. Storing all the raw data in the IIoT devices locally is unwise considering that the end devices’ energy and storage spaces are strictly limited. In addition, the devices are unreliable and vulnerable to many threats because the networks may be deployed in remote and unattended areas. In this paper, we discuss the emerging challenges in the aspects of data processing, secure data storage, efficient data retrieval and dynamic data collection in IIoT. Then, we design a flexible and economical framework to solve the problems above by integrating the fog computing and cloud computing. Based on the time latency requirements, the collected data are processed and stored by the edge server or the cloud server. Specifically, all the raw data are first preprocessed by the edge server and then the time-sensitive data (e.g., control information) are used and stored locally. The non-time-sensitive data (e.g., monitored data) are transmitted to the cloud server to support data retrieval and mining in the future. A series of experiments and simulation are conducted to evaluate the performance of our scheme. The results illustrate that the proposed framework can greatly improve the efficiency and security of data storage and retrieval in IIoT.",CS,AI_ML,0.85,Extracted from log - paper 587
Comparison of Fog Computing & Cloud Computing,"Fog computing is extending cloud computing by transferring computation on the edge of networks such as mobile collaborative devices or fixed nodes with built-in data storage, computing, and communication devices. Fog gives focal points of enhanced proficiency, better security, organize data transfer capacity sparing and versatility. With a specific end goal to give imperative subtle elements of Fog registering, we propose attributes of this region and separate from cloud computing research. Cloud computing is developing innovation which gives figuring assets to a specific assignment on pay per utilize. Cloud computing gives benefit three unique models and the cloud gives shoddy; midway oversaw assets for dependable registering for performing required errands. This paper gives correlation and attributes both Fog and cloud computing differs by outline, arrangement, administrations and devices for associations and clients. This comparison shows that Fog provides more flexible infrastructure and better service of data processing by consuming low network bandwidth instead of shifting whole data to the cloud.",CS,AI_ML,0.85,Extracted from log - paper 588
Cloud Programming Simplified: A Berkeley View on Serverless Computing,"Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing.",CS,AI_ML,0.85,Extracted from log - paper 589
Task scheduling and resource allocation in cloud computing using a heuristic approach,"Cloud computing is required by modern technology. Task scheduling and resource allocation are important aspects of cloud computing. This paper proposes a heuristic approach that combines the modified analytic hierarchy process (MAHP), bandwidth aware divisible scheduling (BATS) + BAR optimization, longest expected processing time preemption (LEPT), and divide-and-conquer methods to perform task scheduling and resource allocation. In this approach, each task is processed before its actual allocation to cloud resources using a MAHP process. The resources are allocated using the combined BATS + BAR optimization method, which considers the bandwidth and load of the cloud resources as constraints. In addition, the proposed system preempts resource intensive tasks using LEPT preemption. The divide-and-conquer approach improves the proposed system, as is proven experimentally through comparison with the existing BATS and improved differential evolution algorithm (IDEA) frameworks when turnaround time and response time are used as performance metrics.",CS,AI_ML,0.85,Extracted from log - paper 590
JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services,"Deep learning models are being deployed in many mobile intelligent applications. End-side services, such as intelligent personal assistants, autonomous cars, and smart home services often employ either simple local models on the mobile or complex remote models on the cloud. However, recent studies have shown that partitioning the DNN computations between the mobile and cloud can increase the latency and energy efficiencies. In this paper, we propose an efficient, adaptive, and practical engine, JointDNN, for collaborative computation between a mobile device and cloud for DNNs in both inference and training phase. JointDNN not only provides an energy and performance efficient method of querying DNNs for the mobile side but also benefits the cloud server by reducing the amount of its workload and communications compared to the cloud-only approach. Given the DNN architecture, we investigate the efficiency of processing some layers on the mobile device and some layers on the cloud server. We provide optimization formulations at layer granularity for forward- and backward-propagations in DNNs, which can adapt to mobile battery limitations and cloud server load constraints and quality of service. JointDNN achieves up to 18 and 32 times reductions on the latency and mobile energy consumption of querying DNNs compared to the status-quo approaches, respectively.",CS,AI_ML,0.85,Extracted from log - paper 591
Reliability and high availability in cloud computing environments: a reference roadmap,"Reliability and high availability have always been a major concern in distributed systems. Providing highly available and reliable services in cloud computing is essential for maintaining customer confidence and satisfaction and preventing revenue losses. Although various solutions have been proposed for cloud availability and reliability, but there are no comprehensive studies that completely cover all different aspects in the problem. This paper presented a ‘Reference Roadmap’ of reliability and high availability in cloud computing environments. A big picture was proposed which was divided into four steps specifying through four pivotal questions starting with ‘Where?’, ‘Which?’, ‘When?’ and ‘How?’ keywords. The desirable result of having a highly available and reliable cloud system could be gained by answering these questions. Each step of this reference roadmap proposed a specific concern of a special portion of the issue. Two main research gaps were proposed by this reference roadmap.",CS,AI_ML,0.85,Extracted from log - paper 592
Understanding determinants of cloud computing adoption using an integrated TAM-TOE model,"– The purpose of this paper is to integrate TAM model and TOE framework for cloud computing adoption at organizational level. , – A conceptual framework was developed using technological and organizational variables of TOE framework as external variables of TAM model while environmental variables were proposed to have direct impact on cloud computing adoption. A questionnaire was used to collect the data from 280 companies in IT, manufacturing and finance sectors in India. The data were analyzed using exploratory and confirmatory factor analyses. Further, structural equation modeling was used to test the proposed model. , – The study identified relative advantage, compatibility, complexity, organizational readiness, top management commitment, and training and education as important variables for affecting cloud computing adoption using perceived ease of use (PEOU) and perceived usefulness (PU) as mediating variables. Also, competitive pressure and trading partner support were found directly affecting cloud computing adoption intentions. The model explained 62 percent of cloud computing adoption. , – The model can be used as a guideline to ensure a positive outcome of the cloud computing adoption in organizations. It also provides relevant recommendations to achieve conducive implementation environment for cloud computing adoption. , – This study integrates two of the information technology adoption models to improve predictive power of resulting model.",CS,AI_ML,0.85,Extracted from log - paper 593
A Modified Hierarchical Attribute-Based Encryption Access Control Method for Mobile Cloud Computing,"Abstract—Cloud computing is an Internet-based computing pattern through which shared resources are provided to devices on-demand. It is an emerging but promising paradigm to integrating mobile devices into cloud computing, and the integration performs in the cloud based hierarchical multi-user data-shared environment. With integrating into cloud computing, security issues such as data confidentiality and user authority may arise in the mobile cloud computing system, and it is concerned as the main constraints to the developments of mobile cloud computing. In order to provide safe and secure operation, a hierarchical access control method using modified hierarchical attribute-based encryption (M-HABE) and a modified three-layer structure is proposed in this paper. In a specific mobile cloud computing model, enormous data which may be from all kinds of mobile devices, such as smart phones, functioned phones and PDAs and so on can be controlled and monitored by the system, and the data can be sensitive to unauthorized third party and constraint to legal users as well. The novel scheme mainly focuses on the data processing, storing and accessing, which is designed to ensure the users with legal authorities to get corresponding classified data and to restrict illegal users and unauthorized legal users get access to the data, which makes it extremely suitable for the mobile cloud computing paradigms.",CS,AI_ML,0.85,Extracted from log - paper 594
Cloud Computing: History and Overview,"Cloud computing has emerged as a new technology and business paradigm in the last couple of years. Cloud computing platforms provide easy access, scalability, reliability, reconfigurability, and high performance from its resources over the Internet without complex infrastructure management by customers. This article presents, a brief history of cloud computing from 1961 when McCarthy at MIT introduced about cloud computing, the evolution of cloud computing from its predecessors such as Utility computing and Grid computing, and development of cloud computing. We have also presented various directions in cloud computing along with advantages, cloud-centric design, mobile cloud, and security. It covers the characteristics, service models, and deployment models of cloud computing. We have presented the applications and security aspects associated with cloud computing.",CS,AI_ML,0.85,Extracted from log - paper 595
The Security Risks of Cloud Computing,"Cloud computing is rapidly increasing for achieving comfortable computing. Cloud computing has essentially security vulnerability of software and hardware. For achieving secure cloud computing, the vulnerabilities of cloud computing could be analyzed in a various and systematic approach from perspective of the service designer, service operator, the designer of cloud security and certifiers of cloud systems. The paper investigates the vulnerabilities and security controls in terms of secure cryptography. For achieving the research aims, this paper analyzes technological security vulnerability, operational weakness and suggests secure ways for cloud systems.",CS,AI_ML,0.85,Extracted from log - paper 596
Collaborative Cloud and Edge Computing for Latency Minimization,"By performing data processing at the network edge, mobile edge computing can effectively overcome the deficiencies of network congestion and long latency in cloud computing systems. To improve edge cloud efficiency with limited communication and computation capacities, we investigate the collaboration between cloud computing and edge computing, where the tasks of mobile devices can be partially processed at the edge node and at the cloud server. First, a joint communication and computation resource allocation problem is formulated to minimize the weighted-sum latency of all mobile devices. Then, the closed-form optimal task splitting strategy is derived as a function of the normalized backhaul communication capacity and the normalized cloud computation capacity. Some interesting and useful insights for the optimal task splitting strategy are also highlighted by analyzing four special scenarios. Based on this, we further transform the original joint communication and computation resource allocation problem into an equivalent convex optimization problem and obtain the closed-form computation resource allocation strategy by leveraging the convex optimization theory. Moreover, a necessary condition is also developed to judge whether a task should be processed at the corresponding edge node only, without offloading to the cloud server. Finally, simulation results confirm our theoretical analysis and demonstrate that the proposed collaborative cloud and edge computing scheme can evidently achieve a better delay performance than the conventional schemes.",CS,AI_ML,0.85,Extracted from log - paper 597
UAV-Assisted Task Offloading in Vehicular Edge Computing Networks,"Vehicular edge computing (VEC) provides an effective task offloading paradigm by pushing cloud resources to the vehicular network edges, e.g., road side units (RSUs). However, overloaded RSUs are likely to occur especially in urban aggregation areas, possibly leading to greatly compromised offloading performance. Inspired by this, this article explores this situation by introducing an unmanned aerial vehicle (UAV) to address the VEC overload problem. Specifically, we formulate a novel online UAV-assisted vehicular task offloading problem to minimize vehicular task delay under the long-term UAV energy constraint. To solve the formulated problem, we first decouple the long-term energy constraint based on the Lyapunov optimization technique. In this way, the problem can be solved in a real-time manner without requiring future information. Then, we construct a Markov chain based on Markov approximation optimization to find out the close-to-optimal UAV-assisted offloading strategies. Furthermore, we derive a mathematical analysis to rigorously demonstrate the offloading performance of the proposed algorithm. Additionally, the simulation results show that the proposed method outperforms the baselines by significantly reducing the vehicular task delay constrained by the long-term UAV energy budget under various system parameters, such as the energy budget and computation workloads.",CS,AI_ML,0.85,Extracted from log - paper 598
REVIEWING THE TRANSFORMATIONAL IMPACT OF EDGE COMPUTING ON REAL-TIME DATA PROCESSING AND ANALYTICS,"Edge computing has emerged as a pivotal paradigm shift in the realm of data processing and analytics, revolutionizing the way organizations handle real-time data. This review presents a comprehensive review of the transformational impact of edge computing on real-time data processing and analytics. Firstly, the review delves into the fundamental concepts of edge computing, elucidating its architectural framework and highlighting its distinct advantages over traditional cloud-centric approaches. By distributing computational resources closer to data sources, edge computing mitigates latency issues and enhances responsiveness, thereby enabling real-time data processing at the edge. Furthermore, this review explores how edge computing facilitates the seamless integration of analytics capabilities into edge devices, empowering organizations to derive actionable insights at the source of data generation. Leveraging advanced analytics algorithms, such as machine learning and artificial intelligence, edge computing enables autonomous decision-making and predictive analytics in real time, fostering innovation across diverse industry verticals. Moreover, the review examines the transformative implications of edge computing on various sectors, including healthcare, manufacturing, transportation, and smart cities. By enabling localized data processing and analytics, edge computing enhances operational efficiency, ensures data privacy and security, and unlocks new opportunities for business optimization and value creation. This review underscores the profound impact of edge computing on real-time data processing and analytics, revolutionizing the way organizations harness data to drive informed decision-making and gain competitive advantage in today's dynamic business landscape. As edge computing continues to evolve, its transformative potential is poised to redefine the future of data-driven innovation and digital transformation. Keywords: Edge, Computing, Analytics, Data, Impact, Review.",CS,AI_ML,0.85,Extracted from log - paper 599
Mobile Edge Computing: A Survey on Architecture and Computation Offloading,"Technological evolution of mobile user equipment (UEs), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the UEs is constrained by limited battery capacity and energy consumption of the UEs. A suitable solution extending the battery life-time of the UEs is to offload the applications demanding huge processing to a conventional centralized cloud. Nevertheless, this option introduces significant execution delay consisting of delivery of the offloaded applications to the cloud and back plus time of the computation at the cloud. Such a delay is inconvenient and makes the offloading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing (MEC), has been introduced. The MEC brings computation and storage resources to the edge of mobile network enabling it to run the highly demanding applications at the UE while meeting strict delay requirements. The MEC computing resources can be exploited also by operators and third parties for specific purposes. In this paper, we first describe major use cases and reference scenarios where the MEC is applicable. After that we survey existing concepts integrating MEC functionalities to the mobile networks and discuss current advancement in standardization of the MEC. The core of this survey is, then, focused on user-oriented use case in the MEC, i.e., computation offloading. In this regard, we divide the research on computation offloading to three key areas: 1) decision on computation offloading; 2) allocation of computing resource within the MEC; and 3) mobility management. Finally, we highlight lessons learned in area of the MEC and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the MEC.",CS,AI_ML,0.85,Extracted from log - paper 600
Intelligent Delay-Aware Partial Computing Task Offloading for Multiuser Industrial Internet of Things Through Edge Computing,"The development of Industrial Internet of Things (IIoT) and Industry 4.0 has completely changed the traditional manufacturing industry. Intelligent IIoT technology usually involves a large number of intensive computing tasks. Resource-constrained IIoT devices often cannot meet the real-time requirements of these tasks. As a promising paradigm, the mobile-edge computing (MEC) system migrates the computation intensive tasks from resource-constrained IIoT devices to nearby MEC servers, thereby obtaining lower delay and energy consumption. However, considering the varying channel conditions as well as the distinct delay requirements for various computing tasks, it is challenging to coordinate the computing task offloading among multiple users. In this article, we propose an autonomous partial offloading system for delay-sensitive computation tasks in multiuser IIoT MEC systems. Our goal is to provide offloading services with minimum delay for better Quality of Service (QoS). Enlighten by the recent advancement of reinforcement learning (RL), we propose two RL-based offloading strategies to automatically optimize the delay performance. Specifically, we first implement the $Q$ -learning algorithm to provide a discrete partial offloading decision. Then, to further optimize the system performance with more flexible task offloading, the offloading decisions are given as continuous based on deep deterministic policy gradient (DDPG). The simulation results show that the $Q$ -learning scheme reduces the delay by 23%, and the DDPG scheme reduces the delay by 30%.",CS,AI_ML,0.85,Extracted from log - paper 601
Edge Computing on IoT for Machine Signal Processing and Fault Diagnosis: A Review,"Edge computing is an emerging paradigm that offloads the computations and analytics workloads onto the Internet of Things (IoT) edge devices to accelerate the computation efficiency, reduce the channel occupation of signal transmission, and reduce the storage and computation workloads on the cloud servers. These distinct merits make it a promising tool for IoT-based machine signal processing and fault diagnosis. This article reviews the edge computing methods in signal processing-based machine fault diagnosis from the aspects of concepts, state-of-the-art methods, case studies, and research prospects. In particular, the lightweight designed algorithms and application-specific hardware platforms of edge computing in the typical fault diagnosis procedures, including signal acquisition, signal preprocessing, feature extraction, and pattern recognition, are reviewed and discussed in detail. The review provides an insight into the edge computing framework, methods, and applications, so as to meet the requirements of IoT-based machine real-time signal processing, low-latency fault diagnosis, and high-efficient predictive maintenance.",CS,AI_ML,0.85,Extracted from log - paper 602
At the Confluence of Artificial Intelligence and Edge Computing in IoT-Based Applications: A Review and New Perspectives,"Given its advantages in low latency, fast response, context-aware services, mobility, and privacy preservation, edge computing has emerged as the key support for intelligent applications and 5G/6G Internet of things (IoT) networks. This technology extends the cloud by providing intermediate services at the edge of the network and improving the quality of service for latency-sensitive applications. Many AI-based solutions with machine learning, deep learning, and swarm intelligence have exhibited the high potential to perform intelligent cognitive sensing, intelligent network management, big data analytics, and security enhancement for edge-based smart applications. Despite its many benefits, there are still concerns about the required capabilities of intelligent edge computing to deal with the computational complexity of machine learning techniques for big IoT data analytics. Resource constraints of edge computing, distributed computing, efficient orchestration, and synchronization of resources are all factors that require attention for quality of service improvement and cost-effective development of edge-based smart applications. In this context, this paper aims to explore the confluence of AI and edge in many application domains in order to leverage the potential of the existing research around these factors and identify new perspectives. The confluence of edge computing and AI improves the quality of user experience in emergency situations, such as in the Internet of vehicles, where critical inaccuracies or delays can lead to damage and accidents. These are the same factors that most studies have used to evaluate the success of an edge-based application. In this review, we first provide an in-depth analysis of the state of the art of AI in edge-based applications with a focus on eight application areas: smart agriculture, smart environment, smart grid, smart healthcare, smart industry, smart education, smart transportation, and security and privacy. Then, we present a qualitative comparison that emphasizes the main objective of the confluence, the roles and the use of artificial intelligence at the network edge, and the key enabling technologies for edge analytics. Then, open challenges, future research directions, and perspectives are identified and discussed. Finally, some conclusions are drawn.",CS,AI_ML,0.85,Extracted from log - paper 603
Joint Optimization of Computing Offloading and Service Caching in Edge Computing-Based Smart Grid,"With the continuous expansion of the power Internet of Things (IoT) and the rapid increase in the number of Smart Devices (SDs), the data generated by SDs has exponentially increased. The traditional cloud-based smart grid cannot meet the low latency and high reliability requirements of emerging applications. By moving computing, data, and services from the centralized cloud to Edge Servers (ESs), edge computing exhibits excellent performance in communication delay and traffic reduction. Simultaneously, service caching also shows attractive advantages in handling the surge in data traffic. In this paper, we consider the joint optimization of computing offloading and service caching in edge computing-based smart grid, and formulate the problem as a Mixed-Integer Non-Linear Program (MINLP), aiming to minimize the task cost of the system. The original problem is decomposed into an equivalent master problem and sub-problem, and a Collaborative Computing Offloading and Resource Allocation Method (CCORAM) is proposed to solve the optimization problem, which includes two low-complexity algorithms. Specifically, a gradient descent allocation algorithm is first proposed to determine the computing resource allocation strategy, and then a game theory-based algorithm is proposed to determine the computing strategy. Simulation results show that CCORAM with low time complexity is very close to the optimal method, and performs much better than other benchmark methods.",CS,AI_ML,0.85,Extracted from log - paper 604
Task Partitioning and Offloading in DNN-Task Enabled Mobile Edge Computing Networks,"Deep neural network (DNN)-task enabled mobile edge computing (MEC) is gaining ubiquity due to outstanding performance of artificial intelligence. By virtue of characteristics of DNN, this paper develops a joint design of task partitioning and offloading for a DNN-task enabled MEC network that consists of a single server and multiple mobile devices (MDs), where the server and each MD employ the well-trained DNNs for task computation. The main contributions of this paper are as follows: First, we propose a layer-level computation partitioning strategy for DNN to partition each MD's task into the subtasks that are either locally computed at the MD or offloaded to the server. Second, we develop a delay prediction model for DNN to characterize the computation delay of each subtask at the MD and the server. Third, we design a slot model and a dynamic pricing strategy for the server to efficiently schedule the offloaded subtasks. Fourth, we jointly optimize the design of task partitioning and offloading to minimize each MD's cost that includes the computation delay, the energy consumption, and the price paid to the server. In particular, we propose two distributed algorithms based on the aggregative game theory to solve the optimization problem. Finally, numerical results demonstrate that the proposed scheme is scalable to different types of DNNs and shows the superiority over the baseline schemes in terms of processing delay and energy consumption.",CS,AI_ML,0.85,Extracted from log - paper 605
Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing,"With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI.",CS,AI_ML,0.85,Extracted from log - paper 606
AI-Enabled Secure Microservices in Edge Computing: Opportunities and Challenges,"The paradigm of edge computing has formed an innovative scope within the domain of the Internet of Things (IoT) through expanding the services of the cloud to the network edge to design distributed architectures and securely enhance decision-making applications. Due to the heterogeneous, distributed and resource-constrained essence of edge Computing, edge applications are required to be developed as a set of lightweight and interdependent modules. As this concept aligns with the objectives of microservice architecture, effective implementation of microservices-based edge applications within IoT networks has the prospective of fully leveraging edge nodes capabilities. Deploying microservices at IoT edge faces plenty of challenges associated with security and privacy. Advances in Artificial Intelligence (AI) (especially Machine Learning), and the easy access to resources with powerful computing providing opportunities for deriving precise models and developing different intelligent applications at the edge of network. In this study, an extensive survey is presented for securing edge computing-based AI Microservices to elucidate the challenges of IoT management and enable secure decision-making systems at the edge. We present recent research studies on edge AI and microservices orchestration and highlight key requirements as well as challenges of securing Microservices at IoT edge. We also propose a Microservices-based edge computing framework that provides secure edge AI algorithms as Microservices utilizing the containerization technology to offer automated and secure AI-based applications at the network edge.",CS,AI_ML,0.85,Extracted from log - paper 607
Edge Computing Driven Low-Light Image Dynamic Enhancement for Object Detection,"With fast increase in volume of mobile multimedia data, how to apply powerful deep learning methods to process data with real-time response becomes a major issue. Meanwhile, edge computing structure helps improve response time and user experience by bringing flexible computation and storage capabilities. Considering both technologies for successful AI-based applications, we propose an edge-computing driven and end-to-end framework to perform tasks of image enhancement and object detection under low-light conditions. The framework consists of a cloud-based enhancement and an edge-based detection stage. In the first stage, we establish connections between edge devices and cloud servers to input re-scaled illumination parts of low-light images, where enhancement subnetworks are dynamically and parallel coupled to compute enhanced illumination parts based on low-light context. During the edge-based detection stage, edge devices could accurately and rapidly detect objects based on cloud-computed informative feature map. Experimental results show the proposed method significantly improves detection performance in low-light conditions with low latency running on edge devices.",CS,AI_ML,0.85,Extracted from log - paper 608
Joint Task Offloading and Resource Allocation for Energy-Constrained Mobile Edge Computing,"We consider the problem of task offloading and resource allocation in mobile edge computing (MEC). To maintain satisfactory quality of experience (QoE) of end-users, mobile devices (MDs) may offload their tasks to edge servers based on the allocated computation (e.g., CPU/GPU cycles and storage) and wireless resources (e.g., bandwidth). However, these resources could not be effectively utilized unless an encouraging resource allocation scheme can be proposed. What’s worse, task offloading incurs additional MEC energy consumption, which inevitably violate the long-term MEC energy budget. Considering these two challenges, we propose an online joint offloading and resource allocation (JORA) framework under the long-term MEC energy constraint, aiming at guaranteeing the end-users’ QoE. To achieve this, we leverage Lyapunov optimization to exploit the optimality of the long-term QoE maximization problem. By constructing an energy deficit queue to guide energy consumption, the problem can be solved in a real-time manner. On this basis, we propose online JORA methods in both centralized and distributed manners. Furthermore, we prove that our proposed methods enable the achievement of the close-to-optimal performance while satisfying the long-term MEC energy constraint. In addition, we conduct extensive simulations and the results show superiority in performance over other methods.",CS,AI_ML,0.85,Extracted from log - paper 609
Accelerating Decentralized Federated Learning in Heterogeneous Edge Computing,"In edge computing (EC), federated learning (FL) enables massive devices to collaboratively train AI models without exposing local data. In order to avoid the possible bottleneck of the parameter server (PS) architecture, we concentrate on the decentralized federated learning (DFL), which adopts peer-to-peer (P2P) communication without maintaining a global model. However, due to the intrinsic features of EC, e.g., resource limitation and heterogeneity, network dynamics and non-IID data, DFL with a fixed P2P topology and/or an identical model compression ratio for all workers results in a slow convergence rate. In this paper, we propose an efficient algorithm (termed <italic>CoCo</italic>) to accelerate DFL by integrating optimization of topology <bold>Co</bold>nstruction and model <bold>Co</bold>mpression. Concretely, we adaptively construct P2P topology and determine specific compression ratios for each worker to conquer the system dynamics and heterogeneity under bandwidth constraints. To reflect how the non-IID data influence the consistency of local models in DFL, we introduce the <italic>consensus distance</italic>, i.e., the discrepancy between local models, as the quantitative metric to guide the fine-grained operations of the joint optimization. Extensive simulations and testbed experiments show that <italic>CoCo</italic> achieves 10× speedup, and reduces the communication cost by about <inline-formula><tex-math notation=""LaTeX"">$50\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>50</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""wang-ieq1-3178378.gif""/></alternatives></inline-formula> on average, compared with the existing DFL baselines.",CS,AI_ML,0.85,Extracted from log - paper 610
Wireless Powered Mobile Edge Computing Networks: A Survey,"Wireless Powered Mobile Edge Computing (WPMEC) is an integration of Mobile Edge Computing (MEC) and Wireless Power Transfer (WPT) technologies, to both improve computing capabilities of mobile devices and energy compensation for their limited battery capabilities. Generally, energy transmitters, mobile devices, and edge servers form a WPMEC system that realizes a closed loop of sending and collecting energy as well as offloading and receiving task data. Due to constraints of time-varying network environments, time-coupled battery levels, and the half-duplex character of mobile devices, the joint design of computation offloading and resource allocation solutions in WPMEC systems has become extremely challenging, and a great number of studies have been devoted to it in recent years. In this article, we first introduce the basic model of the WPMEC system. Then, we present key issues and techniques related to WPMEC. In addition, we summarize solutions for computation offloading and resource allocation to solve critical issues in WPMEC networks. Finally, we discuss some research challenges and open issues.",CS,AI_ML,0.85,Extracted from log - paper 611
Schema Design with Intelligent Multi Modelling Edge Computing Techniques for Industrial Applications,"The Internet has become one of the most essential sources of information and knowledge for consumers. It is important to figure out how to get users' specific needs from the large number of network document resources in a precise and efficient way. Sharing information on activities that are of interest to all members of a group may be done via group suggestion. Too much data and resources are no longer a concern with this system. There has been a comprehensive testing of how big data can be used in the packaging industry. Edge computing with knowledge graphs and LSTMs shows that the proposed system provides more accurate and better suggestions for network document resources. Consequently, it is capable of meeting the individual resource requirements of each user in a more effective manner.",CS,AI_ML,0.85,Extracted from log - paper 612
Reverse Auction-Based Computation Offloading and Resource Allocation in Mobile Cloud-Edge Computing,"This article proposes a novel Reverse Auction-based Computation Offloading and Resource Allocation Mechanism, named RACORAM for the mobile Cloud-Edge computing. The basic idea is that the Cloud Service Center (CSC) recruits edge server owners to replace it to accommodate offloaded computation from nearby resource-constraint Mobile Devices (MDs). In RACORAM, the reverse auction is used to stimulate edge server owners to participate in the offloading process, and the reverse auction-based computation offloading and resource allocation problem is formulated as a Mixed Integer Nonlinear Programming (MINLP) problem, aiming to minimize the cost of the CSC. The original problem is decomposed into an equivalent master problem and subproblem, and low-complexity algorithms are proposed to solve the related optimization problems. Specifically, a Constrained Gradient Descent Allocation Method (CGDAM) is first proposed to determine the computation resource allocation strategy, and then a Greedy Randomized Adaptive Search Procedure based Winning Bid Scheduling Method (GWBSM) is proposed to determine the computation offloading strategy. Meanwhile, the CSC's payment determination for the winning edge server owners is also presented. Simulations are conducted to evaluate the performance of RACORAM, and the results show that RACORAM is very close to the optimal method with significantly reduced computational complexity, and greatly outperforms the other baseline methods in terms of the CSC's cost under different scenarios.",CS,AI_ML,0.85,Extracted from log - paper 613
Aerial Edge Computing on Orbit: A Task Offloading and Allocation Scheme,"As the communication mode with the greatest attention and global network coverage, the Low Earth Orbit (LEO) satellite network has the characteristics of low propagation delay, low link loss and handheld terminal. However, with the increasing requirements of user terminals for network latency and bandwidth, the traditional central cloud computing mode no longer has advantages. Therefore, borrowing from the idea of edge computing in terrestrial networks, Orbital Edge Computing (OEC) technology deploys Multi-access Edge Computing (MEC) servers on LEO satellite constellations to meet the growing demand for real-time and reliability of various applications. Based on the above motivations, this paper proposes an OEC Task Allocation (OEC-TA) algorithm based on the greedy strategy in LEO satellite networks for the Walker Delta satellite constellation, which fully utilizes satellite computing resources to provide services to ground users. Then, we analyze the performance of our proposed algorithm in terms of computational cost. Finally, experimental results show that OEC-TA is better than DEC (Double Edge Computing) and random allocation model on average delay and energy consumption reduction. Compared with DEC, our OEC-TA can reduce the average delay and energy consumption up to 10% and 16.5%, respectively.",CS,AI_ML,0.85,Extracted from log - paper 614
Privacy-Preserving Federated Learning for Industrial Edge Computing via Hybrid Differential Privacy and Adaptive Compression,"With the continuous improvement of hardware computing power, edge computing of industrial data has been gradually applied. In the past decade, the promotion of edge computing has also greatly improved the efficiency of industrial production. Compared with the conventional cloud computing, it not only saves the bandwidth consumption of data transmission, but also ensures the terminal data security to a certain extent. However, the continuous update of attack types also put forward new requirements for the privacy protection of industrial edge computing. So it should fundamentally solve the risk of industrial data leakage in the process of deep model training in edge terminal. In this article, we propose a new federated edge learning framework based on hybrid differential privacy and adaptive compression for industrial data processing. Specifically, it first completes the adaptive gradient compression preparation, then constructs the industrial federated learning model, and finally makes use of adaptive differential privacy model to optimize, so as to complete the privacy protection towards the transmission of gradient parameters in industrial environment. By optimizing the hybrid differential privacy and adaptive compression, we can better prevent the terminal data privacy against inference attacks. The experimental results show that this method is very effective in the industrial edge computing situation, and it also opens up a new direction for the effect of differential privacy in federated learning.",CS,AI_ML,0.85,Extracted from log - paper 615
"Combining Federated Learning and Edge Computing Toward Ubiquitous Intelligence in 6G Network: Challenges, Recent Advances, and Future Directions","Full leverage of the huge volume of data generated on a large number of user devices for providing intelligent services in the 6G network calls for Ubiquitous Intelligence (UI). A key to developing UI lies in the involvement of the large number of network devices, which contribute their data to collaborative Machine Learning (ML) and provide their computational resources to support the learning process. Federated Learning (FL) is a new ML method that enables data owners to collaborate in model training without exposing private data, which allows user devices to contribute their data to developing UI. Edge computing deploys cloud-like capabilities at the network edge, which enables network devices to offer their computational resources for supporting FL. Therefore, a combination of FL and edge computing may greatly facilitate the development of ubiquitous intelligence in the 6G network. In this article, we present a comprehensive survey of the recent developments in technologies for combining FL and edge computing with a holistic vision across the fields of FL and edge computing. We conduct our survey from both the perspective of an FL framework deployed in an edge computing environment (FL in Edge) and the perspective of an edge computing system providing a platform for FL (Edge for FL). From the FL in Edge perspective, we first identify the main challenges to FL in edge computing and then survey the representative technical strategies for addressing the challenges. From the Edge for FL perspective, we first analyze the key requirements for edge computing to support FL and then review the recent advances in edge computing technologies that may be exploited to meet the requirements. Then we discuss open problems and identify some possible directions for future research on combining FL and edge computing, with the hope of arousing the research community’s interest in this emerging and exciting interdisciplinary field.",CS,AI_ML,0.85,Extracted from log - paper 616
Topology-aware Federated Learning in Edge Computing: A Comprehensive Survey,"The ultra-low latency requirements of 5G/6G applications and privacy constraints call for distributed machine learning systems to be deployed at the edge. With its simple yet effective approach, federated learning (FL) is a natural solution for massive user-owned devices in edge computing with distributed and private training data. FL methods based on FedAvg typically follow a naive star topology, ignoring the heterogeneity and hierarchy of the volatile edge computing architectures and topologies in reality. Several other network topologies exist and can address the limitations and bottlenecks of the star topology. This motivates us to survey network topology-related FL solutions. In this paper, we conduct a comprehensive survey of the existing FL works focusing on network topologies. After a brief overview of FL and edge computing networks, we discuss various edge network topologies as well as their advantages and disadvantages. Lastly, we discuss the remaining challenges and future works for applying FL to topology-specific edge networks.",CS,AI_ML,0.85,Extracted from log - paper 617
"Resource Scheduling in Edge Computing: Architecture, Taxonomy, Open Issues and Future Research Directions","An inflection point in the computing industry is occurring with the implementation of the Internet of Things and 5G communications, which has pushed centralized cloud computing toward edge computing resulting in a paradigm shift in computing. The purpose of edge computing is to provide computing, network control, and storage to the network edge to accommodate computationally intense and latency-critical applications at resource-limited endpoints. Edge computing allows edge devices to offload their overflowing computing tasks to edge servers. This procedure may completely exploit the edge server’s computational and storage capabilities and efficiently execute computing operations. However, transferring all the overflowing computing tasks to an edge server leads to long processing delays and surprisingly high energy consumption for numerous computing tasks. Aside from this, unused edge devices and powerful cloud centers may lead to resource waste. Thus, hiring a collaborative scheduling approach based on task properties, optimization targets, and system status with edge servers, cloud centers, and edge devices is critical for the successful operation of edge computing. This paper briefly summarizes the edge computing architecture for information and task processing. Meanwhile, the collaborative scheduling scenarios are examined. Resource scheduling techniques are then discussed and compared based on four collaboration modes. As part of our survey, we present a thorough overview of the various task offloading schemes proposed by researchers for edge computing. Additionally, according to the literature surveyed, we briefly looked at the fairness and load balancing indicators in scheduling. Finally, edge computing resource scheduling issues, challenges, and future directions have discussed.",CS,AI_ML,0.85,Extracted from log - paper 618
Adaptive Federated Learning in Resource Constrained Edge Computing Systems,"Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.",CS,AI_ML,0.85,Extracted from log - paper 619
Serverless Edge Computing—Where We Are and What Lies Ahead,"The edge–cloud continuum combines heterogeneous resources, which are complex to manage. Serverless edge computing is a suitable candidate to manage the continuum by abstracting away the underlying infrastructure, improving developers’ experiences, and optimizing overall resource utilization. However, understanding and overcoming programming support, reliability, and performance engineering challenges are essential for the success of serverless edge computing. In this article, we review and evaluate the maturity of serverless approaches for the edge–cloud continuum. Our review includes commercial, community-driven offerings and approaches from academia. We identify several maturity levels of serverless edge computing and use them as criteria to evaluate the maturity of current state-of-the-art serverless approaches with a special focus on the programming, reliability, and performance challenges. Finally, we lay a road map toward the next generation of serverless edge computing systems.",CS,AI_ML,0.85,Extracted from log - paper 620
Edge Computing with Artificial Intelligence: A Machine Learning Perspective,"Recent years have witnessed the widespread popularity of Internet of things (IoT). By providing sufficient data for model training and inference, IoT has promoted the development of artificial intelligence (AI) to a great extent. Under this background and trend, the traditional cloud computing model may nevertheless encounter many problems in independently tackling the massive data generated by IoT and meeting corresponding practical needs. In response, a new computing model called edge computing (EC) has drawn extensive attention from both industry and academia. With the continuous deepening of the research on EC, however, scholars have found that traditional (non-AI) methods have their limitations in enhancing the performance of EC. Seeing the successful application of AI in various fields, EC researchers start to set their sights on AI, especially from a perspective of machine learning, a branch of AI that has gained increased popularity in the past decades. In this article, we first explain the formal definition of EC and the reasons why EC has become a favorable computing model. Then, we discuss the problems of interest in EC. We summarize the traditional solutions and hightlight their limitations. By explaining the research results of using AI to optimize EC and applying AI to other fields under the EC architecture, this article can serve as a guide to explore new research ideas in these two aspects while enjoying the mutually beneficial relationship between AI and EC.",CS,AI_ML,0.85,Extracted from log - paper 621
"Edge Learning for B5G Networks With Distributed Signal Processing: Semantic Communication, Edge Computing, and Wireless Sensing","To process and transfer large amounts of data in emerging wireless services, it has become increasingly appealing to exploit distributed data communication and learning. Specifically, edge learning (EL) enables local model training on geographically disperse edge nodes and minimizes the need for frequent data exchange. However, the current design of separating EL deployment and communication optimization does not yet reap the promised benefits of distributed signal processing, and sometimes suffers from excessive signalling overhead, long processing delay, and unstable learning convergence. In this paper, we provide an overview on practical distributed EL techniques and their interplay with advanced communication optimization designs. In particular, typical performance metrics for dual-functional learning and communication networks are discussed. Also, recent achievements of enabling techniques for the dual-functional design are surveyed with exemplifications from the mutual perspectives of “communications for learning” and “learning for communications.” The application of EL techniques within a variety of future communication systems are also envisioned for beyond 5G (B5G) wireless networks. For the application in goal-oriented semantic communication, we present a first mathematical model of the goal-oriented source entropy as an optimization problem. In addition, from the viewpoint of information theory, we identify fundamental open problems of characterizing rate regions for communication networks supporting distributed learning-and-computing tasks. We also present technical challenges as well as emerging application opportunities in this field, with the aim of inspiring future research and promoting widespread developments of EL in B5G.",CS,AI_ML,0.85,Extracted from log - paper 622
A Survey on the Convergence of Edge Computing and AI for UAVs: Opportunities and Challenges,"The latest 5G mobile networks have enabled many exciting Internet of Things (IoT) applications that employ unmanned aerial vehicles (UAVs/drones). The success of most UAV-based IoT applications is heavily dependent on artificial intelligence (AI) technologies, for instance, computer vision and path planning. These AI methods must process data and provide decisions while ensuring low latency and low energy consumption. However, the existing cloud-based AI paradigm finds it difficult to meet these strict UAV requirements. Edge AI, which runs AI on-device or on edge servers close to users, can be suitable for improving UAV-based IoT services. This article provides a comprehensive analysis of the impact of edge AI on key UAV technical aspects (i.e., autonomous navigation, formation control, power management, security and privacy, computer vision, and communication) and applications (i.e., delivery systems, civil infrastructure inspection, precision agriculture, search and rescue (SAR) operations, acting as aerial wireless base stations (BSs), and drone light shows). As guidance for researchers and practitioners, this article also explores UAV-based edge AI implementation challenges, lessons learned, and future research directions.",CS,AI_ML,0.85,Extracted from log - paper 623
Federated Learning in Edge Computing: A Systematic Survey,"Edge Computing (EC) is a new architecture that extends Cloud Computing (CC) services closer to data sources. EC combined with Deep Learning (DL) is a promising technology and is widely used in several applications. However, in conventional DL architectures with EC enabled, data producers must frequently send and share data with third parties, edge or cloud servers, to train their models. This architecture is often impractical due to the high bandwidth requirements, legalization, and privacy vulnerabilities. The Federated Learning (FL) concept has recently emerged as a promising solution for mitigating the problems of unwanted bandwidth loss, data privacy, and legalization. FL can co-train models across distributed clients, such as mobile phones, automobiles, hospitals, and more, through a centralized server, while maintaining data localization. FL can therefore be viewed as a stimulating factor in the EC paradigm as it enables collaborative learning and model optimization. Although the existing surveys have taken into account applications of FL in EC environments, there has not been any systematic survey discussing FL implementation and challenges in the EC paradigm. This paper aims to provide a systematic survey of the literature on the implementation of FL in EC environments with a taxonomy to identify advanced solutions and other open problems. In this survey, we review the fundamentals of EC and FL, then we review the existing related works in FL in EC. Furthermore, we describe the protocols, architecture, framework, and hardware requirements for FL implementation in the EC environment. Moreover, we discuss the applications, challenges, and related existing solutions in the edge FL. Finally, we detail two relevant case studies of applying FL in EC, and we identify open issues and potential directions for future research. We believe this survey will help researchers better understand the connection between FL and EC enabling technologies and concepts.",CS,AI_ML,0.85,Extracted from log - paper 624
Federated Learning Meets Blockchain in Edge Computing: Opportunities and Challenges,"Mobile-edge computing (MEC) has been envisioned as a promising paradigm to handle the massive volume of data generated from ubiquitous mobile devices for enabling intelligent services with the help of artificial intelligence (AI). Traditionally, AI techniques often require centralized data collection and training in a single entity, e.g., an MEC server, which is now becoming a weak point due to data privacy concerns and high overhead of raw data communications. In this context, federated learning (FL) has been proposed to provide collaborative data training solutions, by coordinating multiple mobile devices to train a shared AI model without directly exposing their underlying data, which enjoys considerable privacy enhancement. To improve the security and scalability of FL implementation, blockchain as a ledger technology is attractive for realizing decentralized FL training without the need for any central server. Particularly, the integration of FL and blockchain leads to a new paradigm, called FLchain, which potentially transforms intelligent MEC networks into decentralized, secure, and privacy-enhancing systems. This article presents an overview of the fundamental concepts and explores the opportunities of FLchain in MEC networks. We identify several main issues in FLchain design, including communication cost, resource allocation, incentive mechanism, security and privacy protection. The key solutions and the lessons learned along with the outlooks are also discussed. Then, we investigate the applications of FLchain in popular MEC domains, such as edge data sharing, edge content caching and edge crowdsensing. Finally, important research challenges and future directions are also highlighted.",CS,AI_ML,0.85,Extracted from log - paper 625
"A Survey on Mobile Augmented Reality With 5G Mobile Edge Computing: Architectures, Applications, and Technical Aspects","The Augmented Reality (AR) technology enhances the human perception of the world by combining the real environment with the virtual space. With the explosive growth of powerful, less expensive mobile devices, and the emergence of sophisticated communication infrastructure, Mobile Augmented Reality (MAR) applications are gaining increased popularity. MAR allows users to run AR applications on mobile devices with greater mobility and at a lower cost. The emerging 5G communication technologies act as critical enablers for future MAR applications to achieve ultra-low latency and extremely high data rates while Multi-access Edge Computing (MEC) brings enhanced computational power closer to the users to complement MAR. This paper extensively discusses the landscape of MAR through the past and its future prospects with respect to the 5G systems and complementary technology MEC. The paper especially provides an informative analysis of the network formation of current and future MAR systems in terms of cloud, edge, localized, and hybrid architectural options. The paper discusses key application areas for MAR and their future with the advent of 5G technologies. The paper also discusses the requirements and limitations of MAR technical aspects such as communication, mobility management, energy management, service offloading and migration, security, and privacy and analyzes the role of 5G technologies.",CS,AI_ML,0.85,Extracted from log - paper 626
An Overview on Edge Computing Research,"With the rapid development of the Internet of Everything (IoE), the number of smart devices connected to the Internet is increasing, resulting in large-scale data, which has caused problems such as bandwidth load, slow response speed, poor security, and poor privacy in traditional cloud computing models. Traditional cloud computing is no longer sufficient to support the diverse needs of today’s intelligent society for data processing, so edge computing technologies have emerged. It is a new computing paradigm for performing calculations at the edge of the network. Unlike cloud computing, it emphasizes closer to the user and closer to the source of the data. At the edge of the network, it is lightweight for local, small-scale data storage and processing. This article mainly reviews the related research and results of edge computing. First, it summarizes the concept of edge computing and compares it with cloud computing. Then summarize the architecture of edge computing, keyword technology, security and privacy protection, and finally summarize the applications of edge computing.",CS,AI_ML,0.85,Extracted from log - paper 627
Resource Scheduling in Edge Computing: A Survey,"With the proliferation of the Internet of Things (IoT) and the wide penetration of wireless networks, the surging demand for data communications and computing calls for the emerging edge computing paradigm. By moving the services and functions located in the cloud to the proximity of users, edge computing can provide powerful communication, storage, networking, and communication capacity. The resource scheduling in edge computing, which is the key to the success of edge computing systems, has attracted increasing research interests. In this paper, we survey the state-of-the-art research findings to know the research progress in this field. Specifically, we present the architecture of edge computing, under which different collaborative manners for resource scheduling are discussed. Particularly, we introduce a unified model before summarizing the current works on resource scheduling from three research issues, including computation offloading, resource allocation, and resource provisioning. Based on two modes of operation, i.e., centralized and distributed modes, different techniques for resource scheduling are discussed and compared. Also, we summarize the main performance indicators based on the surveyed literature. To shed light on the significance of resource scheduling in real-world scenarios, we discuss several typical application scenarios involved in the research of resource scheduling in edge computing. Finally, we highlight some open research challenges yet to be addressed and outline several open issues as the future research direction.",CS,AI_ML,0.85,Extracted from log - paper 628
Imitation Learning Enabled Task Scheduling for Online Vehicular Edge Computing,"Vehicular edge computing (VEC) is a promising paradigm based on the Internet of vehicles to provide computing resources for end users and relieve heavy traffic burden for cellular networks. In this paper, we consider a VEC network with dynamic topologies, unstable connections and unpredictable movements. Vehicles inside can offload computation tasks to available neighboring VEC clusters formed by onboard resources, with the purpose of both minimizing system energy consumption and satisfying task latency constraints. For online task scheduling, existing researches either design heuristic algorithms or leverage machine learning, e.g., deep reinforcement learning (DRL). However, these algorithms are not efficient enough because of their low searching efficiency and slow convergence speeds for large-scale networks. Instead, we propose an imitation learning enabled online task scheduling algorithm with near-optimal performance from the initial stage. Specially, an expert can obtain the optimal scheduling policy by solving the formulated optimization problem with a few samples offline. For online learning, we train agent policies by following the expert’s demonstration with an acceptable performance gap in theory. Performance results show that our solution has a significant advantage with more than 50 percent improvement compared with the benchmark.",CS,AI_ML,0.85,Extracted from log - paper 629
Service Offloading With Deep Q-Network for Digital Twinning-Empowered Internet of Vehicles in Edge Computing,"With the potential of implementing computing-intensive applications, edge computing is combined with digital twinning (DT)-empowered Internet of vehicles (IoV) to enhance intelligent transportation capabilities. By updating digital twins of vehicles and offloading services to edge computing devices (ECDs), the insufficiency in vehicles’ computational resources can be complemented. However, owing to the computational intensity of DT-empowered IoV, ECD would overload under excessive service requests, which deteriorates the quality of service (QoS). To address this problem, in this article, a multiuser offloading system is analyzed, where the QoS is reflected through the response time of services. Then, a service offloading (SOL) method with deep reinforcement learning, is proposed for DT-empowered IoV in edge computing. To obtain optimized offloading decisions, SOL leverages deep Q-network (DQN), which combines the value function approximation of deep learning and reinforcement learning. Eventually, experiments with comparative methods indicate that SOL is effective and adaptable in diverse environments.",CS,AI_ML,0.85,Extracted from log - paper 630
Multi-Agent Deep Reinforcement Learning for Task Offloading in UAV-Assisted Mobile Edge Computing,"Mobile edge computing can effectively reduce service latency and improve service quality by offloading computation-intensive tasks to the edges of wireless networks. Due to the characteristic of flexible deployment, wide coverage and reliable wireless communication, unmanned aerial vehicles (UAVs) have been employed as assisted edge clouds (ECs) for large-scale sparely-distributed user equipment. Considering the limited computation and energy capacities of UAVs, a collaborative mobile edge computing system with multiple UAVs and multiple ECs is investigated in this paper. The task offloading issue is addressed to minimize the sum of execution delays and energy consumptions by jointly designing the trajectories, computation task allocation, and communication resource management of UAVs. Moreover, to solve the above non-convex optimization problem, a Markov decision process is formulated for the multi-UAV assisted mobile edge computing system. To obtain the joint strategy of trajectory design, task allocation, and power management, a cooperative multi-agent deep reinforcement learning framework is investigated. Considering the high-dimensional continuous action space, the twin delayed deep deterministic policy gradient algorithm is exploited. The evaluation results demonstrate that our multi-UAV multi-EC task offloading method can achieve better performance compared with the other optimization approaches.",CS,AI_ML,0.85,Extracted from log - paper 631
"Edge Computing in Industrial Internet of Things: Architecture, Advances and Challenges","The Industrial Internet of Things (IIoT) is a crucial research field spawned by the Internet of Things (IoT). IIoT links all types of industrial equipment through the network; establishes data acquisition, exchange, and analysis systems; and optimizes processes and services, so as to reduce cost and enhance productivity. The introduction of edge computing in IIoT can significantly reduce the decision-making latency, save bandwidth resources, and to some extent, protect privacy. This paper outlines the research progress concerning edge computing in IIoT. First, the concepts of IIoT and edge computing are discussed, and subsequently, the research progress of edge computing is discussed and summarized in detail. Next, the future architecture from the perspective of edge computing in IIoT is proposed, and its technical progress in routing, task scheduling, data storage and analytics, security, and standardization is analyzed. Furthermore, we discuss the opportunities and challenges of edge computing in IIoT in terms of 5G-based edge communication, load balancing and data offloading, edge intelligence, as well as data sharing security. Finally, we introduce some typical application scenarios of edge computing in IIoT, such as prognostics and health management (PHM), smart grids, manufacturing coordination, intelligent connected vehicles (ICV), and smart logistics.",CS,AI_ML,0.85,Extracted from log - paper 632
Edge Computing for Internet of Everything: A Survey,"In this era of the Internet of Everything (IoE), edge computing has emerged as the critical enabling technology to solve a series of issues caused by an increasing amount of interconnected devices and large-scale data transmission. However, the deficiencies of edge computing paradigm are gradually being magnified in the context of IoE, especially in terms of service migration, security and privacy preservation, and deployment issues of edge node. These issues can not be well addressed by conventional approaches. Thanks to the rapid development of upcoming technologies, such as artificial intelligence (AI), blockchain, and microservices, novel and more effective solutions have emerged and been applied to solve existing challenges. In addition, edge computing can be deeply integrated with technologies in other domains (e.g., AI, blockchain, 6G, and digital twin) through interdisciplinary intersection and practice, releasing the potential for mutual benefit. These promising integrations need to be further explored and researched. In addition, edge computing provides strong support in applications scenarios, such as remote working, new physical retail industries, and digital advertising, which has greatly changed the way we live, work, and study. In this article, we present an up-to-date survey of the edge computing research. In addition to introducing the definition, model, and characteristics of edge computing, we discuss a set of key issues in edge computing and novel solutions supported by emerging technologies in IoE era. Furthermore, we explore the potential and promising trends from the perspective of technology integration. Finally, new application scenarios and the final form of edge computing are discussed.",CS,AI_ML,0.85,Extracted from log - paper 633
Edge-computing-driven Internet of Things: A Survey,"The Internet of Things (IoT) is impacting the world’s connectivity landscape. More and more IoT devices are connected, bringing many benefits to our daily lives. However, the influx of IoT devices poses non-trivial challenges for the existing cloud-based computing paradigm. In the cloud-based architecture, a large amount of IoT data is transferred to the cloud for data management, analysis, and decision making. It could not only cause a heavy workload on the cloud but also result in unacceptable network latency, ultimately undermining the benefits of cloud-based computing. To address these challenges, researchers are looking for new computing models for the IoT. Edge computing, a new decentralized computing model, is valued by more and more researchers in academia and industry. The main idea of edge computing is placing data processing in near-edge devices instead of remote cloud servers. It is promising to build more scalable, low-latency IoT systems. Many studies have been proposed on edge computing and IoT, but a comprehensive survey of this crossover area is still lacking. In this survey, we first introduce the impact of edge computing on the development of IoT and point out why edge computing is more suitable for IoT than other computing paradigms. Then, we analyze the necessity of systematical investigation on the edge-computing-driven IoT (ECDriven-IoT) and summarize new challenges occurring in ECDriven-IoT. We categorize recent advances from bottom to top, covering six aspects of ECDriven-IoT. Finally, we conclude lessons learned and propose some challenging",CS,AI_ML,0.85,Extracted from log - paper 634
Energy-Efficient UAV-Assisted Mobile Edge Computing: Resource Allocation and Trajectory Optimization,"In this paper, we study unmanned aerial vehicle (UAV) assisted mobile edge computing (MEC) with the objective to optimize computation offloading with minimum UAV energy consumption. In the considered scenario, a UAV plays the role of an aerial cloudlet to collect and process the computation tasks offloaded by ground users. Given the service requirements of users, we aim to maximize UAV energy efficiency by jointly optimizing the UAV trajectory, the user transmit power, and computation load allocation. The resulting optimization problem corresponds to nonconvex fractional programming, and the Dinkelbach algorithm and the successive convex approximation (SCA) technique are adopted to solve it. Furthermore, we decompose the problem into multiple subproblems for distributed and parallel problem solving. To cope with the case when the knowledge of user mobility is limited, we adopt a spatial distribution estimation technique to predict the location of ground users so that the proposed approach can still be applied. Simulation results demonstrate the effectiveness of the proposed approach for maximizing the energy efficiency of UAV.",CS,AI_ML,0.85,Extracted from log - paper 635
TCDA: Truthful Combinatorial Double Auctions for Mobile Edge Computing in Industrial Internet of Things,"Mobile edge computing (MEC) emerges as an appealing paradigm to provide time-sensitive computing services for industrial Internet of Things (IIoT) applications. How to guarantee truthfulness and budget-balance under locality constraints is an important issue to the allocation and pricing design of the MEC system. In this paper, we propose a truthful combinatorial double auction mechanism, which integrates the padding concept and the efficient pricing strategy to guarantee desirable properties in constrained MEC environments. This mechanism takes into account the locality characteristics of the MEC systems, where mobile devices (MDs) only offload tasks to edge servers (ESs) in the proximity with various requirements, and ESs only serve their neighboring MDs with limited resources. To be specific, for allocation, a linear programming (LP)-based padding method is used to obtain the near-optimal solution in the polynomial time. For pricing, a critical-value-based pricing strategy and a VCG-based pricing strategy are designed for MDs and ESs to achieve truthfulness and budget-balance. Our theoretical analysis confirms that TCDA is able to hold a set of desirable economic properties, including truthfulness, individual rationality, and budget-balance. Furthermore, simulation results validate the theoretical analysis, and verify the effectiveness and efficiency of TCDA.",CS,AI_ML,0.85,Extracted from log - paper 636
Toward Edge Intelligence: Multiaccess Edge Computing for 5G and Internet of Things,"To satisfy the increasing demand of mobile data traffic and meet the stringent requirements of the emerging Internet-of-Things (IoT) applications such as smart city, healthcare, and augmented/virtual reality (AR/VR), the fifth-generation (5G) enabling technologies are proposed and utilized in networks. As an emerging key technology of 5G and a key enabler of IoT, multiaccess edge computing (MEC), which integrates telecommunication and IT services, offers cloud computing capabilities at the edge of the radio access network (RAN). By providing computational and storage resources at the edge, MEC can reduce latency for end users. Hence, this article investigates MEC for 5G and IoT comprehensively. It analyzes the main features of MEC in the context of 5G and IoT and presents several fundamental key technologies which enable MEC to be applied in 5G and IoT, such as cloud computing, software-defined networking/network function virtualization, information-centric networks, virtual machine (VM) and containers, smart devices, network slicing, and computation offloading. In addition, this article provides an overview of the role of MEC in 5G and IoT, bringing light into the different MEC-enabled 5G and IoT applications as well as the promising future directions of integrating MEC with 5G and IoT. Moreover, this article further elaborates research challenges and open issues of MEC for 5G and IoT. Last but not least, we propose a use case that utilizes MEC to achieve edge intelligence in IoT scenarios.",CS,AI_ML,0.85,Extracted from log - paper 637
Digital Twin-Aided Intelligent Offloading With Edge Selection in Mobile Edge Computing,"In this letter, we study a mobile edge computing (MEC) architecture with the assistance of digital twin (DT) applied for industrial automation where multiple Internet-of-Things (IoT) devices intelligently offload computing tasks to multiple MEC servers to reduce end-to-end latency. To do so, first we propose and formulate a practical end-to-end latency minimization problem in the DT-assisted MEC model subject to the constraints of quality-of-services and computation resource at the IoT devices and MEC servers in industrial IoT networks. Then, we solve the proposed latency minimization problem by iteratively optimizing the transmit power of IoT devices, user association, intelligent task offloading, and estimated CPU processing rate of the devices. Finally, simulation results are conducted to prove the effectiveness of the proposed method in terms of the latency performance compared with some conventional methods.",CS,AI_ML,0.85,Extracted from log - paper 638
Federated Learning in Vehicular Edge Computing: A Selective Model Aggregation Approach,"Federated learning is a newly emerged distributed machine learning paradigm, where the clients are allowed to individually train local deep neural network (DNN) models with local data and then jointly aggregate a global DNN model at the central server. Vehicular edge computing (VEC) aims at exploiting the computation and communication resources at the edge of vehicular networks. Federated learning in VEC is promising to meet the ever-increasing demands of artificial intelligence (AI) applications in intelligent connected vehicles (ICV). Considering image classification as a typical AI application in VEC, the diversity of image quality and computation capability in vehicular clients potentially affects the accuracy and efficiency of federated learning. Accordingly, we propose a selective model aggregation approach, where “fine” local DNN models are selected and sent to the central server by evaluating the local image quality and computation capability. Regarding the implementation of model selection, the central server is not aware of the image quality and computation capability in the vehicular clients, whose privacy is protected under such a federated learning framework. To overcome this information asymmetry, we employ two-dimension contract theory as a distributed framework to facilitate the interactions between the central server and vehicular clients. The formulated problem is then transformed into a tractable problem through successively relaxing and simplifying the constraints, and eventually solved by a greedy algorithm. Using two datasets, i.e., MNIST and BelgiumTSC, our selective model aggregation approach is demonstrated to outperform the original federated averaging (FedAvg) approach in terms of accuracy and efficiency. Meanwhile, our approach also achieves higher utility at the central server compared with the baseline approaches.",CS,AI_ML,0.85,Extracted from log - paper 639
Convergence of Edge Computing and Deep Learning: A Comprehensive Survey,"Ubiquitous sensors and smart devices from factories and communities are generating massive amounts of data, and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people’s lives, from face recognition to ambitious smart factories and cities, developments of artificial intelligence (especially deep learning, DL) based applications and services are thriving. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of “providing artificial intelligence for every person and every organization at everywhere”. Thus, unleashing DL services using resources at the network edge near the data sources has emerged as a desirable solution. Therefore, edge intelligence, aiming to facilitate the deployment of DL services by edge computing, has received significant attention. In addition, DL, as the representative technique of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually beneficial edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely DL training and inference in the customized edge computing framework; 3) challenges and future trends of more pervasive and fine-grained intelligence. We believe that by consolidating information scattered across the communication, networking, and DL areas, this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge DL.",CS,AI_ML,0.85,Extracted from log - paper 640
Dependency-Aware Computation Offloading for Mobile Edge Computing With Edge-Cloud Cooperation,"Most of existing Multi-access edge computing (MEC) studies consider the remote cloud server as a special edge server, the opportunity of edge-cloud collaboration has not been well exploited. We propose a dependency-aware offloading scheme in MEC with edge-cloud cooperation under task dependency constraints. Each mobile device has a limited budget and has to determine which sub-task should be computed locally or should be sent to the edge or remote cloud. To address this issue, we divide the offloading problem into two application finishing time minimization sub-problems with two different cooperation modes, both of which are proved to be NP-hard. We then devise one greedy algorithm with approximation ratio of <inline-formula><tex-math notation=""LaTeX"">$1+\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""chen-ieq1-3037306.gif""/></alternatives></inline-formula> for the first mode with edge-cloud cooperation but no edge-edge cooperation. Then we design an efficient greedy algorithm for the second mode, considering both edge-cloud and edge-edge co-operations. Extensive simulation results show that for the first mode, the proposed greedy algorithm achieves near optimal performance for typical task topologies. On average, it outperforms the modified Hermes benchmark algorithm by about <inline-formula><tex-math notation=""LaTeX"">$23\%\sim 43.6\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>23</mml:mn><mml:mo>%</mml:mo><mml:mo>∼</mml:mo><mml:mn>43</mml:mn><mml:mo>.</mml:mo><mml:mn>6</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""chen-ieq2-3037306.gif""/></alternatives></inline-formula> in terms of application finishing time with given budgets. By further exploiting collaborations among edge servers in the second cooperation mode, the proposed algorithm helps to achieve over 20.3 percent average performance gain on the application finishing time over the first mode under various scenarios. Real-world experiments comply with simulation results.",CS,AI_ML,0.85,Extracted from log - paper 641
Mobile Edge Computing,"Mobile edge computing is a promising paradigm that brings computing resources to mobile users at the network edge, allowing computing-intensive and delay-sensitive applications to be quickly processed by edge servers to satisfy the requirements of mobile users. In this chapter, we first introduce a hierarchical architecture of mobile edge computing that consists of a cloud plane, an edge plane, and a user plane. We then introduce three typical computation offloading decisions. Finally, we review state-of-the-art works on computation offloading and present the use case of joint computation offloading.",CS,AI_ML,0.85,Extracted from log - paper 642
Integration of Blockchain and Edge Computing in Internet of Things: A Survey,"As an important technology to ensure data security, consistency, traceability, etc., blockchain has been increasingly used in Internet of Things (IoT) applications. The integration of blockchain and edge computing can further improve the resource utilization in terms of network, computing, storage, and security. This paper aims to present a survey on the integration of blockchain and edge computing. In particular, we first give an overview of blockchain and edge computing. We then present a general architecture of an integration of blockchain and edge computing system. We next study how to utilize blockchain to benefit edge computing, as well as how to use edge computing to benefit blockchain. We also discuss the issues brought by the integration of blockchain and edge computing system and solutions from perspectives of resource management, joint optimization, data management, computation offloading and security mechanism. Finally, we analyze and summarize the existing challenges posed by the integration of blockchain and edge computing system and the potential solutions in the future.",CS,AI_ML,0.85,Extracted from log - paper 643
Edge computing,"IoT edge computing is a new computing paradigm “in the IoT domain” for performing calculations and processing at the edge of the network, closer to the user and the source of the data. This paradigm is relatively recent, and, together with cloud and fog computing, there may be some confusion about its meaning and implications. This paper aims to help practitioners and researchers better understand what the industry thinks about what IoT edge computing is, and the expected benefits and challenges associated with this paradigm. We conducted a survey using a semi-structured in-depth questionnaire to collect qualitative data from relevant stakeholders from 29 multinational companies and qualitatively analyzed these data using the Constructivist Grounded Theory (Charmaz) method. Several researchers participated in the coding process (collaborative coding). To ensure consensus on the constructs that support the theory and thus improve the rigor of qualitative research, we conducted an intercoder agreement analysis. From the analysis, we have derived a substantive and analytic theory of what companies perceive about IoT edge computing, its benefits and challenges. The theory is substantive in that the scope of validity refers to the 29 surveys processed and analytic in that it analyzes “what is” rather than explaining causality or attempting predictive generalizations. A public repository with all the data related to the information capture process and the products resulting from the analysis of this information is publicly available. This study aims to strengthen the evidence and support practitioners in making better informed decisions about why companies are adopting edge computing and the current challenges they face. Additionally, the testing theory phase shows that the results are aligned with the ISO/IEC TR 30164 standard.",CS,AI_ML,0.85,Extracted from log - paper 644
Profit Maximization Incentive Mechanism for Resource Providers in Mobile Edge Computing,"Mobile edge computing (MEC) has become a promising technique to accommodate demands of resource-constrained mobile devices by offloading the task onto edge clouds nearby. However, most existing works only focus on whether to offload or where to offload the task but ignore the motivations of edge clouds to offer service. To stimulate service provisioning by edge clouds, it is essential to design an incentive mechanism that charges mobile devices and rewards edge clouds. In this paper, we first propose an incentive mechanism in a non-competitive environment. We utilize market-based profit maximization pricing model to establish the relationship between the resources provided by edge clouds and the price charged to mobile devices. By solving the optimization problem, we provide a reasonable pricing strategy to not only ensure the profit of resource providers but guarantee the quality of experience (QoE) of mobile devices. Furthermore, we design an online profit maximization multi-round auction (PMMRA) mechanism for the resource trading between edge clouds as sellers and mobile devices as buyers in a competitive environment. The mechanism can effectively determine the price paid by buyers to use the resources provided by sellers and make the corresponding match between edge clouds and mobile devices. Finally, numerical results show that proposed mechanism outperforms other existing algorithms in maximizing the profit of edge clouds.",CS,AI_ML,0.85,Extracted from log - paper 645
"Computation Offloading in Mobile Cloud Computing and Mobile Edge Computing: Survey, Taxonomy, and Open Issues","Cloud and mobile edge computing (MEC) provides a wide range of computing services for mobile applications. In particular, mobile edge computing enables a computing and storage infrastructure provisioned closely to the end-users at the edge of a cellular network. The small base stations are deployed to establish a mobile edge network that can be coined with cloud infrastructure. A large number of enterprises and individuals rely on services offered by mobile edge and clouds to meet their computational and storage demands. Based on user behavior and demand, the computational tasks are first offloaded from mobile users to the mobile edge network and then executed at one or several specific base stations in the mobile edge network. The MEC architecture has the capability to handle a large number of devices that in turn generate high volumes of traffic. In this work, we first provide a holistic overview of MCC/MEC technology that includes the background and evolution of remote computation technologies. Then, the main part of this paper surveys up-to-date research on the concepts of offloading mechanisms, offloading granularities, and computational offloading techniques. Furthermore, we discuss the offloading mechanism in the static and dynamic environment along with optimization techniques. We further discuss the challenges and potential future directions for MEC research.",CS,AI_ML,0.85,Extracted from log - paper 646
Cost-Effective App User Allocation in an Edge Computing Environment,"Edge computing is a new distributed computing paradigm extending the cloud computing paradigm, offering much lower end-to-end latency, as real-time, latency-sensitive applications can now be deployed on edge servers that are much closer to end-users than distant cloud servers. In edge computing, edge user allocation (EUA) is a critical problem for any app vendors, who need to determine which edge servers will serve which users. This is to satisfy application-specific optimization objectives, e.g., maximizing users’ overall quality of experience, minimizing system costs, and so on. In this article, we focus on the cost-effectiveness of user allocation solutions with two optimization objectives. The primary one is to maximize the number of users allocated to edge servers. The secondary one is to minimize the number of required edge servers, which subsequently reduces the operating costs for app vendors. We first model this problem as a bin packing problem and introduce an approach for finding optimal solutions. However, finding optimal solutions to the <inline-formula><tex-math notation=""LaTeX"">$\mathcal {NP}$</tex-math><alternatives><mml:math><mml:mi mathvariant=""script"">NP</mml:mi></mml:math><inline-graphic xlink:href=""lai-ieq1-3001570.gif""/></alternatives></inline-formula>-hard EUA problem in large-scale scenarios is intractable. Thus, we propose a heuristic to efficiently find sub-optimal solutions to large-scale EUA problems. Extensive experiments conducted on real-world data demonstrate that our heuristic can solve the EUA problem effectively and efficiently, outperforming the state-of-the-art and baseline approaches.",CS,AI_ML,0.85,Extracted from log - paper 647
Adaptive Digital Twin and Multiagent Deep Reinforcement Learning for Vehicular Edge Computing and Networks,"Technological advancements of urban informatics and vehicular intelligence have enabled connected smart vehicles as pervasive edge computing platforms for a plethora of powerful applications. However, varies types of smart vehicles with distinct capacities, diverse applications with different resource demands as well as unpredictive vehicular topology, pose significant challenges on realizing efficient edge computing services. To cope with these challenges, we incorporate digital twin technology and artificial intelligence into the design of a vehicular edge computing network. It centrally exploits potential edge service matching through evaluating cooperation gains in a mirrored edge computing system, while distributively scheduling computation task offloading and edge resource allocation in an multiagent deep reinforcement learning approach. We further propose a coordination graph driven vehicular task offloading scheme, which minimizes offloading costs through efficiently integrating service matching exploitation and intelligent offloading scheduling in both digital twin and physical networks. Numerical results based on real urban traffic datasets demonstrate the efficiency of our proposed schemes.",CS,AI_ML,0.85,Extracted from log - paper 648
Retention-Aware Container Caching for Serverless Edge Computing,"Serverless edge computing adopts an event-based model where Internet-of-Things (IoT) services are executed in lightweight containers only when requested, leading to significantly improved edge resource utilization. Unfortunately, the startup latency of containers degrades the responsiveness of IoT services dramatically. Container caching, while masking this latency, requires retaining resources thus compromising resource efficiency. In this paper, we study the retention-aware container caching problem in serverless edge computing. We leverage the distributed and heterogeneous nature of edge platforms and propose to optimize container caching jointly with request distribution. We reveal step by step that this joint optimization problem can be mapped to the classic ski-rental problem. We first present an online competitive algorithm for a special case where request distribution and container caching are based on a set of carefully designed probability distribution functions. Based on this algorithm, we propose an online algorithm called O-RDC for the general case, which incorporates the resource capacity and network latency by opportunistically distributing requests. We conduct extensive experiments to examine the performance of the proposed algorithms with both synthetic and real-world serverless computing traces. Our results show that ORDC outperforms existing caching strategies of current serverless computing platforms by up to 94.5% in terms of the overall system cost.",CS,AI_ML,0.85,Extracted from log - paper 649
Service Coverage for Satellite Edge Computing,"Recently, increasing investments in satellite-related technologies make the low earth orbit (LEO) satellite constellation a strong complement to terrestrial networks. To mitigate the limitations of the traditional satellite constellation “bent-pipe” architecture, satellite edge computing (SEC) has been proposed by placing computing resources at the LEO satellite constellation. Most existing works focus on space-air-ground integrated network architecture and SEC computing framework. Beyond these works, we are the first to investigate how to efficiently deploy services on the SEC nodes to realize robustness aware service coverage with constrained resources. Facing the challenges of spatial-temporal system dynamics and service coverage-robustness conflict, we propose a novel online service placement algorithm with a theoretical performance guarantee by leveraging Lyapunov optimization and Gibbs sampling. Extensive simulation results show that our algorithm can improve the service coverage by $4.3\times $ compared with the baseline.",CS,AI_ML,0.85,Extracted from log - paper 650
Convergence of Blockchain and Edge Computing for Secure and Scalable IIoT Critical Infrastructures in Industry 4.0,"Critical infrastructure systems are vital to underpin the functioning of a society and economy. Due to the ever-increasing number of Internet-connected Internet-of-Things (IoT)/Industrial IoT (IIoT), and the high volume of data generated and collected, security and scalability are becoming burning concerns for critical infrastructures in industry 4.0. The blockchain technology is essentially a distributed and secure ledger that records all the transactions into a hierarchically expanding chain of blocks. Edge computing brings the cloud capabilities closer to the computation tasks. The convergence of blockchain and edge computing paradigms can overcome the existing security and scalability issues. In this article, we first introduce the IoT/IIoT critical infrastructure in industry 4.0, and then we briefly present the blockchain and edge computing paradigms. After that, we show how the convergence of these two paradigms can enable secure and scalable critical infrastructures. Then, we provide a survey on the state of the art for security and privacy and scalability of IoT/IIoT critical infrastructures. A list of potential research challenges and open issues in this area is also provided, which can be used as useful resources to guide future research.",CS,AI_ML,0.85,Extracted from log - paper 651
Computation Offloading in LEO Satellite Networks With Hybrid Cloud and Edge Computing,"Low earth orbit (LEO) satellite networks can break through geographical restrictions and achieve global wireless coverage, which is an indispensable choice for future mobile communication systems. In this article, we present a hybrid cloud and edge computing LEO satellite (CECLS) network with a three-tier computation architecture, which can provide ground users with heterogeneous computation resources and enable ground users to obtain computation services around the world. With the CECLS architecture, we investigate the computation offloading decisions to minimize the sum energy consumption of ground users, while satisfying the constraints in terms of the coverage time and the computation capability of each LEO satellite. The considered problem leads to a discrete and nonconvex since the objective function and constraints contain binary variables, which makes it difficult to solve. To address this challenging problem, we convert the original nonconvex problem into a linear programming problem by using the binary variables relaxation method. Then, we propose a distributed algorithm by leveraging the alternating direction method of multipliers (ADMMs) to approximate the optimal solution with low computational complexity. Simulation results show that the proposed algorithm can effectively reduce the total energy consumption of ground users.",CS,AI_ML,0.85,Extracted from log - paper 652
An Online Framework for Joint Network Selection and Service Placement in Mobile Edge Computing,"With the rapid development and deployment of 5G wireless technology, mobile edge computing (MEC) has emerged as a new computing paradigm to facilitate a large variety of infrastructures at the network edge to reduce user-perceived communication delay. One of the fundamental problems in this new paradigm is to preserve satisfactory quality-of-service (QoS) for mobile users in light of densely dispersed wireless communication environment and often capacity-constrained MEC nodes. Such user-perceived QoS, typically in terms of the end-to-end delay, is highly vulnerable to both access network bottleneck and communication delay. Previous works have primarily focused on optimizing the communication delay through dynamic service placement, while ignoring the critical effect of access network selection on the access delay. In this work, we study the problem of jointly optimizing the access network selection and service placement for MEC, with the objective of improving the QoS in a cost-efficient manner by judiciously balancing the access delay, communication delay, and service switching cost. Specifically, we propose an efficient online framework to decompose a long-term time-varying optimization problem into a series of one-shot subproblems. To address the NP-hardness of the one-shot problem, we design a computationally-efficient two-phase algorithm based on matching and game theory, which achieves a near-optimal solution. Both rigorous theoretical analysis on the optimality gap and extensive trace-driven simulations are conducted to validate the efficacy of our proposed solution.",CS,AI_ML,0.85,Extracted from log - paper 653
Deep Reinforcement Learning-Based Workload Scheduling for Edge Computing,"Edge computing is a new paradigm for providing cloud computing capacities at the edge of network near mobile users. It offers an effective solution to help mobile devices with computation-intensive and delay-sensitive tasks. However, the edge of network presents a dynamic environment with large number of devices, high mobility of users, heterogeneous applications and intermittent traffic. In such environment, edge computing often suffers from unbalance resource allocation, which leads to task failure and affects system performance. To tackle this problem, we proposed a deep reinforcement learning(DRL)-based workload scheduling approach with the goal of balancing the workload, reducing the service time and the failed task rate. Meanwhile, We adopt Deep-Q-Network(DQN) algorithms to solve the complexity and high dimension of workload scheduling problem. Simulation results show that our proposed approach achieves the best performance in aspects of service time, virtual machine(VM) utilization, and failed tasks rate compared with other approaches. Our DRL-based approach can provide an efficient solution to the workload scheduling problem in edge computing.",CS,AI_ML,0.85,Extracted from log - paper 654
Pyramid: Enabling Hierarchical Neural Networks with Edge Computing,"Machine learning (ML) is powering a rapidly-increasing number of web applications. As a crucial part of 5G, edge computing facilitates edge artificial intelligence (AI) by ML model training and inference at the network edge on edge servers. Compared with centralized cloud AI, edge AI enables low-latency ML inference which is critical to many delay-sensitive web applications, e.g., web AR/VR, web gaming and Web-of-Things applications. Existing studies of edge AI focused on resource and performance optimization in training and inference, leveraging edge computing merely as a tool to accelerate training and inference processes. However, the unique ability of edge computing to process data with context awareness, a powerful feature for building the web-of-things for smart cities, has not been properly explored. In this paper, we propose a novel framework named Pyramid that unleashes the potential of edge AI by facilitating homogeneous and heterogeneous hierarchical ML inferences. We motivate and present Pyramid with traffic prediction as an illustrative example, and evaluate it through extensive experiments conducted on two real-world datasets. The results demonstrate the superior performance of Pyramid neural networks in hierarchical traffic prediction and weather analysis.",CS,AI_ML,0.85,Extracted from log - paper 655
Deep Learning With Edge Computing: A Review,"Deep learning is currently widely used in a variety of applications, including computer vision and natural language processing. End devices, such as smartphones and Internet-of-Things sensors, are generating data that need to be analyzed in real time using deep learning or used to train deep learning models. However, deep learning inference and training require substantial computation resources to run quickly. Edge computing, where a fine mesh of compute nodes are placed close to end devices, is a viable way to meet the high computation and low-latency requirements of deep learning on edge devices and also provides additional benefits in terms of privacy, bandwidth efficiency, and scalability. This paper aims to provide a comprehensive review of the current state of the art at the intersection of deep learning and edge computing. Specifically, it will provide an overview of applications where deep learning is used at the network edge, discuss various approaches for quickly executing deep learning inference across a combination of end devices, edge servers, and the cloud, and describe the methods for training deep learning models across multiple edge devices. It will also discuss open challenges in terms of systems performance, network technologies and management, benchmarks, and privacy. The reader will take away the following concepts from this paper: understanding scenarios where deep learning at the network edge can be useful, understanding common techniques for speeding up deep learning inference and performing distributed training on edge devices, and understanding recent trends and opportunities.",CS,AI_ML,0.85,Extracted from log - paper 656
Orbital Edge Computing: Nanosatellite Constellations as a New Class of Computer System,"Advances in nanosatellite technology and a declining cost of access to space have fostered an emergence of large constellations of sensor-equipped satellites in low-Earth orbit. Many of these satellite systems operate under a ""bent-pipe"" architecture, in which ground stations send commands to orbit and satellites reply with raw data. In this work, we observe that a bent-pipe architecture for Earth-observing satellites breaks down as constellation population increases. Communication is limited by the physical configuration and constraints of the system over time, such as ground station location, nanosatellite antenna size, and energy harvested on orbit. We show quantitatively that nanosatellite constellation capabilities are determined by physical system constraints. We propose an Orbital Edge Computing (OEC) architecture to address the limitations of a bent-pipe architecture. OEC supports edge computing at each camera-equipped nanosatellite so that sensed data may be processed locally when downlinking is not possible. In order to address edge processing latencies, OEC systems organize satellite constellations into computational pipelines. These pipelines parallelize both data collection and data processing based on geographic location and without the need for cross-link coordination. OEC satellites explicitly model constraints of the physical environment via a runtime service. This service uses orbit parameters, physical models, and ground station positions to trigger data collection, predict energy availability, and prepare for communication. We show that an OEC architecture can reduce ground infrastructure over 24x compared to a bent-pipe architecture, and we show that pipelines can reduce system edge processing latency over 617x.",CS,AI_ML,0.85,Extracted from log - paper 657
Dynamic Computation Offloading for Mobile-Edge Computing With Energy Harvesting Devices,"Mobile-edge computing (MEC) is an emerging paradigm to meet the ever-increasing computation demands from mobile applications. By offloading the computationally intensive workloads to the MEC server, the quality of computation experience, e.g., the execution latency, could be greatly improved. Nevertheless, as the on-device battery capacities are limited, computation would be interrupted when the battery energy runs out. To provide satisfactory computation performance as well as achieving green computing, it is of significant importance to seek renewable energy sources to power mobile devices via energy harvesting (EH) technologies. In this paper, we will investigate a green MEC system with EH devices and develop an effective computation offloading strategy. The execution cost, which addresses both the execution latency and task failure, is adopted as the performance metric. A low-complexity online algorithm is proposed, namely, the Lyapunov optimization-based dynamic computation offloading algorithm, which jointly decides the offloading decision, the CPU-cycle frequencies for mobile execution, and the transmit power for computation offloading. A unique advantage of this algorithm is that the decisions depend only on the current system state without requiring distribution information of the computation task request, wireless channel, and EH processes. The implementation of the algorithm only requires to solve a deterministic problem in each time slot, for which the optimal solution can be obtained either in closed form or by bisection search. Moreover, the proposed algorithm is shown to be asymptotically optimal via rigorous analysis. Sample simulation results shall be presented to corroborate the theoretical analysis as well as validate the effectiveness of the proposed algorithm.",CS,AI_ML,0.85,Extracted from log - paper 658
A Survey on the Edge Computing for the Internet of Things,"The Internet of Things (IoT) now permeates our daily lives, providing important measurement and collection tools to inform our every decision. Millions of sensors and devices are continuously producing data and exchanging important messages via complex networks supporting machine-to-machine communications and monitoring and controlling critical smart-world infrastructures. As a strategy to mitigate the escalation in resource congestion, edge computing has emerged as a new paradigm to solve IoT and localized computing needs. Compared with the well-known cloud computing, edge computing will migrate data computation or storage to the network “edge,” near the end users. Thus, a number of computation nodes distributed across the network can offload the computational stress away from the centralized data center, and can significantly reduce the latency in message exchange. In addition, the distributed structure can balance network traffic and avoid the traffic peaks in IoT networks, reducing the transmission latency between edge/cloudlet servers and end users, as well as reducing response times for real-time IoT applications in comparison with traditional cloud services. Furthermore, by transferring computation and communication overhead from nodes with limited battery supply to nodes with significant power resources, the system can extend the lifetime of the individual nodes. In this paper, we conduct a comprehensive survey, analyzing how edge computing improves the performance of IoT networks. We categorize edge computing into different groups based on architecture, and study their performance by comparing network latency, bandwidth occupation, energy consumption, and overhead. In addition, we consider security issues in edge computing, evaluating the availability, integrity, and the confidentiality of security strategies of each group, and propose a framework for security evaluation of IoT networks with edge computing. Finally, we compare the performance of various IoT applications (smart city, smart grid, smart transportation, and so on) in edge computing and traditional cloud computing architectures.",CS,AI_ML,0.85,Extracted from log - paper 659
Survey on Multi-Access Edge Computing Security and Privacy,"The European Telecommunications Standards Institute (ETSI) has introduced the paradigm of Multi-Access Edge Computing (MEC) to enable efficient and fast data processing in mobile networks. Among other technological requirements, security and privacy are significant factors in the realization of MEC deployments. In this paper, we analyse the security and privacy of the MEC system. We introduce a thorough investigation of the identification and the analysis of threat vectors in the ETSI standardized MEC architecture. Furthermore, we analyse the vulnerabilities leading to the identified threat vectors and propose potential security solutions to overcome these vulnerabilities. The privacy issues of MEC are also highlighted, and clear objectives for preserving privacy are defined. Finally, we present future directives to enhance the security and privacy of MEC services.",CS,AI_ML,0.85,Extracted from log - paper 660
A Survey of Recent Advances in Edge-Computing-Powered Artificial Intelligence of Things,"The Internet of Things (IoT) has created a ubiquitously connected world powered by a multitude of wired and wireless sensors generating a variety of heterogeneous data over time in a myriad of fields and applications. To extract complete information from these data, advanced artificial intelligence (AI) technology, especially deep learning (DL), has proved successful in facilitating data analytics, future prediction and decision making. The collective integration of AI and the IoT has greatly promoted the rapid development of AI-of-Things (AIoT) systems that analyze and respond to external stimuli more intelligently without involvement by humans. However, it is challenging or infeasible to process massive amounts of data in the cloud due to the destructive impact of the volume, velocity, and veracity of data and fatal transmission latency on networking infrastructures. These critical challenges can be adequately addressed by introducing edge computing. This article conducts an extensive survey of an end-edge-cloud orchestrated architecture for flexible AIoT systems. Specifically, it begins with articulating fundamental concepts including the IoT, AI and edge computing. Guided by these concepts, it explores the general AIoT architecture, presents a practical AIoT example to illustrate how AI can be applied in real-world applications and summarizes promising AIoT applications. Then, the emerging technologies for AI models regarding inference and training at the edge of the network are reviewed. Finally, the open challenges and future directions in this promising area are outlined.",CS,AI_ML,0.85,Extracted from log - paper 661
Collaborative Cloud-Edge-End Task Offloading in Mobile-Edge Computing Networks With Limited Communication Capability,"Mobile edge computing (MEC) is an emerging computing paradigm for enabling low-latency, high-bandwidth and agile mobile services by deploying computing platform at the edge of network. In order to improve the cloud-edge-end processing efficiency of the tasks within the limited computation and communication capabilities, in this article, we investigate the collaborative computation offloading, computation and communication resource allocation scheme, and develop a collaborative computing framework that the tasks of mobile devices (MDs) can be partially processed at the terminals, edge nodes (EN) and cloud center (CC). Then, we propose the pipeline-based offloading scheme, where both MDs and ENs can offload computation-intensive tasks to a particular EN and CC, according to their computation and communication capacities, respectively. Based on the proposed pipeline offloading strategy, a sum latency of all MDs minimization problem is formulated with the consideration of the offloading strategy, computation resource, delivery rate and power allocation, which is a non-convex problem and difficult to deal with. To solve the optimization problem, by using the classic successive convex approximation (SCA) approach, we transform the non-convex optimization problem into the convex one. Finally, simulation results indicate that the proposed collaboration offloading scheme with the pipeline strategy is efficient and outperforms other offloading schemes.",CS,AI_ML,0.85,Extracted from log - paper 662
PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems,"Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker’s well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named Data_Gen, based on the generative adversarial networks (GANs). This method mainly relies upon the iteratively updated global model parameters to regenerate samples of interested victims. Second, we further propose a novel generative poisoning attack model, named PoisonGAN, against the federated learning framework. This model utilizes the designed Data_Gen method to efficiently reduce the attack assumptions and make attacks feasible in practice. We finally evaluate our data generation and attack models by implementing two types of typical poisoning attack strategies, label flipping and backdoor, on a federated learning prototype. The experimental results demonstrate that these two attack models are effective in federated learning.",CS,AI_ML,0.85,Extracted from log - paper 663
Multi Objective Prioritized Workflow Scheduling Using Deep Reinforcement Based Learning in Cloud Computing,"Workflow Scheduling is a huge challenge in cloud paradigm as many number of workflows dynamically generated from various heterogeneous resources and task dependencies in each workflow varies from each other. Therefore, if a workflow with more number of dependencies is not scheduled onto an appropriate Virtual Machine i.e. with low processing capacity which leads to delay in executing workflows and it results in increase of makespan, cost, energy consumption. In order to effectively schedule complex workflows i.e. with more task dependencies, we propose a novel multi objective workflow scheduling algorithm using Deep reinforcement Learning. Initially, priorities of all workflows calculated based on their dependencies and then calculated priorities of VMs based on electricity cost at datacenters to map workflows onto precise VMs. These priorities are fed to scheduler which uses Deep Q-Network model to dynamically schedule tasks by considering both priorities of tasks and VMs. Extensive simulations carried out on workflowsim by considering realtime scientific workflows (Montage, cybershake, Epigenomics, LIGO). Our proposed MOPWSDRL compared against existing state of art approaches i.e. Heterogeneous Earliest First Deadline, Cat Swarm Optimization, Ant Colony Optimization. Results revealed that our proposed MOPDSWRL outperforms existing state of art algorithms by minimizing makespan, energy consumption.",CS,AI_ML,0.85,Extracted from log - paper 664
Reliability Enhancement Strategies for Workflow Scheduling Under Energy Consumption Constraints in Clouds,"As the demand for Big Data analysis and artificial intelligence technology continues to surge, a significant amount of research has been conducted on cloud computing services. An effective workflow scheduling strategy stands as the pivotal factor in ensuring the quality of cloud services. Dynamic voltage and frequency scaling (DVFS) is an effective energy-saving technology that is extensively used in the development of workflow scheduling algorithms. However, DVFS reduces the processor's running frequency, which increases the possibility of soft errors in workflow execution, thereby lowering the workflow execution reliability. This study proposes an energy-aware reliability enhancement scheduling (EARES) method with a checkpoint mechanism to improve system reliability while meeting the workflow deadline and the energy consumption constraints. The proposed EARES algorithm consists of three phases, namely, workflow application initialization, deadline partitioning, and energy partitioning and virtual machine selection. Numerous experiments are conducted to assess the performance of the EARES algorithm using three real-world scientific workflows. Experimental results demonstrate that the EARES algorithm remarkably improves reliability in comparison with other state-of-the-art algorithms while meeting the deadline and satisfying the energy consumption requirement.",CS,AI_ML,0.85,Extracted from log - paper 665
DB-ACO: A Deadline-Budget Constrained Ant Colony Optimization for Workflow Scheduling in Clouds,"With the development of cloud computing, a growing number of workflows are deployed in cloud platform that can dynamically provide cloud resources on demand for users. In clouds, one basic problem is how to schedule workflow under the deadline constraint and minimize the execution cost. As the capability of cloud resources getting higher, the required cost is also rising. Capability of some resources exceeds the need of users, which leads to higher cost, and the budget of users should be considered. In this paper, a novel scheduling algorithm, named DB-ACO, is proposed to minimize the execution cost for the workflow with deadline and budget constraints. DB-ACO is verified on four typical scientific workflows, and the experiments results show it outperforms four state-of-the-art methods, especially for CyberShake.Note to Practitioners—Budget and deadline are important requirements for users in cloud computing, which are used as constraints. Extensive works have been devoted to minimize the cost of workflows execution with different scheduling strategies. However, most of them only consider one single constraint and assume the constraint is simple and loose, which is impractical in actual scenarios due to higher requirement of users. This paper investigates a novel scheduling algorithm DB-ACO to optimize cost under budget and deadline. DB-ACO combines heuristic and meta-heuristic, it uses ant colony optimization to optimize the execution cost under the deadline and budget constraints: each ant sorts tasks on the basis of the combination of the pheromone trail and heuristic information, the deadline and budget are distributed fairly to each task by a novel distribution method, then the service selection rules are introduced to build solution.",CS,AI_ML,0.85,Extracted from log - paper 666
A Two-stage Multi-population Genetic Algorithm with Heuristics for Workflow Scheduling in Heterogeneous Distributed Computing Environments,"Workflow scheduling in Heterogeneous Distributed Computing Environments (HDCEs) is a NP-hard problem. Although a number of scheduling approaches have been proposed for workflow scheduling in HDCEs, there is still a room and need for improvement. To fill the gaps, this study formulates workflow scheduling problem in HDCEs as a complete, solvable and extensible integer programming mathematical model with precedence and resource constraints that provides a theoretical foundation for developing workflow scheduling strategy. Then, this study develops a novel two-stage multi-population genetic algorithm with heuristics for workflow scheduling. In particular, two-stage multi-population coevolution strategy is employed with designed novel methods for population initialization, genetic operation, individual decoding and improvement. To estimate the validity, extensive experiments are designed and conducted on various scenarios based on real and random workflow applications. The results have shown the practical viability of the proposed algorithm outperforming conventional approaches.",CS,AI_ML,0.85,Extracted from log - paper 667
Workflow Scheduling in Serverless Edge Computing for the Industrial Internet of Things: A Learning Approach,"Serverless edge computing is seen as a promising enabler to execute differentiated Industrial Internet of Things (IIoT) applications without managing the underlying servers and clusters. In IIoT serverless edge computing, IIoT workflow scheduling for cloud-edge collaborative processing is closely related to the service quality of users. However, serverless functions decomposed by IIoT applications are limited in their deployment at the edge due to the resource-constrained nature of edge infrastructures. In addition, the scheduling of complex IIoT applications supported by serverless computing is more challenging. Therefore, considering the limited function deployment and the complex dependencies of serverless workflows, we model the workflow application as directed acyclic graph and formulate the scheduling problem as a multiobjective optimization problem. A dueling double deep Q-network-based solution is proposed to make scheduling decisions under dynamically changing systems. Extensive simulation experiments are conducted to validate the superiority of the proposed scheme.",CS,AI_ML,0.85,Extracted from log - paper 668
ET2FA: A Hybrid Heuristic Algorithm for Deadline-Constrained Workflow Scheduling in Cloud,"Cloud computing is an emerging computational infrastructure for cost-efficient workflow execution that provides flexible and dynamically scalable computing resources at pay-as-you-go pricing. Workflow scheduling, as a typical NP-Complete problem, is one of the major issues in cloud computing. However, in the cloud scenario with unlimited resources, how to generate an efficient and economical workflow scheduling scheme under the deadline constraint is still an extraordinary challenge. In this article, we propose a hybrid heuristic algorithm called enhanced task type first algorithm (ET2FA) to solve deadline-constrained workflow scheduling in cloud with new features such as hibernation and per-second billing. The objectives to be minimized include the total cost and total idle rate. ET2FA involves three phases: 1) Task type first algorithm, which schedules tasks based on topological level and task types, and utilizes a compact-scheduling-condition based VM selection method to assign each task. 2) Delay operation based on block structure, which further optimizes total cost and total idle rate based on block structure properties. 3) Instance hibernate scheduling heuristic, which sets an instance to hibernate if idle for a duration. Extensive simulation experiments based on seven well-known real-world workflow applications show that ET2FA delivers better performance in comparison to the state-of-the-art algorithms.",CS,AI_ML,0.85,Extracted from log - paper 669
Reliability-Aware Multi-Objective Memetic Algorithm for Workflow Scheduling Problem in Multi-Cloud System,"With the development of cloud computing, multi-cloud systems have become common platforms for hosting and executing workflow applications in recent years. However, the complexity of workflow scheduling increases exponentially because of the diversified billing mechanisms, heterogeneous virtual machines, and reliability of multi-cloud systems. This article focuses on a multi-objective workflow scheduling problem in multi-cloud systems (MOWSP-MCS). The makespan, cost, and reliability are considered the optimization objectives from the perspective of users. Compared with the classical multi-objective workflow scheduling in the cloud environment, MOWSP-MCS allows users to apply the backup technique to improve reliability. To solve the MOWSP-MCS, this article proposes a reliability-aware multi-objective memetic algorithm (RA-MOMA) containing a diversification strategy and intensification strategy. In the diversification strategy, several problem-specific genetic operators are introduced to construct the diversified offspring individuals. In the intensification strategy, four problem-specific neighborhood operators are designed based on the critical path and resource utilization rate to improve the quality of the individuals in the archive set. A comprehensive numerical experiment is conducted to evaluate the effectiveness of RA-MOMA. The comparisons with several related algorithms demonstrate the superiority of RA-MOMA for solving the MOWSP-MCS.",CS,AI_ML,0.85,Extracted from log - paper 670
A Two-Stage Estimation of Distribution Algorithm With Heuristics for Energy-Aware Cloud Workflow Scheduling,"With the enormous increase in energy usage by cloud data centers for handling various workflow applications, the energy-aware cloud workflow scheduling has become a hot issue. However, there is still a need and room for improvement in both the model for estimating workflow energy consumption and the algorithm for energy-aware cloud workflow scheduling. To fill these gaps, a new model for estimating the energy consumption of the cloud workflow execution and a novel Two-Stage Estimation of Distribution Algorithm with heuristics (TSEDA) for energy-aware cloud workflow scheduling are proposed based on the relationships among scheduling scheme, host load and power. In particular, in the proposed TSEDA, a new probability model and its updating mechanism are presented, and a two-stage coevolution strategy with some novel heuristic methods for individual generation, decoding and improvement is designed. Extensive experiments are conducted on workflow applications with various sizes and types, and the results show that the proposed TSEDA outperforms conventional algorithms.",CS,AI_ML,0.85,Extracted from log - paper 671
Genetic Programming for Dynamic Workflow Scheduling in Fog Computing,"Dynamic Workflow Scheduling in Fog Computing (DWSFC) is an important optimisation problem with many real-world applications. The current workflow scheduling problems only consider cloud servers but ignore the roles of mobile devices and edge servers. Some applications need to consider the mobile devices, edge, and cloud servers simultaneously, making them work together to generate an effective schedule. In this article, a new problem model for DWSFC is considered and a new simulator is designed for the new DWSFC problem model. The designed simulator takes the mobile devices, edge, and cloud servers as a whole system, where they all can execute tasks. In the designed simulator, two kinds of decision points are considered, which are the routing decision points and the sequencing decision points. To solve this problem, a new Multi-Tree Genetic Programming (MTGP) method is developed to automatically evolve scheduling heuristics that can make effective real-time decisions on these decision points. The proposed MTGP method with a multi-tree representation can handle the routing decision points and sequencing decision points simultaneously. The experimental results show that the proposed MTGP can achieve significantly better test performance (reduce the makespan by up to 50%) on all the tested scenarios than existing state-of-the-art methods.",CS,AI_ML,0.85,Extracted from log - paper 672
Reliability-Aware and Energy-Efficient Workflow Scheduling in IaaS Clouds,"Nowadays, more and more workflow applications with different computing requirements are migrated to clouds and executed with cloud resources. Workflow scheduling becomes a critical problem in the cloud environment, which focuses on meeting various quality of service (QoS) constraints. Workflow reliability and energy consumption are two essential parts in clouds and minimizing energy consumption for scheduling workflow with the reliability constraint is a challenging issue. In response to the challenge, we propose a workflow scheduling algorithm named REWS to reduce energy consumption and satisfy workflow reliability constraints. In REWS, a new sub-reliability constraint prediction strategy is adopted to break down the workflow reliability constraint to task sub-reliability constraints and the effectiveness of this strategy is proved. Moreover, an update method is adopted to adjust the task sub-reliability constraint for reducing energy consumption. In addition, a brief system framework which consists of five parts: workflow analyzer, reliability decomposer, resource manager, workflow scheduler and feedback processer is built to support the algorithm implementation of REWS. We conduct the experiments using both synthetic data and real-world data to evaluate the proposed REWS approach. The results demonstrate the superiority of REWS as compared with the state-of-the-art algorithms. Note to Practitioners—Workflow scheduling is a challenging issue in emerging trends of the cloud environment that focuses on satisfying various QoS constraints. In this paper, we investigate a reliability-aware and energy-efficient workflow scheduling problem in cloud computing. A novel workflow scheduling algorithm called REWS, is designed to reduce the energy consumption and meet the workfolw reliability constraint. The basic idea of REWS is to divide the workflow reliability constraint into task sub-reliability constraints and schedule tasks with an energy-efficient scheduling strategy. We conduct the experiments to evaluate the proposed REWS and the results demonstrate that REWS outperforms the state-of-the-art algorithms.",CS,AI_ML,0.85,Extracted from log - paper 673
MONWS: Multi-Objective Normalization Workflow Scheduling for Cloud Computing,"Cloud computing is a prominent approach for complex scientific and business workflow applications in the pay-as-you-go model. Workflow scheduling poses a challenge in cloud computing due to its widespread applications in physics, astronomy, bioinformatics, and healthcare, etc. Resource allocation for workflow scheduling is problematic due to the computationally intensive nature of the workflow, the interdependence of tasks, and the heterogeneity of cloud resources. During resource allocation, the time and cost of execution are significant issues in the cloud-computing environment, which can potentially degrade the service quality that is provided to end users. This study proposes a method focusing on makespan, average utilization, and cost. The authors propose a task’s dynamic priority for workflow scheduling using MONWS, which uses the min-max algorithm to minimize the finish time and maximize resource utilization by calculating the dynamic threshold value for scheduling tasks on virtual machines. When the experimental results were compared to existing algorithms, MONWS achieved a 35% improvement in makespan, an 8% increase in maximum average cloud utilization, and a 4% decrease in cost.",CS,AI_ML,0.85,Extracted from log - paper 674
Adaptive and Convex Optimization-Inspired Workflow Scheduling for Cloud Environment,"Scheduling large-scale and resource-intensive workflows in cloud infrastructure is one of the main challenges for cloud service providers (CSPs). Cloud infrastructure is more efficient when virtual machines and other resources work up to their full potential. The main factor that influences the quality of cloud services is the distribution of workflow on virtual machines (VMs). Scheduling tasks to VMs depends on the type of workflow and mechanism of resource allocation. Scientific workflows include large-scale data transfer and consume intensive resources of cloud infrastructures. Therefore, scheduling of tasks from scientific workflows on VMs requires efficient and optimized workflow scheduling techniques. This paper proposes an optimised workflow scheduling approach that aims to improve the utilization of cloud resources without increasing execution time and execution cost.",CS,AI_ML,0.85,Extracted from log - paper 675
A Workflow Scheduling Approach With Modified Fuzzy Adaptive Genetic Algorithm in IaaS Clouds,"The emergence of the cloud platform with substantial resources to offer on-demand instigated the researchers to migrate the scientific workflows to the cloud environment. The scheduling of workflows with diverse QoS parameters is not a trivial task, but an NP-Complete problem. Several heuristics for QoS constrained workflows have been investigated. However, most of them focus only on time and cost and do not guarantee high resource utilization. The scheduling of the workflow tasks over the minimum cloud resources under the defined time limit is a grave concern. In this article, an algorithm named MFGA (Modified Fuzzy Adaptive Genetic Algorithm) has been formulated to minimize the makespan and improve resource utilization under both deadline and budget constraints. A fuzzy logic controller has also been devised to control the crossover and mutation rates that prevent MFGA from getting stuck in a local optimum. MFGA has a novel crossover technique that adds the fittest solutions in the population. Additionally, a new mutation technique has also been introduced, which minimizes the makespan and increases the reusability of the resources. The simulation experiments with the real workflows show that the proposed MFGA outperforms other state-of-the-art algorithms.",CS,AI_ML,0.85,Extracted from log - paper 676
Failure-Aware Elastic Cloud Workflow Scheduling,"With an increasing complexity and functionality in cloud data centers, fault tolerance becomes an essential requirement for tasks executed in clouds, especially for workflows with task precedences. Hosts and network devices are the main physical components in a cloud data center. The PB (Primary-Backup) model is a desirable approach to fault tolerance. Many PB-based workflow scheduling algorithms have been proposed for host faults. However, only a few studies focus on cloud workflow scheduling considering network device faults. This paper analyzes the fault-tolerant properties for scheduling dependent tasks and migrating VMs based on the PB model, considering both host and network device faults in a cloud data center. A failure-aware elastic cloud workflow scheduling algorithm is designed for both host and network device fault tolerance. Additionally, an elastic resource provisioning mechanism is proposed and incorporated into the proposed algorithm to improve resource utilization. Performance evaluations on both randomly generated and real-world workflows show that the proposal effectively improves resource utilization while guaranteeing fault tolerance.",CS,AI_ML,0.85,Extracted from log - paper 677
A hybrid algorithm for workflow scheduling in cloud environment,": The advances in cloud computing promote the problem in processing speed. Computing resources in cloud play a vital role in solving user demands, which can be regarded as workflows. Efficient workflow scheduling is a challenge in reducing the task execution time and cost. In recent years, deep reinforcement learning algorithm has been used to solve various combinatorial optimisation problems. However, the trained models often have volatility and can not be applied in real situation. In addition, evolutionary algorithm with a complete framework is a popular method to tackle the scheduling problem. But, it has a poor convergence speed. In this paper, we propose a hybrid algorithm to address the workflow scheduling problem, which combines deep reinforcement algorithm and evolutionary algorithm. The solutions generated by deep reinforcement learning are the initial population in the evolutionary algorithm. Results show that the proposed algorithm is effective.",CS,AI_ML,0.85,Extracted from log - paper 678
Modified firefly algorithm for workflow scheduling in cloud-edge environment,"Edge computing is a novel technology, which is closely related to the concept of Internet of Things. This technology brings computing resources closer to the location where they are consumed by end-users—to the edge of the cloud. In this way, response time is shortened and lower network bandwidth is utilized. Workflow scheduling must be addressed to accomplish these goals. In this paper, we propose an enhanced firefly algorithm adapted for tackling workflow scheduling challenges in a cloud-edge environment. Our proposed approach overcomes observed deficiencies of original firefly metaheuristics by incorporating genetic operators and quasi-reflection-based learning procedure. First, we have validated the proposed improved algorithm on 10 modern standard benchmark instances and compared its performance with original and other improved state-of-the-art metaheuristics. Secondly, we have performed simulations for a workflow scheduling problem with two objectives—cost and makespan. We performed comparative analysis with other state-of-the-art approaches that were tested under the same experimental conditions. Algorithm proposed in this paper exhibits significant enhancements over the original firefly algorithm and other outstanding metaheuristics in terms of convergence speed and results’ quality. Based on the output of conducted simulations, the proposed improved firefly algorithm obtains prominent results and managed to establish improvement in solving workflow scheduling in cloud-edge by reducing makespan and cost compared to other approaches.",CS,AI_ML,0.85,Extracted from log - paper 679
Scoring and Dynamic Hierarchy-Based NSGA-II for Multiobjective Workflow Scheduling in the Cloud,"Cloud computing becomes a promising technology to reduce computation cost by providing users with elastic resources and application-deploying environments as a pay-per-use model. More scientific workflow applications have been moved or are being migrated to the cloud. Scheduling workflows turns to the main bottleneck for increasing resource utilization and quality of service (QoS) for users. This work formulates workflow scheduling as multiobjective optimization problems and proposes a Scoring and Dynamic Hierarchy-based NSGA-II (Nondominated Sorting Genetic Algorithm II), called SDHN for short, to minimize both makespan and cost of workflow execution. First, a scoring criterion is developed to calculate the total score for each individual during population updating, which is used as a quantitative index to evaluate the dominance degree of individuals among the whole population. Hence, SDHN can distinguish individuals within the same dominance level and target its search toward the directions of elite solutions as their different dominance degrees and accordingly improve search efficiency. Second, a population-based dynamic hierarchical structure (HS) and its evolutionary rules are presented to update HS by comparing each child with all parental individuals from bottom to up until finding a proper dominant level. Since traversing all HS levels is not needed in most cases, the number of individual comparisons is reduced and SDHN’s updating efficiency is greatly improved, especially for large-scale and complex applications. Third, to guarantee its converging to the near-optimal solutions, adaptive adjustment strategies (AASs) are designed to prevent the search from falling into local optima or diverging by checking the number of individuals at the highest HS level and then modifying the relevant genetic operations to guide the evolutionary process to approach the global Pareto Front. Extensive experiments are conducted to verify SDHN, and the results show that it outperforms the existing algorithms in the quality and diversity of resulting solutions as well as convergence time. Note to Practitioners—Most scientific applications are computation and/or data-intensive and need large-scale or high-performance resources for their execution. More and more scientists use workflows to manage their applications, but how to efficiently run them in the cloud is a big challenge due to their large scale as well as the dynamic characteristics of the elastic and heterogeneous cloud resources. In this article, we develop a novel multiobjective optimization technique for workflow scheduling such that the makespan and cost can be minimized simultaneously. A scoring criterion, dynamic hierarchical structure and its evolutionary rules, and adaptive adjustment strategies are designed to cooperate with each other and increase the search ability and efficiency of the original and widely used NSGA-II. Adequate experiments are conducted to verify the proposed method’s performance, and the experimental results show that it can provide more near-optimal solutions than the existing methods. It can be readily applied for implementing more efficient and effective cloud data centers to execute large-scale scientific workflows.",CS,AI_ML,0.85,Extracted from log - paper 680
Evolutionary Multi-Objective Workflow Scheduling for Volatile Resources in the Cloud,"The cloud has been widely used as a distributed computing platform for running scientific workflow applications. Most of the cloud providers encourage the use of their underutilized resources as spot instances for much cheaper prices compared with common resources as on-demand instances, however, the promise of lower costs for resources results in the volatility such that spot instances can be interrupted at any time by cloud providers. Many workflow scheduling algorithms have been proposed to deal with volatile resources. In this article, we consider the two most important features of the volatile resources namely fulfillment and interruption rates to fully model the instability of the cloud infrastructure. Subsequently, we propose a novel evolutionary multi-objective workflow scheduling approach to generate a set of trade-off solutions that outperform state-of-the-art algorithms in both makespan and economic costs. In addition, we explore the fluctuation of makespan and costs for our obtained schedules under different levels of fulfillment and interruption rates. Experimental results with the five well-known real-world workflows demonstrate that our evolutionary multi-objective workflow scheduling algorithm is competitive in terms of makespan and cost compared with state-of-the-art on-demand scheduling techniques.",CS,AI_ML,0.85,Extracted from log - paper 681
Cost-Efficient Workflow Scheduling Algorithm for Applications With Deadline Constraint on Heterogeneous Clouds,"In recent years, more and more large-scale data processing and computing workflow applications run on heterogeneous clouds. Such cloud applications with precedence-constrained tasks are usually deadline-constrained and their scheduling is an essential problem faced by cloud providers. Moreover, minimizing the workflow execution cost based on cloud billing periods is also a complex and challenging problem for clouds. In realizing this, we first model the workflow applications as I/O Data-aware Directed Acyclic Graph (DDAG), according to clouds with global storage systems. Then, we mathematically state this deadline-constrained workflow scheduling problem with the goal of minimum execution financial cost. We also prove that the time complexity of this problem is NP-hard by deducing from a multidimensional multiple-choice knapsack problem. Third, we propose a heuristic cost-efficient task scheduling strategy called CETSS, which includes workflow DDAG model building, task subdeadline initialization, greedy workflow scheduling algorithm, and task adjusting method. The greedy workflow scheduling algorithm mainly consists of dynamical task renting billing period sharing method and unscheduled task subdeadline relax technique. We perform rigorous simulations on some synthetic randomly generated applications and real-world applications, such as Epigenomics, CyberShake, and LIGO. The experimental results clearly demonstrate that our proposed heuristic CETSS outperforms the existing algorithms and can effective save the total workflow execution cost. In particular, CETSS is very suitable for large workflow applications.",CS,AI_ML,0.85,Extracted from log - paper 682
Multi-Swarm Co-Evolution Based Hybrid Intelligent Optimization for Bi-Objective Multi-Workflow Scheduling in the Cloud,"Many scientific applications can be well modelled as large-scale workflows. Cloud computing has become a suitable platform for hosting and executing them. Workflow scheduling has gained much attention in recent years. However, since cloud service providers must offer services for multiple users with various QoS demands, scheduling multiple applications with different QoS requirements is highly challenging. This work proposes a Multi-swarm Co-evolution-based Hybrid Intelligent Optimization (MCHO) algorithm for multiple-workflow scheduling to minimize total makespan and cost while meeting the deadline constraint of each workflow. First, we design a multi-swarm co-evolutionary mechanism where three swarms are adopted to sufficiently search for various elite solutions. Second, to improve global search and convergence performance, we embed local and global guiding information into the updating process of a Particle Swarm Optimizer, and develop a swarm cooperation technique. Third, we propose a Genetic Algorithm-based elite enhancement strategy to exploit more non-dominated individuals, and apply the Metropolis Acceptance rule of Simulated Annealing to update the local guiding solution for each swarm so as to prevent it from being stuck into a local optimum at an early stage. Extensive experimental results demonstrate that MCHO outperforms the state-of-art scheduling algorithms with better distributed non-dominated solutions.",CS,AI_ML,0.85,Extracted from log - paper 683
A Cooperative Coevolution Hyper-Heuristic Framework for Workflow Scheduling Problem,"Workflow scheduling problem (WSP) is a well-known combinatorial optimization problem, which is defined to assign a series of interconnected tasks to the available resources to meet user defined Quality of Service (QoS). The guided random search methods and heuristic based methods are two most common methods for solving WSP. However, these methods either require expensive computational cost or heavily rely on human's empirical knowledge, which makes them inconvenient for practical applications. Keeping this in mind, this paper proposes a cooperative coevolution hyper-heuristic framework to solve WSP with an objective of minimizing the completed time of workflow. In particular, in the proposed framework, two heuristic rules, namely, the task selection rule (TSR) and the resource selection rule (RSR), are learned automatically by a cooperative coevolution genetic programming (CCGP) algorithm. The TSR is used to select a ready task for scheduling, while the RSR is used to allocate resources to perform the selected task. To improve the search efficiency, a set of low-level heuristics are defined and used as building blocks to construct the TSR and RSR. Further, to validate the effectiveness of the proposed framework, randomly generated workflow instances and four real-world workflows are used as test cases in the experimental study. Compared with several state-of-the-art methods, e.g., the Heterogeneous Earliest Finish Time (HEFT) and the Predict Earliest Finish Time (PEFT), the high-level heuristics found by our proposed framework demonstrate superior performance on all the test cases in terms of several metrics including the schedule length ratio, speedup and efficiency.",CS,AI_ML,0.85,Extracted from log - paper 684
"Load and Cost-Aware Min-Min Workflow Scheduling Algorithm for Heterogeneous Resources in Fog, Cloud, and Edge Scenarios","Fog computing and Edge computing are few of the latest technologies which are offered as solution to challenges faced in Cloud Computing. Instead of offloading of all the tasks to centralized cloud servers, some of the tasks can be scheduled at intermediate Fog servers or Edge devices. Though this solves most of the problems faced in cloud but also encounter other traditional problems due to resource-related constraints like load balancing, scheduling, etc. In order to address task scheduling and load balancing in Cloud-fog-edge collaboration among servers, we have proposed an improved version of min-min algorithm for workflow scheduling which considers cost, makespan, energy and load balancing in heterogeneous environment. This algorithm is implemented and tested in different offloading scenarios- Cloud only, Fog only, Cloud-fog and Cloud-Fog-Edge collaboration. This approach performed better and the result gives minimum makespan, less energy consumption along with load balancing and marginally less cost when compared to min-min and ELBMM algorithms",CS,AI_ML,0.85,Extracted from log - paper 685
EDQWS: an enhanced divide and conquer algorithm for workflow scheduling in cloud,"A workflow is an effective way for modeling complex applications and serves as a means for scientists and researchers to better understand the details of applications. Cloud computing enables the running of workflow applications on many types of computational resources which become available on-demand. As one of the most important aspects of cloud computing, workflow scheduling needs to be performed efficiently to optimize resources. Due to the existence of various resource types at different prices, workflow scheduling has evolved into an even more challenging problem on cloud computing. The present paper proposes a workflow scheduling algorithm in the cloud to minimize the execution cost of the deadline-constrained workflow. The proposed method, EDQWS, extends the current authors’ previous study (DQWS) and is a two-step scheduler based on divide and conquer. In the first step, the workflow is divided into sub-workflows by defining, scheduling, and removing a critical path from the workflow, similar to DQWS. The process continues until only chain-structured sub-workflows, called linear graphs, remain. In the second step which is linear graph scheduling, a new merging algorithm is proposed that combines the resulting linear graphs so as to reduce the number of used instances and minimize the overall execution cost. In addition, the current work introduces a scoring function to select the most efficient instances for scheduling the linear graphs. Experiments show that EDQWS outperforms its competitors, both in terms of minimizing the monetary costs of executing scheduled workflows and meeting user-defined deadlines. Furthermore, in more than 50% of the examined workflow samples, EDQWS succeeds in reducing the number of resource instances compared to the previously introduced DQWS method.",CS,AI_ML,0.85,Extracted from log - paper 686
Endpoint Communication Contention-Aware Cloud Workflow Scheduling,"Cloud platforms have recently become a popular target execution environment for numerous workflow applications. Hence, effective workflow scheduling strategies in cloud environments are in high demand. However, existing scheduling algorithms are grounded on an idealized target platform model where virtual machines are fully connected, and all communications can be performed concurrently. A significant aspect neglected by them is endpoint communication contention when executing workflows, which has a large impact on workflow makespan. This article investigates how to incorporate contention awareness into cloud workflow scheduling and proposes a new practical scheduling model. Endpoint communication contention-aware List Scheduling Heuristic (ELSH) is designed to minimize workflow makespan. It uses a novel task ranking property and schedules data communications to communication resources besides scheduling tasks to computing resources. Moreover, a rescheduling technique is employed to improve the schedule. In experiments, ELSH is evaluated against the traditional contention-oblivious list scheduling algorithm, which is adapted to address contention during execution in practice. The experimental results reveal that ELSH performs more efficaciously compared with the adapted traditional ones. Note to Practitioners—This article aims to advance the state of the art for workflow scheduling in clouds by taking into account endpoint communication contention that can occur in practice but has largely been neglected in existing investigations. A scheduling method called Endpoint communication contention-aware List Scheduling Heuristic (ELSH) is then proposed to optimize workflow makespan. Experimental results based on synthetic and realistic workflows show that ELSH performs better than traditional scheduling algorithms that fail to consider endpoint communication contention, especially for the workflow with a large communication-to-computation-cost ratio. The proposed approach can be readily put into use and help cloud service providers to offer their customers high-quality services when executing the latter’s workflows.",CS,AI_ML,0.85,Extracted from log - paper 687
Multi-Swarm PSO Algorithm for Static Workflow Scheduling in Cloud-Fog Environments,"Scientific workflow scheduling involves the allocation of workflow tasks to particular computational resources. The generation of optimal solutions to reduce run-time, cost, and energy consumption, as well as ensuring proper load balancing, remains a major challenge. Therefore, this work presents a Multi-Swarm Particle Swarm Optimization (MS-PSO) algorithm to improve the scheduling of scientific workflows in cloud-fog environments. MS-PSO seeks to address the canonical PSO’s problem of premature convergence, which leads it to suboptimal solutions. In MS-PSO, particles are divided into several swarms, with each swarm having its own cognitive and social learning coefficients. This work also develops a weighted sum objective function for the workflow scheduling problem, based on four objectives: makespan, cost, energy and load balancing for cloud and fog tiers. The FogWorkflowSim Toolkit is used in the evaluation process, with the objectives serving as performance metrics. The MS-PSO approach is compared with the canonical PSO, Genetic Algorithm (GA), Differential Evolution (DE) and GA-PSO. The following scientific workflows are used in the simulations: Montage, Cybershake, Epigenomics, LIGO and SIPHT. MS-PSO outperforms the canonical PSO on all scientific workflows and under all performance metrics. It competes fairly well against the other approaches and it is more stable and reliable. It only ranks second to PSO, in terms of execution time. In future, multiple species, incorporating population update mechanisms from several algorithmic frameworks (MS-PSO, DE, GA), will be used for scientific workflow scheduling. Hybdridization of the realized algorithm with dynamic approaches will also be investigated.",CS,AI_ML,0.85,Extracted from log - paper 688
Chaotic-Nondominated-Sorting Owl Search Algorithm for Energy-Aware Multi-Workflow Scheduling in Hybrid Clouds,"Since a single private cloud cannot satisfy the increasing computational requirements for executing multiple workflows simultaneously, hybrid clouds are often adopted to perform such execution. Multi-workflow scheduling is challenging as users may request various applications with different QoS requirements. This work proposes a Chaotic-nondominated-sorting Owl Search Algorithm (COSA) by combining an Owl Search Algorithm (OSA) with a Nondominated Sorting Genetic Algorithm II (NSGA-II) to schedule resource-constrained multiple workflows in hybrid clouds with makespan, cost and energy consumption minimized under the given deadline and budget constraints. First, a hierarchical evolving mechanism is designed to update the better half and worse half of population by NSGA-II and OSA, respectively to guarantee a good trade-off between exploration and exploitation. Second, a chaotic sequence is introduced to adaptively adjust OSA's step size during population evolution for better exploration. Third, we adopt a chaotic operator for searching around the resulting Non-Dominated Solutions (NDS) to improve COSA's local search ability. Experiments are conducted to compare COSA with four peers and the results show its superiority in the number of obtained NDS, diversity preservation and convergence towards the near optimal Pareto set. In particular, it can find at least 19% more NDS than its peers.",CS,AI_ML,0.85,Extracted from log - paper 689
An Effective Cloud Workflow Scheduling Approach Combining PSO and Idle Time Slot-Aware Rules,"Workflow scheduling is a key issue and remains a challenging problem in cloud computing. Faced with the large number of virtual machine (VM) types offered by cloud providers, cloud users need to choose the most appropriate VM type for each task. Multiple task scheduling sequences exist in a workflow application. Different task scheduling sequences have a significant impact on the scheduling performance. It is not easy to determine the most appropriate set of VM types for tasks and the best task scheduling sequence. Besides, the idle time slots on VM instances should be used fully to increase resources' utilization and save the execution cost of a workflow. This paper considers these three aspects simultaneously and proposes a cloud workflow scheduling approach which combines particle swarm optimization (PSO) and idle time slot-aware rules, to minimize the execution cost of a workflow application under a deadline constraint. A new particle encoding is devised to represent the VM type required by each task and the scheduling sequence of tasks. An idle time slot-aware decoding procedure is proposed to decode a particle into a scheduling solution. To handle tasks' invalid priorities caused by the randomness of PSO, a repair method is used to repair those priorities to produce valid task scheduling sequences. The proposed approach is compared with state-of-the-art cloud workflow scheduling algorithms. Experiments show that the proposed approach outperforms the comparative algorithms in terms of both of the execution cost and the success rate in meeting the deadline.",CS,AI_ML,0.85,Extracted from log - paper 690
Real-Time Multiple-Workflow Scheduling in Cloud Environments,"With the development of cloud computing, an increasing number of applications in different fields have been deployed to the cloud. In this process, the real-time scheduling of multiple workflows composed of tasks from these different applications must consider various influencing factors that strongly affect scheduling performance. This paper proposes a real-time multiple-workflow scheduling (RMWS) scheme to schedule workflows dynamically with minimum cost under different deadline constraints. Due to the uncertainty of workflow arrival time and specification, RMWS dynamically allocates tasks and divides the scheduling process into three stages. First, when a new workflow arrives, the latest start time and the latest finish time of each task are calculated according to the deadline, and the subdeadline of each task is obtained by probabilistic upward ranking. Then, each ready task is allocated according to its subdeadline and the increased cost of the virtual machine (VM). Meanwhile, only one waiting task can be assigned to each VM to reduce delay fluctuations. Finally, when the task is completed on the assigned VM, all the parameters of the relevant tasks are updated before allocating them to appropriate VMs. The experimental results based on four real-world workflow traces show that the proposed algorithm is superior to two state-of-the-art algorithms in terms of total rental cost, resource utilization, success rate and deadline deviation under different conditions.",CS,AI_ML,0.85,Extracted from log - paper 691
Dynamic Group Learning Distributed Particle Swarm Optimization for Large-Scale Optimization and Its Application in Cloud Workflow Scheduling,"Cloud workflow scheduling is a significant topic in both commercial and industrial applications. However, the growing scale of workflow has made such a scheduling problem increasingly challenging. Many current algorithms often deal with small- or medium-scale problems (e.g., less than 1000 tasks) and face difficulties in providing satisfactory solutions when dealing with the large-scale problems, due to the curse of dimensionality. To this aim, this article proposes a dynamic group learning distributed particle swarm optimization (DGLDPSO) for large-scale optimization and extends it for the large-scale cloud workflow scheduling. DGLDPSO is efficient for large-scale optimization due to its following two advantages. First, the entire population is divided into many groups, and these groups are coevolved by using the master-slave multigroup distributed model, forming a distributed PSO (DPSO) to enhance the algorithm diversity. Second, a dynamic group learning (DGL) strategy is adopted for DPSO to balance diversity and convergence. When applied DGLDPSO into the large-scale cloud workflow scheduling, an adaptive renumber strategy (ARS) is further developed to make solutions relate to the resource characteristic and to make the searching behavior meaningful rather than aimless. Experiments are conducted on the large-scale benchmark functions set and the large-scale cloud workflow scheduling instances to further investigate the performance of DGLDPSO. The comparison results show that DGLDPSO is better than or at least comparable to other state-of-the-art large-scale optimization algorithms and workflow scheduling algorithms.",CS,AI_ML,0.85,Extracted from log - paper 692
GRP-HEFT: A Budget-Constrained Resource Provisioning Scheme for Workflow Scheduling in IaaS Clouds,"In Infrastructure as a Service (IaaS) Clouds, users are charged to utilize cloud services according to a pay-per-use model. If users intend to run their workflow applications on cloud resources within a specific budget, they have to adjust their demands for cloud resources with respect to this budget. Although several scheduling approaches have introduced solutions to optimize the makespan of workflows on a set of heterogeneous IaaS cloud resources within a certain budget, the hourly-based cost model of some well-known cloud providers (e.g., Amazon EC2 Cloud) can easily lead to a higher makespan and some schedulers may not find any feasible solution. In this article, we propose a novel resource provisioning mechanism and a workflow scheduling algorithm, named Greedy Resource Provisioning and modified HEFT (GRP-HEFT), for minimizing the makespan of a given workflow subject to a budget constraint for the hourly-based cost model of modern IaaS clouds. As a resource provisioning mechanism, we propose a greedy algorithm which lists the instance types according to their efficiency rate. For our scheduler, we modified the HEFT algorithm to consider a budget limit. GRP-HEFT is compared against state-of-the-art workflow scheduling techniques, including MOACS (Multi-Objective Ant Colony System), PSO (Particle Swarm Optimization), and GA (Genetic Algorithm). The experimental results demonstrate that GRP-HEFT outperforms GA, PSO, and MOACS for several well-known scientific workflow applications for different problem sizes on average by 13.64, 19.77, and 11.69 percent, respectively. Also in terms of time complexity, GRP-HEFT outperforms GA, PSO and MOACS.",CS,AI_ML,0.85,Extracted from log - paper 693
An Intelligent Cloud Workflow Scheduling System With Time Estimation and Adaptive Ant Colony Optimization,"The introduction of workflow in cloud computing has afforded a new and efficient way to tackle large-scale applications. As an NP-hard problem, how to schedule cloud workflows effectively and economically with deadline constraints and different kinds of tasks and resources is extraordinarily challenging. To solve this constrained problem, this paper intends to develop an intelligent scheduling system from the perspective of users to reduce expenditure of workflow, subject to the deadline and other execution constraints. A new estimation model of the task execution time is designed according to virtual machine settings in real public clouds and execution data from practical workflows. Based on the new model, an adaptive ant colony optimization algorithm is proposed to meet the quality of service and orchestrate tasks. The adaptiveness of the algorithm is embodied in two aspects. First, an adaptive solution construction method is designed that each solution is built with a dynamically changing resource pool, thus the search space of the algorithm is narrowed down and the execution time is decreased. Second, two heuristics with self-adaptive weight are introduced to adaptively meet different deadline settings. Simulating results on four types of workflows show that the proposed approach is effective and competitive.",CS,AI_ML,0.85,Extracted from log - paper 694
IPPTS: An Efficient Algorithm for Scientific Workflow Scheduling in Heterogeneous Computing Systems,"Efficient scheduling algorithms are key for attaining high performance in heterogeneous computing systems. In this article, we propose a new list scheduling algorithm for assigning task graphs to fully connected heterogeneous processors with an aim to minimize the scheduling length. The proposed algorithm, called Improved Predict Priority Task Scheduling (IPPTS) algorithm has two phases: task prioritization phase, which gives priority to tasks, and processor selection phase, which selects a processor for a task. The IPPTS algorithm has a quadratic time complexity as the related algorithms for the same goal, that is <inline-formula><tex-math notation=""LaTeX"">$O(t^{2} \times p)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>×</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""djigal-ieq1-3041829.gif""/></alternatives></inline-formula>, for <inline-formula><tex-math notation=""LaTeX"">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=""djigal-ieq2-3041829.gif""/></alternatives></inline-formula> tasks and <inline-formula><tex-math notation=""LaTeX"">$p$</tex-math><alternatives><mml:math><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href=""djigal-ieq3-3041829.gif""/></alternatives></inline-formula> processors. Our algorithm reduces the scheduling length significantly by looking ahead in both task prioritization phase and processor selection phase. In this way, the algorithm is looking ahead to schedule a task and its heaviest successor task to the optimistic processor, i.e., the processor that minimizes their computation and communication costs. The experiments based on both randomly generated graphs and graphs of real-world applications show that the IPPTS algorithm significantly outperforms previous list scheduling algorithms in terms of makespan, speedup, makespan standard deviation, efficiency, and frequency of best results.",CS,AI_ML,0.85,Extracted from log - paper 695
Bio-Inspired Workflow Scheduling on HPC Platforms,"Efficient scheduling of tasks in workflows of cloud or grid applications is a key to achieving better utilization of resources as well as timely completion of the user jobs. Many scientific applications comprise several tasks that are dependent in nature and are specified by workflow graphs. The aim of the cloud meta-scheduler is to schedule the user application tasks (and the applications) so as to optimize the resource utilization and to execute the user applications in minimum amount of time. During the past decade, there have been several attempts to use bio-inspired scheduling algorithms to obtain an optimal or near optimal schedule in order to minimize the overall schedule length and to optimize the use of resources. However, as the number of tasks increases, the solution space comprising different tasks-resource mapping sequences increases exponentially. Hence, there is a need to devise mechanisms to improvise the search strategies of the bio-inspired scheduling algorithms for better scheduling solutions in lesser number of iterations/time. The objective of the research work in this paper is to use bio-inspired bacteria foraging optimization algorithm (BFOA) along with other heuristics algorithms for better search of the scheduling solution space for multiple workflows. The idea is to first find a schedule by the heuristic algorithms such as MaxMin, MinMin, and Myopic, and use these as initial solutions (along with other randomly generated solutions) in the search space to get better solutions using BFOA. The performance of our approach with the existing approaches is compared for quality of the scheduling solutions. The results demonstrate that our hybrid approach (MinMin/Myopic with BFOA) outperforms other approaches.",CS,AI_ML,0.85,Extracted from log - paper 696
IoT Workflow Scheduling Using Intelligent Arithmetic Optimization Algorithm in Fog Computing,"Instead of the cloud, the Internet of things (IoT) activities are offloaded into fog computing to boost the quality of services (QoSs) needed by many applications. However, the availability of continuous computing resources on fog computing servers is one of the restrictions for IoT applications since transmitting the large amount of data generated using IoT devices would create network traffic and cause an increase in computational overhead. Therefore, task scheduling is the main problem that needs to be solved efficiently. This study proposes an energy-aware model using an enhanced arithmetic optimization algorithm (AOA) method called AOAM, which addresses fog computing's job scheduling problem to maximize users' QoSs by maximizing the makespan measure. In the proposed AOAM, we enhanced the conventional AOA searchability using the marine predators algorithm (MPA) search operators to address the diversity of the used solutions and local optimum problems. The proposed AOAM is validated using several parameters, including various clients, data centers, hosts, virtual machines, tasks, and standard evaluation measures, including the energy and makespan. The obtained results are compared with other state-of-the-art methods; it showed that AOAM is promising and solved task scheduling effectively compared with the other comparative methods.",CS,AI_ML,0.85,Extracted from log - paper 697
Energy-Efficient Load Balancing Algorithm for Workflow Scheduling in Cloud Data Centers Using Queuing and Thresholds,"Cloud computing is a rapidly growing technology that has been implemented in various fields in recent years, such as business, research, industry, and computing. Cloud computing provides different services over the internet, thus eliminating the need for personalized hardware and other resources. Cloud computing environments face some challenges in terms of resource utilization, energy efficiency, heterogeneous resources, etc. Tasks scheduling and virtual machines (VMs) are used as consolidation techniques in order to tackle these issues. Tasks scheduling has been extensively studied in the literature. The problem has been studied with different parameters and objectives. In this article, we address the problem of energy consumption and efficient resource utilization in virtualized cloud data centers. The proposed algorithm is based on task classification and thresholds for efficient scheduling and better resource utilization. In the first phase, workflow tasks are pre-processed to avoid bottlenecks by placing tasks with more dependencies and long execution times in separate queues. In the next step, tasks are classified based on the intensities of the required resources. Finally, Particle Swarm Optimization (PSO) is used to select the best schedules. Experiments were performed to validate the proposed technique. Comparative results obtained on benchmark datasets are presented. The results show the effectiveness of the proposed algorithm over that of the other algorithms to which it was compared in terms of energy consumption, makespan, and load balancing.",CS,AI_ML,0.85,Extracted from log - paper 698
RETRACTED: Reinforcement learning-based controller for adaptive workflow scheduling in multi-tenant cloud computing,"Multi-tenancy is an essential feature in cloud computing and is a major component to achieve scalability and energy-efficient solution to gain high level of economic benefits. As the cloud, computing is gaining more audiences and high user base, the problem of scheduling the computational workflow for multi-tenant cloud scheduling is becoming a difficult task to achieve. In this study, we present a learning-based scheduler for catering heterogeneous software and hardware resources in the context of multi-tenant cloud computing. The experimentation has been carried out with the help of green cloud simulator and the results are compared with the state of the art techniques like minimum completion time, first come first serve and backfilling. The experimental results exhibit that the presented algorithm provides an effective means of utilizing cloud resources in addition with drastic reduction in cost of operation.",CS,AI_ML,0.85,Extracted from log - paper 699
Multi-Objective Workflow Scheduling With Deep-Q-Network-Based Multi-Agent Reinforcement Learning,"Cloud Computing provides an effective platform for executing large-scale and complex workflow applications with a pay-as-you-go model. Nevertheless, various challenges, especially its optimal scheduling for multiple conflicting objectives, are yet to be addressed properly. The existing multi-objective workflow scheduling approaches are still limited in many ways, e.g., encoding is restricted by prior experts’ knowledge when handling a dynamic real-time problem, which strongly influences the performance of scheduling. In this paper, we apply a deep-Q-network model in a multi-agent reinforcement learning setting to guide the scheduling of multi-workflows over infrastructure-as-a-service clouds. To optimize multi-workflow completion time and user’s cost, we consider a Markov game model, which takes the number of workflow applications and heterogeneous virtual machines as state input and the maximum completion time and cost as rewards. The game model is capable of seeking for correlated equilibrium between make-span and cost criteria without prior experts’ knowledge and converges to the correlated equilibrium policy in a dynamic real-time environment. To validate our proposed approach, we conduct extensive case studies based on multiple well-known scientific workflow templates and Amazon EC2 cloud. The experimental results clearly suggest that our proposed approach outperforms traditional ones, e.g., non-dominated sorting genetic algorithm-II, multi-objective particle swarm optimization, and game-theoretic-based greedy algorithms, in terms of optimality of scheduling plans generated.",CS,AI_ML,0.85,Extracted from log - paper 700
Multiobjective Cloud Workflow Scheduling: A Multiple Populations Ant Colony System Approach,"Cloud workflow scheduling is significantly challenging due to not only the large scale of workflow but also the elasticity and heterogeneity of cloud resources. Moreover, the pricing model of clouds makes the execution time and execution cost two critical issues in the scheduling. This paper models the cloud workflow scheduling as a multiobjective optimization problem that optimizes both execution time and execution cost. A novel multiobjective ant colony system based on a co-evolutionary multiple populations for multiple objectives framework is proposed, which adopts two colonies to deal with these two objectives, respectively. Moreover, the proposed approach incorporates with the following three novel designs to efficiently deal with the multiobjective challenges: 1) a new pheromone update rule based on a set of nondominated solutions from a global archive to guide each colony to search its optimization objective sufficiently; 2) a complementary heuristic strategy to avoid a colony only focusing on its corresponding single optimization objective, cooperating with the pheromone update rule to balance the search of both objectives; and 3) an elite study strategy to improve the solution quality of the global archive to help further approach the global Pareto front. Experimental simulations are conducted on five types of real-world scientific workflows and consider the properties of Amazon EC2 cloud platform. The experimental results show that the proposed algorithm performs better than both some state-of-the-art multiobjective optimization approaches and the constrained optimization approaches.",CS,AI_ML,0.85,Extracted from log - paper 701
A Heuristics-Based Cost Model for Scientific Workflow Scheduling in Cloud,": Scientific Workflow Applications (SWFAs) can deliver collaborative tools useful to researchers in executing large and complex scientific processes. Particularly, Scientific Workflow Scheduling (SWFS) accelerates the computational procedures between the available computational resources and the dependent workflow jobs based on the researchers’ requirements. However, cost optimization is one of the SWFS challenges in handling massive and complicated tasks and requires determining an approximate (near-optimal) solution within polynomial computational time. Motivated by this, current work proposes a novel SWFS cost optimization model effective in solving this challenge. The proposed model contains three main stages: (i) scientific workflow application, (ii) targeted computational environment, and (iii) cost optimization criteria. The model has been used to optimize completion time (makespan) and overall computational cost of SWFS in cloud computing for all considered scenarios in this research context. This will ultimately reduce the cost for service consumers. At the same time, reducing the cost data-intensiveness and computational-intensiveness of SWFAs. The results reveal that the proposed cost optimization model attained an optimal Job completion time (makespan) and total computational cost for small and large sizes of the considered dataset. In contrast, hybrid and hyper-based approaches consistently achieved better results for the medium-sized dataset.",CS,AI_ML,0.85,Extracted from log - paper 702
A Knowledge-Based Adaptive Discrete Water Wave Optimization for Solving Cloud Workflow Scheduling,"Workflow scheduling in cloud environments has become a significant topic in both commercial and industrial applications. However, it is still an extraordinarily challenge to generate effective and economical scheduling schemes under the deadline constraint especially for the large scale workflow applications. To address the issue, this article investigates the cloud workflow scheduling problem with the aim of minimizing the whole cost of workflow execution whereas maintaining its execution time under a predetermined deadline. A novel knowledge-based adaptive discrete water wave optimization (KADWWO) algorithm is developed based on the problem-specific knowledge of cloud workflow scheduling. In the proposed KADWWO, a discrete propagation operator is designed based on the idle time knowledge of hourly-based cost model to adaptively explore the huge search space. The adaptive refraction operator is employed to avoid stagnation and expand the available resource pool. Meanwhile, the dynamic grouping based breaking operator is designed to exploit the excellent block structure knowledge of task allocation scheme and corresponding resource to intensify the local region and accelerate convergence. Extensive simulation experiments on the well-known scientific workflow demonstrate that the KADWWO approach outperforms several recent state-of-the-art algorithms.",CS,AI_ML,0.85,Extracted from log - paper 703
Hybrid Workflow Scheduling on Edge Cloud Computing Systems,"Internet of Things applications can be represented as workflows in which stream and batch processing are combined to accomplish data analytics objectives in many application domains such as smart home, health care, bioinformatics, astronomy, and education. The main challenge of this combination is the differentiation of service quality constraints between batch and stream computations. Stream processing is highly latency-sensitive while batch processing is more likely resource-intensive. In this work, we propose an end-to-end hybrid workflow scheduling on an edge cloud system as a two-stage framework. In the first stage, we propose a resource estimation algorithm based on a linear optimization approach, gradient descent search (GDS), and in the second stage, we propose a cluster-based provisioning and scheduling technique for hybrid workflows on heterogeneous edge cloud resources. We provide a multi-objective optimization model for execution time and monetary cost under constraints of deadline and throughput. Results demonstrate the framework performance in controlling the execution of hybrid workflows by efficiently tuning several parameters including stream arrival rate, processing throughput, and workflow complexity. In comparison to a meta-heuristics technique using Particle Swarm Optimization (PSO), the proposed scheduler provides significant improvement for large-scale hybrid workflows in terms of execution time and cost with an average of 8% and 35%, respectively.",CS,AI_ML,0.85,Extracted from log - paper 704
Cybersecurity compliance in financial institutions: A comparative analysis of global standards and regulations,"Cybersecurity is a critical concern for financial institutions worldwide, given the increasing frequency and sophistication of cyberattacks. This paper conducts a comparative analysis of global standards and regulations governing cybersecurity compliance in financial institutions. By examining the regulatory frameworks of key jurisdictions, including the United States, the European Union, and Asia-Pacific countries, this study aims to identify common trends, differences, and best practices in cybersecurity compliance. The analysis begins by outlining the regulatory landscape for cybersecurity in financial institutions, highlighting the key objectives and principles underlying these regulations. It then compares the regulatory frameworks of different regions, focusing on areas such as data protection, incident response, and risk management. By examining the specific requirements and guidelines set forth by each jurisdiction, this study identifies the strengths and weaknesses of current cybersecurity regulations and offers recommendations for enhancing compliance and resilience. One of the key findings of this study is the increasing convergence of global cybersecurity standards, driven by the interconnected nature of the financial sector and the need for harmonized regulatory approaches. While differences in regulatory frameworks still exist, particularly in areas such as data protection and breach notification, there is a growing recognition of the need for international cooperation and information sharing to combat cyber threats effectively. The study also highlights the challenges faced by financial institutions in achieving cybersecurity compliance, including resource constraints, evolving cyber threats, and the complexity of regulatory requirements. It underscores the importance of implementing robust cybersecurity measures, such as encryption, multi-factor authentication, and regular security audits, to mitigate these challenges. In conclusion, this comparative analysis provides valuable insights into the global landscape of cybersecurity compliance in financial institutions. By identifying common trends and best practices, this study aims to assist policymakers, regulators, and financial institutions in enhancing their cybersecurity posture and effectively addressing the evolving cyber threat landscape.",CS,AI_ML,0.85,Extracted from log - paper 705
From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy,"Undoubtedly, the evolution of Generative AI (GenAI) models has been the highlight of digital transformation in the year 2022. As the different GenAI models like ChatGPT and Google Bard continue to foster their complexity and capability, it’s critical to understand its consequences from a cybersecurity perspective. Several instances recently have demonstrated the use of GenAI tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. This research paper highlights the limitations, challenges, potential risks, and opportunities of GenAI in the domain of cybersecurity and privacy. The work presents the vulnerabilities of ChatGPT, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. This paper demonstrates successful example attacks like Jailbreaks, reverse psychology, and prompt injection attacks on the ChatGPT. The paper also investigates how cyber offenders can use the GenAI tools in developing cyber attacks, and explore the scenarios where ChatGPT can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. This paper then examines defense techniques and uses GenAI tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. We will also discuss the social, legal, and ethical implications of ChatGPT. In conclusion, the paper highlights open challenges and future directions to make this GenAI secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts.",CS,AI_ML,0.85,Extracted from log - paper 706
CYBERSECURITY IN BANKING: A GLOBAL PERSPECTIVE WITH A FOCUS ON NIGERIAN PRACTICES,"The paper review cybersecurity practices in banking, with a specific focus on Nigerian banks. Cybersecurity has become a paramount concern in the banking industry worldwide, given the escalating frequency and sophistication of cyber threats. This study provides an overview of the global landscape of cybersecurity in banking, with a specific focus on practices observed in Nigeria. The global banking sector is witnessing a surge in digital transformation, marked by the adoption of advanced technologies and online financial services. However, this digitization brings with it unprecedented cybersecurity challenges, ranging from data breaches and ransomware attacks to sophisticated financial fraud. Financial institutions globally are compelled to fortify their cybersecurity frameworks to protect sensitive customer information, ensure the integrity of financial transactions, and maintain trust in the digital financial ecosystem. Nigeria, as a key player in the African banking landscape, faces unique cybersecurity challenges and has developed distinct strategies to safeguard its financial institutions. This study explores the cybersecurity practices employed by Nigerian banks, considering regulatory frameworks, incident response mechanisms, and collaborative efforts with international cybersecurity entities. The analysis encompasses case studies illustrating real-world cyber threats and incidents in both global and Nigerian banking contexts. It investigates the effectiveness of cybersecurity measures implemented by Nigerian banks, shedding light on the nation's response to evolving cyber risks. Furthermore, the study delves into collaborative initiatives between Nigerian banks, regulatory bodies, and international cybersecurity organizations to enhance information sharing, threat intelligence, and collective defense mechanisms. It also explores the role of public awareness campaigns in fostering a cyber-resilient banking environment. The insights provided aim to contribute to the collective understanding of cybersecurity challenges in the global banking sector while offering a nuanced perspective on Nigerian practices. As digital financial ecosystems continue to evolve, the findings underscore the importance of ongoing adaptation, collaboration, and innovation in safeguarding the integrity and trustworthiness of banking systems, both on a global scale and within the Nigerian context. Keywords: Cybersecurity, Banking, Global Perspective, Nigerian, Cybersecurity, Artificial Intelligence.",CS,AI_ML,0.85,Extracted from log - paper 707
The integration of artificial intelligence in cybersecurity measures for sustainable finance platforms: An analysis,"This study delves into the integration of Artificial Intelligence (AI) in cybersecurity measures within smart cities, aiming to uncover both the challenges and opportunities this fusion presents. With the burgeoning reliance on interconnected digital infrastructures and the vast data ecosystems within urban environments, smart cities are increasingly susceptible to sophisticated cyber threats. Through a systematic literature review and content analysis, this research identifies the unique cybersecurity vulnerabilities faced by smart cities and evaluates how AI technologies can fortify urban cybersecurity frameworks. The methodology encompasses a comprehensive review of recent scholarly articles, industry reports, and case studies to assess the role of AI in enhancing threat detection, response, and prevention mechanisms. Key findings reveal that AI-driven cybersecurity solutions significantly enhance the resilience of smart cities against cyber threats by providing advanced analytical capabilities and real-time threat intelligence. However, the study also highlights the critical need for robust ethical and privacy considerations in the deployment of AI technologies. Strategic recommendations are provided for policymakers, urban planners, and technology leaders, emphasizing the importance of integrating secure AI-enabled infrastructure and fostering public-private partnerships. The study concludes with suggestions for future research directions, focusing on the ethical implications of AI in cybersecurity and the development of scalable AI solutions for diverse urban contexts. Keywords: Artificial Intelligence, Cybersecurity, Smart Cities, Urban Resilience.",CS,AI_ML,0.85,Extracted from log - paper 708
Cybersecurity Threats Detection in Intelligent Networks using Predictive Analytics Approaches,"The modern scenario of network vulnerabilities necessitates the adoption of sophisticated detection and mitigation strategies. Predictive analytics is surfaced to be a powerful tool in the fight against cybercrime, offering unparalleled capabilities for automating tasks, analyzing vast amounts of data, and identifying complex patterns that might elude human analysts. This paper presents a comprehensive overview of how AI is transforming the field of cybersecurity. Machine intelligence can bring revolution to cybersecurity by providing advanced defense capabilities. Addressing ethical concerns, ensuring model explainability, and fostering collaboration between researchers and developers are crucial for maximizing the positive impact of AI in this critical domain.",CS,AI_ML,0.85,Extracted from log - paper 709
Developing comprehensive cybersecurity frameworks for protecting green infrastructure: Conceptual models and practical applications,"This study investigates the critical intersection of cybersecurity and green infrastructure (GI), aiming to elucidate the challenges, opportunities, and strategic approaches necessary for safeguarding these essential systems against cyber threats. Employing a systematic literature review and content analysis, the research scrutinizes peer-reviewed articles, industry reports, and regulatory publications from 2014 to 2024. The methodology focuses on identifying prevalent cybersecurity vulnerabilities within GI, the evolution of protective practices, the impact of regulatory frameworks, and the strategic implications for diverse stakeholders. Key findings reveal a complex landscape where the integration of digital technologies in GI introduces both innovative solutions and new vulnerabilities. The study highlights the pivotal role of international standards and regulatory bodies in shaping cybersecurity strategies, underscoring the necessity for a holistic approach that encompasses technological, regulatory, and human factors. Strategic recommendations advocate for interdisciplinary collaboration, enhanced regulatory frameworks, and stakeholder engagement to fortify the cybersecurity of GI. The research underscores the imperative of embedding cybersecurity into the fabric of GI planning and management. It calls for future research to explore predictive models and proactive measures, ensuring the resilience and sustainability of green infrastructure in an increasingly digitalized urban environment. This study contributes to the burgeoning discourse on securing sustainable urban systems against cyber threats, offering a foundation for further exploration and development in the field.",CS,AI_ML,0.85,Extracted from log - paper 710
CYBERSECURITY CHALLENGES IN THE AGE OF AI: THEORETICAL APPROACHES AND PRACTICAL SOLUTIONS,"In the ever-evolving landscape of cybersecurity, the proliferation of artificial intelligence (AI) technologies introduces both promising advancements and daunting challenges. This paper explores the theoretical underpinnings and practical implications of addressing cybersecurity challenges in the age of AI. With the integration of AI into various facets of digital infrastructure, including threat detection, authentication, and response mechanisms, cyber threats have become increasingly sophisticated and difficult to mitigate. Theoretical approaches delve into understanding the intricate interplay between AI algorithms, human behavior, and adversarial tactics, elucidating the underlying mechanisms of cyber attacks and defense strategies. However, this complexity also engenders novel vulnerabilities, as AI-driven attacks leverage machine learning algorithms to evade traditional security measures, posing formidable challenges to organizations across sectors. As such, practical solutions necessitate a multifaceted approach, encompassing robust threat intelligence, adaptive defense mechanisms, and ethical considerations to safeguard against AI-driven cyber threats effectively. Leveraging AI for cybersecurity defense holds promise in enhancing detection capabilities, automating response actions, and augmenting human analysts' capabilities. Yet, inherent limitations, such as algorithmic biases, data privacy concerns, and the potential for AI-enabled attacks, underscore the need for a comprehensive risk management framework. Regulatory frameworks and industry standards play a crucial role in shaping the development and deployment of AI-powered cybersecurity solutions, ensuring accountability, transparency, and compliance with ethical principles. Moreover, fostering interdisciplinary collaboration and investing in cybersecurity education and training are vital for cultivating a skilled workforce equipped to navigate the evolving threat landscape. By integrating theoretical insights with practical strategies, this paper elucidates key challenges and opportunities in securing AI-driven systems, offering insights for policymakers, researchers, and practitioners alike. Keywords: Cybersecurity; Artificial Intelligence; Threat Detection; Defense Strategies; Ethical Considerations; Regulatory Frameworks.",CS,AI_ML,0.85,Extracted from log - paper 711
A Critical Cybersecurity Analysis and Future Research Directions for the Internet of Things: A Comprehensive Review,"The emergence of the Internet of Things (IoT) technology has brought about tremendous possibilities, but at the same time, it has opened up new vulnerabilities and attack vectors that could compromise the confidentiality, integrity, and availability of connected systems. Developing a secure IoT ecosystem is a daunting challenge that requires a systematic and holistic approach to identify and mitigate potential security threats. Cybersecurity research considerations play a critical role in this regard, as they provide the foundation for designing and implementing security measures that can address emerging risks. To achieve a secure IoT ecosystem, scientists and engineers must first define rigorous security specifications that serve as the foundation for developing secure devices, chipsets, and networks. Developing such specifications requires an interdisciplinary approach that involves multiple stakeholders, including cybersecurity experts, network architects, system designers, and domain experts. The primary challenge in IoT security is ensuring the system can defend against both known and unknown attacks. To date, the IoT research community has identified several key security concerns related to the architecture of IoT systems. These concerns include issues related to connectivity, communication, and management protocols. This research paper provides an all-inclusive and lucid review of the current state of anomalies and security concepts related to the IoT. We classify and analyze prevalent security distresses regarding IoT’s layered architecture, including connectivity, communication, and management protocols. We establish the foundation of IoT security by examining the current attacks, threats, and cutting-edge solutions. Furthermore, we set security goals that will serve as the benchmark for assessing whether a solution satisfies the specific IoT use cases.",CS,AI_ML,0.85,Extracted from log - paper 712
The Role of AI in Cybersecurity: Addressing Threats in the Digital Age,"In the contemporary digital landscape, cybersecurity stands as a paramount concern due to the increasing sophistication and frequency of cyber threats. Artificial Intelligence (AI) has emerged as a potent tool in fortifying defenses against these evolving threats. This paper examines the multifaceted role of AI in cybersecurity, elucidating its applications in threat detection, vulnerability assessment, incident response, and predictive analysis. By leveraging machine learning algorithms, AI systems can swiftly analyze vast troves of data to identify anomalous patterns indicative of potential security breaches. Moreover, AI-driven technologies enable proactive defense mechanisms, empowering organizations to preemptively mitigate risks and safeguard sensitive information. However, the deployment of AI in cybersecurity also raises pertinent ethical and privacy considerations, necessitating a balanced approach towards its implementation. Through a comprehensive analysis, this paper underscores the imperative of integrating AI into cybersecurity frameworks to effectively mitigate threats in the digital age.",CS,AI_ML,0.85,Extracted from log - paper 713
Machine learning in cybersecurity: A review of threat detection and defense mechanisms,"The cybersecurity concerns get increasingly intricate as the digital world progresses. In light of the increasing complexity of cyber threats, it is imperative to develop and implement advanced and flexible security strategies. Machine Learning (ML) has become a potent tool in strengthening cybersecurity, providing the capacity to scrutinise extensive information, recognise trends, and improve threat detection and defence methods. This paper examines the significance of ML in the field of cybersecurity, with a special emphasis on the identification of threats and the implementation of protective measures. By incorporating ML algorithms into cybersecurity frameworks, organisations may automate decision-making processes, facilitating prompt responses to ever-changing threats. The initial segment explores the terrain of cyber threats, highlighting the necessity for dynamic and aggressive security methods. Conventional solutions that rely on signatures are frequently inadequate when it comes to handling sophisticated, shape-shifting attacks. ML algorithms, in contrast, have exceptional proficiency in identifying nuanced patterns and irregularities within extensive datasets, therefore offering a more efficient method of detecting potential threats. The second section delves into several ML methodologies utilised in cybersecurity, including supervised and unsupervised learning, deep learning, and reinforcement learning. Every approach is assessed based on its suitability for threat detection, demonstrating its advantages and constraints. Furthermore, the relevance of feature engineering and data pretreatment in improving machine learning models for cybersecurity applications. The versatility of ML algorithms allows them to grow with emerging threats, making them a useful tool in the ever-changing arena of cyber warfare. The final segment focuses on real-world applications of machine learning in cybersecurity, presenting successful use cases across sectors. From anomaly detection to behavior analysis, ML algorithms contribute to the discovery of dangerous activity, lowering false positives and strengthening the overall security posture. Lastly, the paper covers the obstacles and ethical issues related to the adoption of ML in cybersecurity. Issues like as adversarial assaults, skewed datasets, and the interpretability of ML models are examined, highlighting the necessity for a holistic strategy that integrates modern technology with ethical considerations. The fusion of human expertise and machine intelligence offers a formidable defense against evolving cyber threats, paving the way for a more resilient and secure digital future.",CS,AI_ML,0.85,Extracted from log - paper 714
DATA CONFIDENTIALITY AND INTEGRITY: A REVIEW OF ACCOUNTING AND CYBERSECURITY CONTROLS IN SUPERANNUATION ORGANIZATIONS,"In an era dominated by digital transformation, superannuation organizations face unprecedented challenges in safeguarding the confidentiality and integrity of sensitive financial data. This review explores the intricate relationship between accounting practices and cybersecurity controls within the context of superannuation entities. By examining the existing literature, regulatory frameworks, and industry best practices, this paper synthesizes the key considerations essential for ensuring robust data protection. The study delves into the critical role of accounting systems in managing financial information and the subsequent implications for data confidentiality. It investigates how evolving accounting standards and practices intersect with cybersecurity protocols to fortify the integrity of financial records within superannuation organizations. The dynamic nature of cyber threats necessitates a comprehensive analysis of technological safeguards, risk management frameworks, and compliance measures to uphold data confidentiality. Furthermore, the review underscores the imperative for a multidimensional approach to cybersecurity in the superannuation sector. It discusses the integration of advanced technologies such as encryption, blockchain, and anomaly detection alongside traditional accounting controls to create a resilient defense against emerging threats. The exploration extends to the examination of employee training programs, incident response strategies, and third-party risk assessments as integral components of a comprehensive cybersecurity posture. As superannuation organizations navigate the complex landscape of data management, a holistic understanding of the interplay between accounting and cybersecurity controls becomes paramount. This review contributes to the existing body of knowledge by providing insights into the challenges and opportunities presented by the evolving technological landscape, offering practitioners and policymakers a foundation for enhancing data confidentiality and integrity in the superannuation sector. Keywords: Data Confidentiality; Accounting; Cybersecurity; Superannuation Organization; Data Integrity.",CS,AI_ML,0.85,Extracted from log - paper 715
LSTM Recurrent Neural Networks for Cybersecurity Named Entity Recognition,"The automated and timely conversion of cybersecurity information from unstructured online sources, such as blogs and articles to more formal representations has become a necessity for many applications in the domain nowadays. Named Entity Recognition (NER) is one of the early phases towards this goal. It involves the detection of the relevant domain entities, such as product, version, attack name, etc. in technical documents. Although generally considered a simple task in the information extraction field, it is quite challenging in some domains like cybersecurity because of the complex structure of its entities. The state of the art methods require time-consuming and labor intensive feature engineering that describes the properties of the entities, their context, domain knowledge, and linguistic characteristics. The model demonstrated in this paper is domain independent and does not rely on any features specific to the entities in the cybersecurity domain, hence does not require expert knowledge to perform feature engineering. The method used relies on a type of recurrent neural networks called Long Short-Term Memory (LSTM) and the Conditional Random Fields (CRFs) method. The results we obtained showed that this method outperforms the state of the art methods given an annotated corpus of a decent size.",CS,AI_ML,0.85,Extracted from log - paper 716
CYBERSECURITY DYNAMICS IN NIGERIAN BANKING: TRENDS AND STRATEGIES REVIEW,"This paper provides an in-depth review of the cybersecurity dynamics within the Nigerian banking sector, emphasizing recent trends and strategic approaches to address emerging challenges. As a review paper, it synthesizes existing literature, reports, and case studies to offer a comprehensive understanding of the current cybersecurity landscape in Nigerian banks. The focus is on identifying the predominant cyber threats, analyzing the sector's response strategies, and evaluating the effectiveness of these measures in the context of Nigeria's unique socio-economic and regulatory environment. Our analysis reveals a notable escalation in cyber threats, particularly phishing, ransomware, and insider attacks, which have been intensified by the rapid digital transformation in banking services. The review identifies key factors contributing to these challenges, such as the increasing sophistication of cybercriminals, the digital literacy gap among customers, and the evolving nature of cyber threats. It also examines the strategic responses of Nigerian banks, including the adoption of advanced security technologies, enhanced staff training, and collaboration with government and international cybersecurity bodies.The paper concludes that Nigerian banks have made significant strides in fortifying their cybersecurity defenses. However, it also highlights the need for more robust regulatory frameworks, increased customer awareness initiatives, and a shift towards more integrated and proactive cybersecurity strategies. The findings of this review underscore the critical need for continuous evolution and investment in cybersecurity measures to effectively counter the dynamic and complex nature of cyber threats in the Nigerian banking sector. Keywords: Cybersecurity, Cybersecurity Dynamics, Nigerian Banking Sector, Digital Transformation, Cyber Threats, Phishing Attacks, Ransomware, Insider Threats, Regulatory Framework, Central Bank of Nigeria (CBN), Compliance Challenges, Security Technologies, Cybersecurity Awareness, Artificial Intelligence (AI), Machine Learning (ML), Blockchain Technology.",CS,AI_ML,0.85,Extracted from log - paper 717
Large Language Models in Cybersecurity: State-of-the-Art,"The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 718
MASTERING COMPLIANCE: A COMPREHENSIVE REVIEW OF REGULATORY FRAMEWORKS IN ACCOUNTING AND CYBERSECURITY,"In the rapidly evolving landscape of business and technology, the intersection of accounting and cybersecurity has become a focal point for organizations striving to maintain integrity, security, and regulatory adherence. This paper presents a meticulous examination of regulatory frameworks governing both accounting and cybersecurity domains. The study aims to provide a comprehensive understanding of the intricate compliance landscape, offering valuable insights for practitioners, policymakers, and scholars. The investigation unfolds through a dual lens, meticulously dissecting the regulatory intricacies surrounding financial reporting in accounting and the safeguarding of digital assets in cybersecurity. A critical analysis of prominent global regulatory bodies, such as the Financial Accounting Standards Board (FASB), the International Financial Reporting Standards (IFRS), and cybersecurity standards like ISO 27001 and NIST Cybersecurity Framework, forms the cornerstone of this research. The paper delves into the historical evolution of accounting and cybersecurity regulations, identifying key milestones and paradigm shifts that have shaped the current regulatory environment. It explores the synergies and dissonances between these two critical domains, shedding light on how compliance efforts in one area may impact the other. Furthermore, the study investigates the challenges and opportunities presented by emerging technologies such as blockchain, artificial intelligence, and cloud computing in the context of regulatory compliance. By examining real-world case studies and industry best practices, this thesis provides practical insights for organizations seeking to navigate the complex terrain of compliance in an era of digital transformation. The paper offers a holistic and forward-looking perspective on the regulatory frameworks governing accounting and cybersecurity. Through its comprehensive analysis, the thesis aims to equip professionals and academics with the knowledge and tools necessary to navigate the intricate regulatory landscape, fostering a proactive and adaptive approach to compliance in the dynamic business environment. Keywords: Regulatory Frameworks, Accounting, Cybersecurity, Cloud Computing, Blockchain",CS,AI_ML,0.85,Extracted from log - paper 719
Harnessing adversarial machine learning for advanced threat detection: AI-driven strategies in cybersecurity risk assessment and fraud prevention,"The abstract is ""The rapid evolution of cyber threats necessitates innovative defenses, particularly in the domains of risk assessment and fraud detection. This paper explores the integration of Artificial Intelligence (AI) and Adversarial Machine Learning (ML) techniques as a formidable strategy against increasingly sophisticated cyber-attacks. We present a comprehensive framework that leverages AI to dynamically assess cybersecurity risks and detect fraudulent activities with unprecedented accuracy and speed. Firstly, we delve into the foundational principles of adversarial machine learning, outlining how these techniques can be employed to simulate potential cyber threats, thereby enabling the development of more resilient AI-driven cybersecurity systems. We highlight the dual role of adversarial ML in both enhancing security defenses and potentially serving as a vector for sophisticated attacks, underscoring the importance of developing robust, adversarial-resistant models. Subsequently, we introduce a novel adaptive risk assessment methodology that incorporates real-time data analysis, machine learning algorithms, and predictive modeling to accurately identify and prioritize threats. This method adapts to the evolving digital landscape, ensuring that cybersecurity measures are always one step ahead of potential attackers. In the context of fraud detection, we explore -how AI algorithms can sift through vast datasets to detect anomalies and patterns indicative of fraudulent behavior. Through case studies and empirical analysis, we demonstrate the effectiveness of AI in identifying fraud across various sectors, from financial transactions to online identity verification processes. Our research contributes to the cybersecurity field by providing a detailed examination of how AI and adversarial ML can be harnessed to fortify digital defenses, improve risk assessment techniques, and enhance fraud detection capabilities. The insights garnered from this study not only advance theoretical understanding but also offer practical guidance for organizations seeking to implement AI-driven security solutions. As cyber threats continue to evolve, the integration of AI and adversarial ML in cybersecurity strategies will be paramount in safeguarding digital assets and maintaining the integrity of online systems.""",CS,AI_ML,0.85,Extracted from log - paper 720
Artificial intelligence (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI,"As Artificial Intelligence (AI) rapidly advances and integrates into various domains, cybersecurity emerges as a critical field grappling with both the benefits and pitfalls of AI technologies. This paper explores the multifaceted dimensions of AI-driven cyberattacks, offering insights into their implications, mitigation strategies, underlying motivations, and profound societal impacts. The research centres on developing and presenting the AI Cybersecurity Dimensions (AICD) Framework, a comprehensive, multidimensional schema designed to guide academics, policymakers, and industry professionals in understanding and combating the evolving challenges posed by AI-driven cyber threats. The research unveils the complex dynamics of offensive AI, stressing the need for adaptive defences and ethical considerations. Concurrently, the study highlights adversarial AI threats, calling for proactive measures to address their potential ramifications. Through rigorous textual analyses and extensive literature reviews, the paper underscores the urgency for interdisciplinary approaches to bridge the technology-humanity chasm traditionally observed in cybersecurity discussions. By synthesising these diverse elements, the AICD Framework emerges as an instrumental tool for holistic understanding and practical interventions in the AI-infused cybersecurity landscape. The paper concludes with an urgent call for collaborative efforts in research and practice to navigate the intricate challenges and capitalise on the opportunities borne from the convergence of AI and cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 721
"The future of Cybersecurity in renewable energy systems: A review, identifying challenges and proposing strategic solutions","This study provides a comprehensive examination of cybersecurity within renewable energy systems, highlighting the critical role of cybersecurity measures in ensuring the sustainability and reliability of these systems. With the increasing reliance on renewable energy sources, the need for robust cybersecurity frameworks to protect against evolving cyber threats has never been more pressing. Through a systematic literature review and content analysis, this research identifies the prevalent cyber threats and vulnerabilities specific to renewable energy infrastructures, evaluates the effectiveness of current cybersecurity measures, and explores cutting-edge technologies and practices in the field. The methodology encompasses a detailed analysis of peer-reviewed academic journals, conference proceedings, industry reports, and white papers published from 2010 to 2024. This approach facilitates the identification of gaps in current cybersecurity practices and the proposal of strategic solutions to address these challenges. Key insights reveal the significance of adopting advanced cybersecurity technologies, such as artificial intelligence and machine learning algorithms, to enhance threat detection and mitigation efforts. The study concludes with strategic recommendations for industry practitioners and policymakers, emphasizing the importance of a proactive cybersecurity posture, collaboration and information sharing, investment in cybersecurity training, and the development of specific cybersecurity standards and regulations for the renewable energy sector. Future research directions are suggested to further explore innovative cybersecurity solutions and assess their implications for renewable energy systems. This study underscores the necessity of integrating robust cybersecurity measures to safeguard the future of sustainable energy. Keywords: Cybersecurity, Renewable Energy, Cyber Threats, Vulnerabilities, Advanced Cybersecurity Technologies.",CS,AI_ML,0.85,Extracted from log - paper 722
A Situation Based Predictive Approach for Cybersecurity Intrusion Detection and Prevention Using Machine Learning and Deep Learning Algorithms in Wireless Sensor Networks of Industry 4.0,"Industry 4.0 is fundamentally based on networked systems. Real-time communication between machines, sensors, devices, and people makes it easier to transmit the data needed to make decisions. Informed decision-making is empowered by the comprehensive insights and analytics made possible by this connectedness in conjunction with information transparency. Industry 4.0-based wireless sensor networks (WSNs) are an integral part of modern industrial operations however, these networks face escalating cybersecurity threats. These networks are always vulnerable to cyber-attacks as they continuously collect data and optimize processes. Increased connections make people more susceptible to cyberattacks, necessitating the use of strong cybersecurity measures to protect sensitive data. This study proposes a predictive framework intended to intelligently prioritize and prevent cybersecurity intrusions on WSNs in Industry 4.0. The proposed framework enhances the cybersecurity of WSNs in Industry 4.0 using a multi-criteria approach. It implements machine-learning and deep-learning algorithms for cybersecurity intrusion detection in WSNs of Industry 4.0 and provides prevention by assigning priorities to the threats based on the situation and nature of the attacks. We implemented three models, i.e., Decision Tree, MLP, and Autoencoder, as proposed algorithms in the framework. For multidimensional classification and detection of cybersecurity intrusions, we implemented Decision Tree and MLP models. For binary classification and detection of cybersecurity intrusions in WSNs of Industry 4.0, we implemented Autoencoder model. Simulation results show that the Decision Tree model provides an accuracy of 99.48%, precision of 99.49%, recall of 99.48%, and F1 score of 99.49% in the detection and classification of cybersecurity intrusions. The MLP model provides an accuracy of 99.52%, precision of 99.5%, recall of 99.5%, and F1 score of 99.5% in the detection and classification of cybersecurity intrusions. The implementation of Autoencoder with binary classification yields an accuracy of 91%, a precision of 92%, a recall of 91%, and an F1 score of 91%. The benchmark models, i.e., Random Forest (RF) for multidimensional classification and Logistic Regression (LR) for binary classification, have also been implemented. We compared the performance of the benchmark models with the models implemented in the proposed framework, revealing that the models in the proposed framework",CS,AI_ML,0.85,Extracted from log - paper 723
Aligning sustainable development goals with cybersecurity strategies: Ensuring a secure and sustainable future,"This study explores the critical intersection between cybersecurity and sustainable development, aiming to understand how cybersecurity measures can support the achievement of the Sustainable Development Goals (SDGs). Employing a systematic literature review and content analysis, the research scrutinizes peer-reviewed articles, conference proceedings, and reports from international organizations, focusing on literature published from 2010 to 2024. The inclusion criteria targeted works that directly address the role of cybersecurity in sustainable development, particularly those discussing emerging technologies and their potential to enhance digital security in support of the SDGs. The exclusion criteria filtered out non-peer-reviewed articles, opinion pieces, and studies not explicitly linking cybersecurity with sustainable development efforts. Key findings highlight the indispensable role of cybersecurity in safeguarding digital infrastructure essential for achieving SDGs, emphasizing the transformative potential of innovations such as blockchain technology and artificial intelligence in enhancing cybersecurity measures. The study identifies significant challenges at the intersection of cybersecurity and sustainability, including emerging threats and the need for a global framework to integrate cybersecurity within sustainable development efforts. Strategic recommendations for stakeholders encompass fostering international cooperation, investing in cybersecurity education, and promoting inclusive cybersecurity practices. Finally, the study underscores the necessity of integrating advanced cybersecurity measures with sustainable development initiatives. Enhanced cybersecurity is pivotal for creating a secure, resilient, and sustainable digital future, thereby supporting the global pursuit of the Sustainable Development Goals.",CS,AI_ML,0.85,Extracted from log - paper 724
Best practices in cybersecurity for green building management systems: Protecting sustainable infrastructure from cyber threats,"This study explores the critical intersection of cybersecurity and sustainable infrastructure, with a focus on Green Building Management Systems (GBMS). Recognizing the increasing sophistication of cyber threats and the integration of digital technologies in sustainable buildings, this research aims to understand the challenges and prospects of cybersecurity within this context. Employing a systematic literature review and content analysis, the study examines peer-reviewed articles, conference proceedings, and industry reports from 2010 to 2024. The methodology facilitates a comprehensive understanding of the evolution, current practices, and future directions of cybersecurity measures in sustainable infrastructure. Key findings reveal that robust cybersecurity measures are foundational to protecting the digital and physical assets underpinning sustainable infrastructure. The study identifies core principles of cybersecurity, such as resilience and the integration of cybersecurity with sustainability efforts, as crucial for enhancing the security posture of GBMS. Looking ahead, the research anticipates a future where cybersecurity measures are seamlessly integrated into sustainable buildings, ensuring resilience against cyber threats while advancing sustainability goals. Strategic recommendations include adopting international standards, fostering interdisciplinary collaboration, investing in cybersecurity education, and leveraging emerging technologies. The study concludes that advancing research in cybersecurity technologies tailored for sustainable infrastructure is essential for navigating the complexities of cybersecurity in green building management.",CS,AI_ML,0.85,Extracted from log - paper 725
Artificial intelligence in cybersecurity: Protecting national infrastructure: A USA review,"Artificial Intelligence (AI) has emerged as a transformative force in the field of cybersecurity, playing a pivotal role in safeguarding national infrastructure. This review focuses on the application of AI technologies within the context of the United States, examining their efficacy in fortifying critical systems against evolving cyber threats. The paper delves into various AI-driven cybersecurity strategies, ranging from anomaly detection and predictive analysis to threat intelligence and automated response mechanisms. The integration of AI in cybersecurity not only enhances the speed and accuracy of threat detection but also addresses the dynamic nature of cyber threats. The specific AI technologies employed in the United States, including machine learning, natural language processing, and neural networks, highlighting their contributions to bolstering the resilience of national infrastructure are also examined. Furthermore, the challenges and ethical considerations associated with the widespread adoption of AI in cybersecurity are assessed. It discusses the need for robust regulatory frameworks to govern the deployment of AI in sensitive domains and emphasizes the importance of collaboration between government agencies, private enterprises, and research institutions to foster innovation and address emerging threats. In conclusion, this review provides a comprehensive analysis of the role of AI in cybersecurity within the United States, emphasizing its significance in protecting critical national infrastructure. By exploring technological advancements, challenges, and ethical considerations, this paper contributes to the ongoing discourse on leveraging AI to safeguard against the ever-evolving landscape of cyber threats.",CS,AI_ML,0.85,Extracted from log - paper 726
AI-Driven Cybersecurity: Balancing Advancements and Safeguards,"As Artificial Intelligence (AI) continues its rapid evolution, its profound influence on cybersecurity becomes increasingly evident. This study delves into the pivotal role of AI in fortifying cybersecurity measures, emphasizing its capacity for enhanced threat detection, automated response mechanisms, and the development of resilient security frameworks. However, alongside its promise, recognition of AI's susceptibility to exploitation in sophisticated cyber-attacks exists, underscoring the imperative for continual advancements in AI-driven security solutions. This research offers a nuanced perspective on AI's impact on cybersecurity, advocating for the proactive integration of AI strategies, sustained research efforts, and formulating ethical guidelines. Adopting supervised machine learning (ML) algorithms like decision trees, support vector machines, and neural networks aims to harness AI's potential to bolster cybersecurity while concurrently addressing associated risks, paving the way for a secure digital landscape. Regarding accuracy, the neural network outperforms other models by 98%.",CS,AI_ML,0.85,Extracted from log - paper 727
COMPREHENSIVE REVIEW ON CYBERSECURITY: MODERN THREATS AND ADVANCED DEFENSE STRATEGIES,"In the rapidly evolving landscape of cyberspace, the prevalence of sophisticated cyber threats has escalated, posing formidable challenges to individuals, organizations, and nations. This comprehensive review explores the contemporary panorama of cybersecurity, focusing on the latest threats and the advanced defense strategies employed to mitigate them. The analysis encompasses a wide spectrum of cyber threats, including malware, ransomware, phishing attacks, and advanced persistent threats (APTs), shedding light on their evolving tactics, techniques, and procedures. The review delves into the intricate world of cybercrime, emphasizing the motives behind attacks and the diverse range of threat actors involved, from individual hackers to state-sponsored entities. By examining recent case studies and real-world incidents, the review provides valuable insights into the dynamic nature of cyber threats, emphasizing the need for proactive and adaptive cybersecurity measures. Furthermore, the review critically evaluates cutting-edge defense mechanisms and strategies deployed to counteract these threats. It explores advancements in artificial intelligence, machine learning, and behavioral analytics, showcasing their pivotal roles in bolstering cybersecurity defenses. Additionally, the review discusses the importance of threat intelligence sharing, collaborative efforts, and international cooperation to fortify the global cyber defense ecosystem. As cybersecurity extends beyond technical measures, the review also addresses the human element, emphasizing the significance of cybersecurity awareness training and the role of employees in fortifying organizational resilience. It scrutinizes regulatory frameworks and compliance standards that play a crucial role in shaping cybersecurity policies and practices. By synthesizing the latest research, industry best practices, and expert insights, this comprehensive review aims to provide a holistic understanding of the current state of cybersecurity, offering practitioners, policymakers, and researchers a valuable resource to navigate the intricate challenges posed by modern cyber threats and to develop effective defense strategies for the digital age. Keywords: Cybersecurity, Threats, Defense Strategy, Cyber Threats, Review.",CS,AI_ML,0.85,Extracted from log - paper 728
CYBERSECURITY IN THE FINANCIAL SECTOR: A COMPARATIVE ANALYSIS OF THE USA AND NIGERIA,"This paper provides a comprehensive review and comparative analysis of cybersecurity challenges and strategies within the financial sectors of the United States of America (USA) and Nigeria. It aims to elucidate the complexities and variances in cybersecurity practices, focusing on the different approaches taken by these nations to safeguard their financial data against increasing cyber threats. Through a detailed examination of existing literature, including academic journals, industry reports, and cybersecurity incident databases, this study identifies the unique and common cybersecurity vulnerabilities, regulatory environments, and defense mechanisms employed by the financial sectors in both countries. The review reveals that the USA's financial sector benefits from advanced cybersecurity technologies and a strong regulatory framework, yet faces challenges related to sophisticated cyber-attacks and the management of insider threats. Conversely, Nigeria's financial sector grapples with issues such as limited cybersecurity awareness, technological constraints, and evolving regulatory frameworks. Despite these disparities, both countries share the necessity of enhancing their cybersecurity posture to combat the evolving nature of cyber threats effectively. Conclusively, the paper argues that addressing cybersecurity in the financial sector necessitates a comprehensive approach that includes not only technological solutions but also the strengthening of regulatory policies, enhancement of cybersecurity awareness, and fostering international collaboration. The comparative analysis underscores the importance of adopting best practices from each country's experience, aiming to bolster the resilience of financial institutions against cyber threats in an increasingly interconnected world. Keywords: Cybersecurity, Financial Sector, United States, Nigeria, Digital Infrastructure, Technological Vulnerabilities, Regulatory Complexities, Human Factors, Advanced Detection, Prevention Technologies, Machine Learning, Anomaly Detection, Cybersecurity Frameworks, Awareness, Training, Culture, Public-Private Partnerships, Threat Intelligence, Best Practices, Innovation, Regulatory Foresight, Human Capital Development, Stability, Integrity, Collaboration",CS,AI_ML,0.85,Extracted from log - paper 729
Digital Transformation and Cybersecurity Challenges for Businesses Resilience: Issues and Recommendations,"This systematic literature review explores the digital transformation (DT) and cybersecurity implications for achieving business resilience. DT involves transitioning organizational processes to IT solutions, which can result in significant changes across various aspects of an organization. However, emerging technologies such as artificial intelligence, big data and analytics, blockchain, and cloud computing drive digital transformation worldwide while increasing cybersecurity risks for businesses undergoing this process. This literature survey article highlights the importance of comprehensive knowledge of cybersecurity threats during DT implementation to prevent interruptions due to malicious activities or unauthorized access by attackers aiming at sensitive information alteration, destruction, or extortion from users. Cybersecurity is essential to DT as it protects digital assets from cyber threats. We conducted a systematic literature review using the PRISMA methodology in this research. Our literature review found that DT has increased efficiency and productivity but poses new challenges related to cybersecurity risks, such as data breaches and cyber-attacks. We conclude by discussing future vulnerabilities associated with DT implementation and provide recommendations on how organizations can mitigate these risks through effective cybersecurity measures. The paper recommends a staged cybersecurity readiness framework for business organizations to be prepared to pursue digital transformation.",CS,AI_ML,0.85,Extracted from log - paper 730
A REVIEW OF CYBERSECURITY STRATEGIES IN MODERN ORGANIZATIONS: EXAMINING THE EVOLUTION AND EFFECTIVENESS OF CYBERSECURITY MEASURES FOR DATA PROTECTION,"In an era where digital threats are increasingly pervasive, understanding the evolution and efficacy of cybersecurity strategies in modern organizations is paramount. This study provides a comprehensive analysis of the dynamic landscape of cybersecurity, exploring its progression from traditional methods to innovative, technology-driven approaches. The digital age has ushered in complex cyber threats, necessitating robust cybersecurity measures. This study examines cybersecurity strategies' historical development, current trends, and future directions across different organizational contexts and industries. The primary aim is to assess the evolution and effectiveness of cybersecurity measures, identify existing gaps, and understand the interplay between human behavior, technology, and policy in cybersecurity. The paper encompasses a comprehensive methodological framework for cybersecurity analysis, exploring the effectiveness of traditional versus modern approaches, the role of AI and ML, and the impact of international policies. It also presents case studies to illustrate successes and failures in cybersecurity implementation. Key findings reveal a significant shift towards advanced technologies like AI and ML in cybersecurity, the critical role of human factors in shaping cybersecurity outcomes, and the influence of international policies in standardizing cybersecurity practices. The study concludes that effective cybersecurity strategies require a balanced approach, combining technological advancements with understanding human factors and adherence to international standards. Recommendations include continuous education and training, adopting holistic cybersecurity strategies, and aligning with international policies. Keywords: Cybersecurity, Artificial Intelligence, Machine Learning, International Policies, Human Factors, Cyber Threats.",CS,AI_ML,0.85,Extracted from log - paper 731
Cybersecurity risks in online banking: A detailed review and preventive strategies applicatio,"In an era where the digital transformation of the banking sector intersects with the escalating complexity of cyber threats, this paper endeavors to dissect the multifaceted realm of cybersecurity within the banking industry. With a backdrop of increasing online banking adoption and the concomitant rise in cybercrime, the study aims to illuminate the current cybersecurity landscape, evaluate the efficacy of existing frameworks and propose strategic enhancements to fortify digital defenses. Employing a methodological amalgam of literature review and analysis of recent cybersecurity incidents, this investigation delves into the intricacies of cyber threats, the financial repercussions of breaches and the robustness of current cybersecurity measures in banking. The scope of this paper encompasses a comprehensive examination of recent cyber incidents, an assessment of the financial impact of cyber-attacks, an evaluation of the effectiveness of existing cybersecurity frameworks and the formulation of strategic recommendations for bolstering cybersecurity measures. Through this scholarly inquiry, key findings emerge, highlighting the critical need for dynamic cybersecurity strategies that integrate advanced technologies, promote regulatory compliance and foster a culture of cybersecurity awareness. Conclusively, the study posits that the banking sector must embrace a holistic and adaptive approach to cybersecurity, underscored by strategic investments in technology, education, and collaboration. Recommendations advocate for the integration of Big Data analytics, artificial intelligence and continuous risk assessment methodologies to navigate the evolving cyber threat landscape effectively. This paper serves as a clarion call to banking institutions, urging a reinvigorated commitment to cybersecurity resilience in safeguarding financial assets and customer trust against the backdrop of digital transformation.",CS,AI_ML,0.85,Extracted from log - paper 732
CYBERSECURITY CHALLENGES IN SMART CITIES: A CASE REVIEW OF AFRICAN METROPOLISES,"The rapid urbanization and digital transformation of cities across Africa have given rise to the concept of Smart Cities, where advanced technologies are integrated to enhance efficiency, sustainability, and the overall quality of urban life. However, this paradigm shift towards interconnected and technology-driven urban environments brings forth a host of cybersecurity challenges that demand careful consideration. This paper explores the cybersecurity challenges in Smart Cities, focusing on a case review of African metropolises. African cities, emblematic of the global urbanization trend, are embracing Smart City initiatives to address urban challenges and foster economic development. While these initiatives promise improved services and enhanced connectivity, they concurrently expose cities to a myriad of cybersecurity threats. The interconnectedness of devices and systems in Smart Cities creates a vast attack surface, making them susceptible to cyber-attacks ranging from data breaches to infrastructure disruptions. This case review delves into specific instances of cybersecurity challenges faced by African metropolises in their quest for technological advancement. It analyzes the vulnerabilities in critical infrastructure, such as energy grids, transportation systems, and healthcare networks, highlighting the potential risks associated with inadequate cybersecurity measures. Moreover, the paper sheds light on the socio-economic implications of cyber threats in Smart Cities, emphasizing the importance of resilient cybersecurity frameworks in safeguarding citizen data and urban functionality. In conclusion, the paper underscores the urgent need for comprehensive cybersecurity strategies tailored to the unique challenges faced by Smart Cities in Africa. The findings aim to contribute to a better understanding of the intricate relationship between urbanization, technology, and cybersecurity, offering insights that can inform policy decisions, technological implementations, and collaborative efforts to build secure and resilient Smart Cities in the African context. Keywords: Cybersecurity, Smart Cities, Africa, Metropolis, Review.",CS,AI_ML,0.85,Extracted from log - paper 733
CYBERSECURITY AWARENESS AND EDUCATION PROGRAMS: A REVIEW OF EMPLOYEE ENGAGEMENT AND ACCOUNTABILITY,"As organizations continue to grapple with the escalating threat landscape of cyber-attacks, the imperative to fortify their cybersecurity defenses becomes increasingly paramount. This review delves into the critical realm of cybersecurity awareness and education programs, focusing on the pivotal factors of employee engagement and accountability. The effectiveness of these programs in cultivating a cyber-resilient workforce is scrutinized through an extensive examination of existing literature, empirical studies, and industry practices. The review begins by exploring the foundational elements of cybersecurity awareness programs, elucidating the significance of imparting knowledge and instilling a culture of vigilance among employees. It examines the diverse methodologies employed in these programs, ranging from interactive workshops and simulated phishing exercises to online modules and gamified learning platforms. A comparative analysis of these approaches sheds light on their respective strengths and limitations. A central theme of this review revolves around the nexus between employee engagement and cybersecurity resilience. It delves into the psychological and behavioral aspects of engagement, assessing how motivational factors and tailored learning experiences contribute to heightened cybersecurity awareness. The impact of organizational culture and leadership support on fostering a sense of responsibility among employees is also explored, emphasizing the need for a holistic approach that transcends mere compliance. Furthermore, the review investigates the role of accountability in sustaining the efficacy of cybersecurity initiatives. It examines the mechanisms employed by organizations to enforce adherence to security policies and protocols, emphasizing the role of robust monitoring systems, clear communication channels, and consequence management. Case studies and real-world examples are integrated to illustrate instances of successful accountability frameworks and their influence on overall cybersecurity posture. This review synthesizes key findings and identifies emerging trends in cybersecurity awareness and education programs, with a particular focus on optimizing employee engagement and fostering a culture of accountability. The insights gleaned from this analysis provide a roadmap for organizations seeking to fortify their defenses against evolving cyber threats by cultivating a vigilant and proactive workforce. Keywords: Cybersecurity, Education, Cyber threat, Employee engagement, Accountability.",CS,AI_ML,0.85,Extracted from log - paper 734
Cybersecurity’s Role in Environmental Protection and Sustainable Development: Bridging Technology and Sustainability Goals,"This study investigates the pivotal role of cybersecurity in bolstering environmental protection and sustainable development, a critical yet underexplored nexus in contemporary research. Employing a systematic literature review and content analysis, the research scrutinizes peer-reviewed articles, conference proceedings, and industry reports from 2015 to 2023, sourced from databases such as IEEE Xplore, ScienceDirect, and Google Scholar. The methodology is anchored in a rigorous search strategy, leveraging keywords related to cybersecurity, sustainability, and communication technologies, and adheres to defined inclusion and exclusion criteria to ensure the relevance and quality of the literature reviewed. Key findings highlight cybersecurity as an indispensable enabler of sustainable development initiatives, safeguarding the technological infrastructure essential for environmental conservation efforts. The study identifies evolving cyber threats as a significant challenge, necessitating adaptive security measures that anticipate and mitigate potential vulnerabilities. Furthermore, it underscores the opportunities presented by advanced cybersecurity technologies, such as artificial intelligence and blockchain, in enhancing the security and efficiency of sustainable practices. Strategic recommendations emphasize the need for comprehensive cybersecurity frameworks, stakeholder collaboration, cybersecurity education, and alignment with regulatory standards to fortify the resilience of sustainability initiatives against cyber threats. The study concludes that integrating robust cybersecurity measures is paramount in the pursuit of sustainable development goals, calling for ongoing vigilance, innovation, and interdisciplinary collaboration to navigate the complex landscape of digital threats and opportunities. This research contributes valuable insights into the critical intersection of cybersecurity and sustainability, offering a foundation for future studies and strategic initiatives aimed at securing sustainable development in the digital age. Keywords: Cybersecurity, Sustainable Development, Environmental Protection, Advanced Security Technologies.",CS,AI_ML,0.85,Extracted from log - paper 735
Investigate the multifaceted dynamics of cybersecurity practices and their impact on the quality of e-government services: evidence from the KSA,"Purpose The Kingdom of Saudi Arabia (KSA) is embracing digital transformation and e-government services, aiming to improve efficiency, accessibility and citizen-centricity. Nonetheless, the country faces challenges such as evolving cyber threats. The purpose of this study is to investigate the factors influencing cybersecurity practices to ensure the reliability and security of e-government services. Design/methodology/approach This paper investigates the multifaceted dynamics of cybersecurity practices and their impact on the quality and effectiveness of e-government services. Five key factors explored include organizational culture, technology infrastructure, adherence to standards and regulations, employee training and awareness and financial investment in cybersecurity. This study used a quantitative method to gather data from 320 participants. The researcher collected 285 completed questionnaires, excluding unusable or incomplete responses, and analyzed the final data set using partial least squares structural equation modeling. Findings The findings show that financial investment in cybersecurity, employee training and awareness and adherence to cybersecurity regulations significantly influence the adoption of robust cybersecurity practices. However, the relationship between organizational culture and cybersecurity practices is less straightforward. The research establishes a strong positive correlation between cybersecurity practices and e-government service quality, highlighting the role of security in fostering public trust and user satisfaction and meeting the evolving needs of citizens and businesses. Originality/value This research contributes valuable empirical evidence to the fields of e-government and cybersecurity, offering insights that can inform evidence-based policy decisions and resource allocation. By understanding the nuanced dynamics at play, Saudi Arabia is better poised to fortify its digital governance infrastructure and provide secure, high-quality e-government services to its constituents.",CS,AI_ML,0.85,Extracted from log - paper 736
Cyber Threat Intelligence Mining for Proactive Cybersecurity Defense: A Survey and New Perspectives,"Today’s cyber attacks have become more severe and frequent, which calls for a new line of security defenses to protect against them. The dynamic nature of new-generation threats, which are evasive, resilient, and complex, makes traditional security systems based on heuristics and signatures struggle to match. Organizations aim to gather and share real-time cyber threat information and then turn it into threat intelligence for preventing attacks or, at the very least, responding quickly in a proactive manner. Cyber Threat Intelligence (CTI) mining, which uncovers, processes, and analyzes valuable information about cyber threats, is booming. However, most organizations today mainly focus on basic use cases, such as integrating threat data feeds with existing network and firewall systems, intrusion prevention systems, and Security Information and Event Management systems (SIEMs), without taking advantage of the insights that such new intelligence can deliver. In order to make the most of CTI so as to significantly strengthen security postures, we present a comprehensive review of recent research efforts on CTI mining from multiple data sources in this article. Specifically, we provide and devise a taxonomy to summarize the studies on CTI mining based on the intended purposes (i.e., cybersecurity-related entities and events, cyber attack tactics, techniques and procedures, profiles of hackers, indicators of compromise, vulnerability exploits and malware implementation, and threat hunting), along with a comprehensive review of the current state-of-the-art. Lastly, we discuss research challenges and possible future research directions for CTI mining.",CS,AI_ML,0.85,Extracted from log - paper 737
Recent Advances on Federated Learning for Cybersecurity and Cybersecurity for Federated Learning for Internet of Things,"Decentralized paradigm in the field of cybersecurity and machine learning (ML) for the emerging Internet of Things (IoT) has gained a lot of attention from the government, academia, and industries in recent years. Federated cybersecurity (FC) is regarded as a revolutionary concept to make the IoT safer and more efficient in the future. This emerging concept has the potential of detecting security threats, taking countermeasures, and limiting the spreading of threats over the IoT network system efficiently. An objective of cybersecurity is achieved by forming the federation of the learned and shared model on top of various participants. Federated learning (FL), which is regarded as a privacy-aware ML model, is particularly useful to secure the vulnerable IoT environment. In this article, we start with background and comparison of centralized learning, distributed on-site learning, and FL, which is then followed by a survey of the application of FL to cybersecurity for IoT. This survey primarily focuses on the security aspect but it also discusses several approaches that address the performance issues (e.g., accuracy, latency, resource constraint, and others) associated with FL, which may impact the security and overall performance of the IoT. To anticipate the future evolution of this new paradigm, we discuss the main ongoing research efforts, challenges, and research trends in this area. With this article, readers can have a more thorough understanding of FL for cybersecurity as well as cybersecurity for FL, different security attacks, and countermeasures.",CS,AI_ML,0.85,Extracted from log - paper 738
Cybersecurity of Smart Inverters in the Smart Grid: A Survey,"The penetration of distributed energy resources (DERs) in smart grids significantly increases the number of field devices owned and controlled by consumers, aggregators, third parties, and utilities. As the interface between DER and power grids, DER inverters are becoming smarter with various grid-support functions and communication capabilities. Meanwhile, the cybersecurity risks of smart inverters are also on the rise due to the extensive utilization of information and communication technologies. The potential negative impacts of cyberattacks on smart inverters have attracted significant attention from scholars and organizations. To advance the research on smart inverter cybersecurity and provide insights into its technical achievements, barriers, and future directions, this article will give a comprehensive review of critical attacks and defense strategies for smart inverters and inverter-based systems like microgrids. We start this survey with an overview of the smart inverter introduction, including device- and grid-level architectures, grid-support functions, and communication protocols. We then review various cyberattacks and defense strategies in different categories and scenarios tailed with discussions including their feasibility and remaining gaps. Finally, we discuss the opportunities and challenges of emerging technologies that can secure smart inverters. We hope this survey can inspire efforts to close research gaps and develop more mature cybersecurity solutions for smart inverters in the smart grid.",CS,AI_ML,0.85,Extracted from log - paper 739
Towards Artificial Intelligence-Based Cybersecurity: The Practices and ChatGPT Generated Ways to Combat Cybercrime,"Today, cybersecurity is considered one of the most noteworthy topics that are circulated frequently among companies in order to protect their data from hacking operations. The emergence of cyberspace contributed to the growth of electronic systems. It is a virtual digital space through which interconnection is established between computers and smartphones connected within the Internet of Things environment. This space is critical in building a safe digital environment free of threats and cybercrime. It is only possible to make a digital environment with the presence of cyberspace, which contains modern technologies that make this environment safe and far from unauthorized individuals. Cybersecurity has a wide range of challenges and obstacles in performance, and it is difficult for companies to face them. In this report, the most significant practices, sound, and good strategies will be studied to stop cybercrime and make a digital environment that guarantees data transfers between electronic devices safely and without the presence of malicious software. This report concluded that the procedures provided by cybersecurity are required and must be taken care of and developed.",CS,AI_ML,0.85,Extracted from log - paper 740
"Cybersecurity for Sustainable Smart Healthcare: State of the Art, Taxonomy, Mechanisms, and Essential Roles","Cutting-edge technologies have been widely employed in healthcare delivery, resulting in transformative advances and promising enhanced patient care, operational efficiency, and resource usage. However, the proliferation of networked devices and data-driven systems has created new cybersecurity threats that jeopardize the integrity, confidentiality, and availability of critical healthcare data. This review paper offers a comprehensive evaluation of the current state of cybersecurity in the context of smart healthcare, presenting a structured taxonomy of its existing cyber threats, mechanisms and essential roles. This study explored cybersecurity and smart healthcare systems (SHSs). It identified and discussed the most pressing cyber threats and attacks that SHSs face, including fake base stations, medjacking, and Sybil attacks. This study examined the security measures deployed to combat cyber threats and attacks in SHSs. These measures include cryptographic-based techniques, digital watermarking, digital steganography, and many others. Patient data protection, the prevention of data breaches, and the maintenance of SHS integrity and availability are some of the roles of cybersecurity in ensuring sustainable smart healthcare. The long-term viability of smart healthcare depends on the constant assessment of cyber risks that harm healthcare providers, patients, and professionals. This review aims to inform policymakers, healthcare practitioners, and technology stakeholders about the critical imperatives and best practices for fostering a secure and resilient smart healthcare ecosystem by synthesizing insights from multidisciplinary perspectives, such as cybersecurity, healthcare management, and sustainability research. Understanding the most recent cybersecurity measures is critical for controlling escalating cyber threats and attacks on SHSs and networks and encouraging intelligent healthcare delivery.",CS,AI_ML,0.85,Extracted from log - paper 741
Counterattacking Cyber Threats: A Framework for the Future of Cybersecurity,"Amidst the rapid advancements in the digital landscape, the convergence of digitization and cyber threats presents new challenges for organizational security. This article presents a comprehensive framework that aims to shape the future of cyber security. This framework responds to the complexities of modern cyber threats and provides guidance to organizations to enhance their resilience. The primary focus lies in the integration of capabilities with resilience. By combining these elements into cyber security practices, organizations can improve their ability to predict, mitigate, respond to, and recover from cyber disasters. This article emphasizes the importance of organizational leadership, accountability, and innovation in achieving cyber resilience. As cyber threat challenges continue to evolve, this framework offers strategic guidance to address the intricate dynamics between digitization and cyber security, moving towards a safer and more robust digital environment in the future.",CS,AI_ML,0.85,Extracted from log - paper 742
Machine Learning in Cybersecurity: Techniques and Challenges,"In the computer world, data science is the force behind the recent dramatic changes in cybersecurity's operations and technologies. The secret to making a security system automated and intelligent is to extract patterns or insights related to security incidents from cybersecurity data and construct appropriate data-driven models. Data science, also known as diverse scientific approaches, machine learning techniques, processes, and systems, is the study of actual occurrences via the use of data. Due to its distinctive qualities, such as flexibility, scalability, and the capability to quickly adapt to new and unknowable obstacles, machine learning techniques have been used in many scientific fields. Due to notable advancements in social networks, cloud and web technologies, online banking, mobile environments, smart grids, etc., cyber security is a rapidly expanding sector that requires a lot of attention. Such a broad range of computer security issues have been effectively addressed by various machine learning techniques. This article covers several machine-learning applications in cyber security. Phishing detection, network intrusion detection, keystroke dynamics authentication, cryptography, human interaction proofs, spam detection in social networks, smart meter energy consumption profiling, and security concerns with machine learning techniques themselves are all covered in this study. The methodology involves collecting a large dataset of phishing and legitimate instances, extracting relevant features such as email headers, content, and URLs, and training a machine-learning model using supervised learning algorithms. Machine learning models can effectively identify phishing emails and websites with high accuracy and low false positive rates. To enhance phishing detection, it is recommended to continuously update the training dataset to include new phishing techniques and to employ ensemble methods that combine multiple machine learning models for better performance.",CS,AI_ML,0.85,Extracted from log - paper 743
The Significance of Machine Learning and Deep Learning Techniques in Cybersecurity: A Comprehensive Review,"People in the modern era spend most of their lives in virtual environments that offer a range of public and private services and social platforms. Therefore, these environments need to be protected from cyber attackers that can steal data or disrupt systems. Cybersecurity refers to a collection of technical, organizational, and executive means for preventing the unauthorized use or misuse of electronic information and communication systems to ensure the continuity of their work, guarantee the confidentiality and privacy of personal data, and protect consumers from threats and intrusions. Accordingly, this article explores the cybersecurity practices that protect computer systems from attacks, hacking, and data thefts and investigates the role of artificial intelligence in this domain. This article also summarizes the most significant literature that explore the roles and effects of machine learning and deep learning techniques in cybersecurity. Results show that machine learning and deep learning techniques play significant roles in protecting computer systems from unauthorized entry and in controlling system penetration by predicting and understanding the behaviour and traffic of malicious software.",CS,AI_ML,0.85,Extracted from log - paper 744
ChatGPT: Exploring the Role of Cybersecurity in the Protection of Medical Information,"ChatGPT is a large language model developed by OpenAI. It is trained on a dataset of conversational text and can be used to generate human-like responses to prompts in a variety of languages and formats. It can be used for tasks such as chatbots, language translation, and text completion. The role of ChatGPT is to generate human-like text based on a given prompt or context. It can be used in a variety of applications such as chatbots, language translation, text completion, and question answering. Additionally, it can be fine-tuned for specific tasks such as generating product descriptions or summarizing articles. It can also be used to generate creative writing such as poetry and stories. It can be integrated into a wide range of industries from customer service to entertainment, to research.",CS,AI_ML,0.85,Extracted from log - paper 745
A comprehensive study on cybersecurity challenges and opportunities in the IoT world,"It has become possible to link anything and everything to the Internet in recent decades due to the expanding Internet of Things (IoT). As a result, our usage of technology has changed a lot, causing digital disruption in the real world. IoT allows drones, sensors, digital set‐top boxes, surveillance cameras, wearable technology, and medical equipment to be connected to the internet. Healthcare, manufacturing, utilities, transportation, and housing are among the various sectors that has become intelligent. Recently, we have seen a surge in cybersecurity challenges and opportunities for the improvement of various IoT applications. Although cybersecurity and the IoT are extensively researched, there is a dearth of studies that exclusively focus on the evolution of cybersecurity challenges in the area of AI and machine learning, blockchain and zero trust, lightweight security, integration of IoT with 5G networks, and many more in the IoT world. The availability of environment‐capturing sensors and internet‐connected tracking devices allows for private life surveillance and cloud data transmission. Therefore, a significant problem for researchers and developers is to ensure the CIA (Confidentiality, Integrity, and Availability) security triangle for people. This paper presents a comprehensive study of cybersecurity applications, challenges, and opportunities in the IoT world. The IoT architectural layer, attacks against the IoT layer, and related issues are highlighted. Furthermore, cybersecurity issues and challenges in IoT along with the strength and weaknesses of existing techniques are discussed in detail. Our study will provide insight into various current cybersecurity research trends in the IoT world.",CS,AI_ML,0.85,Extracted from log - paper 746
Deep Learning Based Attack Detection for Cyber-Physical System Cybersecurity: A Survey,"With the booming of cyber attacks and cyber criminals against cyber-physical systems (CPSs), detecting these attacks remains challenging. It might be the worst of times, but it might be the best of times because of opportunities brought by machine learning (ML), in particular deep learning (DL). In general, DL delivers superior performance to ML because of its layered setting and its effective algorithm for extract useful information from training data. DL models are adopted quickly to cyber attacks against CPS systems. In this survey, a holistic view of recently proposed DL solutions is provided to cyber attack detection in the CPS context. A six-step DL driven methodology is provided to summarize and analyze the surveyed literature for applying DL methods to detect cyber attacks against CPS systems. The methodology includes CPS scenario analysis, cyber attack identification, ML problem formulation, DL model customization, data acquisition for training, and performance evaluation. The reviewed works indicate great potential to detect cyber attacks against CPS through DL modules. Moreover, excellent performance is achieved partly because of several high-quality datasets that are readily available for public use. Furthermore, challenges, opportunities, and research trends are pointed out for future research.",CS,AI_ML,0.85,Extracted from log - paper 747
Current trends in AI and ML for cybersecurity: A state-of-the-art survey,"Abstract This paper provides a comprehensive survey of the state-of-the-art use of Artificial Intelligence (AI) and Machine Learning (ML) in the field of cybersecurity. The paper illuminates key applications of AI and ML in cybersecurity, while also addressing existing challenges and posing unresolved questions for future research. The paper also emphasizes the ethical and legal implications associated with their implementation. The researchers conducted a thorough survey by reviewing numerous papers and articles from respected sources such as IEEE, ACM, and Springer. Their focus centered on the latest AI and ML breakthroughs in cybersecurity, while also exploring current challenges and open research questions. The results indicate that integrating AI and ML into cybersecurity systems shows great potential for future research and development. Intrusion detection and response, malware detection, and network security are among the most promising applications identified. According to the survey, 45% of organizations have already implemented AI and ML in their cybersecurity systems, while an additional 35% plan to do so. However, 20% of organizations believe that it is not yet the right time for adopting these technologies. Overall, this paper serves as a reliable reference for researchers and practitioners in the field of cybersecurity, offering a comprehensive overview of the use of AI and ML. It not only highlights the potential applications but also addresses the challenges and research gaps. Additionally, the paper raises awareness about the ethical and legal considerations associated with leveraging AI and ML in the cybersecurity domain.",CS,AI_ML,0.85,Extracted from log - paper 748
Cybersecurity Risk Analysis of Electric Vehicles Charging Stations,"The increasing availability of Electric Vehicles (EVs) is driving a shift away from traditional gasoline-powered vehicles. Subsequently, the demand for Electric Vehicle Charging Systems (EVCS) is rising, leading to the significant growth of EVCS as public and private charging infrastructure. The cybersecurity-related risks in EVCS have significantly increased due to the growing network of EVCS. In this context, this paper presents a cybersecurity risk analysis of the network of EVCS. Firstly, the recent advancements in the EVCS network, recent EV adaptation trends, and charging use cases are described as a background of the research area. Secondly, cybersecurity aspects in EVCS have been presented considering infrastructure and protocol-centric vulnerabilities with possible cyber-attack scenarios. Thirdly, threats in EVCS have been validated with real-time data-centric analysis of EV charging sessions. The paper also highlights potential open research issues in EV cyber research as new knowledge for domain researchers and practitioners.",CS,AI_ML,0.85,Extracted from log - paper 749
"Cyberattacks in Smart Grids: Challenges and Solving the Multi-Criteria Decision-Making for Cybersecurity Options, Including Ones That Incorporate Artificial Intelligence, Using an Analytical Hierarchy Process","Smart grids have emerged as a transformative technology in the power sector, enabling efficient energy management. However, the increased reliance on digital technologies also exposes smart grids to various cybersecurity threats and attacks. This article provides a comprehensive exploration of cyberattacks and cybersecurity in smart grids, focusing on critical components and applications. It examines various cyberattack types and their implications on smart grids, backed by real-world case studies and quantitative models. To select optimal cybersecurity options, the study proposes a multi-criteria decision-making (MCDM) approach using the analytical hierarchy process (AHP). Additionally, the integration of artificial intelligence (AI) techniques in smart-grid security is examined, highlighting the potential benefits and challenges. Overall, the findings suggest that “security effectiveness” holds the highest importance, followed by “cost-effectiveness”, “scalability”, and “Integration and compatibility”, while other criteria (i.e., “performance impact”, “manageability and usability”, “compliance and regulatory requirements”, “resilience and redundancy”, “vendor support and collaboration”, and “future readiness”) contribute to the evaluation but have relatively lower weights. Alternatives such as “access control and authentication” and “security information and event management” with high weighted sums are crucial for enhancing cybersecurity in smart grids, while alternatives such as “compliance and regulatory requirements” and “encryption” have lower weighted sums but still provide value in their respective criteria. We also find that “deep learning” emerges as the most effective AI technique for enhancing cybersecurity in smart grids, followed by “hybrid approaches”, “Bayesian networks”, “swarm intelligence”, and “machine learning”, while “fuzzy logic”, “natural language processing”, “expert systems”, and “genetic algorithms” exhibit lower effectiveness in addressing smart-grid cybersecurity. The article discusses the benefits and drawbacks of MCDM-AHP, proposes enhancements for its use in smart-grid cybersecurity, and suggests exploring alternative MCDM techniques for evaluating security options in smart grids. The approach aids decision-makers in the smart-grid field to make informed cybersecurity choices and optimize resource allocation.",CS,AI_ML,0.85,Extracted from log - paper 750
Cybersecurity Challenges for Manufacturing Systems 4.0: Assessment of the Business Impact Level,"An ever-growing number of companies are moving toward the Industry 4.0 paradigm, adopting a range of advanced technologies (e.g., smart sensors, big data analytics, and cloud computing) and networking their manufacturing systems. This improves the efficiency and effectiveness of operations but also introduces new cybersecurity challenges. In this article, the impact assessment methodology is applied in the context of manufacturing systems 4.0 (also known as smart manufacturing systems, cyber manufacturing systems, or digital manufacturing systems), thus identifying the critical assets to be protected against cyber-attacks and assessing the business impacts in the case of subtractive and additive technologies. The research design of the single case study with multiple units of analysis is applied. In particular, a large company, a leader in the manufacturing of aeronautical components, is considered a representative case study, and its two main types of manufacturing cells that is, those based on networked computer numerical control machines and 3-D printers, are taken as applicative cases for the methodology. The application of the impact assessment methodology in the manufacturing context 4.0 of aeronautical components represents a useful guide for researchers in the field of cybersecurity and for companies intending to implement it in their smart manufacturing environments. In particular, based on this study, companies can define the critical manufacturing data to protect against cyber-attacks, isolate the business impacts in case of cybersecurity breaches, correlate the identified business impacts with the specific data category, and assess the level of business impacts.",CS,AI_ML,0.85,Extracted from log - paper 751
More than malware: unmasking the hidden risk of cybersecurity regulations,"Cybersecurity investments are made within a complex and ever-evolving environment, where regulatory changes represent a significant risk factor. While cybersecurity regulations aim to minimize cyber risks and enhance protection, the uncertainty arising from frequent changes or new regulations can significantly impact organizational response strategies. This paper explores the determinants and implications of regulatory risks associated with cybersecurity, aiming to provide a deeper understanding of how these risks influence strategic decision-making. The study delves into the suggestion of preventive and mitigative controls that enable businesses to adapt to and mitigate potential disruptions caused by regulatory changes, thereby preserving their established cybersecurity practices. Another key contribution of this study is the introduction of a stochastic econometric model that illustrates how regulatory risks and uncertainties can affect investment behaviors, often prompting a “wait-and-see” stance. This model synthesizes the complex relationship among investment choices, regulatory changes, and cybersecurity risks, providing insights into the dynamic nature of cybersecurity investment strategies. The research findings offer valuable guidance for risk management and strategic planning in cybersecurity investments. By comprehensively understanding the drivers and impacts of regulatory risks, businesses and policymakers can develop more effective risk evaluation and management approaches. This is essential for sustaining a strong cybersecurity posture while navigating the changing regulatory environment.",CS,AI_ML,0.85,Extracted from log - paper 752
"AI-Driven Cybersecurity: An Overview, Security Intelligence Modeling and Research Directions","Artificial intelligence (AI) is one of the key technologies of the Fourth Industrial Revolution (or Industry 4.0), which can be used for the protection of Internet-connected systems from cyber threats, attacks, damage, or unauthorized access. To intelligently solve today’s various cybersecurity issues, popular AI techniques involving machine learning and deep learning methods, the concept of natural language processing, knowledge representation and reasoning, as well as the concept of knowledge or rule-based expert systems modeling can be used. Based on these AI methods, in this paper, we present a comprehensive view on “AI-driven Cybersecurity” that can play an important role for intelligent cybersecurity services and management. The security intelligence modeling based on such AI methods can make the cybersecurity computing process automated and intelligent than the conventional security systems. We also highlight several research directions within the scope of our study, which can help researchers do future research in the area. Overall, this paper’s ultimate objective is to serve as a reference point and guidelines for cybersecurity researchers as well as industry professionals in the area, especially from an intelligent computing or AI-based technical point of view.",CS,AI_ML,0.85,Extracted from log - paper 753
Evaluating the adoption of cybersecurity and its influence on organizational performance,"Cyberattacks negatively impact the performance of enterprises all around the globe. While organizations invest more in cybersecurity to avoid cyberattacks, studies on the factors affecting their overall cybersecurity adoption and awareness are sparse. In this paper, by integrating the diffusion of innovation theory (DOI), technology acceptance model (TAM), and technology-organization-environment (TOE) with the balanced scorecard approach, we propose a comprehensive set of factors that influence cybersecurity adoption and assess the effects of these factors on organizational performance. Data are collected through a survey of IT experts in small and medium-sized enterprises (SMEs) in the United Kingdom, with 147 valid responses. Structural equation modeling based on a statistical package for the social sciences (SPSS) was used to assess the model. The findings identify and confirm the importance of eight factors affecting SMEs' cybersecurity adoption. Moreover, cybersecurity technology adoption is found to positively impacts organizational performance. The proposed framework depicts variables influencing cybersecurity technology adoption and assesses their importance. The outcomes of this study provide a basis for future research and can be adopted by IT and cybersecurity managers to identify the most appropriate cybersecurity technologies that positively impact their company's performance.",CS,AI_ML,0.85,Extracted from log - paper 754
Exploring the Top Five Evolving Threats in Cybersecurity: An In-Depth Overview,"The term cybersecurity refers to an environment capable of protecting digital devices, networks and information from unauthorized access and preventing data theft or alteration. It is composed of a collection of carefully crafted techniques, processes, and practices to protect sensitive information and deterring cyber-attacks. In the recent period, the domain of cybersecurity has undergone rapid growth in response to the increasing cyber threats. Cybersecurity includes important tactics that help protect the digital environment, which are firewalls, encryption, secure passwords, and threat detection and response systems. Employees must be trained on these tactics. This article will discuss the five most pressing challenges facing the cybersecurity industry today that must be taken into account by businesses, organizations, and individuals in order to secure their confidential data from cybercrime. The conclusion of the article highlighted the significance of growing awareness about cybersecurity risks in order to effectively handle digital environments and protect them from any electronic threats.",CS,AI_ML,0.85,Extracted from log - paper 755
Review of strategic alignment: Accounting and cybersecurity for data confidentiality and financial security,"In the contemporary landscape of rapidly evolving technological advancements and the increasing prevalence of cyber threats, organizations face a critical imperative to align their accounting practices with robust cybersecurity measures. This review explores the symbiotic relationship between accounting and cybersecurity in safeguarding data confidentiality and ensuring financial security. Focusing on the intersection of these two domains, we examine the strategic alignment required to fortify organizations against the escalating challenges posed by cyber threats to sensitive financial information. The review begins by delving into the intricate connection between accounting processes and the protection of financial data, emphasizing the pivotal role of accurate financial reporting and transparent disclosure in maintaining stakeholder trust. Subsequently, it scrutinizes the evolving threat landscape, identifying cyber risks that specifically target financial systems and data. The analysis underscores the need for a comprehensive strategic approach that integrates accounting practices with cybersecurity protocols to effectively mitigate these risks. Furthermore, the review investigates the contemporary tools and technologies that facilitate the integration of accounting and cybersecurity, enhancing organizations' ability to detect, prevent, and respond to cyber threats. It explores the adoption of advanced encryption methods, intrusion detection systems, and artificial intelligence-driven analytics to bolster data confidentiality and financial security. In examining case studies and best practices, this review highlights successful instances of organizations aligning accounting and cybersecurity strategies to achieve a cohesive defense against financial cyber threats. Lessons learned from these cases offer valuable insights for practitioners and decision-makers seeking to implement effective measures within their own organizational contexts. Ultimately, this review contributes to the evolving discourse on strategic alignment by emphasizing the imperative of synergizing accounting practices with cybersecurity initiatives. As organizations navigate an increasingly complex and interconnected business environment, a holistic approach that unifies financial integrity and cyber resilience becomes paramount for ensuring sustained success and safeguarding against the multifaceted challenges of the digital age.",CS,AI_ML,0.85,Extracted from log - paper 756
Cybersecurity of Autonomous Vehicles: A Systematic Literature Review of Adversarial Attacks and Defense Models,"Autonomous driving (AD) has developed tremendously in parallel with the ongoing development and improvement of deep learning (DL) technology. However, the uptake of artificial intelligence (AI) in AD as the core enabling technology raises serious cybersecurity issues. An enhanced attack surface has been spurred on by the rising digitization of vehicles and the integration of AI features. The performance of the autonomous vehicle (AV)-based applications is constrained by the DL models' susceptibility to adversarial attacks despite their great potential. Hence, AI-enabled AVs face numerous security threats, which prevent the large-scale adoption of AVs. Therefore, it becomes crucial to evolve existing cybersecurity practices to deal with risks associated with the increased uptake of AI. Furthermore, putting defense models into practice against adversarial attacks has grown in importance as a field of study amongst researchers. Therefore, this study seeks to provide an overview of the most recent adversarial defensive and attack models developed in the domain of AD.",CS,AI_ML,0.85,Extracted from log - paper 757
Cybersecurity knowledge graphs,"Cybersecurity knowledge graphs, which represent cyber-knowledge with a graph-based data model, provide holistic approaches for processing massive volumes of complex cybersecurity data derived from diverse sources. They can assist security analysts to obtain cyberthreat intelligence, achieve a high level of cyber-situational awareness, discover new cyber-knowledge, visualize networks, data flow, and attack paths, and understand data correlations by aggregating and fusing data. This paper reviews the most prominent graph-based data models used in this domain, along with knowledge organization systems that define concepts and properties utilized in formal cyber-knowledge representation for both background knowledge and specific expert knowledge about an actual system or attack. It is also discussed how cybersecurity knowledge graphs enable machine learning and facilitate automated reasoning over cyber-knowledge.",CS,AI_ML,0.85,Extracted from log - paper 758
The elephant in the room: cybersecurity in healthcare,"Cybersecurity has seen an increasing frequency and impact of cyberattacks and exposure of Protected Health Information (PHI). The uptake of an Electronic Medical Record (EMR), the exponential adoption of Internet of Things (IoT) devices, and the impact of the COVID-19 pandemic has increased the threat surface presented for cyberattack by the healthcare sector. Within healthcare generally and, more specifically, within anaesthesia and Intensive Care, there has been an explosion in wired and wireless devices used daily in the care of almost every patient—the Internet of Medical Things (IoMT); ventilators, anaesthetic machines, infusion pumps, pacing devices, organ support and a plethora of monitoring modalities. All of these devices, once connected to a hospital network, present another opportunity for a malevolent party to access the hospital systems, either to gain PHI for financial, political or other gain or to attack the systems directly to cause erroneous monitoring, altered settings of any device and even to access the EMR via this IoMT window. This exponential increase in the IoMT and the increasing wireless connectivity of anaesthesia and ICU devices as well as implantable devices presents a real and present danger to patient safety. There has, at the same time, been a chronic underfunding of cybersecurity in healthcare. This lack of cybersecurity investment has left the sector exposed, and with the monetisation of PHI, the introduction of technically unsecure IoT devices for monitoring and direct patient care, the healthcare sector is presenting itself for further devastating cyberattacks or breaches of PHI. Coupled with the immense strain that the COVID-19 pandemic has placed on healthcare and the changes in working patterns of many caregivers, this has further amplified the exposure of the sector to cyberattacks.",CS,AI_ML,0.85,Extracted from log - paper 759
Cybersecurity for Blockchain-Based IoT Systems: A Review,"The Internet of Things (IoT) has become a pervasive technology with various applications ranging from smart homes and cities to industrial automation and healthcare. However, the increasing adoption of IoT devices has also raised significant concerns about cybersecurity and privacy. Blockchain, as a distributed and immutable ledger technology, has been proposed as a potential solution to enhance the security and privacy of IoT systems. Blockchain-based IoT systems offer several benefits, such as decentralization, transparency, and data integrity. However, they also pose unique cybersecurity challenges that need to be addressed for their secure and reliable deployment. In this paper, we review the existing literature and highlight the key challenges in cybersecurity for blockchain-based IoT systems. We categorize these challenges into three main areas: (i) IoT device security, (ii) blockchain security, and (iii) integration of IoT devices with blockchain (network security). Through an in-depth analysis, we present the current state of research and discuss potential solutions for each challenge. Additionally, we contribute by identifying future research directions to address these challenges and enhance the cybersecurity of blockchain-based IoT systems.",CS,AI_ML,0.85,Extracted from log - paper 760
Cybersecurity data science: an overview from machine learning perspective,"In a computing context, cybersecurity is undergoing massive shifts in technology and its operations in recent days, and data science is driving the change. Extracting security incident patterns or insights from cybersecurity data and building corresponding data-driven model, is the key to make a security system automated and intelligent. To understand and analyze the actual phenomena with data, various scientific methods, machine learning techniques, processes, and systems are used, which is commonly known as data science. In this paper, we focus and briefly discuss on cybersecurity data science, where the data is being gathered from relevant cybersecurity sources, and the analytics complement the latest data-driven patterns for providing more effective security solutions. The concept of cybersecurity data science allows making the computing process more actionable and intelligent as compared to traditional ones in the domain of cybersecurity. We then discuss and summarize a number of associated research issues and future directions. Furthermore, we provide a machine learning based multi-layered framework for the purpose of cybersecurity modeling. Overall, our goal is not only to discuss cybersecurity data science and relevant methods but also to focus the applicability towards data-driven intelligent decision making for protecting the systems from cyber-attacks.",CS,AI_ML,0.85,Extracted from log - paper 761
Detecting Cybersecurity Attacks in Internet of Things Using Artificial Intelligence Methods: A Systematic Literature Review,"In recent years, technology has advanced to the fourth industrial revolution (Industry 4.0), where the Internet of things (IoTs), fog computing, computer security, and cyberattacks have evolved exponentially on a large scale. The rapid development of IoT devices and networks in various forms generate enormous amounts of data which in turn demand careful authentication and security. Artificial intelligence (AI) is considered one of the most promising methods for addressing cybersecurity threats and providing security. In this study, we present a systematic literature review (SLR) that categorize, map and survey the existing literature on AI methods used to detect cybersecurity attacks in the IoT environment. The scope of this SLR includes an in-depth investigation on most AI trending techniques in cybersecurity and state-of-art solutions. A systematic search was performed on various electronic databases (SCOPUS, Science Direct, IEEE Xplore, Web of Science, ACM, and MDPI). Out of the identified records, 80 studies published between 2016 and 2021 were selected, surveyed and carefully assessed. This review has explored deep learning (DL) and machine learning (ML) techniques used in IoT security, and their effectiveness in detecting attacks. However, several studies have proposed smart intrusion detection systems (IDS) with intelligent architectural frameworks using AI to overcome the existing security and privacy challenges. It is found that support vector machines (SVM) and random forest (RF) are among the most used methods, due to high accuracy detection another reason may be efficient memory. In addition, other methods also provide better performance such as extreme gradient boosting (XGBoost), neural networks (NN) and recurrent neural networks (RNN). This analysis also provides an insight into the AI roadmap to detect threats based on attack categories. Finally, we present recommendations for potential future investigations.",CS,AI_ML,0.85,Extracted from log - paper 762
"Federated Learning for Cybersecurity: Concepts, Challenges, and Future Directions","Federated learning (FL) is a recent development in artificial intelligence, which is typically based on the concept of decentralized data. As cyberattacks are frequently happening in the various applications deployed in real time, most industrialists are hesitating to move forward in adopting the technology of the Internet of Everything. This article aims to provide an extensive study on how FL could be utilized for providing better cybersecurity and prevent various cyberattacks in real time. We present an extensive survey of the various FL models currently developed by researchers for providing authentication, privacy, trust management, and attack detection. We also discuss few real-time use cases that have been deployed recently and how FL is adopted in them for preserving privacy of data and improving the performance of the system. Based on the study, we conclude this article with some prominent challenges and future directions on which the researchers can focus for adopting FL in real-time scenarios.",CS,AI_ML,0.85,Extracted from log - paper 763
The Role of Machine Learning in Cybersecurity,"Machine Learning (ML) represents a pivotal technology for current and future information systems, and many domains already leverage the capabilities of ML. However, deployment of ML in cybersecurity is still at an early stage, revealing a significant discrepancy between research and practice. Such a discrepancy has its root cause in the current state of the art, which does not allow us to identify the role of ML in cybersecurity. The full potential of ML will never be unleashed unless its pros and cons are understood by a broad audience. This article is the first attempt to provide a holistic understanding of the role of ML in the entire cybersecurity domain—to any potential reader with an interest in this topic. We highlight the advantages of ML with respect to human-driven detection methods, as well as the additional tasks that can be addressed by ML in cybersecurity. Moreover, we elucidate various intrinsic problems affecting real ML deployments in cybersecurity. Finally, we present how various stakeholders can contribute to future developments of ML in cybersecurity, which is essential for further progress in this field. Our contributions are complemented with two real case studies describing industrial applications of ML as defense against cyber-threats.",CS,AI_ML,0.85,Extracted from log - paper 764
Cyber risk and cybersecurity: a systematic review of data availability,"Cybercrime is estimated to have cost the global economy just under USD 1 trillion in 2020, indicating an increase of more than 50% since 2018. With the average cyber insurance claim rising from USD 145,000 in 2019 to USD 359,000 in 2020, there is a growing necessity for better cyber information sources, standardised databases, mandatory reporting and public awareness. This research analyses the extant academic and industry literature on cybersecurity and cyber risk management with a particular focus on data availability. From a preliminary search resulting in 5219 cyber peer-reviewed studies, the application of the systematic methodology resulted in 79 unique datasets. We posit that the lack of available data on cyber risk poses a serious problem for stakeholders seeking to tackle this issue. In particular, we identify a lacuna in open databases that undermine collective endeavours to better manage this set of risks. The resulting data evaluation and categorisation will support cybersecurity researchers and the insurance industry in their efforts to comprehend, metricise and manage cyber risks.",CS,AI_ML,0.85,Extracted from log - paper 765
Cybersecurity in precision agriculture: Protecting data integrity and privacy,"Precision agriculture, an innovative approach to farming that leverages data-driven technologies, has revolutionized the agricultural sector by enhancing productivity, resource efficiency, and sustainability. However, the increasing reliance on digital tools and connected devices has introduced significant cybersecurity challenges, particularly concerning data integrity and privacy. This paper explores the critical importance of cybersecurity in precision agriculture, focusing on protecting sensitive agricultural data from breaches, unauthorized access, and potential manipulation. As precision agriculture systems collect vast amounts of data through sensors, drones, and GPS-enabled devices, the risk of cyber threats has escalated. These threats can compromise the integrity of critical data, leading to inaccurate decision-making, financial losses, and disruption of agricultural operations. Moreover, the interconnected nature of precision agriculture systems makes them vulnerable to cyberattacks that could have widespread implications across the agricultural supply chain. This study examines the key cybersecurity challenges in precision agriculture, including the protection of data at rest and in transit, the safeguarding of privacy in data sharing among stakeholders, and the implementation of robust encryption and authentication mechanisms. It also highlights the importance of developing industry-specific cybersecurity standards and best practices tailored to the unique needs of the agricultural sector. Furthermore, the paper discusses the ethical considerations of data privacy in precision agriculture, emphasizing the need to balance technological advancement with the protection of farmers' and consumers' rights. The potential consequences of inadequate cybersecurity measures, such as the loss of trust in digital farming technologies and the erosion of competitive advantage, are also addressed. In conclusion, as precision agriculture continues to evolve, ensuring the integrity and privacy of agricultural data through effective cybersecurity measures is paramount. This will not only protect the agricultural sector from emerging cyber threats but also foster the sustainable growth and adoption of precision agriculture technologies. Keywords: Cybersecurity, Precision Agriculture, Data Privacy, Data Integrity, Protecting.",CS,AI_ML,0.85,Extracted from log - paper 766
Regulatory cybersecurity governance in the making: the formation of ENISA and its struggle for epistemic authority,"ABSTRACT Over the last decades, cybersecurity has become a top priority for the European Union (EU). As a contribution to scholarship on the ‘regulatory security state’, we analyze how the European Union Agency for Cybersecurity (ENISA), emerged and stabilized as the EU's key agency for cybersecurity. We use data from policy documents, secondary sources, and semi-structured interviews to show how ENISA struggled to become a relevant actor by carving out a specific role for itself. In particular, we show how challenging it was for the agency to acquire epistemic authority. Although the trajectory of ENISA supports attempts to govern through regulation, it also shows that its role was never a given, only functions as part of a larger whole, and continues to be subject to change. Our article indicates that the study of security governance must remain ontologically flexible to capture hybrid forms and political struggles.",CS,AI_ML,0.85,Extracted from log - paper 767
A Dynamic and Adaptive Cybersecurity Governance Framework,"Cybersecurity protects cyberspace from a wide range of cyber threats to reduce overall business risk, ensure business continuity, and maximize business opportunities and return on investments. Cybersecurity is well achieved by using appropriate sets of security governance frameworks. To this end, various Information Technology (IT) and cybersecurity governance frameworks have been reviewed along with their benefits and limitations. The major limitations of the reviewed frameworks are; they are complex and have complicated structures to implement, they are expensive and require high skill IT and security professionals. Moreover, the frameworks require many requirement checklists for implementation and auditing purposes and a lot of time and resources. To fill the limitations mentioned above, a simple, dynamic, and adaptive cybersecurity governance framework is proposed that provides security related strategic direction, ensures that security risks are managed appropriately, and ensures that organizations’ resources are utilized optimally. The framework incorporated different components not considered in the existing frameworks, such as research and development, public-private collaboration framework, regional and international cooperation framework, incident management, business continuity, disaster recovery frameworks, and compliance with laws and regulations. Moreover, the proposed framework identifies and includes some of the existing frameworks’ missed and overlapped components, processes, and activities. It has nine components, five activities, four outcomes, and seven processes. Performance metrics, evaluation, and monitoring techniques are also proposed. Moreover, it follows a risk based approach to address the current and future technology and threat landscapes. The design science research method was used in this research study to solve the problem mentioned. Using the design science research method, the problem was identified. Based on the problem, research objectives were articulated; the objective of this research was solved by developing a security governance framework considering different factors which were not addressed in the current works. Finally, performance metrics were proposed to evaluate the implementation of the governance framework.",CS,AI_ML,0.85,Extracted from log - paper 768
"Exploring Cybersecurity Education and Training Techniques: A Comprehensive Review of Traditional, Virtual Reality, and Augmented Reality Approaches","Considering the alarming increase in cyberattacks and their potential financial implications, the importance of cybersecurity education and training cannot be overstated. This paper presents a systematic literature review that examines different cybersecurity education and training techniques with a focus on symmetry. It primarily focuses on traditional cybersecurity education techniques and emerging technologies, such as virtual reality (VR) and augmented reality (AR), through the lens of symmetry. The main objective of this study is to explore the existing cybersecurity training techniques, identify the challenges involved, and assess the effectiveness of cybersecurity training based on VR and AR while emphasizing the concept of symmetry. Through careful selection criteria, 66 primary studies were selected from a total of 150 pertinent research studies. This article offers valuable insights into the pros and cons of conventional training approaches, explores the use of VR and AR in cybersecurity education concerning symmetry, and thoroughly discusses the challenges associated with these technologies. The findings of this review contribute significantly to the continuing efforts in cybersecurity education by offering recommendations for improving employees’ knowledge, engagement, and motivation in cybersecurity training programs while maintaining symmetry in the learning process.",CS,AI_ML,0.85,Extracted from log - paper 769
Exploring the Frontiers of Cybersecurity Behavior: A Systematic Review of Studies and Theories,"Cybersecurity procedures and policies are prevalent countermeasures for protecting organizations from cybercrimes and security incidents. Without considering human behaviors, implementing these countermeasures will remain useless. Cybersecurity behavior has gained much attention in recent years. However, a systematic review that provides extensive insights into cybersecurity behavior through different technologies and services and covers various directions in large-scale research remains lacking. Therefore, this study retrieved and analyzed 2210 articles published on cybersecurity behavior. The retrieved articles were then thoroughly examined to meet the inclusion and exclusion criteria, in which 39 studies published between 2012 and 2021 were ultimately picked for further in-depth analysis. The main findings showed that the protection motivation theory (PMT) dominated the list of theories and models examining cybersecurity behavior. Cybersecurity behavior and intention behavior counted for the highest purpose for most studies, with fewer studies focusing on cybersecurity awareness and compliance behavior. Most examined studies were conducted in individualistic contexts with limited exposure to collectivistic societies. A total of 56% of the analyzed studies focused on the organizational level, indicating that the individual level is still in its infancy stage. To address the research gaps in cybersecurity behavior at the individual level, this review proposes a number of research agendas that can be considered in future research. This review is believed to improve our understanding by revealing the full potential of cybersecurity behavior and opening the door for further research opportunities.",CS,AI_ML,0.85,Extracted from log - paper 770
Prevention of Phishing Attacks Using AI-Based Cybersecurity Awareness Training,"Machine learning has been described as an effective measure in avoiding most cyberattacks. The development of AI has therefore promoted increased security for most computer attacks. Phishing attacks are risky and can be prevented through AI-based solutions. This factor suggests the need for increased awareness of cybersecurity through AI. Developing awareness for most people will prevent these types of attacks. The research paper describes how the awareness of AI-based cybersecurity could ensure a reduction of phishing attacks. The paper, therefore, showcases the effectiveness of AI-based cybersecurity awareness training and how it may influence cyber-attacks.",CS,AI_ML,0.85,Extracted from log - paper 771
Organizational Learning from Cybersecurity Performance: Effects on Cybersecurity Investment Decisions,"IS literature has identified various economic, performance, and environmental factors affecting cybersecurity investment decisions. However, economic modeling approaches dominate, and research on cybersecurity performance as an antecedent to investments has taken a backseat. Neglecting the role of performance indicators ignores real-world concerns driving actual cybersecurity investment decision-making. We investigate two critical aspects of cybersecurity performance: breach costs and breach identification source, as antecedents to cybersecurity investment decisions. We use organizational learning to theorize how performance feedback from these two aspects of cybersecurity breaches influences subsequent investment decisions. Using firm-level data on 722 firms in the UK, we find that higher breach costs are more likely to elicit increases in cybersecurity investments. This relationship is further strengthened if a third party identifies the breach instead of the focal firm. We contribute to the literature on cybersecurity investments and incident response. The findings stress the need for firms to analyze aspects of their cybersecurity performance and use them as feedback for investment decisions, making these decisions data-driven and based on firm-specific needs.",CS,AI_ML,0.85,Extracted from log - paper 772
"Cybercompetitions: A survey of competitions, tools, and systems to support cybersecurity education","Over the last decade, industry and academia have worked towards raising students’ interests in cybersecurity through game-like competitions to fill a shortfall of cybersecurity professionals. Rising interest in video games in combination with gamification techniques make learning fun, easy, and addictive. It is crucial that cybersecurity curricula enhance and expose cybersecurity education to a diversified student body to meet workforce demands. Gamification through cybercompetitions is one method to achieve that. With a vast list of options for competition type, focus areas, learning outcomes, and participant experience levels we need to systematize knowledge of attributes that ameliorate cybercompetitions. In the wake of the COVID-19 pandemic and global lock-downs, competition hosts scrambled to move platforms from local to online infrastructure due to poor interoperability between competition software. We derive a list of takeaways including the lack of interoperability between state-of-the-art competition systems, breaking the high knowledge barrier to participate, addressing competition type diversity, then suggest potential solutions and research questions moving forward. Our paper aims to systematize cybersecurity, access control, and programming competitions by surveying the history of these events. We explore the types of competitions that have been hosted and categorize them based on focus areas related to the InfoSEC Color Wheel. We then explore state-of-the-art technologies that enable these types of competitions, and finally, present our takeaways.",CS,AI_ML,0.85,Extracted from log - paper 773
The role of data science in transforming business operations: Case studies from enterprises,"Data science has emerged as a pivotal force in transforming business operations across various industries, driving innovation, operational efficiency, and strategic decision-making. This review paper explores the multifaceted role of data science in business, examining key concepts, historical integration, and strategic advantages. It discusses the application of data science in diverse business domains, highlighting techniques such as predictive analytics, sentiment analysis, and optimization that have revolutionized marketing, supply chain management, finance, and customer service. The paper further analyzes data science's tangible and intangible benefits, including cost reduction, improved customer experience, and enhanced productivity, which collectively contribute to a competitive edge in the market. The paper reflects on emerging trends like artificial intelligence, machine learning, ethical data practices, and the integration of blockchain and IoT, which are set to shape the future of data-driven business operations. It offers recommendations for businesses to prepare for this evolving landscape, emphasizing the importance of a data-centric culture, robust infrastructure, and collaboration. The paper concludes by underscoring the critical role of data science in fostering sustained business success in an increasingly data-driven world. Keywords: Data Science, Business Transformation, Predictive Analytics, Operational Efficiency, Artificial Intelligence, Strategic Decision-Making.",CS,AI_ML,0.85,Extracted from log - paper 774
Disruptive Technologies and Operations Management in the Industry 4.0 Era and Beyond,"In the Industry 4.0 era, automation and data analytics emerge as the major forces to enhance efficiency in operations management (OM). Disruptive technologies, such as artificial intelligence, robotics, blockchain, 3D printing, 5G, Internet‐of‐Thing, digital twins, and augmented reality, are widely applied. They potentially will bring a radical change to real world operations. In this study, we first explore several major disruptive technologies, examine the corresponding OM studies, and highlight their current applications in the industry. Then, we discuss the pros and cons associated with the use of these technologies and uncover the potential human–machine conflicting areas. After that, we propose measures which may be able to achieve human–machine reconciles in the coming Industry 5.0 era. A concept of “sustainable social welfare” which includes worker welfare, privacy, etc. is proposed and the roles played by policy makers are also discussed. Finally, a future research agenda, which covers topics in both the Industry 4.0 and Industry 5.0 eras, is established.",CS,AI_ML,0.85,Extracted from log - paper 775
OM Forum - Distributed Ledgers and Operations: What Operations Management Researchers Should Know About Blockchain Technology,"Problem definition: Blockchain is a form of distributed ledger technology. While it has grown in prominence, its full potential and possible downsides are not fully understood yet, especially with respect to operations management (OM). Academic/practical relevance: This article fills this gap. Methodology: After briefly reviewing the technical foundations, we explore multiple business and policy aspects. Results: We identify five key strengths, the corresponding five main weaknesses, and three research themes of applying blockchain technology to OM. The key strengths are (1) visibility, (2) aggregation, (3) validation, (4) automation, and (5) resiliency. The corresponding weaknesses are (1) lack of privacy, (2) lack of standardization, (3) garbage in, garbage out, (4) black box effect, and (5) inefficiency. The three research themes are (1) information, (2) automation, and (3) tokenization. Managerial implications: We illustrate these research themes with multiple promising research problems, ranging from classical inventory management, to new areas of ethical OM, and to questions of industrial organization.",CS,AI_ML,0.85,Extracted from log - paper 776
"REVIEW ON THE EVOLUTION AND IMPACT OF IOT-DRIVEN PREDICTIVE MAINTENANCE: ASSESSING ADVANCEMENTS, THEIR ROLE IN ENHANCING SYSTEM LONGEVITY, AND SUSTAINABLE OPERATIONS IN BOTH MECHANICAL AND ELECTRICAL REALMS","This study provides a comprehensive review of the evolution and impact of Internet of Things (IoT)-driven predictive maintenance, focusing on advancements in technology, their role in enhancing system longevity, and promoting sustainable operations in mechanical and electrical systems. The primary objective was to assess how IoT integration has transformed traditional maintenance approaches, leading to improved system durability and reliability. Utilizing a systematic literature review methodology, the study involved sourcing data from peer-reviewed journals, conference proceedings, and industry reports. A content analysis approach was employed to analyze the data, focusing on themes such as technological advancements, sustainability considerations, and industry-specific applications of IoT in predictive maintenance. Key findings reveal significant advancements in IoT applications, particularly the integration of advanced data analytics, artificial intelligence, and machine learning in predictive maintenance strategies. These advancements have led to more accurate and timely maintenance interventions, contributing to enhanced system longevity and operational efficiency. The study also highlights the emergence of green IoT practices and the challenges and opportunities in the future landscape of IoT in predictive maintenance. The study concludes that IoT-driven predictive maintenance is pivotal for sustainable industrial operations, with opportunities lying in addressing challenges through innovative solutions and robust regulatory frameworks. Recommendations for industry and policy include fostering sustainable IoT practices and prioritizing energy efficiency. Future research directions involve exploring the integration of IoT with emerging technologies and investigating the long-term environmental impacts of IoT deployments. Keywords: Predictive Maintenance, System Longevity, Sustainable Operations, Internet of Things.",CS,AI_ML,0.85,Extracted from log - paper 777
Blockchain technology: implications for operations and supply chain management,"PurposeThis paper aims to encourage the study of blockchain technology from an operations and supply chain management (OSCM) perspective, identifying potential areas of application, and to provide an agenda for future research.Design/methodology/approachAn explanation and analysis of blockchain technology is provided to identify implications for the field of OSCM.FindingsThe hype around the opportunities that digital ledger technologies offer is high. For OSCM, a myriad of ways in which blockchain could transform practice are identified, including enhancing product safety and security; improving quality management; reducing illegal counterfeiting; improving sustainable supply chain management; advancing inventory management and replenishment; reducing the need for intermediaries; impacting new product design and development; and reducing the cost of supply chain transactions. The immature state of practice and research surrounding blockchain means there is an opportunity for OSCM researchers to study the technology in its early stages and shape its adoption.Research limitations/implicationsThe paper provides a platform for new research that addresses gaps in knowledge and advances the field of OSCM. A research agenda is developed around six key themes.Practical implicationsThere are many opportunities for organisations to obtain an advantage by making use of blockchain technology ahead of the competition, enabling them to enhance their market position. But it is important that managers examine the characteristics of their products, services and supply chains to determine whether they need or would benefit sufficiently from the adoption of blockchain. Moreover, it is important that organisations build human capital expertise that allows them to develop, implement and exploit applications of this technology to maximum reward.Originality/valueThis is one of the first papers in a leading international OSCM journal to analyse blockchain technology, thereby complementing a recent article on digital supply chains that omitted blockchain.",CS,AI_ML,0.85,Extracted from log - paper 778
Developing advanced predictive modeling techniques for optimizing business operations and reducing costs,"In today's competitive business landscape, organizations are increasingly turning to predictive modeling techniques to enhance operational efficiency and reduce costs. By leveraging data analytics, machine learning, and statistical methods, predictive models enable businesses to anticipate market trends, optimize resource allocation, and make data-driven decisions. This review explores the development of advanced predictive modeling techniques to optimize various business processes, from inventory management and supply chain optimization to customer relationship management and financial forecasting. The integration of predictive analytics into business operations can significantly reduce costs by automating workflows, minimizing waste, and enhancing accuracy in demand forecasting, key components of successful predictive modeling include robust data collection, preprocessing, and feature engineering, followed by the selection of appropriate algorithms and model evaluation. Challenges such as data quality, scalability, and ethical concerns are addressed, highlighting the need for transparency and explainable Artificial Intelligence in predictive applications. Furthermore, this review examines real-world case studies where businesses have successfully implemented predictive models to improve profitability and streamline operations. By identifying patterns and trends from historical data, these models support proactive decision-making ultimately leading to improved performance and cost savings. This concludes with insights into emerging technologies, such as deep learning and IoT, which are poised to further enhance predictive capabilities. As businesses continue to embrace digital transformation, predictive modeling will play a critical role in driving sustainable growth and competitive advantage. Keywords: Advanced Predictive Modeling, Optimizing Business Operations, Statistical Methods, Review.",CS,AI_ML,0.85,Extracted from log - paper 779
Industry 4.0: Opportunities and Challenges for Operations Management,"Industry 4.0 connotes a new industrial revolution centered around cyber-physical systems. It posits that the real-time connection of physical and digital systems, along with new enabling technologies, will change the way that work is done and therefore, how work should be managed. It has the potential to break, or at least change, the traditional operations trade-offs among the competitive priorities of cost, flexibility, speed, and quality. This article describes the technologies inherent in Industry 4.0 and the opportunities and challenges for research in this area. The focus is on goods-producing industries, which includes both the manufacturing and agricultural sectors. Specific technologies discussed include additive manufacturing, the internet of things, blockchain, advanced robotics, and artificial intelligence.",CS,AI_ML,0.85,Extracted from log - paper 780
How will artificial intelligence and Industry 4.0 emerging technologies transform operations management?,"Emerging technologies such as artificial intelligence, blockchain, additive manufacturing, advanced robotics, autonomous vehicles, and the Internet of Things are frequently mentioned as part of “Industry 4.0.” As such, how will they influence operations and supply chain management? We answer this question by providing a brief review of the evolution of technologies and operations management (OM) over time. Because terms such as “Industry 4.0” do not have a precise definition, we focus on more fundamental issues raised by Industry 4.0 emerging technologies for research in OM. We propose a theory of disruptive debottlenecking and the SACE framework by classifying emerging technologies in terms of the functionalities they enable: sense, analyze, collaborate, and execute. Subsequently, we review the nascent but rapidly growing literature at the interface between digital technologies and OM. Our review suggests that one way to assess the value of Industry 4.0 technologies can be via their influence on adding revenues, differentiating, reducing costs, optimizing risks, innovating, and transforming business models and processes. Finally, we conclude by proposing an agenda for further research.",CS,AI_ML,0.85,Extracted from log - paper 781
Unit Operations Of Chemical Engineering,"chemical engineering chemical engineering essentials for, home american journal of chemical engineering, bachelor of science in chemical engineering american, specialty polymers high performance polymers solvay, chemical engineering degrees top universities, chemical process wikipedia, perry s chemical engineers handbook eighth edition, wolfram and mathematica solutions for chemical engineering, aquatherm engineering consultants india pvt ltd edit, naval reserve officers training corps scholarship, college of engineering california state university long, unit and door heaters armstrong international, chemical engineering cput, water treatment products and services h2o engineering, journal of chemical amp engineering data acs publications, wbdg wbdg whole building design guide, chemical engineering for non chemical engineers aiche, proposed syllabus for b tech program in chemical engineering, phase out of the national diploma in chemical engineering, chemical engineering free books at ebd, operations council sgs, chemical plants india caustic soda plants chemical plants, martindale s calculators on line center mathematics, kraft recovery operations course tappi org, patent technology centers management uspto, chemical recycling makes waste plastic a resource, faculty of engineering amp technology vaal university of, chemical engineer wikipedia, index chemical engineering conferences asia events, european training network for chemical engineering, diploma of engineering curtin college, search unit standards south african qualifications authority, bcit chemical and environmental technology process, perry s chemical engineers handbook 9th edition, electrical amp systems engineering washington university, csulb chemical engineering california state university, visual encyclopedia of chemical engineeringabout gold membership gold level membership allows you full access to the chemical engineering archives dating back to 1986 quickly search and retrieve all articles and back issues, in chemical engineering process design is the design of processes for desired physical and or chemical transformation of materials process design is central to chemical engineering and it can be considered to be the summit of that field bringing together all of the fields components, at the department of chemical engineering we provide you with a challenging and contemporary chemical engineering degree program enhanced by the research accomplishments of our faculty members we value excellence in teaching quality research and service along with the intellectual development of students in a challenging rewarding academic environment, the largest selection of the highest performing polymers solvay is the industry leader in specialty polymers offering the broadest selection of high performance thermoplastic resins fluoroelastomers and fluorinated fluids, what is chemical engineering so what is chemical engineering chemical engineering is a multi disciplinary branch of engineering that combines natural and experimental sciences such as chemistry and physics along with life sciences such as biology microbiology and biochemistry plus mathematics and economics to design develop produce transform transport operate and manage the, in a scientific sense a chemical process is a method or means of somehow changing one or more chemicals or chemical compounds such a chemical process can occur by itself or be caused by an outside force and involves a chemical reaction",CS,AI_ML,0.85,Extracted from log - paper 782
Agricultural Robotics for Field Operations,"Modern agriculture is related to a revolution that occurred in a large group of technologies (e.g., informatics, sensors, navigation) within the last decades. In crop production systems, there are field operations that are quite labour-intensive either due to their complexity or because of the fact that they are connected to sensitive plants/edible product interaction, or because of the repetitiveness they require throughout a crop production cycle. These are the key factors for the development of agricultural robots. In this paper, a systematic review of the literature has been conducted on research and commercial agricultural robotics used in crop field operations. This study underlined that the most explored robotic systems were related to harvesting and weeding, while the less studied were the disease detection and seeding robots. The optimization and further development of agricultural robotics are vital, and should be evolved by producing faster processing algorithms, better communication between the robotic platforms and the implements, and advanced sensing systems.",CS,AI_ML,0.85,Extracted from log - paper 783
A hybrid meta-heuristic scheduler algorithm for optimization of workflow scheduling in cloud heterogeneous computing environment,"Purpose Improvement of workflow scheduling in distributed engineering systems Design/methodology/approach The authors proposed a hybrid meta heuristic optimization algorithm. Findings The authors have made improvement in hybrid approach by exploiting of genetic algorithm and simulated annealing plus points. Originality/value To the best of the authors’ knowledge, this paper presents a novel theorem and novel hybrid approach.",CS,AI_ML,0.85,Extracted from log - paper 784
IFFO: An Improved Fruit Fly Optimization Algorithm for Multiple Workflow Scheduling Minimizing Cost and Makespan in Cloud Computing Environments,"Cloud computing platforms have been extensively using scientific workflows to execute large-scale applications. However, multiobjective workflow scheduling with scientific standards to optimize QoS parameters is a challenging task. Various metaheuristic scheduling techniques have been proposed to satisfy the QoS parameters like makespan, cost, and resource utilization. Still, traditional metaheuristic approaches are incompetent to maintain agreeable equilibrium between exploration and exploitation of the search space because of their limitations like getting trapped in local optimum value at later evolution stages and higher-dimensional nonlinear optimization problem. This paper proposes an improved Fruit Fly Optimization (IFFO) algorithm to minimize makespan and cost for scheduling multiple workflows in the cloud computing environment. The proposed algorithm is evaluated using CloudSim for scheduling multiple workflows. The comparative results depict that the proposed algorithm IFFO outperforms FFO, PSO, and GA.",CS,AI_ML,0.85,Extracted from log - paper 785
Makespan-Driven Workflow Scheduling in Clouds Using Immune-Based PSO Algorithm,"Cloud Computing is becoming more and more popular for solving problems that need high concurrency and a lot of resources. Many traditional areas of research choose to solve their problems through the cloud, and workflow scheduling is one of them. Cloud computing brings many benefits, meanwhile, due to the almost “infinite” amount of resources for users, it also brings new challenges for scheduling and optimization, in which cost and makespan are the most concerned issues for workflow scheduling. Users want to obtain a low cost and fast makespan solution. This paper focuses on how to find an optimized solution to achieve better cost-makespan at the same time under the constraint of deadline. In order to solve this problem, an immune particle swarm optimization algorithm (IMPSO) is proposed, which effectively improves the quality and speed of the optimization. The proposed IMPSO overcomes the problem of slow convergence of PSO, which is easy to fall into local optimization. Experiments show the efficiency and effectiveness of the proposed approach.",CS,AI_ML,0.85,Extracted from log - paper 786
A Survey on QoS Requirements Based on Particle Swarm Optimization Scheduling Techniques for Workflow Scheduling in Cloud Computing,"Cloud computing is an innovative technology that deploys networks of servers, located in wide remote areas, for performing operations on a large amount of data. In cloud computing, a workflow model is used to represent different scientific and web applications. One of the main issues in this context is scheduling large workflows of tasks with scientific standards on the heterogeneous cloud environment. Other issues are particular to public cloud computing. These include the need for the user to be satisfied with the quality of service (QoS) parameters, such as scalability and reliability, as well as maximize the end-users resource utilization rate. This paper surveys scheduling algorithms based on particle swarm optimization (PSO). This is aimed at assisting users to decide on the most suitable QoS consideration for large workflows in infrastructure as a service (IaaS) cloud applications and mapping tasks to resources. Besides, the scheduling schemes are categorized according to the variant of the PSO algorithm implemented. Their objectives, characteristics, limitations and testing tools have also been highlighted. Finally, further directions for future research are identified.",CS,AI_ML,0.85,Extracted from log - paper 787
Dynamic resource provisioning for workflow scheduling under uncertainty in edge computing environment,"Edge computing, an extension of cloud computing, is introduced to provide sufficient computing and storage resources for mobile devices. Moreover, a series of computing tasks in a mobile device are set as structured computing processes and flows to achieve effective management by the workflow. However, the execution uncertainty caused by performance degradation, service failure, and new service additions remains a huge challenge to the user's service experience. In order to address the uncertainty, a software‐defined network (SDN)‐based edge computing framework and a dynamic resource provisioning (UARP) method are proposed in this paper. The UARP method is implemented in the proposed framework and addresses the uncertainty through the advantages of SDN. In addition, the nondominated sorting genetic algorithm‐III is employed to optimize two goals, that is, the energy consumption and the completion time, to obtain balanced scheduling strategies. The comparative experiments are performed and the results show that the UARP method is superior to other methods in addressing the uncertainty, while reducing energy consumption and shortening the completion time.",CS,AI_ML,0.85,Extracted from log - paper 788
A New Double Rank-based Multi-workflow Scheduling with Multi-objective Optimization in Cloud Environments,"Workflow scheduling in clouds has been extensively researched. Many workflows from different users could be submitted to clouds at the same time and cloud providers should handle them simultaneously. So, it is necessary to consider the problem of scheduling multi-workflow. In addition, cloud computing systems can offer some special features, like Pay-Per-Use and Quality of Service (QoS) over the Internet. The scheduler has to consider the tradeoffs between different QoS parameters in order to satisfy the QoS requirements. Hence, how to schedule multiple heterogeneous workflows in the meanwhile to balance multiple objectives is a big challenge. The majority of the existing multi-workflow scheduling algorithms are based on QoS constrained approaches and attempt to optimize one objective while taking other QoS factors as constraints. Meanwhile, most of the multi-objective optimization scheduling works aim to deal with single-workflow. Conversely, this paper focuses on QoS optimization approaches by finding trade-off schedules to execute multi-workflow on cloud computing resources so as to balance multi-objective. To this end, a new double rank-based task sequencing method is proposed and integrated with a multi-objective heuristic algorithm for multi-workflow scheduling. Different algorithms are evaluated using various well-known real-world workflows and simulated workflows. The performance evaluation results demonstrate that the proposed approach is capable of generating efficient schedules with high quality in terms of meeting multi-objective for multiple workflows.",CS,AI_ML,0.85,Extracted from log - paper 789
Self adaptive fruit fly algorithm for multiple workflow scheduling in cloud computing environment,"Purpose In general, cloud computing is a model of on-demand business computing that grants a convenient access to shared configurable resources on the internet. With the increment of workload and difficulty of tasks that are submitted by cloud consumers; “how to complete these tasks effectively and rapidly with limited cloud resources?” is becoming a challenging question. The major point of a task scheduling approach is to identify a trade-off among user needs and resource utilization. However, tasks that are submitted by varied users might have diverse needs of computing time, memory space, data traffic, response time, etc. This paper aims to proposes a new way of task scheduling. Design/methodology/approach To make the workflow completion in an efficient way and to reduce the cost and flow time, this paper proposes a new way of task scheduling. Here, a self-adaptive fruit fly optimization algorithm (SA-FFOA) is used for scheduling the workflow. The proposed multiple workflow scheduling model compares its efficiency over conventional methods in terms of analysis such as performance analysis, convergence analysis and statistical analysis. From the outcome of the analysis, the betterment of the proposed approach is proven with effective workflow scheduling. Findings The proposed algorithm is more superior regarding flow time with the minimum value, and the proposed model is enhanced over FFOA by 0.23%, differential evolution by 2.48%, artificial bee colony (ABC) by 2.85%, particle swarm optimization (PSO) by 2.46%, genetic algorithm (GA) by 2.33% and expected time to compute (ETC) by 2.56%. While analyzing the make span case, the proposed algorithm is 0.28%, 0.15%, 0.38%, 0.20%, 0.21% and 0.29% better than the conventional methods such as FFOA, DE, ABC, PSO, GA and ETC, respectively. Moreover, the proposed model has attained less cost, which is 2.14% better than FFOA, 2.32% better than DE, 3.53% better than ABC, 2.43% better than PSO, 2.07% better than GA and 2.90% better than ETC, respectively. Originality/value This paper presents a new way of task scheduling for making the workflow completion in an efficient way and for reducing the cost and flow time. This is the first paper uses SA-FFOA for scheduling the workflow.",CS,AI_ML,0.85,Extracted from log - paper 790
Performance Modeling and Workflow Scheduling of Microservice-Based Applications in Clouds,"Microservice has been increasingly recognized as a promising architectural style for constructing large-scale cloud-based applications within and across organizational boundaries. This microservice-based architecture greatly increases application scalability, but meanwhile incurs an expensive performance overhead, which calls for a careful design of performance modeling and task scheduling. However, these problems have thus far remained largely unexplored. In this paper, we develop a performance modeling and prediction method for independent microservices, design a three-layer performance model for microservice-based applications, formulate a Microservice-based Application Workflow Scheduling problem for minimum end-to-end delay under a user-specified Budget Constraint (MAWS-BC), and propose a heuristic microservice scheduling algorithm. The performance modeling and prediction method are validated and justified by experimental results generated through a well-known microservice benchmark on disparate computing nodes, and the performance superiority of the proposed scheduling solution is illustrated by extensive simulation results in comparison with existing algorithms.",CS,AI_ML,0.85,Extracted from log - paper 791
"Leveraging Network Data Analytics Function and Machine Learning for Data Collection, Resource Optimization, Security and Privacy in 6G Networks","The full deployment of sixth-generation (6G) networks is inextricably connected with a holistic network redesign able to deal with various emerging challenges, such as integration of heterogeneous technologies and devices, as well as support of latency and bandwidth demanding applications. In such a complex environment, resource optimization, and security and privacy enhancement can be quite demanding, due to the vast and diverse data generation endpoints and associated hardware elements. Therefore, efficient data collection mechanisms are needed that can be deployed at any network infrastructure. In this context, the network data analytics function (NWDAF) has already been defined in the fifth-generation (5G) architecture from Release 15 of 3GPP, that can perform data collection from various network functions (NFs). When combined with advanced machine learning (ML) techniques, a full-scale network optimization can be supported, according to traffic demands and service requirements. In addition, the collected data from NWDAF can be used for anomaly detection and thus, security and privacy enhancement. Therefore, the main goal of this paper is to present the current state-of-the-art on the role of the NWDAF towards data collection, resource optimization and security enhancement in next generation broadband networks. Furthermore, various key enabling technologies for data collection and threat mitigation in the 6G framework are identified and categorized, along with advanced ML approaches. Finally, a high level architectural approach is presented and discussed, based on the NWDAF, for efficient data collection and ML model training in large scale heterogeneous environments.",CS,AI_ML,0.85,Extracted from log - paper 792
Enterprise cloud resource optimization and management based on cloud operations,"The so-called automated operation and maintenance refers to a large number of repetitive tasks in daily IT operations (from simple daily checks, configuration changes and software installation to organizational scheduling of the entire change process) from manual execution in the past to standardized, streamlined and automated operations. This article delves into the realm of enterprise cloud resource optimization and management, leveraging automated operations (autoOps) as a fundamental strategy. As industries like banking witness exponential growth and innovation in IT systems, the complexity of managing resources escalates. Automated operations have emerged as a critical component, transitioning from manual interventions to encompass standardization, workflow optimization, and architectural enhancements. Through real-world deployments and theoretical frameworks, it elucidates effective strategies for optimizing and governing enterprise cloud resources, thereby enhancing efficiency, security, and resilience in IT operations.",CS,AI_ML,0.85,Extracted from log - paper 793
Smart Load-Based Resource Optimization Model to Enhance the Performance of Device-to-Device Communication in 5G-WPAN,"In wireless personal area networks (WPANs), devices can communicate with each other without relying on a central router or access point. They can improve performance and efficiency by allowing devices to share resources directly; however, managing resource allocation and optimizing communication between devices can be challenging. This paper proposes an intelligent load-based resource optimization model to enhance the performance of device-to-device communication in 5G-WPAN. Intelligent load-based resource optimization in device-to-device communication is a strategy used to maximize the efficiency and effectiveness of resource usage in device-to-device (D2D) communications. This optimization strategy is based on optimizing the network’s resource load by managing resource utilization and ensuring that the network is not overloaded. It is achieved by monitoring the current load on the network and then adjusting the usage of resources, such as bandwidth and power, to optimize the overall performance. This type of optimization is essential in D2D communication since it can help reduce costs and improve the system’s performance. The proposed model has achieved 86.00% network efficiency, 93.74% throughput, 91.94% reduced latency, and 92.85% scalability.",CS,AI_ML,0.85,Extracted from log - paper 794
"Toward Efficient 6G IoT Networks: A Perspective on Resource Optimization Strategies, Challenges, and Future Directions","The next generation (6G) wireless communication technology has super advantages in high transmission rates scenarios. Internet of Things (IoT) has been applied in recent years due to its wide connection. However, effective resource optimization methods must be analyzed to meet the high requirements of key performance indicators in 6G wireless IoT networks. This paper discusses a general investigation of the resource optimization strategies in the 6G IoT system. The study aims to find the main solutions to optimize 6G IoT network performance. First, an overall summary of current research is preferred resource optimization in latency, reliability, Energy Efficiency (EE), Spectrum Efficiency (SE), bandwidth utilization efficiency, rate, and power efficiency. Second, we propose the multi-indicator tradeoff strategies associated with the latest resource optimization approaches and investigate optimal strategies. Furthermore, we show the limitations of the current resource optimization methods and discuss the future works of resource optimization for IoT devices in 6G communication. Our survey aims to help researchers optimize 6G IoT network performance using advanced techniques.",CS,AI_ML,0.85,Extracted from log - paper 795
Importance of Circular Economy for Resource Optimization in Various Industry Sectors – A Review-based Opportunity Analysis,"Purpose: The circular economy concept is of significant importance across various industry sectors, including the primary, secondary, tertiary, and quaternary sectors. This concept has a profound impact across all industry sectors by promoting sustainable practices, resource efficiency, waste reduction, and innovation. By embracing circular principles, industries can contribute to the transition towards a more sustainable and resilient economy while creating economic value and minimizing environmental impact. Methodology: The importance of the circular economy is discussed in various industry sectors by means of a systematic review to know the current status and SWOC and ABCD analysis to know the Opportunity of exploring this field. Results: Based on analysis, comparison, evaluation, and interpretation of the circular economy in all four industry sectors, the importance of Circular Economy for Resource Optimization are suggested. Outcome/Values/Novelty: The importance of the Circular Economy in primary, secondary, tertiary, and quaternary industries are evaluated by knowing the current status and the opportunity are analyzed using SWOC and ABCD analysis frameworks. Type of Paper: Exploratory Analysis",CS,AI_ML,0.85,Extracted from log - paper 796
Fair Energy-Efficient Resource Optimization for Green Multi-NOMA-UAV Assisted Internet of Things,"Owing to the advantages of better air-ground channel and higher flexibility, unmanned aerial vehicle (UAV) has been widely applied in the field of Internet of Things (IoT) to increase the communication coverage. However, the UAV with limited energy is facing severe energy shortage when serving more and more IoT devices. In this paper, we propose a non-orthogonal multiple access (NOMA) based green multi-UAV assisted IoT system to increase user capacity while improving the energy utilization of each UAV. Considering the limited energy budget of each UAV, we formulate a fair energy-efficient resource optimization problem under the constraints of maximum transmit power of each UAV, minimum communication rate requirement of each user and UAV mobility. By alternately optimizing the communication scheduling, transmit power allocation and UAV trajectory with the Dinkelbach method and the successive convex approximation (SCA), the energy-efficient fairness can be achieved between the UAVs by maximizing the minimum energy efficiency of the UAVs. The simulation results indicate the fair energy efficiency between the UAVs can be obtained through the alternative resource optimization, and the proposed multi-NOMA-UAV assisted IoT can get higher energy efficiency than the traditional orthogonal multiple access (OMA)-UAV assisted IoT.",CS,AI_ML,0.85,Extracted from log - paper 797
Fair Energy-Efficient Resource Optimization for Multi-UAV Enabled Internet of Things,"Unmanned aerial vehicle (UAV) enabled Internet of Things (IoT) can keep network connectivity when the ground infrastructures are paralyzed. However, its transmission perform will be restricted due to the limited energy of the UAV. In this paper, a multi-UAV enabled IoT is proposed, where the UAVs as base stations send information to the ground IoT nodes via downlink within the flight time. And a fair energy-efficient resource optimization scheme for the IoT is studied to ensure fair energy consumption of multiple UAVs. The optimization problem seeks to maximize the minimum energy efficiency of each UAV by jointly optimizing communication scheduling, power allocations and trajectories of the UAVs. We decompose the non-convex optimization problem into three sub-optimization problems and solve them by Dlinkelbach method and successive convex approximation (SCA). Then a joint optimization algorithm is put forward to obtain the global optimal solutions by iteratively optimizing the three sub-optimization problems. The simulations results show that the multi-UAV enabled IoT can achieve significant performance improvement, and the energy efficiency between UAVs can achieve relative fairness by the fair resource optimization.",CS,AI_ML,0.85,Extracted from log - paper 798
Resource Optimization Tool,"Logistics Management Institute (LMI) is a consulting firm that innovates technological solutions for its clients spanning across the defense, space, and healthcare industries. The company is interested in expanding their workforce analytics toolset by digitizing the project assignment process. Current processes are vulnerable to inaccurate estimates of employee skills, project requirements, and project priority. Manually crafting assignments from complex inputs can result in schedules with unequal employee utilization, long project time horizons, and suboptimal project sequencing. LMI has digital tools that quantify employee skills and project requirements and now requires a tool to produce an optimized project schedule. The team developed several mathematical models that are optimized using different solvers, heuristics, and software that converts the mathematical model output into human-readable schedules and performance metrics. This project has provided LMI with a product they can utilize internally or sell to customers to efficiently allocate resources. The optimized assignment approach produces solutions that show several improvements compared to an algorithmic approximation of the current state, with increases in average employee utilization of up to 41% and decreases in makespan of up to 60%.",CS,AI_ML,0.85,Extracted from log - paper 799
Eigen: End-to-end Resource Optimization for Large-Scale Databases on the Cloud,"Increasingly, cloud database vendors host large-scale geographically distributed clusters to provide cloud database services. When managing the clusters, we observe that it is challenging to simultaneously maximizing the resource allocation ratio and resource availability. This problem becomes more severe in modern cloud database clusters, where resource allocations occur more frequently and on a greater scale. To improve the resource allocation ratio without hurting resource availability, we introduce Eigen, a large-scale cloud-native cluster management system for large-scale databases on the cloud. Based on a resource flow model, we propose a hierarchical resource management system and three resource optimization algorithms that enable end-to-end resource optimization. Furthermore, we demonstrate the system optimization that promotes user experience by reducing scheduling latencies and improving scheduling throughput. Eigen has been launched in a large-scale public-cloud production environment for 30+ months and served more than 30+ regions (100+ available zones) globally. Based on the evaluation of real-world clusters and simulated experiments, Eigen can improve the allocation ratio by over 27% (from 60% to 87.0%) on average, while the ratio of delayed resource provisions is under 0.1%.",CS,AI_ML,0.85,Extracted from log - paper 800
Vehicle Selection and Resource Optimization for Federated Learning in Vehicular Edge Computing,"As a distributed deep learning paradigm, federated learning (FL) provides a powerful tool for the accurate and efficient processing of on-board data in vehicular edge computing (VEC). However, FL involves the training and transmission of model parameters, which consumes the vehicles’ precious energy resources and takes up much time. It is a departure from many applications with severe real-time requirements in VEC. And the capabilities and data quality of each vehicle are distinct that will affect the performance of training the model. Therefore, it is crucial to select the appropriate vehicles to participate in learning tasks and optimize resource allocation under learning time and energy consumption constraints. In this paper, taking the vehicle position and velocity into consideration, we formulate a min-max optimization problem to jointly optimize the on-board computation capability, transmission power, and local model accuracy to achieve the minimum cost in the worst case of FL. Specifically, we propose a greedy algorithm to select vehicles with higher image quality dynamically, and it keeps the system’s overall cost to a minimum in FL. The formulated optimization problem is a nonlinear programming problem, so we decompose it into two subproblems. For the resource allocation problem, we use the Lagrangian dual problem and the subgradient projection method to approximate the optimal value iteratively. For the local model accuracy problem, we develop an adaptive harmony algorithm for heuristic search. The simulation results show that our proposed algorithms have well convergence and effectiveness and achieve a tradeoff between cost and fairness.",CS,AI_ML,0.85,Extracted from log - paper 801
Joint Computation and Communication Resource Optimization for Beyond Diagonal UAV-IRS Empowered MEC Networks,"Recent advancements in 6G systems signal a leap towards universal connectivity and ultra-reliable, low-latency communications for real-time data devices. Yet, these advancements encounter obstacles such as limited device battery life and computational power, along with urban signal blockages. To counter these, Intelligent Reconfigurable Surfaces (IRS) within Mobile Edge Cloud (MEC) infrastructures offer enhanced computing to overcome device limitations and create alternative communication paths. Despite these improvements, connectivity issues remain for remote areas. Our paper presents the Beyond Diagonal IRS (BD-IRS or IRS 2.0), integrated with UAVs in MEC networks (BD-IRS-UAV), providing on-demand links for remote users to offload tasks, tackling resource and battery limitations. We propose a joint optimization strategy to reduce system's worst-case latency and UAV hovering time by optimizing BD-IRS-UAV deployment and resource allocation. This challenge is approached by dividing it into two sub-problems: BD-IRS-UAV Placement and Computational Resource Optimization, and Communication Resource Optimization, each solved iteratively. This design significantly enhances system performance, showing a $17.75\%$ increase over traditional diagonal IRS and a $25.43\%$ improvement over IRS on buildings, with a $13.44\%$ enhancement in worst-case latency compared to binary offloading schemes.",CS,AI_ML,0.85,Extracted from log - paper 802
Online Trajectory and Resource Optimization for Stochastic UAV-enabled MEC System,"The recent development of unmanned aerial vehicle (UAV) and mobile edge computing (MEC) technologies provides flexible and resilient computation services to mobile users out of the terrestrial computing service coverage. In this paper, we consider a UAV-enabled MEC platform that serves multiple mobile ground users with random movements and task arrivals. We aim to minimize the average weighted energy consumption of all users subject to the average UAV energy consumption and data queue stability constraints. We formulate the problem as a multi-stage stochastic optimization, and adopt Lyapunov optimization to convert it into per-slot deterministic problems with fewer optimizing variables. We design two reduced-complexity methods that solve the resource allocation and the UAV movement either in two sequential steps or jointly in one step. Both methods can guarantee to satisfy the average UAV energy and queue stability constraints, meanwhile achieving a tradeoff between the user energy consumption and the length of queue backlog. Simulation results show that the two methods significantly outperform the other benchmark methods including a learning-based method in reducing the energy consumption of ground users. In between, the proposed joint optimization method achieves better performance than the two-stage method at the cost of higher computational complexity.",CS,AI_ML,0.85,Extracted from log - paper 803
Joint Trajectory and Resource Optimization for UAV-Aided Two-Way Relay Networks,"Unmanned aerial vehicle (UAV)-aided two-way relay networks with multiple terrestrial user pairs is investigated, where a fix-wing UAV is served as a two-way relay to assist information transmission. We aim to maximize the total rate of the two-way relay networks while the quality of service (QoS) being guaranteed. A novel relay strategy, namely time-slots pairing, is proposed by exploiting physical-layer network coding (PNC) protocol. Specifically, the UAV receives and buffers the signals from the scheduled user pair when the UAV approaches one of them, then forwards the modulated signal when the UAV is close to the other. As a result, a non-convex total rate maximization problem is formulated by joint trajectory design and resource optimization. To solve this problem, we first decompose it into three sub-problems, and then a three-step iterative algorithm that can effectively handle the non-convex problem with no worse than a polynomial complexity is developed leveraging the block coordinate descent (BCD) technique and successive convex approximation (SCA) method. Simulation results finally illustrate that: 1) Compared with amplify-and-forward (AF) or decode-and-forward (DF) protocols, the proposed design based on PNC protocol can significantly improve the total rate of the two-way relay networks; 2) the time-slots pairing relay strategy always achieves a significant enhancement on performance than the traditional non-time-slots pairing relay strategy; 3) the time-slots pairing relay strategy is more favorable for the mobile PNC relay networks since it can well resist the degradation of the entire networks caused by users’ increased service quality requirements.",CS,AI_ML,0.85,Extracted from log - paper 804
Joint Online Route Planning and Resource Optimization for Multitarget Tracking in Airborne Radar Systems,"Reasonable route planning and resource allocation strategy in the airborne radar systems (ARS), can sufficiently utilize the limited resources and promote the multitarget tracking (MTT) performance. However, using separately the route planning and resource optimization method cannot take full advantage of the airborne platform. Considering this issue, we propose a joint online route planning and resource optimization strategy in the ARS to improve the system capability for MTT. First, a kinematic model of the ARS, including the mathematical expression between the radar states and the system control parameters, is introduced, which integrates the route planning to the radar scheduling scheme. Next, the posterior Cramér–Rao lower bound about route planning and resource optimization variables for the tracking targets is derived. Then, a scaled-based utility function is established to quantify the MTT performance. Hereafter, a nonconvex problem is formulated by minimizing the utility function with route and resource constraints, and then a efficient three-stage partition-based solution is proposed. Finally, simulation experiments demonstrate the effectiveness of the proposed algorithm. Furthermore, over the traditional benchmark algorithm, the tracking performance of the proposed approach improves 30.44%.",CS,AI_ML,0.85,Extracted from log - paper 805
GADGET: Online Resource Optimization for Scheduling Ring-All-Reduce Learning Jobs,"Fueled by advances in distributed deep learning (DDL), recent years have witnessed a rapidly growing demand for resource-intensive distributed/parallel computing to process DDL computing jobs. To resolve network communication bottleneck and load balancing issues in distributed computing, the so-called ""ring-all-reduce"" decentralized architecture has been increasingly adopted to remove the need for dedicated parameter servers. To date, however, there remains a lack of theoretical understanding on how to design resource optimization algorithms for efficiently scheduling ring-all-reduce DDL jobs in computing clusters. This motivates us to fill this gap by proposing a series of new resource scheduling designs for ring-all-reduce DDL jobs. Our contributions in this paper are threefold: i) We propose a new resource scheduling analytical model for ring-all-reduce deep learning, which covers a wide range of objectives in DDL performance optimization (e.g., excessive training avoidance, energy efficiency, fairness); ii) Based on the proposed performance analytical model, we develop an efficient resource scheduling algorithm called GADGET (greedy ring-all-reduce distributed graph embedding technique), which enjoys a provable strong performance guarantee; iii) We conduct extensive trace-driven experiments to demonstrate the effectiveness of the GADGET approach and its superiority over the state of the art.",CS,AI_ML,0.85,Extracted from log - paper 806
Composed Resource Optimization for Multitarget Tracking in Active and Passive Radar Network,"In this article, a composed resource optimization (CRO) scheme is developed for an active and passive radar network engaged in multiple target tracking (MTT). The motivation of the CRO scheme is to collaboratively optimize the transmit resources of active radars, as well as the receiving processing resources of passive radars, to improve the overall MTT performance. We utilize the predicted conditional Cramér–Rao lower bound to evaluate the impact of allocation strategies on tracking performance and formulate the CRO as a mixed-integer nonlinear program problem since the adaptable parameters w.r.t. the target selection process are in binary form. To solve the problem, we propose an alternating direction method of multiplier-based algorithm. This algorithm transforms the original problem into an equality constrained problem by introducing two auxiliary vectors. In such a case, the CRO problem can be tackled by alternately solving several simple subproblems. Specifically, the subproblem w.r.t. the resource vector is convex, and the subproblems w.r.t. the auxiliary vectors are separable. Simulation results demonstrate that the proposed CRO scheme outperforms the traditional allocation schemes in terms of MTT performance. In addition, the performance of the CRO scheme is close to the optimal performance provided by the exhaustive method, but the computation load of the CRO scheme is lower than that of the exhaustive method. Finally, physical interpretations are presented to support our conclusions.",CS,AI_ML,0.85,Extracted from log - paper 807
The Effect of Big Data Analytics Capability on Competitive Performance: The Mediating Role of Resource Optimization and Resource Bricolage,"Although big data analytics capability (BDAC) leads to competitive performance, the mechanism of the relationship is still unclear. To narrow the research gap, this paper investigates the mediating roles of two forms of resource integration (resource optimization and resource bricolage) in the relationship between two forms of BDAC [big data analytics (BDA) management capability and BDA technology capability] and competitive performance. Supported by Partial Least Squares-Structural Equation Modeling (PLS-SEM) and the cross-sectional survey data from 219 Chinese enterprises, the results show that the resource bricolage plays a significantly mediating role in the relationships between BDA management capability and competitive performance as well as in the relationship between BDA technology capability and competitive performance. Furthermore, the mediating effect in the former relationship is stronger than that in the latter relationship. Additionally, BDA technology capability only has a direct effect on resource bricolage, while BDA management capability has a stronger effect on resource optimization than that on resource bricolage. Finally, resource bricolage has a stronger impact on competitive performance than resource optimization. These findings contribute to understanding how enterprises could apply different forms of BDAC to other kinds of resource integration to achieve outstanding competitive performance.",CS,AI_ML,0.85,Extracted from log - paper 808
LBRO: Load Balancing for Resource Optimization in Edge Computing,"Mobile cloud computing and edge computing-based solutions provide means to offload tasks for resource-limited mobile devices. Mobile cloud computing provides remote cloud solutions while edge computing provides closer proximity-based solutions. Remote cloud solutions suffer from network latency and limited bandwidth challenges due to distance and dependency on the Internet. However, these challenges are addressed by edge-based solutions since the edge node is available in the same network. The use of Internet of Things-based solutions considering future Information Communication Technology infrastructure is on the rise resulting in the massive growth of digital equipment increasing the load at edge devices. Hence, some load balancing mechanism is required at the edge level to avoid resource congestion. The load balancing at the edge must consider the user’s preferences about edge resources such as personal computers or mobile devices. A user must declare which resources can be spared for other devices to avoid overprovisioning essential resources. We present Load Balancing for Resource Optimization (LBRO), a collaborative cloudlet platform to address load balancing challenges in edge computing considering users’ preferences. A comparative analysis of the proposed approach with the conventional edge-based approach yields that the proposed approach provides significantly improved results in terms of CPU, memory, and disk utilization.",CS,AI_ML,0.85,Extracted from log - paper 809
Distributed Resource Optimization for NOMA Transmission in Beamforming SATCOM,"This work studies the application of nonorthogonal transmission in beamforming (BF) based forward links for next-generation satellite communication (SATCOM) with multiple gateways. With the aim of enhancing the throughput of BF SATCOM systems, the state-of-the-art nonorthogonal multiple access (NOMA) technique is exploited by serving multiple users per beam in the same time slot. In this regard, the feeder link limitations and multibeam satellite payload constraints must be considered for BF design and power allocation (PA) optimization in nonorthogonal SATCOM. To address these challenges, distributed resource optimization strategies are investigated for BF and flexible payload power resource allocation in multigateway (multi-GW) nonorthogonal SATCOM systems. Specifically, a per-feed available power-constrained BF strategy via maximization of the worst-user signal-to-leakage-and-noise ratio (SLNR) is explored with local channel state information (CSI) for a distributed operation of GWs. As an upper-bound performance limit, a centralized multilayer BF strategy is processed in a central unit with full global CSI and data sharing. After the BF direction optimization, a weighted sum-rate maximization-based (WSRM-based) power resource optimization strategy is locally applied at each GW to efficiently use the power resources for higher performance increment. The nonconvex WSRM problem, under the constraints of the practical satellite payload power budget, successful successive interference cancellation (SIC) decoding, and minimum data rate, is recast into an equivalent weighted sum-MSE minimization (WMMSE) counterpart for a tractable solution. Finally, an efficient user scheduling is designed to enable the operator to capture a substantial system-throughput gain. Accurate simulations are conducted with the near-to-real coverage area (footprints), the random distributions of users, and interference, relying on geographical locations of users. The results over a realistic simulation environment show the efficiency of our strategies.",CS,AI_ML,0.85,Extracted from log - paper 810
CroApp: A CNN-Based Resource Optimization Approach in Edge Computing Environment,"With the emergence of various convolutional neural network (CNN)-based applications and the rapid growth of CNN model scale, the resource-constricted end devices can hardly deploy CNN-based applications. Current work optimizes the CNN model on edge servers and deploys the optimized model on devices in an edge computing environment. However, most of them only optimize the resource consumption within or across models solely, whereas neglecting the other side. In this article, we propose a novel CNN-based resource optimization approach (CroApp) that not only optimizes the resource consumption within the CNN model but also pays attention to resource optimization across the applications. Specifically, we adopt model compression as the “inner-model” optimization method, as well as computation sharing as the “intermodel” optimization method. First, during “inner-model” optimization, the CroApp prunes unnecessary parameters within the model on edge servers to reduce the scale of the model. Then, during “intermodel” optimization, the CroApp trains a set of shareable models based on the pruned model and sends these shareable models to end devices. Finally, the CroApp adaptively adjusts the shared models to reduce resource consumption. The experimental results show that the CroApp outperforms the state-of-the-art approaches in terms of resource reduction, scalability, and application performance.",CS,AI_ML,0.85,Extracted from log - paper 811
Target Capacity Based Resource Optimization for Multiple Target Tracking in Radar Network,"In this paper, a target capacity based resource optimization (TC-RO) scheme is developed for multiple target tracking (MTT) application in radar networks. The key idea of this scheme is to coordinate the transmit power and dell time resource usage of multiple radars in order to increase the number of the targets that can be tracked with predetermined accuracy requirements. We adopt the Bayesian Cramér-Rao lower bound as a metric function to quantify the MTT accuracies, and build the TC-RO scheme as a non-smooth and non-convex optimization problem. To deal with this problem, we design an efficient three-step solution technique which incorporates relaxation and fine-tuning process. Specifically, we first relax the resulting optimization problem as a smooth one by applying sigmoid-type transformation to its objective, and then develop an appropriate method to find a local minimum to the relaxed non-convex problem with guaranteed convergence. After that, the local minimum of the relaxed problem is used as an initial point and a fine-tuning process is performed to search for a reasonable feasible solution to the original non-smooth optimization problem. Simulation results demonstrate that the proposed TC-RO scheme can greatly increase the target capacity of the radar network when compared with the traditional uniform allocation scheme.",CS,AI_ML,0.85,Extracted from log - paper 812
Flexible Resource Optimization for GEO Multibeam Satellite Communication System,"Conventional GEO satellite communication systems rely on a multibeam foot-print with a uniform resource allocation to provide connectivity to users. However, applying uniform resource allocation is inefficient in presence of non-uniform demand distribution. To overcome this limitation, the next generation of broadband GEO satellite systems will enable flexibility in terms of power and bandwidth assignment, enabling on-demand resource allocation. In this paper, we propose a novel satellite resource assignment design whose goal is to satisfy the beam traffic demand by making use of the minimum transmit power and utilized bandwidth. The motivation behind the proposed design is to maximize the satellite spectrum utilization by pushing the spectrum reuse to affordable limits in terms of tolerable interference. The proposed problem formulation results in a non-convex optimization structure, for which we propose an efficient tractable solution. We validate the proposed method with extensive numerical results, which demonstrate the efficiency of the proposed approach with respect to benchmark schemes.",CS,AI_ML,0.85,Extracted from log - paper 813
A Lightweight Authentication Protocol for UAV Networks Based on Security and Computational Resource Optimization,"The widespread use of Unmanned Aerial Vehicles (UAV) has made the security and computing resource application efficiency of UAV a hot topic in the security field of the Internet of Things. In this paper, an optimized lightweight identity security authentication protocol, Optimized Identity Authentication Protocol (ODIAP) is proposed for Internet of Drones (IoD) networks. The protocol is targeted to the security risks faced by IoD networks, and proposes the security authentication mechanism consisting of 3 phases and 7 authentication processes, which enables the protocol has both forward and backward security, and can resist mainstream network attacks. Meanwhile, this paper fully considers the computational load and proposes the identity information generation and verification method based on the Chinese residual theorem, which reduces the computational load of resource-constrained nodes and shifts the complex computational process to server nodes with abundant computational resources. Moreover, after security protocol analysis and tool verification based on the automated security verification tool Proverif, the protocol in this paper has complete security. At the same time, the performance analysis and comparison with other mainstream protocols shows that this protocol effectively optimizes the use of computing resources without compromising security.",CS,AI_ML,0.85,Extracted from log - paper 814
Joint Waveform Control and Resource Optimization for Maneuvering Targets Tracking in Netted Colocated MIMO Radar Systems,"In netted colocated multiple-input multiple-output (MIMO) radar system (NCMRS), constrained to the limited resource, one must decide when to run the NCMRS, which nodes are to be activated, how much energy should be consumed, and which waveform type as well as parameter should be transmitted by each activated node. To this end, in this article, we consider the problem of joint waveform control and space-time resource management for maneuvering targets tracking in NCMRS. First, the joint waveform type as well as parameter selection and space-time resource management optimization model is proposed, where both the network system resource consumption and the overall tracking performance of maneuvering targets are considered and employed as the network comprehensive cost. Then, to solve the established optimization problem efficiently, a modified particle swarm optimization (MPSO)-based joint waveform control and space-time resource optimization (JWCSTRO) algorithm is put forward. In the MPSO-based JWCSTRO algorithm, the system sampling period, the activated nodes, the node-target assignment, and the subarray number, transmit energy and transmit waveform type, as well as parameter of each activated node can be adjusted jointly and adaptively, where the former five implement the space-time resource optimization and the last one achieves the waveform control in NCMRS. Finally, numerical simulation results demonstrate the effectiveness of the proposed MPSO-based JWCSTRO algorithm. Furthermore, compared with the five other benchmark algorithms, the maximum and minimum improvement ratios of the network comprehensive cost of the proposed MPSO-based JWCSTRO algorithm are 44.89% and 21.68%, respectively.",CS,AI_ML,0.85,Extracted from log - paper 815
Resource Optimization and Delay Guarantee Virtual Network Function Placement for Mapping SFC Requests in Cloud Networks,"Since the advent of network function virtualization (NFV), cloud service providers (CSPs) can implement traditional dedicated network devices as software and flexibly instantiate network functions (NFs) on common off-the-shelf servers. NFV technology enables CSPs to deploy their NFs to a cloud data center in the form of virtual network functions (VNFs) without costly capital expenditures and operating expenses. However, it is an essential but intractable issue for CSPs to devise a suitable VNF placement scheme to optimize network resource consumption and improve network performance. In this article, we focus on the VNF placement problem for mapping users’ service function chain requests (SFCRs) in cloud networks. To enhance network resource utilization, we consider the fundamental resource overheads and implementation method of VNFs. The VNF placement problem is formulated as an integer linear programming model with the aim of minimizing the total network resource consumption while guaranteeing the delay requirements of SFCRs. We devise a two-phase optimization solution (TPOS) to solve the problem. TPOS contains a mapping phase to map SFCRs on servers and an adjustment phase to optimize the placement of VNFs and VNF requests. Evaluation results demonstrate that TPOS can derive near-optimal server resource consumption and significantly enhance network resource utilization. TPOS can guarantee the delay requirements of SFCRs and outperform contrastive schemes in terms of activated servers, SFCR acceptance ratio, and average VNF utilization.",CS,AI_ML,0.85,Extracted from log - paper 816
Joint Trajectory-Resource Optimization in UAV-Enabled Edge-Cloud System With Virtualized Mobile Clone,"This article studies an unmanned aerial vehicle (UAV)-enabled edge-cloud system, where UAV acts as a mobile edge computing (MEC) server interplaying with remote central cloud to provide computation services to ground terminals (GTs). The UAV-enabled edge-cloud system implements a virtualized network function, namely, mobile clone (MC), for each GT to help execute their offloaded tasks. Through such network function virtualization (NFV) implemented on top of the UAV-enabled edge-cloud system, GTs can have extended computation capability and prolonged battery lifetime. We aim to jointly optimize the allocation of resource and the UAV trajectory in the 3-D spaces to minimize the overall energy consumption of the UAV. The proposed solution, therefore, can extend the endurance of the UAV and support reliable MC functions for GTs. This article solves the complicated optimization problem through a block coordinate descent algorithm in an iterative way. In each iteration, the allocation of resource is modeled as a multiple constrained optimization problem given predefined UAV trajectory, which can be reformulated into a more tractable convex form and solved by successive convex optimization and Lagrange duality. Second, given the allocated resource, the optimization of the trajectory of rotary-wing/fixed-wing UAV can be formulated into a series of convex quadratically constrained quadratically program (QCQP) problems and solved by the standard convex optimization techniques. After the block coordinate descent algorithm converges to a prescribed accuracy, a high-quality suboptimal solution can be found. According to the simulation, the numerical results verify the effectiveness of our proposed solution in contrast to the baseline solutions.",CS,AI_ML,0.85,Extracted from log - paper 817
Cognitive Carrier Resource Optimization for Internet-of-Vehicles in 5G-Enhanced Smart Cities,"Internet-of-Vehicles (IoV), an important part of Intelligent Transportation Systems, is one of the most strategic applications in smart cities initiatives. The mMTC and URLLC functions of 5G are especially crucial for ensuring the connectivity and communication needs of rapidly moving IoVs. In this backdrop, network virtualization, cognitive computing along with smart spectrum resource management to the virtual networks will play a key role in solving the spectrum resource challenge. In this article, we propose a dynamic carrier resource allocation scheme for supporting IoV systems in smart cities enabled by cloud radio access networks (CRAN)-based 5G carriers. In CRAN-based 5G networks, the carrier resource allocated to the virtual networks can be centrally managed and shared to meet the dynamic demand of cell capacities caused by the rapid movement of IoVs, and the response to this dynamic allocation will become more time critical. The proposed cognitive carrier resource optimization is achieved by enhancing the ability to predict movement of IoVs, hence the dynamically changing demand for carrier resources. As an enhancement of the traditional Markov Model, our prediction model introduces vehicles' mobility analysis in order to allow the construction of a more precise flow transition matrix to improve the prediction result. Numerical results are provided to show the performance improvement of the proposed method.",CS,AI_ML,0.85,Extracted from log - paper 818
Ant colony resource optimization for Industrial IoT and CPS,"Internet‐of‐Things (IoT) enabled cyber‐physical systems (CPS) is a system in which communication between the physical devices and the cyber environment runs independently without any user interaction. Several optimization algorithms have been used for determining the optimal solutions that can reduce the production cost and/or enhance the production efficiency with in limited time‐periods. However, existing optimization approaches have failed to solve the issues in the complex manufacturing process. To overcome this issue, a novel technique called directed acyclic graph theory based multiobjective oppositional learnt artificial ant colony resource optimization (DAGT‐MOLAACRO) technique has been introduced in this study for solving the complex manufacturing process in the industry. Initially, IoT devices are used in the industrial sector for sensing and collecting data. Then the collected data is sent to the cyberspace of the CPS system with the least latency. Then, the CPS system collects the data generated from the industrial IoT devices that is stored in cyberspace with lesser memory consumption. MOLAACRO is applied to find the optimal solution among the population that satisfies the resource constraints by constructing the directed acyclic graph. In this way, the DAGT‐MOLAACRO technique reduces the time complexity with minimal latency and computation overhead. For verification purposes, our experimental work has been carried out using different performance metrics such as data latency, time complexity, and computation overhead with respect to the number of IoT devices and the amount of data collected. The results show that the DAGT‐MOLAACRO technique has better performance with reductions in terms of time complexity by 10%, latency by 17%, and the computation overhead by 11% against the existing works in literature.",CS,AI_ML,0.85,Extracted from log - paper 819
Fuzzy-multi-mode Resource-constrained Discrete Time-cost-resource Optimization in Project Scheduling Using ENSCBO,"Construction companies are required to employ effective methods of project planning and scheduling in today's competitive environment. Time and cost are critical factors in project success, and they can vary based on the type and amount of resources used for activities, such as labor, tools, and materials. In addition, resource leveling strategies that are used to limit fluctuations in a project's resource consumption also affect project time and cost. The multi-mode resource-constrained discrete-time–cost-resource optimization (MRC-DTCRO) is an optimization tool that is developed for scheduling of a set of activities involving multiple execution modes with the aim of minimizing time, cost, and resource moment. Moreover, uncertainty in cost should be accounted for in project planning because activities are exposed to risks that can cause delays and budget overruns. This paper presents a fuzzy-multi-mode resource-constrained discrete-time–cost-resource optimization (F-MRC-DTCRO) model for the time-cost-resource moment tradeoff in a fuzzy environment while satisfying all the project constraints. In the proposed model, fuzzy numbers are used to characterize the uncertainty of direct cost of activities. Using this model, different risk acceptance levels of the decision maker can be addressed in the optimization process. A newly developed multi-objective optimization algorithm called ENSCBO is used to search non-dominated solutions to the fuzzy multi-objective model. Finally, the developed model is applied to solve a benchmark test problem. The results indicate that incorporating the fuzzy structure of uncertainty in costs to previously developed MRC-DTCRO models facilitates the decision-making process and provides more realistic solutions.",CS,AI_ML,0.85,Extracted from log - paper 820
Resource optimization for the quantum Internet,"The quantum Internet enables networking based on the fundamentals of quantum mechanics. Here, we define methods and procedures of resource prioritization and resource balancing for the quantum Internet. The aim of the proposed solutions is to optimize the resource allocation mechanisms and to reduce the resource consumptions of the network entities.",CS,AI_ML,0.85,Extracted from log - paper 821
Federated Learning for Edge Networks: Resource Optimization and Incentive Mechanism,"Recent years have witnessed a rapid proliferation of smart Internet of Things (IoT) devices. IoT devices with intelligence require the use of effective machine learning paradigms. Federated learning can be a promising solution for enabling IoT-based smart applications. In this article, we present the primary design aspects for enabling federated learning at the network edge. We model the incentive- based interaction between a global server and participating devices for federated learning via a Stackelberg game to motivate the participation of the devices in the federated learning process. We present several open research challenges with their possible solutions. Finally, we provide an outlook on future research.",CS,AI_ML,0.85,Extracted from log - paper 822
Transfer Learning-powered Resource Optimization for Green Computing in 5G-Aided Industrial Internet of Things,"Objective: Green computing meets the needs of a low-carbon society and it is an important aspect of promoting social sustainable development and technological progress. In the investigation, green computing for resource management and allocation issues is only discussed. Therefore, in the context of the 5G communication network, the investigation of the data classification and resource optimization of the Internet of Things are conducted. Method: The virtualization architecture of the heterogeneous wireless network resource based on 5G technology is designed. The related investigation is conducted based on 5G network and Internet of Things technology. Under the traditional method, the transfer learning is introduced to improve the AdaBoost (Adaptive Boosting) algorithm to classify the data. The investigated complete resource reuse method is used to optimize resources. A method that a sub-channel can be reused by a cellular link and any number of D2D links at the same time is proposed to conduct resource optimization investigation. Results: The investigation indicates that the classification accuracy of the algorithm is excellent for the data classification of the Internet of Things and has different advantages in various aspects compared with other algorithms. The designed algorithm can find a larger set of resource reuse and have a significant increase in spectrum utilization efficiency. Conclusion: The investigation can contribute to the boom in the Internet of Things in terms of data classification and resource optimization based on 5G.",CS,AI_ML,0.85,Extracted from log - paper 823
Stochastic Resource Optimization of Random Access for Transmitters With Correlated Activation,"For a range of scenarios arising in sensor networks, control and edge computing, communication is event-triggered; that is, in response to the environment of the communicating devices. A key feature of device activity in this setting is correlation, which is particularly relevant for sensing of physical phenomena such as earthquakes or flooding. Such correlation introduces a new challenge in the design of resource allocation and scheduling for random access that aim to maximize throughput or expected sum-rate, which do not admit a closed-form expression. In this letter, we develop stochastic resource optimization algorithms to design a random access scheme that provably converge with probability one to locally optimal solutions of the throughput and the sum-rate. A key feature of the stochastic optimization algorithm is that the number of parameters that need to be estimated grows at most linearly in the number of devices. We show via simulations that our algorithms outperform existing approaches in terms of the expected sum-rate by up to 30% for a moderate number of available slots.",CS,AI_ML,0.85,Extracted from log - paper 824
Resource Optimization for Signal Recognition in Satellite MEC with Federated Learning,"Currently, limited resources and data privacy have influenced the development of signal modulation recognition in satellite communications. To solve the above issues, mobile edge computing (MEC) and federated learning (FL) are considered to signal modulation recognition on satellite communications in this paper. FL enables distributed learning with local datasets and allows model parameters instead of the whole training datasets being shared during learning process. MEC technology reduces transmission delay and energy consumption via setting up edge servers on Medium Earth Orbit (MEO) satellites close to Low Earth Orbit (LEO) satellites, High Attitude Platforms (HAPs) and Unmanned Aerial Vehicles (UAVs) replacing central server on Geostationary Earth Orbit (GEO) satellites. To accelerate the learning process and improve the resource allocation strategy simultaneously, we formulate a joint delay ratio and energy efficiency optimization problem. We introduce a Q-learning based algorithm to solve the problem. The experimental results indicate that the accuracy of the proposed scheme is almost the same as the scheme without resource optimization while the required resource is 10% less than the classical algorithms. Meanwhile, the computation complexity of the Q-learning based algorithm is $O(n^{2})$, much lower than the ergodic scheme $(O(n^{3}))$.",CS,AI_ML,0.85,Extracted from log - paper 825
Multilevel Task Offloading and Resource Optimization of Edge Computing Networks Considering UAV Relay and Green Energy,"Unmanned aerial vehicle (UAV)-assisted relay mobile edge computing (MEC) network is a prominent concept, where network deployment is flexible and network coverage is wide. In scenarios such as emergency communications and low-cost coverage, optimization of offloading methods and resource utilization are important ways to improve system effectiveness due to limited terminal and UAV energy and hardware equipment. A multilevel edge computing network resource optimization model on the basis of UAV fusion that provides relay forwarding and offload services is established by considering the initial energy state of the UAV, the green energy charging function, and the reliability of computing offload. With normalized system utility function maximization as the goal, a Markov decision process algorithm meets the needs of the practical application scene and provides a flexible and effective unloading mode. This algorithm is adopted to solve the optimal offloading mode and the optimal resource utilization scheme. Simulations verify the effectiveness and reliability of the proposed multilevel offloading model. The proposed model can optimize system resource allocation and effectively improve the utility function and user experience of computing offloading systems.",CS,AI_ML,0.85,Extracted from log - paper 826
Joint Availability Guarantee and Resource Optimization of Virtual Network Function Placement in Data Center Networks,"Network Function Virtualization (NFV) is a promising technology that decouples network functions from the physical device on which they deployed. Network service in NFV is deployed as Service Function Chain (SFC) that consists of an ordered set of Virtual Network Functions (VNFs). In this paper, we focus on the VNF placement problem in data center networks considering availability guarantee and resource optimization. Firstly, we define an availability model that takes both physical device failures and VNF failures into consideration when evaluating the availability of SFC. Secondly, we propose a novel Joint Path-VNF (JPV) backup model that combines path backup and VNF backup in a joint way. In the JPV backup model, resource consumption can be effectively reduced. Finally, we design an Affinity-Based Algorithm (ABA) to reduce physical link consumption when map VNFs. The evaluation results show that ABA and JPV can achieve better availability improvement (99.99%) with less resource (reduce 40% PL consumption).",CS,AI_ML,0.85,Extracted from log - paper 827
"Artificial Intelligence for Predictive Maintenance Applications: Key Components, Trustworthiness, and Future Trends","Predictive maintenance (PdM) is a policy applying data and analytics to predict when one of the components in a real system has been destroyed, and some anomalies appear so that maintenance can be performed before a breakdown takes place. Using cutting-edge technologies like data analytics and artificial intelligence (AI) enhances the performance and accuracy of predictive maintenance systems and increases their autonomy and adaptability in complex and dynamic working environments. This paper reviews the recent developments in AI-based PdM, focusing on key components, trustworthiness, and future trends. The state-of-the-art (SOTA) techniques, challenges, and opportunities associated with AI-based PdM are first analyzed. The integration of AI technologies into PdM in real-world applications, the human–robot interaction, the ethical issues emerging from using AI, and the testing and validation abilities of the developed policies are later discussed. This study exhibits the potential working areas for future research, such as digital twin, metaverse, generative AI, collaborative robots (cobots), blockchain technology, trustworthy AI, and Industrial Internet of Things (IIoT), utilizing a comprehensive survey of the current SOTA techniques, opportunities, and challenges allied with AI-based PdM.",CS,AI_ML,0.85,Extracted from log - paper 828
Marine Propulsion Health Monitoring: Integrating Neural Networks and IoT Sensor Fusion in Predictive Maintenance,"The maritime sector is shifting towards predictive maintenance to improve marine propulsion system dependability and efficiency. This research introduces neural networks and IoT sensor fusion for marine propulsion health monitoring. Real-time operational data is collected by a sophisticated sensor array spanning crucial propulsion system components. Fusing sensor data using modern IoT algorithms gives a comprehensive overview of system health. The suggested technique uses neural networks for predictive maintenance. A deep learning model analyses sensor-fused data to detect flaws or performance deterioration. Training the neural network on past data from various operating situations allows it to adapt and forecast faults. The model's capacity to learn and develop improves its vessel operating state adaptation. Neural networks and IoT sensor fusion offer early defect identification and dynamic maintenance schedules. Low downtime, operating expenses, and marine propulsion system longevity are achieved using this strategy. Case studies and simulations indicate that the suggested system can predict and avoid significant failures, making it suitable for marine use.",CS,AI_ML,0.85,Extracted from log - paper 829
AI in renewable energy: A review of predictive maintenance and energy optimization,"In the dynamic landscape of the burgeoning renewable energy sector, optimizing energy output, ensuring robust infrastructure maintenance, and seamless integration into the grid present formidable challenges. This paper delves into the transformative potential of artificial intelligence (AI) as a solution to these critical issues. The focus of this study is on the current state of AI applications within the renewable energy domain, particularly honing in on its profound impact on predictive maintenance and energy optimization across diverse sources such as solar, wind, and hydro. By examining the underlying AI techniques employed in this context, the research seeks to unravel the intricacies of how AI contributes to enhancing the efficiency and sustainability of renewable energy systems. A critical component of this exploration involves the analysis of successful case studies, illustrating real-world applications where AI has made substantial strides in predictive maintenance and energy optimization. These cases provide tangible evidence of the practical implications of incorporating AI into renewable energy practices. The research explores AI’s role in renewable energy, focusing on emerging trends and future directions. It aims to understand AI’s transformative influence on optimization, sustainability, and energy efficiency, fostering a more resilient and efficient energy landscape. AI is revolutionizing the renewable energy sector, transforming infrastructure maintenance, energy generation optimization, and integrating renewable sources into the grid. Its advanced analytics, predictive capabilities, and optimization are crucial in achieving global renewable energy targets. As AI technology evolves, its impact on the renewable energy landscape will deepen, paving the way for a cleaner, more sustainable future. By harnessing AI’s power, we can accelerate the transition towards a renewable energy future, ensuring a thriving planet for future generations.",CS,AI_ML,0.85,Extracted from log - paper 830
Artificial intelligence (AI) in renewable energy: A review of predictive maintenance and energy optimization,"The integration of Artificial Intelligence (AI) in the renewable energy sector has emerged as a transformative force, enhancing the efficiency and sustainability of energy systems. This paper provides a comprehensive review of the application of AI in two critical aspects of renewable energy in relation to predictive maintenance and energy optimization. Predictive maintenance, enabled by AI, has revolutionized the renewable energy landscape by predicting and preventing equipment failures before they occur. Utilizing machine learning algorithms, AI analyzes vast amounts of data from sensors and historical performance to identify patterns indicative of potential faults. This proactive approach not only minimizes downtime but also extends the lifespan of renewable energy infrastructure, resulting in substantial cost savings and improved reliability. Furthermore, AI plays a pivotal role in optimizing the energy output of renewable sources. Through advanced data analytics and real-time monitoring, AI algorithms can adapt to changing environmental conditions, predicting energy production patterns and optimizing resource allocation. This ensures maximum energy yield from renewable sources, making them more competitive with traditional energy sources. The paper delves into specific AI techniques such as deep learning, neural networks, and predictive analytics employed for predictive maintenance and energy optimization in various renewable energy systems like solar, wind, and hydropower. Challenges and opportunities associated with implementing AI in renewable energy are discussed, including data security, interoperability, and the need for standardized frameworks. The synthesis of AI technologies with renewable energy not only addresses operational challenges but also contributes to the global transition towards sustainable and clean energy solutions. This review serves as a valuable resource for researchers, practitioners, and policymakers seeking insights into the evolving landscape of AI applications in the renewable energy sector. As technology continues to advance, the synergies between AI and renewable energy are poised to shape the future of the global energy paradigm.",CS,AI_ML,0.85,Extracted from log - paper 831
Transforming equipment management in oil and gas with AI-Driven predictive maintenance,"The oil and gas industry faces significant challenges in managing equipment maintenance due to the complexity and criticality of its assets. Traditional maintenance approaches are often reactive and inefficient, leading to costly downtime and safety risks. However, the emergence of artificial intelligence (AI) and predictive maintenance technologies offers a transformative solution to these challenges. This paper explores the role of AI-driven predictive maintenance in revolutionizing equipment management in the oil and gas sector. AI-driven predictive maintenance leverages machine learning algorithms to analyze equipment data and predict when maintenance is required before a breakdown occurs. By monitoring equipment performance in real-time, AI can identify potential issues early, allowing operators to take proactive maintenance actions. This approach helps minimize downtime, reduce maintenance costs, and improve overall equipment reliability and safety. The implementation of AI-driven predictive maintenance requires a comprehensive strategy that includes data collection, analysis, and integration with existing maintenance practices. Successful adoption of AI-driven predictive maintenance can lead to significant benefits for oil and gas companies, including increased equipment uptime, extended asset lifespan, and enhanced operational efficiency. This paper reviews the current landscape of equipment management in the oil and gas industry, highlighting the limitations of traditional maintenance practices and the need for a more proactive approach. It then examines the principles and benefits of AI-driven predictive maintenance, showcasing real-world examples of its successful implementation. Finally, the paper discusses the challenges and considerations for implementing AI-driven predictive maintenance and provides recommendations for oil and gas companies looking to transform their equipment management practices. Keywords: Transforming Equipment; Management; Oil and Gas; AI-Driven; Predictive Maintenance.",CS,AI_ML,0.85,Extracted from log - paper 832
Toward Physics-Informed Machine-Learning-Based Predictive Maintenance for Power Converters—A Review,"Predictive maintenance for power electronic converters has emerged as a critical area of research and development. With the rapid advancements in deep-learning techniques, new possibilities have emerged for enhancing the performance and reliability of power converters. However, addressing challenges related to data resources, physical consistency, and generalizability has become crucial in achieving optimal strategies. This comprehensive review article presents an insightful overview of the recent advancements in the field of predictive maintenance for power converters. It explores three paradigms: model-based approaches, data-driven techniques, and the emerging concept of physics-informed machine learning (PIML). By leveraging the integration of physical knowledge into machine-learning architectures, PIML holds great promise for overcoming the aforementioned concerns. Drawing upon the current state-of-art, this review identifies common trends, practical challenges, and significant research opportunities in the domain of predictive maintenance for power converters. The analysis covers a broad spectrum of approaches used for parameter identification, feature engineering, fault detection, and remaining useful life estimation. This article not only provides a comprehensive survey of recent methodologies but also highlights future trends, serving as a resource for researchers and practitioners involved in the development of predictive maintenance strategies for power converters.",CS,AI_ML,0.85,Extracted from log - paper 833
"Explainable Predictive Maintenance of Rotating Machines Using LIME, SHAP, PDP, ICE","Artificial Intelligence (AI) is a key component in Industry 4.0. Rotating machines are critical components in manufacturing industries. In the vast world of Industry 4.0, where an IoT network acts as a monitoring and decision-making system, predictive maintenance is quickly gaining importance. Predictive maintenance is a method that uses AI to handle potential problems before they cause breakdowns in operations, processes or systems. However, there is a significant issue with the AI models’ (also known as “black boxes”) inability to explain their decisions. This interpretability is vital for making maintenance decisions and validating the model’s reliability, leading to improved trust and acceptance of AI-driven predictive maintenance strategies. Explainable AI is the solution because it provides human-understandable insights into how the AI model arrives at its predictions. In this regard, the paper presents Explainable AI-based predictive maintenance of Industrial rotating machines. The proposed approach unfolds in four comprehensive stages: 1) Multi-sensor based multi-fault (5 different fault classes) data acquisition; 2) frequency-domain statistical feature extraction; and c) comparison of results for multiple AI algorithms, and d) XAI integration using “Local Interpretable Model Agnostic Explanation (LIME)”, “SHapley Additive exPlanation (SHAP)”, “Partial Dependence Plot (PDP)” and “Individual Conditional Expectation (ICE)” to interpret the results.",CS,AI_ML,0.85,Extracted from log - paper 834
Advancements in predictive maintenance for aging oil and gas infrastructure,"The oil and gas industry relies heavily on aging infrastructure to extract, transport, and process hydrocarbons. As these assets age, the risk of failures and downtime increases, leading to safety hazards and costly repairs. Predictive maintenance has emerged as a valuable strategy to mitigate these risks by using data-driven insights to predict equipment failures and schedule maintenance proactively. This review highlights advancements in predictive maintenance technologies for aging oil and gas infrastructure, focusing on the benefits and challenges of implementation. Advancements in sensor technology and data analytics have significantly improved the effectiveness of predictive maintenance in the oil and gas industry. Sensors installed on critical equipment collect real-time data on temperature, pressure, vibration, and other key parameters, providing insights into equipment health and performance. Data analytics tools analyze this data to identify patterns and trends indicative of potential failures, enabling operators to take preventive action before a breakdown occurs. Machine learning algorithms have also played a crucial role in enhancing predictive maintenance capabilities. These algorithms can process large volumes of data and learn from past equipment failures to predict future issues accurately. By continuously learning from new data, machine learning algorithms can improve their predictive accuracy over time, leading to more effective maintenance strategies. Despite these advancements, implementing predictive maintenance in aging oil and gas infrastructure poses several challenges. One major challenge is integrating new sensor technology with existing equipment, which may require retrofitting or upgrading existing assets. Another challenge is managing the vast amounts of data generated by sensors and analytics tools, which can strain existing IT infrastructure and require specialized expertise to analyze effectively. In conclusion, advancements in predictive maintenance technologies offer significant benefits for aging oil and gas infrastructure. By leveraging sensor technology, data analytics, and machine learning, operators can predict equipment failures, reduce downtime, and extend the life of critical assets. However, implementing these technologies requires careful planning and investment to overcome challenges related to integration, data management, and expertise.",CS,AI_ML,0.85,Extracted from log - paper 835
The internet of things,"When the Internet emerged more than two decades ago, it changed everything. But the Internet of Everything makes that pale in comparison. After two decades of networking and communication, 99% of things are still not networked. The Internet of Everything will disrupt several industries. That means new opportunities, businesses, experiences, and services, and big opportunities for people, companies, and countries. The Internet of Everything demands an intelligent network — a distributed, application-centric networking, computing and storage platform that connects things together, connects things to the Network and connects people and things to the cloud in ways that just weren't possible, or even imaginable, before.",CS,AI_ML,0.85,Extracted from log - paper 836
Internet of Things for Smart Cities,"The Internet of Things (IoT) shall be able to incorporate transparently and seamlessly a large number of different and heterogeneous end systems, while providing open access to selected subsets of data for the development of a plethora of digital services. Building a general architecture for the IoT is hence a very complex task, mainly because of the extremely large variety of devices, link layer technologies, and services that may be involved in such a system. In this paper, we focus specifically to an urban IoT system that, while still being quite a broad category, are characterized by their specific application domain. Urban IoTs, in fact, are designed to support the Smart City vision, which aims at exploiting the most advanced communication technologies to support added-value services for the administration of the city and for the citizens. This paper hence provides a comprehensive survey of the enabling technologies, protocols, and architecture for an urban IoT. Furthermore, the paper will present and discuss the technical solutions and best-practice guidelines adopted in the Padova Smart City project, a proof-of-concept deployment of an IoT island in the city of Padova, Italy, performed in collaboration with the city municipality.",CS,AI_ML,0.85,Extracted from log - paper 837
Blockchains and Smart Contracts for the Internet of Things,"Motivated by the recent explosion of interest around blockchains, we examine whether they make a good fit for the Internet of Things (IoT) sector. Blockchains allow us to have a distributed peer-to-peer network where non-trusting members can interact with each other without a trusted intermediary, in a verifiable manner. We review how this mechanism works and also look into smart contracts-scripts that reside on the blockchain that allow for the automation of multi-step processes. We then move into the IoT domain, and describe how a blockchain-IoT combination: 1) facilitates the sharing of services and resources leading to the creation of a marketplace of services between devices and 2) allows us to automate in a cryptographically verifiable manner several existing, time-consuming workflows. We also point out certain issues that should be considered before the deployment of a blockchain network in an IoT setting: from transactional privacy to the expected value of the digitized assets traded on the network. Wherever applicable, we identify solutions and workarounds. Our conclusion is that the blockchain-IoT combination is powerful and can cause significant transformations across several industries, paving the way for new business models and novel, distributed applications.",CS,AI_ML,0.85,Extracted from log - paper 838
Context Aware Computing for The Internet of Things: A Survey,"As we are moving towards the Internet of Things (IoT), the number of sensors deployed around the world is growing at a rapid pace. Market research has shown a significant growth of sensor deployments over the past decade and has predicted a significant increment of the growth rate in the future. These sensors continuously generate enormous amounts of data. However, in order to add value to raw sensor data we need to understand it. Collection, modelling, reasoning, and distribution of context in relation to sensor data plays critical role in this challenge. Context-aware computing has proven to be successful in understanding sensor data. In this paper, we survey context awareness from an IoT perspective. We present the necessary background by introducing the IoT paradigm and context-aware fundamentals at the beginning. Then we provide an in-depth analysis of context life cycle. We evaluate a subset of projects (50) which represent the majority of research and commercial solutions proposed in the field of context-aware computing conducted over the last decade (2001-2011) based on our own taxonomy. Finally, based on our evaluation, we highlight the lessons to be learnt from the past and some possible directions for future research. The survey addresses a broad range of techniques, methods, models, functionalities, systems, applications, and middleware solutions related to context awareness and IoT. Our goal is not only to analyse, compare and consolidate past research work but also to appreciate their findings and discuss their applicability towards the IoT.",CS,AI_ML,0.85,Extracted from log - paper 839
"Industrial Internet of Things: Challenges, Opportunities, and Directions","Internet of Things (IoT) is an emerging domain that promises ubiquitous connection to the Internet, turning common objects into connected devices. The IoT paradigm is changing the way people interact with things around them. It paves the way for creating pervasively connected infrastructures to support innovative services and promises better flexibility and efficiency. Such advantages are attractive not only for consumer applications, but also for the industrial domain. Over the last few years, we have been witnessing the IoT paradigm making its way into the industry marketplace with purposely designed solutions. In this paper, we clarify the concepts of IoT, Industrial IoT, and Industry 4.0. We highlight the opportunities brought in by this paradigm shift as well as the challenges for its realization. In particular, we focus on the challenges associated with the need of energy efficiency, real-time performance, coexistence, interoperability, and security and privacy. We also provide a systematic overview of the state-of-the-art research efforts and potential research directions to solve Industrial IoT challenges.",CS,AI_ML,0.85,Extracted from log - paper 840
The Internet of Things for Health Care: A Comprehensive Survey,"The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.",CS,AI_ML,0.85,Extracted from log - paper 841
Federated Learning for Internet of Things: A Comprehensive Survey,"The Internet of Things (IoT) is penetrating many facets of our daily life with the proliferation of intelligent services and applications empowered by artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and processing that may not be feasible in realistic application scenarios due to the high scalability of modern IoT networks and growing data privacy concerns. Federated Learning (FL) has emerged as a distributed collaborative AI approach that can enable many intelligent IoT applications, by allowing for AI training at distributed IoT devices without the need for data sharing. In this article, we provide a comprehensive survey of the emerging applications of FL in IoT networks, beginning from an introduction to the recent advances in FL and IoT to a discussion of their integration. Particularly, we explore and analyze the potential of FL for enabling a wide range of IoT services, including IoT data sharing, data offloading and caching, attack detection, localization, mobile crowdsensing, and IoT privacy and security. We then provide an extensive survey of the use of FL in various key IoT applications such as smart healthcare, smart transportation, Unmanned Aerial Vehicles (UAVs), smart cities, and smart industry. The important lessons learned from this review of the FL-IoT services and applications are also highlighted. We complete this survey by highlighting the current challenges and possible directions for future research in this booming area.",CS,AI_ML,0.85,Extracted from log - paper 842
A Comprehensive Survey on Internet of Things (IoT) Toward 5G Wireless Systems,"Recently, wireless technologies have been growing actively all around the world. In the context of wireless technology, fifth-generation (5G) technology has become a most challenging and interesting topic in wireless research. This article provides an overview of the Internet of Things (IoT) in 5G wireless systems. IoT in the 5G system will be a game changer in the future generation. It will open a door for new wireless architecture and smart services. Recent cellular network LTE (4G) will not be sufficient and efficient to meet the demands of multiple device connectivity and high data rate, more bandwidth, low-latency quality of service (QoS), and low interference. To address these challenges, we consider 5G as the most promising technology. We provide a detailed overview of challenges and vision of various communication industries in 5G IoT systems. The different layers in 5G IoT systems are discussed in detail. This article provides a comprehensive review on emerging and enabling technologies related to the 5G system that enables IoT. We consider the technology drivers for 5G wireless technology, such as 5G new radio (NR), multiple-input–multiple-output antenna with the beamformation technology, mm-wave commutation technology, heterogeneous networks (HetNets), the role of augmented reality (AR) in IoT, which are discussed in detail. We also provide a review on low-power wide-area networks (LPWANs), security challenges, and its control measure in the 5G IoT scenario. This article introduces the role of AR in the 5G IoT scenario. This article also discusses the research gaps and future directions. The focus is also on application areas of IoT in 5G systems. We, therefore, outline some of the important research directions in 5G IoT.",CS,AI_ML,0.85,Extracted from log - paper 843
A Study of LoRa: Long Range & Low Power Networks for the Internet of Things,"LoRa is a long-range, low-power, low-bitrate, wireless telecommunications system, promoted as an infrastructure solution for the Internet of Things: end-devices use LoRa across a single wireless hop to communicate to gateway(s), connected to the Internet and which act as transparent bridges and relay messages between these end-devices and a central network server. This paper provides an overview of LoRa and an in-depth analysis of its functional components. The physical and data link layer performance is evaluated by field tests and simulations. Based on the analysis and evaluations, some possible solutions for performance enhancements are proposed.",CS,AI_ML,0.85,Extracted from log - paper 844
"Internet of Things: Architectures, Protocols, and Applications","The Internet of Things (IoT) is defined as a paradigm in which objects equipped with sensors, actuators, and processors communicate with each other to serve a meaningful purpose. In this paper, we survey state-of-the-art methods, protocols, and applications in this new emerging area. This survey paper proposes a novel taxonomy for IoT technologies, highlights some of the most important technologies, and profiles some applications that have the potential to make a striking difference in human life, especially for the differently abled and the elderly. As compared to similar survey papers in the area, this paper is far more comprehensive in its coverage and exhaustively covers most major technologies spanning from sensors to applications.",CS,AI_ML,0.85,Extracted from log - paper 845
"iFogSim: A toolkit for modeling and simulation of resource management techniques in the Internet of Things, Edge and Fog computing environments","Internet of Things (IoT) aims to bring every object (eg, smart cameras, wearable, environmental sensors, home appliances, and vehicles) online, hence generating massive volume of data that can overwhelm storage systems and data analytics applications. Cloud computing offers services at the infrastructure level that can scale to IoT storage and processing requirements. However, there are applications such as health monitoring and emergency response that require low latency, and delay that is caused by transferring data to the cloud and then back to the application can seriously impact their performances. To overcome this limitation, Fog computing paradigm has been proposed, where cloud services are extended to the edge of the network to decrease the latency and network congestion. To realize the full potential of Fog and IoT paradigms for real‐time analytics, several challenges need to be addressed. The first and most critical problem is designing resource management techniques that determine which modules of analytics applications are pushed to each edge device to minimize the latency and maximize the throughput. To this end, we need an evaluation platform that enables the quantification of performance of resource management policies on an IoT or Fog computing infrastructure in a repeatable manner. In this paper we propose a simulator, called iFogSim, to model IoT and Fog environments and measure the impact of resource management techniques in latency, network congestion, energy consumption, and cost. We describe two case studies to demonstrate modeling of an IoT environment and comparison of resource management policies. Moreover, scalability of the simulation toolkit of RAM consumption and execution time is verified under different circumstances.",CS,AI_ML,0.85,Extracted from log - paper 846
A Survey on 5G Networks for the Internet of Things: Communication Technologies and Challenges,"The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world via heterogeneous smart devices through seamless connectivity. The current demand for machine-type communications (MTC) has resulted in a variety of communication technologies with diverse service requirements to achieve the modern IoT vision. More recent cellular standards like long-term evolution (LTE) have been introduced for mobile devices but are not well suited for low-power and low data rate devices such as the IoT devices. To address this, there is a number of emerging IoT standards. Fifth generation (5G) mobile network, in particular, aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT. In this paper, the state-of-the-art of the IoT application requirements along with their associated communication technologies are surveyed. In addition, the third generation partnership project cellular-based low-power wide area solutions to support and enable the new service requirements for Massive to Critical IoT use cases are discussed in detail, including extended coverage global system for mobile communications for the Internet of Things, enhanced machine-type communications, and narrowband-Internet of Things. Furthermore, 5G new radio enhancements for new service requirements and enabling technologies for the IoT are introduced. This paper presents a comprehensive review related to emerging and enabling technologies with main focus on 5G mobile networks that is envisaged to support the exponential traffic growth for enabling the IoT. The challenges and open research directions pertinent to the deployment of massive to critical IoT applications are also presented in coming up with an efficient context-aware congestion control mechanism.",CS,AI_ML,0.85,Extracted from log - paper 847
Internet of Things (IoT): A Literature Review,"One of the buzzwords in the Information Technology is Internet of Things (IoT). The future is Internet of Things, which will transform the real world objects into intelligent virtual objects. The IoT aims to unify everything in our world under a common infrastructure, giving us not only control of things around us, but also keeping us informed of the state of the things. In Light of this, present study addresses IoT concepts through systematic review of scholarly research papers, corporate white papers, professional discussions with experts and online databases. Moreover this research article focuses on definitions, geneses, basic requirements, characteristics and aliases of Internet of Things. The main objective of this paper is to provide an overview of Internet of Things, architectures, and vital technologies and their usages in our daily life. However, this manuscript will give good comprehension for the new researchers, who want to do research in this field of Internet of Things (Technological GOD) and facilitate knowledge accumulation in efficiently.",CS,AI_ML,0.85,Extracted from log - paper 848
Smart Farming: Internet of Things (IoT)-Based Sustainable Agriculture,"Smart farming is a development that has emphasized information and communication technology used in machinery, equipment, and sensors in network-based hi-tech farm supervision cycles. Innovative technologies, the Internet of Things (IoT), and cloud computing are anticipated to inspire growth and initiate the use of robots and artificial intelligence in farming. Such ground-breaking deviations are unsettling current agriculture approaches, while also presenting a range of challenges. This paper investigates the tools and equipment used in applications of wireless sensors in IoT agriculture, and the anticipated challenges faced when merging technology with conventional farming activities. Furthermore, this technical knowledge is helpful to growers during crop periods from sowing to harvest; and applications in both packing and transport are also investigated.",CS,AI_ML,0.85,Extracted from log - paper 849
Blockchain for Internet of Things: A Survey,"Internet of Things (IoT) is reshaping the incumbent industry to smart industry featured with data-driven decision-making. However, intrinsic features of IoT result in a number of challenges, such as decentralization, poor interoperability, privacy, and security vulnerabilities. Blockchain technology brings the opportunities in addressing the challenges of IoT. In this paper, we investigate the integration of blockchain technology with IoT. We name such synthesis of blockchain and IoT as blockchain of things (BCoT). This paper presents an in-depth survey of BCoT and discusses the insights of this new paradigm. In particular, we first briefly introduce IoT and discuss the challenges of IoT. Then, we give an overview of blockchain technology. We next concentrate on introducing the convergence of blockchain and IoT and presenting the proposal of BCoT architecture. We further discuss the issues about using blockchain for fifth generation beyond in IoT as well as industrial applications of BCoT. Finally, we outline the open research directions in this promising area.",CS,AI_ML,0.85,Extracted from log - paper 850
Mobile Unmanned Aerial Vehicles (UAVs) for Energy-Efficient Internet of Things Communications,"In this paper, the efficient deployment and mobility of multiple unmanned aerial vehicles (UAVs), used as aerial base stations to collect data from ground Internet of Things (IoT) devices, are investigated. In particular, to enable reliable uplink communications for the IoT devices with a minimum total transmit power, a novel framework is proposed for jointly optimizing the 3D placement and the mobility of the UAVs, device-UAV association, and uplink power control. First, given the locations of active IoT devices at each time instant, the optimal UAVs’ locations and associations are determined. Next, to dynamically serve the IoT devices in a time-varying network, the optimal mobility patterns of the UAVs are analyzed. To this end, based on the activation process of the IoT devices, the time instances at which the UAVs must update their locations are derived. Moreover, the optimal 3D trajectory of each UAV is obtained in a way that the total energy used for the mobility of the UAVs is minimized while serving the IoT devices. Simulation results show that, using the proposed approach, the total-transmit power of the IoT devices is reduced by 45% compared with a case, in which stationary aerial base stations are deployed. In addition, the proposed approach can yield a maximum of 28% enhanced system reliability compared with the stationary case. The results also reveal an inherent tradeoff between the number of update times, the mobility of the UAVs, and the transmit power of the IoT devices. In essence, a higher number of updates can lead to lower transmit powers for the IoT devices at the cost of an increased mobility for the UAVs.",CS,AI_ML,0.85,Extracted from log - paper 851
IoT Solutions in Agriculture: Enhancing Efficiency and Productivity,"The agricultural sector is on the brink of a transformative era with the emergence of Internet of Things (IoT) technologies. This paper delves into integrating IoT solutions in agriculture, focusing on how these technologies can significantly enhance efficiency, productivity, and sustainability. It explores various IoT applications, including precision farming, automated irrigation, soil monitoring, and pest control, and discusses their benefits and challenges. The study underlines the immense potential of IoT in shaping the future of agriculture by harnessing real-time data, advanced analytics, and intelligent decision-making systems.",CS,AI_ML,0.85,Extracted from log - paper 852
CICIoT2023: A Real-Time Dataset and Benchmark for Large-Scale Attacks in IoT Environment,"Nowadays, the Internet of Things (IoT) concept plays a pivotal role in society and brings new capabilities to different industries. The number of IoT solutions in areas such as transportation and healthcare is increasing and new services are under development. In the last decade, society has experienced a drastic increase in IoT connections. In fact, IoT connections will increase in the next few years across different areas. Conversely, several challenges still need to be faced to enable efficient and secure operations (e.g., interoperability, security, and standards). Furthermore, although efforts have been made to produce datasets composed of attacks against IoT devices, several possible attacks are not considered. Most existing efforts do not consider an extensive network topology with real IoT devices. The main goal of this research is to propose a novel and extensive IoT attack dataset to foster the development of security analytics applications in real IoT operations. To accomplish this, 33 attacks are executed in an IoT topology composed of 105 devices. These attacks are classified into seven categories, namely DDoS, DoS, Recon, Web-based, brute force, spoofing, and Mirai. Finally, all attacks are executed by malicious IoT devices targeting other IoT devices. The dataset is available on the CIC Dataset website.",CS,AI_ML,0.85,Extracted from log - paper 853
Integration of IoT-Enabled Technologies and Artificial Intelligence (AI) for Smart City Scenario: Recent Advancements and Future Trends,"As the global population grows, and urbanization becomes more prevalent, cities often struggle to provide convenient, secure, and sustainable lifestyles due to the lack of necessary smart technologies. Fortunately, the Internet of Things (IoT) has emerged as a solution to this challenge by connecting physical objects using electronics, sensors, software, and communication networks. This has transformed smart city infrastructures, introducing various technologies that enhance sustainability, productivity, and comfort for urban dwellers. By leveraging Artificial Intelligence (AI) to analyze the vast amount of IoT data available, new opportunities are emerging to design and manage futuristic smart cities. In this review article, we provide an overview of smart cities, defining their characteristics and exploring the architecture of IoT. A detailed analysis of various wireless communication technologies employed in smart city applications is presented, with extensive research conducted to determine the most appropriate communication technologies for specific use cases. The article also sheds light on different AI algorithms and their suitability for smart city applications. Furthermore, the integration of IoT and AI in smart city scenarios is discussed, emphasizing the potential contributions of 5G networks coupled with AI in advancing modern urban environments. This article contributes to the existing literature by highlighting the tremendous opportunities presented by integrating IoT and AI, paving the way for the development of smart cities that significantly enhance the quality of life for urban dwellers while promoting sustainability and productivity. By exploring the potential of IoT, AI, and their integration, this review article provides valuable insights into the future of smart cities, demonstrating how these technologies can positively impact urban environments and the well-being of their inhabitants.",CS,AI_ML,0.85,Extracted from log - paper 854
Exploring the Full Potentials of IoT for Better Financial Growth and Stability: A Comprehensive Survey,"Cutting-edge technologies, with a special emphasis on the Internet of Things (IoT), tend to operate as game changers, generating enormous alterations in both traditional and modern enterprises. Understanding multiple uses of IoT has become vital for effective financial management, given the ever-changing nature of organizations and the technological disruptions that come with this paradigm change. IoT has proven to be a powerful tool for improving operational efficiency, decision-making processes, overall productivity, and data management. As a result of the continuously expanding data volume, there is an increasing demand for a robust IT system capable of adeptly handling all enterprise processes. Consequently, businesses must develop suitable IoT architectures that can efficiently address these continually evolving requirements. This research adopts an incremental explanatory approach, guided by the principles of the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA). A rigorous examination of 84 research papers has allowed us to delve deeply into the current landscape of IoT research. This research aims to provide a complete and cohesive overview of the existing body of knowledge on IoT. This is accomplished by combining a rigorous empirical approach to categorization with ideas from specialized literature in the IoT sector. This study actively contributes to the ongoing conversation around IoT by recognizing and critically examining current difficulties. This, consequently, opens new research possibilities and promotes future developments in this ever-changing sector.",CS,AI_ML,0.85,Extracted from log - paper 855
Edge-IIoTset: A New Comprehensive Realistic Cyber Security Dataset of IoT and IIoT Applications for Centralized and Federated Learning,"In this paper, we propose a new comprehensive realistic cyber security dataset of IoT and IIoT applications, called Edge-IIoTset, which can be used by machine learning-based intrusion detection systems in two different modes, namely, centralized and federated learning. Specifically, the dataset has been generated using a purpose-built IoT/IIoT testbed with a large representative set of devices, sensors, protocols and cloud/edge configurations. The IoT data are generated from various IoT devices (more than 10 types) such as Low-cost digital sensors for sensing temperature and humidity, Ultrasonic sensor, Water level detection sensor, pH Sensor Meter, Soil Moisture sensor, Heart Rate Sensor, Flame Sensor, etc.). Furthermore, we identify and analyze fourteen attacks related to IoT and IIoT connectivity protocols, which are categorized into five threats, including, DoS/DDoS attacks, Information gathering, Man in the middle attacks, Injection attacks, and Malware attacks. In addition, we extract features obtained from different sources, including alerts, system resources, logs, network traffic, and propose new 61 features with high correlations from 1176 found features. After processing and analyzing the proposed realistic cyber security dataset, we provide a primary exploratory data analysis and evaluate the performance of machine learning approaches (i.e., traditional machine learning as well as deep learning) in both centralized and federated learning modes. The Edge-IIoTset dataset can be publicly accessed from [1].",CS,AI_ML,0.85,Extracted from log - paper 856
A Novel Deep Learning-Based Intrusion Detection System for IoT Networks,"The impressive growth rate of the Internet of Things (IoT) has drawn the attention of cybercriminals more than ever. The growing number of cyber-attacks on IoT devices and intermediate communication media backs the claim. Attacks on IoT, if they remain undetected for an extended period, cause severe service interruption resulting in financial loss. It also imposes the threat of identity protection. Detecting intrusion on IoT devices in real-time is essential to make IoT-enabled services reliable, secure, and profitable. This paper presents a novel Deep Learning (DL)-based intrusion detection system for IoT devices. This intelligent system uses a four-layer deep Fully Connected (FC) network architecture to detect malicious traffic that may initiate attacks on connected IoT devices. The proposed system has been developed as a communication protocol-independent system to reduce deployment complexities. The proposed system demonstrates reliable performance for simulated and real intrusions during the experimental performance analysis. It detects the Blackhole, Distributed Denial of Service, Opportunistic Service, Sinkhole, and Workhole attacks with an average accuracy of 93.74%. The proposed intrusion detection system’s precision, recall, and F1-score are 93.71%, 93.82%, and 93.47%, respectively, on average. This innovative deep learning-based IDS maintains a 93.21% average detection rate which is satisfactory for improving the security of IoT networks.",CS,AI_ML,0.85,Extracted from log - paper 857
AI for UAV-Assisted IoT Applications: A Comprehensive Review,"With the rapid development of the Internet of Things (IoT), there are a dramatically increasing number of devices, leading to the fact that only using terrestrial infrastructure can hardly provide high-quality services to all devices. Due to their flexibility, maneuverability, and economy, unmanned aerial vehicles (UAVs) are widely used to improve the performance of IoT networks. UAVs can not only provide wireless access to IoT devices in the absence of a terrestrial network but can also perform rich IoT services and applications, such as video surveillance, cargo transportation, pesticide spraying, and so forth. However, due to the high complexity, dynamics, and heterogeneity of the UAV-assisted IoT networks, growing attention has focused on using artificial intelligence (AI)-based methods to optimize, schedule, and orchestrate UAV-assisted IoT networks. In this article, we comprehensively analyze the impact of applying advanced AI architectures, models, and methods to different aspects of UAV-assisted IoT networks, including key IoT technologies, tasks, and applications. In addition, this article also explores challenges and discusses potential research directions of AI-enabled UAV-assisted IoT networks.",CS,AI_ML,0.85,Extracted from log - paper 858
"Unleashing the Power of IoT: A Comprehensive Review of IoT Applications and Future Prospects in Healthcare, Agriculture, Smart Homes, Smart Cities, and Industry 4.0","The Internet of Things (IoT) technology and devices represent an exciting field in computer science that is rapidly emerging worldwide. The demand for automation and efficiency has also been a contributing factor to the advancements in this technology. The proliferation of IoT devices coincides with advancements in wireless networking technologies, driven by the enhanced connectivity of the internet. Today, nearly any everyday object can be connected to the network, reflecting the growing demand for automation and efficiency. This paper reviews the emergence of IoT devices, analyzed their common applications, and explored the future prospects in this promising field of computer science. The examined applications encompass healthcare, agriculture, and smart cities. Although IoT technology exhibits similar deployment trends, this paper will explore different fields to discern the subtle nuances that exist among them. To comprehend the future of IoT, it is essential to comprehend the driving forces behind its advancements in various industries. By gaining a better understanding of the emergence of IoT devices, readers will develop insights into the factors that have propelled their growth and the conditions that led to technological advancements. Given the rapid pace at which IoT technology is advancing, this paper provides researchers with a deeper understanding of the factors that have brought us to this point and the ongoing efforts that are actively shaping the future of IoT. By offering a comprehensive analysis of the current landscape and potential future developments, this paper serves as a valuable resource to researchers seeking to contribute to and navigate the ever-evolving IoT ecosystem.",CS,AI_ML,0.85,Extracted from log - paper 859
"IoT-Enabled Smart Agriculture: Architecture, Applications, and Challenges","The growth of the global population coupled with a decline in natural resources, farmland, and the increase in unpredictable environmental conditions leads to food security is becoming a major concern for all nations worldwide. These problems are motivators that are driving the agricultural industry to transition to smart agriculture with the application of the Internet of Things (IoT) and big data solutions to improve operational efficiency and productivity. The IoT integrates a series of existing state-of-the-art solutions and technologies, such as wireless sensor networks, cognitive radio ad hoc networks, cloud computing, big data, and end-user applications. This study presents a survey of IoT solutions and demonstrates how IoT can be integrated into the smart agriculture sector. To achieve this objective, we discuss the vision of IoT-enabled smart agriculture ecosystems by evaluating their architecture (IoT devices, communication technologies, big data storage, and processing), their applications, and research timeline. In addition, we discuss trends and opportunities of IoT applications for smart agriculture and also indicate the open issues and challenges of IoT application in smart agriculture. We hope that the findings of this study will constitute important guidelines in research and promotion of IoT solutions aiming to improve the productivity and quality of the agriculture sector as well as facilitating the transition towards a future sustainable environment with an agroecological approach.",CS,AI_ML,0.85,Extracted from log - paper 860
A Survey on Indoor Positioning Systems for IoT-Based Applications,"The Internet of Things (IoT), as a pervasive paradigm, is becoming an integral part of the tech industry and academic research in recent years. It forms a ubiquitous heterogeneous network connecting humans and things. The basic premise is acquiring data from the environment with sensors and remote intelligent management via actuators. For IoT service providers, time and place are functional parameters. Whereas most IoT scenarios are in indoor spaces and GPS cannot fully cover them, applying an indoor positioning system (IPS) is necessary. Besides, indoor enabling technologies can leverage the capability of IoT in context-aware services. In this article, we aim to provide a panoramic view of IPSs and localization services with the centrality of IoT. First, we explain the main concepts and review the latest positioning methods, techniques, and technologies with IoT remarks. Then, we discuss technical implementation challenges and open issues with feasible solutions. Finally, we mentioned location-based services (LBSs), real IoT applications, and active vendors in the realm of positioning services. This article provides a real insight into LBSs in IoT for future research.",CS,AI_ML,0.85,Extracted from log - paper 861
Design and Implementation of ESP32-Based IoT Devices,"The Internet of Things (IoT) has become a transformative technology with great potential in various sectors, including home automation, industrial control, environmental monitoring, agriculture, wearables, health monitoring, and others. The growing presence of IoT devices stimulates schools and academic institutions to integrate IoT into the educational process, since IoT skills are in demand in the labor market. This paper presents educational IoT tools and technologies that simplify the design, implementation, and testing of IoT applications. The article presents the introductory IoT course that students perform initially and then presents some of the projects that they develop and implement on their own later in the project.",CS,AI_ML,0.85,Extracted from log - paper 862
"Internet of Things (IoT) Security Intelligence: A Comprehensive Overview, Machine Learning Solutions and Research Directions","The Internet of Things (IoT) is one of the most widely used technologies today, and it has a significant effect on our lives in a variety of ways, including social, commercial, and economic aspects. In terms of automation, productivity, and comfort for consumers across a wide range of application areas, from education to smart cities, the present and future IoT technologies hold great promise for improving the overall quality of human life. However, cyber-attacks and threats greatly affect smart applications in the environment of IoT. The traditional IoT security techniques are insufficient with the recent security challenges considering the advanced booming of different kinds of attacks and threats. Utilizing artificial intelligence (AI) expertise, especially machine and deep learning solutions , is the key to delivering a dynamically enhanced and up-to-date security system for the next-generation IoT system. Throughout this article, we present a comprehensive picture on IoT security intelligence , which is built on machine and deep learning technologies that extract insights from raw data to intelligently protect IoT devices against a variety of cyber-attacks. Finally, based on our study, we highlight the associated research issues and future directions within the scope of our study. Overall, this article aspires to serve as a reference point and guide, particularly from a technical standpoint, for cybersecurity experts and researchers working in the context of IoT.",CS,AI_ML,0.85,Extracted from log - paper 863
"IoT-Enabled Smart Cities: A Review of Concepts, Frameworks and Key Technologies","In recent years, smart cities have been significantly developed and have greatly expanded their potential. In fact, novel advancements to the Internet of things (IoT) have paved the way for new possibilities, representing a set of key enabling technologies for smart cities and allowing the production and automation of innovative services and advanced applications for the different city stakeholders. This paper presents a review of the research literature on IoT-enabled smart cities, with the aim of highlighting the main trends and open challenges of adopting IoT technologies for the development of sustainable and efficient smart cities. This work first provides a survey on the key technologies proposed in the literature for the implementation of IoT frameworks, and then a review of the main smart city approaches and frameworks, based on classification into eight domains, which extends the traditional six domain classification that is typically adopted in most of the related works.",CS,AI_ML,0.85,Extracted from log - paper 864
N-BaIoT—Network-Based Detection of IoT Botnet Attacks Using Deep Autoencoders,"The proliferation of IoT devices that can be more easily compromised than desktop computers has led to an increase in IoT-based botnet attacks. To mitigate this threat, there is a need for new methods that detect attacks launched from compromised IoT devices and that differentiate between hours- and milliseconds-long IoT-based attacks. In this article, we propose a novel network-based anomaly detection method for the IoT called N-BaIoT that extracts behavior snapshots of the network and uses deep autoencoders to detect anomalous network traffic from compromised IoT devices. To evaluate our method, we infected nine commercial IoT devices in our lab with two widely known IoT-based botnets, Mirai and BASHLITE. The evaluation results demonstrated our proposed methods ability to accurately and instantly detect the attacks as they were being launched from the compromised IoT devices that were part of a botnet.",CS,AI_ML,0.85,Extracted from log - paper 865
TON_IoT Telemetry Dataset: A New Generation Dataset of IoT and IIoT for Data-Driven Intrusion Detection Systems,"Although the Internet of Things (IoT) can increase efficiency and productivity through intelligent and remote management, it also increases the risk of cyber-attacks. The potential threats to IoT applications and the need to reduce risk have recently become an interesting research topic. It is crucial that effective Intrusion Detection Systems (IDSs) tailored to IoT applications be developed. Such IDSs require an updated and representative IoT dataset for training and evaluation. However, there is a lack of benchmark IoT and IIoT datasets for assessing IDSs-enabled IoT systems. This paper addresses this issue and proposes a new data-driven IoT/IIoT dataset with the ground truth that incorporates a label feature indicating normal and attack classes, as well as a type feature indicating the sub-classes of attacks targeting IoT/IIoT applications for multi-classification problems. The proposed dataset, which is named TON_IoT, includes Telemetry data of IoT/IIoT services, as well as Operating Systems logs and Network traffic of IoT network, collected from a realistic representation of a medium-scale network at the Cyber Range and IoT Labs at the UNSW Canberra (Australia). This paper also describes the proposed dataset of the Telemetry data of IoT/IIoT services and their characteristics. TON_IoT has various advantages that are currently lacking in the state-of-the-art datasets: i) it has various normal and attack events for different IoT/IIoT services, and ii) it includes heterogeneous data sources. We evaluated the performance of several popular Machine Learning (ML) methods and a Deep Learning model in both binary and multi-class classification problems for intrusion detection purposes using the proposed Telemetry dataset.",CS,AI_ML,0.85,Extracted from log - paper 866
"Cellular, Wide-Area, and Non-Terrestrial IoT: A Survey on 5G Advances and the Road Toward 6G","The next wave of wireless technologies is proliferating in connecting things among themselves as well as to humans. In the era of the Internet of Things (IoT), billions of sensors, machines, vehicles, drones, and robots will be connected, making the world around us smarter. The IoT will encompass devices that must wirelessly communicate a diverse set of data gathered from the environment for myriad new applications. The ultimate goal is to extract insights from this data and develop solutions that improve quality of life and generate new revenue. Providing large-scale, long-lasting, reliable, and near real-time connectivity is the major challenge in enabling a smart connected world. This paper provides a comprehensive survey on existing and emerging communication solutions for serving IoT applications in the context of cellular, wide-area, as well as non-terrestrial networks. Specifically, wireless technology enhancements for providing IoT access in the fifth-generation (5G) and beyond cellular networks, and communication networks over the unlicensed spectrum are presented. Aligned with the main key performance indicators of 5G and beyond 5G networks, we investigate solutions and standards that enable energy efficiency, reliability, low latency, and scalability (connection density) of current and future IoT networks. The solutions include grant-free access and channel coding for short-packet communications, non-orthogonal multiple access, and on-device intelligence. Further, a vision of new paradigm shifts in communication networks in the 2030s is provided, and the integration of the associated new technologies like artificial intelligence, non-terrestrial networks, and new spectra is elaborated. In particular, the potential of using emerging deep learning and federated learning techniques for enhancing the efficiency and security of IoT communication are discussed, and their promises and challenges are introduced. Finally, future research directions toward beyond 5G IoT networks are pointed out.",CS,AI_ML,0.85,Extracted from log - paper 867
A Survey on Federated Learning for Resource-Constrained IoT Devices,"Federated learning (FL) is a distributed machine learning strategy that generates a global model by learning from multiple decentralized edge clients. FL enables on-device training, keeping the client’s local data private, and further, updating the global model based on the local model updates. While FL methods offer several advantages, including scalability and data privacy, they assume there are available computational resources at each edge-device/client. However, the Internet-of-Things (IoT)-enabled devices, e.g., robots, drone swarms, and low-cost computing devices (e.g., Raspberry Pi), may have limited processing ability, low bandwidth and power, or limited storage capacity. In this survey article, we propose to answer this question: how to train distributed machine learning models for resource-constrained IoT devices? To this end, we first explore the existing studies on FL, relative assumptions for distributed implementation using IoT devices, and explore their drawbacks. We then discuss the implementation challenges and issues when applying FL to an IoT environment. We highlight an overview of FL and provide a comprehensive survey of the problem statements and emerging challenges, particularly during applying FL within heterogeneous IoT environments. Finally, we point out the future research directions for scientists and researchers who are interested in working at the intersection of FL and resource-constrained IoT environments.",CS,AI_ML,0.85,Extracted from log - paper 868
Enabling Massive IoT Toward 6G: A Comprehensive Survey,"Nowadays, many disruptive Internet-of-Things (IoT) applications emerge, such as augmented/virtual reality online games, autonomous driving, and smart everything, which are massive in number, data intensive, computation intensive, and delay sensitive. Due to the mismatch between the fifth generation (5G) and the requirements of such massive IoT-enabled applications, there is a need for technological advancements and evolutions for wireless communications and networking toward the sixth-generation (6G) networks. 6G is expected to deliver extended 5G capabilities at a very high level, such as Tbps data rate, sub-ms latency, cm-level localization, and so on, which will play a significant role in supporting massive IoT devices to operate seamlessly with highly diverse service requirements. Motivated by the aforementioned facts, in this article, we present a comprehensive survey on 6G-enabled massive IoT. First, we present the drivers and requirements by summarizing the emerging IoT-enabled applications and the corresponding requirements, along with the limitations of 5G. Second, visions of 6G are provided in terms of core technical requirements, use cases, and trends. Third, a new network architecture provided by 6G to enable massive IoT is introduced, i.e., space–air–ground–underwater/sea networks enhanced by edge computing. Fourth, some breakthrough technologies, such as machine learning and blockchain, in 6G are introduced, where the motivations, applications, and open issues of these technologies for massive IoT are summarized. Finally, a use case of fully autonomous driving is presented to show 6G supports massive IoT.",CS,AI_ML,0.85,Extracted from log - paper 869
"A Survey on IoT Security: Application Areas, Security Threats, and Solution Architectures","The Internet of Things (IoT) is the next era of communication. Using the IoT, physical objects can be empowered to create, receive, and exchange data in a seamless manner. Various IoT applications focus on automating different tasks and are trying to empower the inanimate physical objects to act without any human intervention. The existing and upcoming IoT applications are highly promising to increase the level of comfort, efficiency, and automation for the users. To be able to implement such a world in an ever-growing fashion requires high security, privacy, authentication, and recovery from attacks. In this regard, it is imperative to make the required changes in the architecture of the IoT applications for achieving end-to-end secure IoT environments. In this paper, a detailed review of the security-related challenges and sources of threat in the IoT applications is presented. After discussing the security issues, various emerging and existing technologies focused on achieving a high degree of trust in the IoT applications are discussed. Four different technologies, blockchain, fog computing, edge computing, and machine learning, to increase the level of security in IoT are discussed.",CS,AI_ML,0.85,Extracted from log - paper 870
"IoT in Smart Cities: A Survey of Technologies, Practices and Challenges","Internet of Things (IoT) is a system that integrates different devices and technologies, removing the necessity of human intervention. This enables the capacity of having smart (or smarter) cities around the world. By hosting different technologies and allowing interactions between them, the internet of things has spearheaded the development of smart city systems for sustainable living, increased comfort and productivity for citizens. The IoT for Smart Cities has many different domains and draws upon various underlying systems for its operation. In this paper, we provide a holistic coverage of the Internet of Things in Smart Cities. We start by discussing the fundamental components that make up the IoT based Smart City landscape followed by the technologies that enable these domains to exist in terms of architectures utilized, networking technologies used as well as the Artificial Algorithms deployed in IoT based Smart City systems. This is then followed up by a review of the most prevalent practices and applications in various Smart City domains. Lastly, the challenges that deployment of IoT systems for smart cities encounter along with mitigation measures.",CS,AI_ML,0.85,Extracted from log - paper 871
IOT BASED SMART AGRICULTURE,"Agriculture plays a vital role in the development of an agricultural country. Around 70% of people depend on farming in our country, and one-third of the nation's income comes from agriculture. Issues concerning agriculture have been continually impeding the improvement of the nation. The main answer for this issue is smart agriculture by modernizing the current customary strategies for farming. Henceforth the undertaking targets making agribusiness smart using computerization and IoT advances. The featuring highlights of this project incorporate smart GPSbased remote-controlled robots to perform errands like weeding, sensing moisture, animal and bird scaring, spraying, and so on. Also, it contains smart irrigation with savvy control and canny dynamic dependent on precise ongoing field information. Thirdly, smart warehouse management includes temperature support, humidity maintenance, and burglary discovery in the stockroom. Controlling of every one of these tasks will be through any far-off shrewd gadget or PC associated with the Web, and the tasks will be performed by interfacing sensors, Wi-Fi or ZigBee modules, camera and actuators with miniature regulator and raspberry pi",CS,AI_ML,0.85,Extracted from log - paper 872
Federated-Learning-Based Anomaly Detection for IoT Security Attacks,"The Internet of Things (IoT) is made up of billions of physical devices connected to the Internet via networks that perform tasks independently with less human intervention. Such brilliant automation of mundane tasks requires a considerable amount of user data in digital format, which, in turn, makes IoT networks an open source of personally identifiable information data for malicious attackers to steal, manipulate, and perform nefarious activities. A huge interest has been developed over the past years in applying machine learning (ML)-assisted approaches in the IoT security space. However, the assumption in many current works is that big training data are widely available and transferable to the main server because data are born at the edge and are generated continuously by IoT devices. This is to say that classic ML works on the legacy set of entire data located on a central server, which makes it the least preferred option for domains with privacy concerns on user data. To address this issue, we propose the federated-learning (FL)-based anomaly detection approach to proactively recognize intrusion in IoT networks using decentralized on-device data. Our approach uses federated training rounds on gated recurrent units (GRUs) models and keeps the data intact on local IoT devices by sharing only the learned weights with the central server of FL. Also, the approach’s ensembler part aggregates the updates from multiple sources to optimize the global ML model’s accuracy. Our experimental results demonstrate that our approach outperforms the classic/centralized machine learning (non-FL) versions in securing the privacy of user data and provides an optimal accuracy rate in attack detection.",CS,AI_ML,0.85,Extracted from log - paper 873
Classifying IoT Devices in Smart Environments Using Network Traffic Characteristics,"The Internet of Things (IoT) is being hailed as the next wave revolutionizing our society, and smart homes, enterprises, and cities are increasingly being equipped with a plethora of IoT devices. Yet, operators of such smart environments may not even be fully aware of their IoT assets, let alone whether each IoT device is functioning properly safe from cyber-attacks. In this paper, we address this challenge by developing a robust framework for IoT device classification using traffic characteristics obtained at the network level. Our contributions are fourfold. First, we instrument a smart environment with 28 different IoT devices spanning cameras, lights, plugs, motion sensors, appliances, and health-monitors. We collect and synthesize traffic traces from this infrastructure for a period of six months, a subset of which we release as open data for the community to use. Second, we present insights into the underlying network traffic characteristics using statistical attributes such as activity cycles, port numbers, signalling patterns, and cipher suites. Third, we develop a multi-stage machine learning based classification algorithm and demonstrate its ability to identify specific IoT devices with over 99 percent accuracy based on their network activity. Finally, we discuss the trade-offs between cost, speed, and performance involved in deploying the classification framework in real-time. Our study paves the way for operators of smart environments to monitor their IoT assets for presence, functionality, and cyber-security without requiring any specialized devices or protocols.",CS,AI_ML,0.85,Extracted from log - paper 874
"Internet of Things (IoT) for Next-Generation Smart Systems: A Review of Current Challenges, Future Trends and Prospects for Emerging 5G-IoT Scenarios","The Internet of Things (IoT)-centric concepts like augmented reality, high-resolution video streaming, self-driven cars, smart environment, e-health care, etc. have a ubiquitous presence now. These applications require higher data-rates, large bandwidth, increased capacity, low latency and high throughput. In light of these emerging concepts, IoT has revolutionized the world by providing seamless connectivity between heterogeneous networks (HetNets). The eventual aim of IoT is to introduce the plug and play technology providing the end-user, ease of operation, remotely access control and configurability. This paper presents the IoT technology from a bird’s eye view covering its statistical/architectural trends, use cases, challenges and future prospects. The paper also presents a detailed and extensive overview of the emerging 5G-IoT scenario. Fifth Generation (5G) cellular networks provide key enabling technologies for ubiquitous deployment of the IoT technology. These include carrier aggregation, multiple-input multiple-output (MIMO), massive-MIMO (M-MIMO), coordinated multipoint processing (CoMP), device-to-device (D2D) communications, centralized radio access network (CRAN), software-defined wireless sensor networking (SD-WSN), network function virtualization (NFV) and cognitive radios (CRs). This paper presents an exhaustive review for these key enabling technologies and also discusses the new emerging use cases of 5G-IoT driven by the advances in artificial intelligence, machine and deep learning, ongoing 5G initiatives, quality of service (QoS) requirements in 5G and its standardization issues. Finally, the paper discusses challenges in the implementation of 5G-IoT due to high data-rates requiring both cloud-based platforms and IoT devices based edge computing.",CS,AI_ML,0.85,Extracted from log - paper 875
"Internet of Things (IoT): A Review of Its Enabling Technologies in Healthcare Applications, Standards Protocols, Security, and Market Opportunities","The Internet of Things (IoT) is a methodology or a system that encompasses real-world things to interact and communicate with each other with the assistance of networking technologies. This article describes surveys on advances in IoT-based healthcare methods and reviews the state-of-the-art technologies in detail. Moreover, this review classifies an existing IoT-based healthcare network and represents a summary of all perspective networks. IoT healthcare protocols are analyzed in this context and provide a broad discussion on it. It also initiates a comprehensive survey on IoT healthcare applications and services. Extensive insights into IoT healthcare security, its requirements, challenges, and privacy issues are visualized in IoT surrounding healthcare. In this review, we analyze security and privacy features consisting of data protection, network architecture, Quality of Services (QoS), app development, and continuous monitoring of healthcare that are facing difficulties in many IoT-based healthcare architectures. To mitigate the security problems, an IoT-based security architectural model has been proposed in this review. Furthermore, this review discloses the market opportunity that will enhance the IoT healthcare market development. To conduct the survey, we searched through established journal and conference databases using specific keywords to find scholarly works. We applied a filtering mechanism to collect only papers that were relevant to our research works. The selected papers were then examined carefully to understand their contributions/research focus. Eventually, the paper reviews were analyzed to identify any existing research gaps and untouched areas of research and to discover possible features for sustainable IoT healthcare development.",CS,AI_ML,0.85,Extracted from log - paper 876
Hierarchical Adversarial Attacks Against Graph-Neural-Network-Based IoT Network Intrusion Detection System,"The advancement of Internet of Things (IoT) technologies leads to a wide penetration and large-scale deployment of IoT systems across an entire city or even country. While IoT systems are capable of providing intelligent services, the large amount of data collected and processed in IoT systems also raises serious security concerns. Many research efforts have been devoted to design intelligent network intrusion detection system (NIDS) to prevent misuse of IoT data across smart applications. However, existing approaches may suffer from the issue of limited and imbalanced attack data when training the detection model, which make the system vulnerable especially for those unknown type attacks. In this study, a novel hierarchical adversarial attack (HAA) generation method is introduced to realize the level-aware black-box adversarial attack strategy, targeting the graph neural network (GNN)-based intrusion detection in IoT systems with a limited budget. By constructing a shadow GNN model, an intelligent mechanism based on a saliency map technique is designed to generate adversarial examples by effectively identifying and modifying the critical feature elements with minimal perturbations. A hierarchical node selection algorithm based on random walk with restart (RWR) is developed to select a set of more vulnerable nodes with high attack priority, considering their structural features, and overall loss changes within the targeted IoT network. The proposed HAA generation method is evaluated using the open-source data set UNSW-SOSR2019 with three baseline methods. Comparison results demonstrate its ability in degrading the classification precision by more than 30% in the two state-of-the-art GNN models, GCN and JK-Net, respectively, for NIDS in IoT environments.",CS,AI_ML,0.85,Extracted from log - paper 877
Anomaly Detection for IoT Time-Series Data: A Survey,"Anomaly detection is a problem with applications for a wide variety of domains; it involves the identification of novel or unexpected observations or sequences within the data being captured. The majority of current anomaly detection methods are highly specific to the individual use case, requiring expert knowledge of the method as well as the situation to which it is being applied. The Internet of Things (IoT) as a rapidly expanding field offers many opportunities for this type of data analysis to be implemented, however, due to the nature of the IoT, this may be difficult. This review provides a background on the challenges which may be encountered when applying anomaly detection techniques to IoT data, with examples of applications for the IoT anomaly detection taken from the literature. We discuss a range of approaches that have been developed across a variety of domains, not limited to IoT due to the relative novelty of this application. Finally, we summarize the current challenges being faced in the anomaly detection domain with a view to identifying potential research opportunities for the future.",CS,AI_ML,0.85,Extracted from log - paper 878
VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model,"With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",CS,AI_ML,0.85,Extracted from log - paper 879
AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control,"Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness.",CS,AI_ML,0.85,Extracted from log - paper 880
FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios,"Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/",CS,AI_ML,0.85,Extracted from log - paper 881
Actor-Critics Can Achieve Optimal Sample Efficiency,"Actor-critic algorithms have become a cornerstone in reinforcement learning (RL), leveraging the strengths of both policy-based and value-based methods. Despite recent progress in understanding their statistical efficiency, no existing work has successfully learned an $\epsilon$-optimal policy with a sample complexity of $O(1/\epsilon^2)$ trajectories with general function approximation when strategic exploration is necessary.   We address this open problem by introducing a novel actor-critic algorithm that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$ regret when the Bellman eluder dimension $d$ does not increase with $T$ at more than a $\log T$ rate.   Here, $\mathcal{F}$ is the critic function class, $\mathcal{A}$ is the action space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm integrates optimism, off-policy critic estimation targeting the optimal Q-function, and rare-switching policy resets.   We extend this to the setting of Hybrid RL, showing that initializing the critic with offline data yields sample efficiency gains compared to purely offline or online RL. Further, utilizing access to offline data, we provide a \textit{non-optimistic} provably efficient actor-critic algorithm that only additionally requires $N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$ in exchange for omitting optimism, where $c_{\text{off}}^*$ is the single-policy concentrability coefficient and $N_{\text{off}}$ is the number of offline samples. This addresses another open problem in the literature. We further provide numerical experiments to support our theoretical findings.",CS,AI_ML,0.85,Extracted from log - paper 882
Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid,"Assured safe-separation is essential for achieving seamless high-density operation of airborne vehicles in a shared airspace. To equip resource-constrained aerial systems with this safety-critical capability, we present ViSafe, a high-speed vision-only airborne collision avoidance system. ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by tightly integrating a learning-based edge-AI framework with a custom multi-camera hardware prototype designed under SWaP-C constraints. By leveraging perceptual input-focused control barrier functions (CBF) to design, encode, and enforce safety thresholds, ViSafe can provide provably safe runtime guarantees for self-separation in high-speed aerial operations. We evaluate ViSafe's performance through an extensive test campaign involving both simulated digital twins and real-world flight scenarios. By independently varying agent types, closure rates, interaction geometries, and environmental conditions (e.g., weather and lighting), we demonstrate that ViSafe consistently ensures self-separation across diverse scenarios. In first-of-its-kind real-world high-speed collision avoidance tests with closure rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous collision avoidance, establishing a new standard for safety in high-speed aerial navigation.",CS,AI_ML,0.85,Extracted from log - paper 883
Graph Drawing for LLMs: An Empirical Evaluation,"Our work contributes to the fast-growing literature on the use of Large Language Models (LLMs) to perform graph-related tasks. In particular, we focus on usage scenarios that rely on the visual modality, feeding the model with a drawing of the graph under analysis. We investigate how the model's performance is affected by the chosen layout paradigm, the aesthetics of the drawing, and the prompting technique used for the queries. We formulate three corresponding research questions and present the results of a thorough experimental analysis. Our findings reveal that choosing the right layout paradigm and optimizing the readability of the input drawing from a human perspective can significantly improve the performance of the model on the given task. Moreover, selecting the most effective prompting technique is a challenging yet crucial task for achieving optimal performance.",CS,AI_ML,0.85,Extracted from log - paper 884
"Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance","In human-agent teams, openly sharing goals is often assumed to enhance planning, collaboration, and effectiveness. However, direct communication of these goals is not always feasible, requiring teammates to infer their partner's intentions through actions. Building on this, we investigate whether an AI agent's ability to share its inferred understanding of a human teammate's goals can improve task performance and perceived collaboration. Through an experiment comparing three conditions-no recognition (NR), viable goals (VG), and viable goals on-demand (VGod) - we find that while goal-sharing information did not yield significant improvements in task performance or overall satisfaction scores, thematic analysis suggests that it supported strategic adaptations and subjective perceptions of collaboration. Cognitive load assessments revealed no additional burden across conditions, highlighting the challenge of balancing informativeness and simplicity in human-agent interactions. These findings highlight the nuanced trade-off of goal-sharing: while it fosters trust and enhances perceived collaboration, it can occasionally hinder objective performance gains.",CS,AI_ML,0.85,Extracted from log - paper 885
Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time,"This paper proposes an integration of temporal logical reasoning and Partially Observable Markov Decision Processes (POMDPs) to achieve interpretable decision-making under uncertainty with macro-actions. Our method leverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus (EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guide Monte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon, significantly reducing inference time while ensuring robust performance. Such macro-actions are learnt via Inductive Logic Programming (ILP) from a few traces of execution (belief-action pairs), thus eliminating the need for manually designed heuristics and requiring only the specification of the POMDP transition model. In the Pocman and Rocksample benchmark scenarios, our learned macro-actions demonstrate increased expressiveness and generality when compared to time-independent heuristics, indeed offering substantial computational efficiency improvements.",CS,AI_ML,0.85,Extracted from log - paper 886
Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models,"Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are essential for evaluating white matter integrity and structural connectivity in neuroimaging. However, the spatial misalignment between FA maps and tractography atlases hinders their effective integration into predictive models. To address this issue, we propose a CycleGAN based approach for generating FA maps directly from T1-weighted MRI scans, representing the first application of this technique to both healthy and tumour-affected tissues. Our model, trained on unpaired data, produces high fidelity maps, which have been rigorously evaluated using Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in tumour regions. Radiological assessments further underscore the model's potential to enhance clinical workflows by providing an AI-driven alternative that reduces the necessity for additional scans.",CS,AI_ML,0.85,Extracted from log - paper 887
Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems,"Recommender Systems (RSs) aim to provide personalized recommendations for users. A newly discovered bias, known as sentiment bias, uncovers a common phenomenon within Review-based RSs (RRSs): the recommendation accuracy of users or items with negative reviews deteriorates compared with users or items with positive reviews. Critical users and niche items are disadvantaged by such unfair recommendations. We study this problem from the perspective of counterfactual inference with two stages. At the model training stage, we build a causal graph and model how sentiment influences the final rating score. During the inference stage, we decouple the direct and indirect effects to mitigate the impact of sentiment bias and remove the indirect effect using counterfactual inference. We have conducted extensive experiments, and the results validate that our model can achieve comparable performance on rating prediction for better recommendations and effective mitigation of sentiment bias. To the best of our knowledge, this is the first work to employ counterfactual inference on sentiment bias mitigation in RSs.",CS,AI_ML,0.85,Extracted from log - paper 888
ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant,"Recent advances in personalized MLLMs enable effective capture of user-specific concepts, supporting both recognition of personalized concepts and contextual captioning. However, humans typically explore and reason over relations among objects and individuals, transcending surface-level information to achieve more personalized and contextual understanding. To this end, existing methods may face three main limitations: Their training data lacks multi-object sets in which relations among objects are learnable. Building on the limited training data, their models overlook the relations between different personalized concepts and fail to reason over them. Their experiments mainly focus on a single personalized concept, where evaluations are limited to recognition and captioning tasks. To address the limitations, we present a new dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard graph prompting methods are designed to align KGs within the model's semantic space. We establish the ReGraP Benchmark, which contains diverse task types: multiple-choice, fill-in-the-blank, True/False, and descriptive questions in both open- and closed-ended settings. The proposed benchmark is designed to evaluate the relational reasoning and knowledge-connection capability of personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and other competitive MLLMs. Results show that the proposed model not only learns personalized knowledge but also performs relational reasoning in responses, achieving the SoTA performance compared with the competitive methods. All the codes and datasets are released at: https://github.com/xyfyyds/ReGraP.",CS,AI_ML,0.85,Extracted from log - paper 889
Binding threshold units with artificial oscillatory neurons,"Artificial Kuramoto oscillatory neurons were recently introduced as an alternative to threshold units. Empirical evidence suggests that oscillatory units outperform threshold units in several tasks including unsupervised object discovery and certain reasoning problems. The proposed coupling mechanism for these oscillatory neurons is heterogeneous, combining a generalized Kuramoto equation with standard coupling methods used for threshold units. In this research note, we present a theoretical framework that clearly distinguishes oscillatory neurons from threshold units and establishes a coupling mechanism between them. We argue that, from a biological standpoint, oscillatory and threshold units realise distinct aspects of neural coding: roughly, threshold units model intensity of neuron firing, while oscillatory units facilitate information exchange by frequency modulation. To derive interaction between these two types of units, we constrain their dynamics by focusing on dynamical systems that admit Lyapunov functions. For threshold units, this leads to Hopfield associative memory model, and for oscillatory units it yields a specific form of generalized Kuramoto model. The resulting dynamical systems can be naturally coupled to form a Hopfield-Kuramoto associative memory model, which also admits a Lyapunov function. Various forms of coupling are possible. Notably, oscillatory neurons can be employed to implement a low-rank correction to the weight matrix of a Hopfield network. This correction can be viewed either as a form of Hebbian learning or as a popular LoRA method used for fine-tuning of large language models. We demonstrate the practical realization of this particular coupling through illustrative toy experiments.",CS,AI_ML,0.85,Extracted from log - paper 890
ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders,"Despite the extensive use of deep autoencoders (AEs) in critical applications, their adversarial robustness remains relatively underexplored compared to classification models. AE robustness is characterized by the Lipschitz bounds of its components. Existing robustness evaluation frameworks based on white-box attacks do not fully exploit the vulnerabilities of intermediate ill-conditioned layers in AEs. In the context of optimizing imperceptible norm-bounded additive perturbations to maximize output damage, existing methods struggle to effectively propagate adversarial loss gradients throughout the network, often converging to less effective perturbations. To address this, we propose a novel layer-conditioning-based adversarial optimization objective that effectively guides the adversarial map toward regions of local Lipschitz bounds by enhancing loss gradient information propagation during attack optimization. We demonstrate through extensive experiments on state-of-the-art AEs that our adversarial objective results in stronger attacks, outperforming existing methods in both universal and sample-specific scenarios. As a defense method against this attack, we introduce an inference-time adversarially trained defense plugin that mitigates the effects of adversarial examples.",CS,AI_ML,0.85,Extracted from log - paper 891
BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems,"Learning-enabled planning and control algorithms are increasingly popular, but they often lack rigorous guarantees of performance or safety. We introduce an algorithm for computing underapproximate backward reachable sets of nonlinear discrete time neural feedback loops. We then use the backward reachable sets to check goal-reaching properties. Our algorithm is based on overapproximating the system dynamics function to enable computation of underapproximate backward reachable sets through solutions of mixed-integer linear programs. We rigorously analyze the soundness of our algorithm and demonstrate it on a numerical example. Our work expands the class of properties that can be verified for learning-enabled systems.",CS,AI_ML,0.85,Extracted from log - paper 892
Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability,"Human decision-making in cognitive tasks and daily life exhibits considerable variability, shaped by factors such as task difficulty, individual preferences, and personal experiences. Understanding this variability across individuals is essential for uncovering the perceptual and decision-making mechanisms that humans rely on when faced with uncertainty and ambiguity. We present a computational framework BAM (Boundary Alignment & Manipulation framework) that combines perceptual boundary sampling in ANNs and human behavioral experiments to systematically investigate this phenomenon. Our perceptual boundary sampling algorithm generates stimuli along ANN decision boundaries that intrinsically induce significant perceptual variability. The efficacy of these stimuli is empirically validated through large-scale behavioral experiments involving 246 participants across 116,715 trials, culminating in the variMNIST dataset containing 19,943 systematically annotated images. Through personalized model alignment and adversarial generation, we establish a reliable method for simultaneously predicting and manipulating the divergent perceptual decisions of pairs of participants. This work bridges the gap between computational models and human individual difference research, providing new tools for personalized perception analysis.",CS,AI_ML,0.85,Extracted from log - paper 893
Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation,"In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation often consists of multiple components from other agents or dynamic entities in the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalization capability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework.",CS,AI_ML,0.85,Extracted from log - paper 894
BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation,"Public deliberation, as in open discussion of issues of public concern, often suffers from scattered and shallow discourse, poor sensemaking, and a disconnect from actionable policy outcomes. This paper introduces BCause, a discussion system leveraging generative AI and human-machine collaboration to transform unstructured dialogue around public issues (such as urban living, policy changes, and current socio-economic transformations) into structured, actionable democratic processes. We present three innovations: (i) importing and transforming unstructured transcripts into argumentative discussions, (ii) geo-deliberated problem-sensing via a Telegram bot for local issue reporting, and (iii) smart reporting with customizable widgets (e.g., summaries, topic modelling, policy recommendations, clustered arguments). The system's human-AI partnership preserves critical human participation to ensure ethical oversight, contextual relevance, and creative synthesis.",CS,AI_ML,0.85,Extracted from log - paper 895
LlamaFirewall: An open source guardrail system for building secure AI agents,"Large language models (LLMs) have evolved from simple chatbots into autonomous agents capable of performing complex tasks such as editing production code, orchestrating workflows, and taking higher-stakes actions based on untrusted inputs like webpages and emails. These capabilities introduce new security risks that existing security measures, such as model fine-tuning or chatbot-focused guardrails, do not fully address. Given the higher stakes and the absence of deterministic solutions to mitigate these risks, there is a critical need for a real-time guardrail monitor to serve as a final layer of defense, and support system level, use case specific safety policy definition and enforcement. We introduce LlamaFirewall, an open-source security focused guardrail framework designed to serve as a final layer of defense against security risks associated with AI Agents. Our framework mitigates risks such as prompt injection, agent misalignment, and insecure code risks through three powerful guardrails: PromptGuard 2, a universal jailbreak detector that demonstrates clear state of the art performance; Agent Alignment Checks, a chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment, which, while still experimental, shows stronger efficacy at preventing indirect injections in general scenarios than previously proposed approaches; and CodeShield, an online static analysis engine that is both fast and extensible, aimed at preventing the generation of insecure or dangerous code by coding agents. Additionally, we include easy-to-use customizable scanners that make it possible for any developer who can write a regular expression or an LLM prompt to quickly update an agent's security guardrails.",CS,AI_ML,0.85,Extracted from log - paper 896
OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents,"In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.",CS,AI_ML,0.85,Extracted from log - paper 897
Real-Time Person Image Synthesis Using a Flow Matching Model,"Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images conditioned on a target pose and a source image. This task plays a key role in various real-world applications, such as sign language video generation, AR/VR, gaming, and live streaming. In these scenarios, real-time PGPIS is critical for providing immediate visual feedback and maintaining user immersion.However, achieving real-time performance remains a significant challenge due to the complexity of synthesizing high-fidelity images from diverse and dynamic human poses. Recent diffusion-based methods have shown impressive image quality in PGPIS, but their slow sampling speeds hinder deployment in time-sensitive applications. This latency is particularly problematic in tasks like generating sign language videos during live broadcasts, where rapid image updates are required. Therefore, developing a fast and reliable PGPIS model is a crucial step toward enabling real-time interactive systems. To address this challenge, we propose a generative model based on flow matching (FM). Our approach enables faster, more stable, and more efficient training and sampling. Furthermore, the proposed model supports conditional generation and can operate in latent space, making it especially suitable for real-time PGPIS applications where both speed and quality are critical. We evaluate our proposed method, Real-Time Person Image Synthesis Using a Flow Matching Model (RPFM), on the widely used DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves near-real-time sampling speeds while maintaining performance comparable to the state-of-the-art models. Our methodology trades off a slight, acceptable decrease in generated-image accuracy for over a twofold increase in generation speed, thereby ensuring real-time performance.",CS,AI_ML,0.85,Extracted from log - paper 898
Ergodic Generative Flows,"Generative Flow Networks (GFNs) were initially introduced on directed acyclic graphs to sample from an unnormalized distribution density. Recent works have extended the theoretical framework for generative methods allowing more flexibility and enhancing application range. However, many challenges remain in training GFNs in continuous settings and for imitation learning (IL), including intractability of flow-matching loss, limited tests of non-acyclic training, and the need for a separate reward model in imitation learning. The present work proposes a family of generative flows called Ergodic Generative Flows (EGFs) which are used to address the aforementioned issues. First, we leverage ergodicity to build simple generative flows with finitely many globally defined transformations (diffeomorphisms) with universality guarantees and tractable flow-matching loss (FM loss). Second, we introduce a new loss involving cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It is designed for IL training without a separate reward model. We evaluate IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning experiments with a target reward, using the FM loss.",CS,AI_ML,0.85,Extracted from log - paper 899
Rapid AI-based generation of coverage paths for dispensing applications,"Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial role in the design of power electronics and electronic control units. Up to now, this is done manually by experts or by using optimization approaches with a high computational effort. We propose a novel AI-based approach to generate dispense paths for TIM and similar dispensing applications. It is a drop-in replacement for optimization-based approaches. An Artificial Neural Network (ANN) receives the target cooling area as input and directly outputs the dispense path. Our proposed setup does not require labels and we show its feasibility on multiple target areas. The resulting dispense paths can be directly transferred to automated manufacturing equipment and do not exhibit air entrapments. The approach of using an ANN to predict process parameters for a desired target state in real-time could potentially be transferred to other manufacturing processes.",CS,AI_ML,0.85,Extracted from log - paper 900
Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID,"The personalization of Stable Diffusion for generating professional portraits from amateur photographs is a burgeoning area, with applications in various downstream contexts. This paper investigates the impact of augmentations on improving facial resemblance when using two prominent personalization techniques: DreamBooth and InstantID. Through a series of experiments with diverse subject datasets, we assessed the effectiveness of various augmentation strategies on the generated headshots' fidelity to the original subject. We introduce FaceDistance, a wrapper around FaceNet, to rank the generations based on facial similarity, which aided in our assessment. Ultimately, this research provides insights into the role of augmentations in enhancing facial resemblance in SDXL-generated portraits, informing strategies for their effective deployment in downstream applications.",CS,AI_ML,0.85,Extracted from log - paper 901
A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning,"Inconsistent outputs and hallucinations from large language models (LLMs) are major obstacles to reliable AI systems. When different proprietary reasoning models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI, are given the same complex request, they often produce divergent results due to variations in training and inference. This paper proposes a novel consensus mechanism, inspired by distributed ledger technology, to validate and converge these outputs, treating each RM as a black-box peer. Building on the Hashgraph consensus algorithm, our approach employs gossip-about-gossip communication and virtual voting to achieve agreement among an ensemble of RMs. We present an architectural design for a prototype system in which RMs iteratively exchange and update their answers, using information from each round to improve accuracy and confidence in subsequent rounds. This approach goes beyond simple majority voting by incorporating the knowledge and cross-verification content of every model. We justify the feasibility of this Hashgraph-inspired consensus for AI ensembles and outline its advantages over traditional ensembling techniques in reducing nonfactual outputs. Preliminary considerations for implementation, evaluation criteria for convergence and accuracy, and potential challenges are discussed. The proposed mechanism demonstrates a promising direction for multi-agent AI systems to self-validate and deliver high-fidelity responses in complex tasks.",CS,AI_ML,0.85,Extracted from log - paper 902
STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game,"We introduce STORY2GAME, a novel approach to using Large Language Models to generate text-based interactive fiction games that starts by generating a story, populates the world, and builds the code for actions in a game engine that enables the story to play out interactively. Whereas a given set of hard-coded actions can artificially constrain story generation, the ability to generate actions means the story generation process can be more open-ended but still allow for experiences that are grounded in a game state. The key to successful action generation is to use LLM-generated preconditions and effects of actions in the stories as guides for what aspects of the game state must be tracked and changed by the game engine when a player performs an action. We also introduce a technique for dynamically generating new actions to accommodate the player's desire to perform actions that they think of that are not part of the story. Dynamic action generation may require on-the-fly updates to the game engine's state representation and revision of previously generated actions. We evaluate the success rate of action code generation with respect to whether a player can interactively play through the entire generated story.",CS,AI_ML,0.85,Extracted from log - paper 903
Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks,"Deep learning has substantially advanced the Single Image Super-Resolution (SISR). However, existing researches have predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. In this paper, we introduce the concept of ""Universality"" and its associated definitions which extend the traditional notion of ""Generalization"" to encompass the modules' ease of transferability, thus revealing the relationships between module universality and model generalizability. Then we propose the Universality Assessment Equation (UAE), a metric for quantifying how readily a given module could be transplanted across models. Guided by the UAE results of standard residual blocks and other plug-and-play modules, we further design two optimized modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB). Through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, extreme-industrial imagery and on-device deployments, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity.",CS,AI_ML,0.85,Extracted from log - paper 904
From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition,"In this paper, we introduce a novel paradigm for reservoir computing (RC) that leverages a pool of cultured biological neurons as the reservoir substrate, creating a biological reservoir computing (BRC). This system operates similarly to an echo state network (ESN), with the key distinction that the neural activity is generated by a network of cultured neurons, rather than being modeled by traditional artificial computational units. The neuronal activity is recorded using a multi-electrode array (MEA), which enables high-throughput recording of neural signals. In our approach, inputs are introduced into the network through a subset of the MEA electrodes, while the remaining electrodes capture the resulting neural activity. This generates a nonlinear mapping of the input data to a high-dimensional biological feature space, where distinguishing between data becomes more efficient and straightforward, allowing a simple linear classifier to perform pattern recognition tasks effectively. To evaluate the performance of our proposed system, we present an experimental study that includes various input patterns, such as positional codes, bars with different orientations, and a digit recognition task. The results demonstrate the feasibility of using biological neural networks to perform tasks traditionally handled by artificial neural networks, paving the way for further exploration of biologically-inspired computing systems, with potential applications in neuromorphic engineering and bio-hybrid computing.",CS,AI_ML,0.85,Extracted from log - paper 905
Augmenting Human Cognition through Everyday AR,"As spatial computing and multimodal LLMs mature, AR is tending to become an intuitive ""thinking tool,"" embedding semantic and context-aware intelligence directly into everyday environments. This paper explores how always-on AR can seamlessly bridge digital cognition and physical affordances, enabling proactive, context-sensitive interactions that enhance human task performance and understanding.",CS,AI_ML,0.85,Extracted from log - paper 906
A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM),"Generative models can unintentionally memorize training data, posing significant privacy risks. This paper addresses the memorization phenomenon in time series imputation models, introducing the Loss-Based with Reference Model (LBRM) algorithm. The LBRM method leverages a reference model to enhance the accuracy of membership inference attacks, distinguishing between training and test data. Our contributions are twofold: first, we propose an innovative method to effectively extract and identify memorized training data, significantly improving detection accuracy. On average, without fine-tuning, the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased by approximately 60\%. Second, we validate our approach through membership inference attacks on two types of architectures designed for time series imputation, demonstrating the robustness and versatility of the LBRM approach in different contexts. These results highlight the significant enhancement in detection accuracy provided by the LBRM approach, addressing privacy risks in time series imputation models.",CS,AI_ML,0.85,Extracted from log - paper 907
am-ELO: A Stable Framework for Arena-based LLM Evaluation,"Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating's probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.",CS,AI_ML,0.85,Extracted from log - paper 908
Blending 3D Geometry and Machine Learning for Multi-View Stereopsis,"Traditional multi-view stereo (MVS) methods primarily depend on photometric and geometric consistency constraints. In contrast, modern learning-based algorithms often rely on the plane sweep algorithm to infer 3D geometry, applying explicit geometric consistency (GC) checks only as a post-processing step, with no impact on the learning process itself. In this work, we introduce GC MVSNet plus plus, a novel approach that actively enforces geometric consistency of reference view depth maps across multiple source views (multi view) and at various scales (multi scale) during the learning phase (see Fig. 1). This integrated GC check significantly accelerates the learning process by directly penalizing geometrically inconsistent pixels, effectively halving the number of training iterations compared to other MVS methods. Furthermore, we introduce a densely connected cost regularization network with two distinct block designs simple and feature dense optimized to harness dense feature connections for enhanced regularization. Extensive experiments demonstrate that our approach achieves a new state of the art on the DTU and BlendedMVS datasets and secures second place on the Tanks and Temples benchmark. To our knowledge, GC MVSNet plus plus is the first method to enforce multi-view, multi-scale supervised geometric consistency during learning. Our code is available.",CS,AI_ML,0.85,Extracted from log - paper 909
An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation,"Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a given use case can be complex and expensive. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To address this gap, we present a comprehensive study involving 5 HPO algorithms over 5 datasets from diverse domains, including a new one collected for this work on real-world product documentation. Our study explores the largest HPO search space considered to date, with two optimized evaluation metrics. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with iterative random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing models first is preferable to the prevalent practice of optimizing sequentially according to the RAG pipeline order.",CS,AI_ML,0.85,Extracted from log - paper 910
Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis,"The rise of QR code based phishing (""Quishing"") poses a growing cybersecurity threat, as attackers increasingly exploit QR codes to bypass traditional phishing defenses. Existing detection methods predominantly focus on URL analysis, which requires the extraction of the QR code payload, and may inadvertently expose users to malicious content. Moreover, QR codes can encode various types of data beyond URLs, such as Wi-Fi credentials and payment information, making URL-based detection insufficient for broader security concerns. To address these gaps, we propose the first framework for quishing detection that directly analyzes QR code structure and pixel patterns without extracting the embedded content. We generated a dataset of phishing and benign QR codes and we used it to train and evaluate multiple machine learning models, including Logistic Regression, Decision Trees, Random Forest, Naive Bayes, LightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of 0.9106, demonstrating the feasibility of QR-centric detection. Through feature importance analysis, we identify key visual indicators of malicious intent and refine our feature set by removing non-informative pixels, improving performance to an AUC of 0.9133 with a reduced feature space. Our findings reveal that the structural features of QR code correlate strongly with phishing risk. This work establishes a foundation for quishing mitigation and highlights the potential of direct QR analysis as a critical layer in modern phishing defenses.",CS,AI_ML,0.85,Extracted from log - paper 911
Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories,"Centralized and distributed systems are two main approaches to organizing ICT infrastructure, each with its pros and cons. Centralized systems concentrate resources in one location, making management easier but creating single points of failure. Distributed systems, on the other hand, spread resources across multiple nodes, offering better scalability and fault tolerance, but requiring more complex management. The choice between them depends on factors like application needs, scalability, and data sensitivity. Centralized systems suit applications with limited scalability and centralized control, while distributed systems excel in large-scale environments requiring high availability and performance. This paper explores a distributed document repository system developed for the Italian Ministry of Justice, using edge repositories to analyze textual data and metadata, enhancing semantic exploration capabilities.",CS,AI_ML,0.85,Extracted from log - paper 912
The Steganographic Potentials of Language Models,"The potential for large language models (LLMs) to hide messages within plain text (steganography) poses a challenge to detection and thwarting of unaligned AI agents, and undermines faithfulness of LLMs reasoning. We explore the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL) to: (1) develop covert encoding schemes, (2) engage in steganography when prompted, and (3) utilize steganography in realistic scenarios where hidden reasoning is likely, but not prompted. In these scenarios, we detect the intention of LLMs to hide their reasoning as well as their steganography performance. Our findings in the fine-tuning experiments as well as in behavioral non fine-tuning evaluations reveal that while current models exhibit rudimentary steganographic abilities in terms of security and capacity, explicit algorithmic guidance markedly enhances their capacity for information concealment.",CS,AI_ML,0.85,Extracted from log - paper 913
Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents,"Large Language Models (LLMs) represent a landmark achievement in Artificial Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks such as text generation, code completion, and conversational coherence. These capabilities stem from their architecture, which mirrors human procedural memory -- the brain's ability to automate repetitive, pattern-driven tasks through practice. However, as LLMs are increasingly deployed in real-world applications, it becomes impossible to ignore their limitations operating in complex, unpredictable environments. This paper argues that LLMs, while transformative, are fundamentally constrained by their reliance on procedural memory. To create agents capable of navigating ``wicked'' learning environments -- where rules shift, feedback is ambiguous, and novelty is the norm -- we must augment LLMs with semantic memory and associative learning systems. By adopting a modular architecture that decouples these cognitive functions, we can bridge the gap between narrow procedural expertise and the adaptive intelligence required for real-world problem-solving.",CS,AI_ML,0.85,Extracted from log - paper 914
MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks,"Large Language Models (LLMs) have demonstrated significant promise for various applications in healthcare. However, their efficacy in the Arabic medical domain remains unexplored due to the lack of high-quality domain-specific datasets and benchmarks. This study introduces MedArabiQ, a novel benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and including multiple choice questions, fill-in-the-blank, and patient-doctor question answering. We first constructed the dataset using past medical exams and publicly available datasets. We then introduced different modifications to evaluate various LLM capabilities, including bias mitigation. We conducted an extensive evaluation with five state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude 3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of new high-quality benchmarks that span different languages to ensure fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, we provide a foundation for future research aimed at evaluating and enhancing the multilingual capabilities of LLMs for the equitable use of generative AI in healthcare.",CS,AI_ML,0.85,Extracted from log - paper 915
Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications,"Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for diagnosing heart diseases and evaluating cardiac health. However, the limited availability of large-scale, high-quality CMR datasets poses a major challenge to the effective application of artificial intelligence (AI) in this domain. Even the amount of unlabeled data and the health status it covers are difficult to meet the needs of model pretraining, which hinders the performance of AI models on downstream tasks. In this study, we present Cardiac Phenotype-Guided CMR Generation (CPGG), a novel approach for generating diverse CMR data that covers a wide spectrum of cardiac health status. The CPGG framework consists of two stages: in the first stage, a generative model is trained using cardiac phenotypes derived from CMR data; in the second stage, a masked autoregressive diffusion model, conditioned on these phenotypes, generates high-fidelity CMR cine sequences that capture both structural and functional features of the heart in a fine-grained manner. We synthesized a massive amount of CMR to expand the pretraining data. Experimental results show that CPGG generates high-quality synthetic CMR data, significantly improving performance on various downstream tasks, including diagnosis and cardiac phenotypes prediction. These gains are demonstrated across both public and private datasets, highlighting the effectiveness of our approach. Code is availabel at https://anonymous.4open.science/r/CPGG.",CS,AI_ML,0.85,Extracted from log - paper 916
Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense,"The growing need for Trusted AI (TAI) highlights the importance of interpretability and robustness in machine learning models. However, many existing tools overlook graph data and rarely combine these two aspects into a single solution. Graph Neural Networks (GNNs) have become a popular approach, achieving top results across various tasks. We introduce GNN-AID (Graph Neural Network Analysis, Interpretation, and Defense), an open-source framework designed for graph data to address this gap. Built as a Python library, GNN-AID supports advanced trust methods and architectural layers, allowing users to analyze graph datasets and GNN behavior using attacks, defenses, and interpretability methods.   GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models, and support for any GNNs through customizable interfaces. It also includes a web interface with tools for graph visualization and no-code features like an interactive model builder, simplifying the exploration and analysis of GNNs. The framework also supports MLOps techniques, ensuring reproducibility and result versioning to track and revisit analyses efficiently.   GNN-AID is a flexible tool for developers and researchers. It helps developers create, analyze, and customize graph models, while also providing access to prebuilt datasets and models for quick experimentation. Researchers can use the framework to explore advanced topics on the relationship between interpretability and robustness, test defense strategies, and combine methods to protect against different types of attacks.   We also show how defenses against evasion and poisoning attacks can conflict when applied to graph data, highlighting the complex connections between defense strategies.   GNN-AID is available at \href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}",CS,AI_ML,0.85,Extracted from log - paper 917
Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation,"This research paper investigates the application of Large Language Models (LLMs) in healthcare, specifically focusing on enhancing medical decision support through Retrieval-Augmented Generation (RAG) integrated with hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation (QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By embedding and retrieving context-relevant healthcare information, the system significantly improves response accuracy. QLoRA facilitates notable parameter efficiency and memory optimization, preserving the integrity of medical information through specialized quantization techniques. Our research also shows that our model performs relatively well on various medical benchmarks, indicating that it can be used to make basic medical suggestions. This paper details the system's technical components, including its architecture, quantization methods, and key healthcare applications such as enhanced disease prediction from patient symptoms and medical history, treatment suggestions, and efficient summarization of complex medical reports. We touch on the ethical considerations-patient privacy, data security, and the need for rigorous clinical validation-as well as the practical challenges of integrating such systems into real-world healthcare workflows. Furthermore, the lightweight quantized weights ensure scalability and ease of deployment even in low-resource hospital environments. Finally, the paper concludes with an analysis of the broader impact of LLMs on healthcare and outlines future directions for LLMs in medical settings.",CS,AI_ML,0.85,Extracted from log - paper 918
DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation,"Radiology Report Generation (RRG) automates the creation of radiology reports from medical imaging, enhancing the efficiency of the reporting process. Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating the ability to compare current and prior exams, facilitating the tracking of temporal changes in clinical findings. Existing LRRG approaches only extract features from prior and current images using a visual pre-trained encoder, which are then concatenated to generate the final report. However, these methods struggle to effectively capture both spatial and temporal correlations during the feature extraction process. Consequently, the extracted features inadequately capture the information of difference across exams and thus underrepresent the expected progressions, leading to sub-optimal performance in LRRG. To address this, we develop a novel dynamic difference-aware temporal residual network (DDaTR). In DDaTR, we introduce two modules at each stage of the visual encoder to capture multi-level spatial correlations. The Dynamic Feature Alignment Module (DFAM) is designed to align prior features across modalities for the integrity of prior clinical information. Prompted by the enriched prior features, the dynamic difference-aware module (DDAM) captures favorable difference information by identifying relationships across exams. Furthermore, our DDaTR employs the dynamic residual network to unidirectionally transmit longitudinal information, effectively modelling temporal correlations. Extensive experiments demonstrated superior performance over existing methods on three benchmarks, proving its efficacy in both RRG and LRRG tasks.",CS,AI_ML,0.85,Extracted from log - paper 919
Automatic Calibration for Membership Inference Attack on Large Language Models,"Membership Inference Attacks (MIAs) have recently been employed to determine whether a specific text was part of the pre-training data of Large Language Models (LLMs). However, existing methods often misinfer non-members as members, leading to a high false positive rate, or depend on additional reference models for probability calibration, which limits their practicality. To overcome these challenges, we introduce a novel framework called Automatic Calibration Membership Inference Attack (ACMIA), which utilizes a tunable temperature to calibrate output probabilities effectively. This approach is inspired by our theoretical insights into maximum likelihood estimation during the pre-training of LLMs. We introduce ACMIA in three configurations designed to accommodate different levels of model access and increase the probability gap between members and non-members, improving the reliability and robustness of membership inference. Extensive experiments on various open-source LLMs demonstrate that our proposed attack is highly effective, robust, and generalizable, surpassing state-of-the-art baselines across three widely used benchmarks. Our code is available at: \href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.",CS,AI_ML,0.85,Extracted from log - paper 920
Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant,"Medical AI assistants support doctors in disease diagnosis, medical image analysis, and report generation. However, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings. We propose RCMed, a full-stack AI assistant that improves multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis through hierarchical vision-language grounding. A self-reinforcing correlation mechanism allows visual features to inform language context, while language semantics guide pixel-wise attention, forming a closed loop that refines both modalities. This correlation is enhanced by a color region description strategy, translating anatomical structures into semantically rich text to learn shape-location-text relationships across scales. Trained on 20 million image-mask-description triplets, RCMed achieves state-of-the-art precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It achieved a 23.5% relative improvement in cell segmentation from microscopy images over prior methods. RCMed's strong vision-language alignment enables exceptional generalization, with state-of-the-art performance in external validation across 20 clinically significant cancer types, including novel tasks. This work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric AI healthcare.",CS,AI_ML,0.85,Extracted from log - paper 921
SPAP: Structured Pruning via Alternating Optimization and Penalty Methods,"The deployment of large language models (LLMs) is often constrained by their substantial computational and memory demands. While structured pruning presents a viable approach by eliminating entire network components, existing methods suffer from performance degradation, reliance on heuristic metrics, or expensive finetuning. To address these challenges, we propose SPAP (Structured Pruning via Alternating Optimization and Penalty Methods), a novel and efficient structured pruning framework for LLMs grounded in optimization theory. SPAP formulates the pruning problem through a mixed-integer optimization model, employs a penalty method that effectively makes pruning decisions to minimize pruning errors, and introduces an alternating minimization algorithm tailored to the splittable problem structure for efficient weight updates and performance recovery. Extensive experiments on OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at 30% sparsity) and proportional memory reductions. Our work offers a practical, optimization-driven solution for pruning LLMs while preserving model performance.",CS,AI_ML,0.85,Extracted from log - paper 922
Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten,"Free play is a fundamental aspect of early childhood education, supporting children's cognitive, social, emotional, and motor development. However, assessing children's development during free play poses significant challenges due to the unstructured and spontaneous nature of the activity. Traditional assessment methods often rely on direct observations by teachers, parents, or researchers, which may fail to capture comprehensive insights from free play and provide timely feedback to educators. This study proposes an innovative approach combining Large Language Models (LLMs) with learning analytics to analyze children's self-narratives of their play experiences. The LLM identifies developmental abilities, while performance scores across different play settings are calculated using learning analytics techniques. We collected 2,224 play narratives from 29 children in a kindergarten, covering four distinct play areas over one semester. According to the evaluation results from eight professionals, the LLM-based approach achieved high accuracy in identifying cognitive, motor, and social abilities, with accuracy exceeding 90% in most domains. Moreover, significant differences in developmental outcomes were observed across play settings, highlighting each area's unique contributions to specific abilities. These findings confirm that the proposed approach is effective in identifying children's development across various free play settings. This study demonstrates the potential of integrating LLMs and learning analytics to provide child-centered insights into developmental trajectories, offering educators valuable data to support personalized learning and enhance early childhood education practices.",CS,AI_ML,0.85,Extracted from log - paper 923
Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection,"Speech-based AI models are emerging as powerful tools for detecting depression and the presence of Post-traumatic stress disorder (PTSD), offering a non-invasive and cost-effective way to assess mental health. However, these models often struggle with gender bias, which can lead to unfair and inaccurate predictions. In this study, our study addresses this issue by introducing a domain adversarial training approach that explicitly considers gender differences in speech-based depression and PTSD detection. Specifically, we treat different genders as distinct domains and integrate this information into a pretrained speech foundation model. We then validate its effectiveness on the E-DAIC dataset to assess its impact on performance. Experimental results show that our method notably improves detection performance, increasing the F1-score by up to 13.29 percentage points compared to the baseline. This highlights the importance of addressing demographic disparities in AI-driven mental health assessment.",CS,AI_ML,0.85,Extracted from log - paper 924
Safer Prompts: Reducing IP Risk in Visual Generative AI,"Visual Generative AI models have demonstrated remarkable capability in generating high-quality images from simple inputs like text prompts. However, because these models are trained on images from diverse sources, they risk memorizing and reproducing specific content, raising concerns about intellectual property (IP) infringement. Recent advances in prompt engineering offer a cost-effective way to enhance generative AI performance. In this paper, we evaluate the effectiveness of prompt engineering techniques in mitigating IP infringement risks in image generation. Our findings show that Chain of Thought Prompting and Task Instruction Prompting significantly reduce the similarity between generated images and the training data of diffusion models, thereby lowering the risk of IP infringement.",CS,AI_ML,0.85,Extracted from log - paper 925
Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs,"Large Language Models (LLMs) have shown promise for generative recommender systems due to their transformative capabilities in user interaction. However, ensuring they do not recommend out-of-domain (OOD) items remains a challenge. We study two distinct methods to address this issue: RecLM-ret, a retrieval-based method, and RecLM-cgen, a constrained generation method. Both methods integrate seamlessly with existing LLMs to ensure in-domain recommendations. Comprehensive experiments on three recommendation datasets demonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing LLM-based recommender models in accuracy while eliminating OOD recommendations, making it the preferred method for adoption. Additionally, RecLM-cgen maintains strong generalist capabilities and is a lightweight plug-and-play module for easy integration into LLMs, offering valuable practical benefits for the community. Source code is available at https://github.com/microsoft/RecAI",CS,AI_ML,0.85,Extracted from log - paper 926
Absolute Zero: Reinforced Self-play Reasoning with Zero Data,"Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",CS,AI_ML,0.85,Extracted from log - paper 927
"AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning","Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering methodology designed to bridge this gap using standard LLM chat interfaces (zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical analysis of experimental chemistry manuscripts, featuring a hierarchical, modular architecture (structured via Markdown) that defines detailed analysis workflows. We develop this PWP prompt through iterative application of meta-prompting techniques and meta-reasoning aimed at systematically codifying expert review workflows, including tacit knowledge. Submitted once at the start of a session, this PWP prompt equips the LLM with persistent workflows triggered by subsequent queries, guiding modern reasoning LLMs through systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM identifying major methodological flaws in a test case while mitigating LLM input bias and performing complex tasks, including distinguishing claims from evidence, integrating text/photo/figure analysis to infer parameters, executing quantitative feasibility checks, comparing estimates against claims, and assessing a priori plausibility. To ensure transparency and facilitate replication, we provide full prompts, detailed demonstration analyses, and logs of interactive chats as supplementary resources. Beyond the specific application, this work offers insights into the meta-development process itself, highlighting the potential of PWP, informed by detailed workflow formalization, to enable sophisticated analysis using readily available LLMs for complex scientific tasks.",CS,AI_ML,0.85,Extracted from log - paper 928
Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning,"Deep learning models have shown encouraging capabilities for mapping accurately forests at medium resolution with TanDEM-X interferometric SAR data. Such models, as most of current state-of-the-art deep learning techniques in remote sensing, are trained in a fully-supervised way, which requires a large amount of labeled data for training and validation. In this work, our aim is to exploit the high-resolution capabilities of the TanDEM-X mission to map forests at 6 m. The goal is to overcome the intrinsic limitations posed by midresolution products, which affect, e.g., the detection of narrow roads within vegetated areas and the precise delineation of forested regions contours. To cope with the lack of extended reliable reference datasets at such a high resolution, we investigate self-supervised learning techniques for extracting highly informative representations from the input features, followed by a supervised training step with a significantly smaller number of reliable labels. A 1 m resolution forest/non-forest reference map over Pennsylvania, USA, allows for comparing different training approaches for the development of an effective forest mapping framework with limited labeled samples. We select the best-performing approach over this test region and apply it in a real-case forest mapping scenario over the Amazon rainforest, where only very few labeled data at high resolution are available. In this challenging scenario, the proposed self-supervised framework significantly enhances the classification accuracy with respect to fully-supervised methods, trained using the same amount of labeled data, representing an extremely promising starting point for large-scale, very high-resolution forest mapping with TanDEM-X data.",CS,AI_ML,0.85,Extracted from log - paper 929
SD-VSum: A Method and Dataset for Script-Driven Video Summarization,"In this work, we introduce the task of script-driven video summarization, which aims to produce a summary of the full-length video by selecting the parts that are most relevant to a user-provided script outlining the visual content of the desired summary. Following, we extend a recently-introduced large-scale dataset for generic video summarization (VideoXum) by producing natural language descriptions of the different human-annotated summaries that are available per video. In this way we make it compatible with the introduced task, since the available triplets of ``video, summary and summary description'' can be used for training a method that is able to produce different summaries for a given video, driven by the provided script about the content of each summary. Finally, we develop a new network architecture for script-driven video summarization (SD-VSum), that relies on the use of a cross-modal attention mechanism for aligning and fusing information from the visual and text modalities. Our experimental evaluations demonstrate the advanced performance of SD-VSum against state-of-the-art approaches for query-driven and generic (unimodal and multimodal) summarization from the literature, and document its capacity to produce video summaries that are adapted to each user's needs about their content.",CS,AI_ML,0.85,Extracted from log - paper 930
"Artificial Behavior Intelligence: Technology, Challenges, and Future Directions","Understanding and predicting human behavior has emerged as a core capability in various AI application domains such as autonomous driving, smart healthcare, surveillance systems, and social robotics. This paper defines the technical framework of Artificial Behavior Intelligence (ABI), which comprehensively analyzes and interprets human posture, facial expressions, emotions, behavioral sequences, and contextual cues. It details the essential components of ABI, including pose estimation, face and emotion recognition, sequential behavior analysis, and context-aware modeling. Furthermore, we highlight the transformative potential of recent advances in large-scale pretrained models, such as large language models (LLMs), vision foundation models, and multimodal integration models, in significantly improving the accuracy and interpretability of behavior recognition. Our research team has a strong interest in the ABI domain and is actively conducting research, particularly focusing on the development of intelligent lightweight models capable of efficiently inferring complex human behaviors. This paper identifies several technical challenges that must be addressed to deploy ABI in real-world applications including learning behavioral intelligence from limited data, quantifying uncertainty in complex behavior prediction, and optimizing model structures for low-power, real-time inference. To tackle these challenges, our team is exploring various optimization strategies including lightweight transformers, graph-based recognition architectures, energy-aware loss functions, and multimodal knowledge distillation, while validating their applicability in real-time environments.",CS,AI_ML,0.85,Extracted from log - paper 931
Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation,"The recent surge in the popularity of diffusion models for image synthesis has attracted new attention to their potential for generation tasks in other domains. However, their applications to symbolic music generation remain largely under-explored because symbolic music is typically represented as sequences of discrete events and standard diffusion models are not well-suited for discrete data. We represent symbolic music as image-like pianorolls, facilitating the use of diffusion models for the generation of symbolic music. Moreover, this study introduces a novel diffusion model that incorporates our proposed Transformer-Mamba block and learnable wavelet transform. Classifier-free guidance is utilised to generate symbolic music with target chords. Our evaluation shows that our method achieves compelling results in terms of music quality and controllability, outperforming the strong baseline in pianoroll generation. Our code is available at https://github.com/jinchengzhanggg/proffusion.",CS,AI_ML,0.85,Extracted from log - paper 932
Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices,"This paper presents a comprehensive evaluation of lightweight deep learning models for image classification, emphasizing their suitability for deployment in resource-constrained environments such as low-memory devices. Five state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using four key performance metrics: classification accuracy, inference time, floating-point operations (FLOPs), and model size. Additionally, we investigate the impact of hyperparameter tuning, data augmentation, and training paradigms by comparing pretrained models with scratch-trained counterparts, focusing on MobileNetV3 Small. Our findings reveal that transfer learning significantly enhances model accuracy and computational efficiency, particularly for complex datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest accuracy, while MobileNetV3 offers the best balance between accuracy and efficiency, and SqueezeNet excels in inference speed and compactness. This study highlights critical trade-offs between accuracy and efficiency, offering actionable insights for deploying lightweight models in real-world applications where computational resources are limited. By addressing these challenges, this research contributes to optimizing deep learning systems for edge computing and mobile platforms.",CS,AI_ML,0.85,Extracted from log - paper 933
Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach,"Foundation models constitute a significant advancement in computer vision: after a single, albeit costly, training phase, they can address a wide array of tasks. In the field of Earth observation, over 75 remote sensing vision foundation models have been developed in the past four years. However, none has consistently outperformed the others across all available downstream tasks. To facilitate their comparison, we propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning on each one. This method is based on what we call ""capabilities encoding."" The utility of this novel approach is twofold: we demonstrate its potential to simplify the selection of a foundation model for a given new task, and we employ it to offer a fresh perspective on the existing literature, suggesting avenues for future research. Codes are available at https://github.com/pierreadorni/capabilities-encoding.",CS,AI_ML,0.85,Extracted from log - paper 934
The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning,"We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel approach for flexible policy representation and imitation learning in robot manipulation. MiDiGap enables learning from as few as five demonstrations using only camera observations and generalizes across a wide range of challenging tasks. It excels at long-horizon behaviors such as making coffee, highly constrained motions such as opening doors, dynamic actions such as scooping with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns these tasks on a CPU in less than a minute and scales linearly to large datasets. We also develop a rich suite of tools for inference-time steering using evidence such as collision signals and robot kinematic constraints. This steering enables novel generalization capabilities, including obstacle avoidance and cross-embodiment policy transfer. MiDiGap achieves state-of-the-art performance on diverse few-shot manipulation benchmarks. On constrained RLBench tasks, it improves policy success by 76 percentage points and reduces trajectory cost by 67%. On multimodal tasks, it improves policy success by 48 percentage points and increases sample efficiency by a factor of 20. In cross-embodiment transfer, it more than doubles policy success. We make the code publicly available at https://midigap.cs.uni-freiburg.de.",CS,AI_ML,0.85,Extracted from log - paper 935
Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces,"Modern automation systems increasingly rely on modular architectures, with capabilities and skills as one solution approach. Capabilities define the functions of resources in a machine-readable form and skills provide the concrete implementations that realize those capabilities. However, the development of a skill implementation conforming to a corresponding capability remains a time-consuming and challenging task. In this paper, we present a method that treats capabilities as contracts for skill implementations and leverages large language models to generate executable code based on natural language user input. A key feature of our approach is the integration of existing software libraries and interface technologies, enabling the generation of skill implementations across different target languages. We introduce a framework that allows users to incorporate their own libraries and resource interfaces into the code generation process through a retrieval-augmented generation architecture. The proposed method is evaluated using an autonomous mobile robot controlled via Python and ROS 2, demonstrating the feasibility and flexibility of the approach.",CS,AI_ML,0.85,Extracted from log - paper 936
Physics-inspired Energy Transition Neural Network for Sequence Learning,"Recently, the superior performance of Transformers has made them a more robust and scalable solution for sequence modeling than traditional recurrent neural networks (RNNs). However, the effectiveness of Transformer in capturing long-term dependencies is primarily attributed to their comprehensive pair-modeling process rather than inherent inductive biases toward sequence semantics. In this study, we explore the capabilities of pure RNNs and reassess their long-term learning mechanisms. Inspired by the physics energy transition models that track energy changes over time, we propose a effective recurrent structure called the``Physics-inspired Energy Transition Neural Network"" (PETNN). We demonstrate that PETNN's memory mechanism effectively stores information over long-term dependencies. Experimental results indicate that PETNN outperforms transformer-based methods across various sequence tasks. Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower complexity. Our study presents an optimal foundational recurrent architecture and highlights the potential for developing effective recurrent neural networks in fields currently dominated by Transformer.",CS,AI_ML,0.85,Extracted from log - paper 937
RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation,"Large language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to identify the most relevant MCP(s) for a given query from an external index before engaging the LLM. Only the selected tool descriptions are passed to the model, drastically reducing prompt size and simplifying decision-making. Experiments, including an MCP stress test, demonstrate RAG-MCP significantly cuts prompt tokens (e.g., by over 50%) and more than triples tool selection accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables scalable and accurate tool integration for LLMs.",CS,AI_ML,0.85,Extracted from log - paper 938
Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models,"While modern Requirements Engineering (RE) heavily relies on natural language processing and Machine Learning (ML) techniques, their effectiveness is limited by the scarcity of high-quality datasets. This paper introduces Synthline, a Product Line (PL) approach that leverages Large Language Models to systematically generate synthetic RE data for classification-based use cases. Through an empirical evaluation conducted in the context of using ML for the identification of requirements specification defects, we investigated both the diversity of the generated data and its utility for training downstream models. Our analysis reveals that while synthetic datasets exhibit less diversity than real data, they are good enough to serve as viable training resources. Moreover, our evaluation shows that combining synthetic and real data leads to substantial performance improvements. Specifically, hybrid approaches achieve up to 85% improvement in precision and a 2x increase in recall compared to models trained exclusively on real data. These findings demonstrate the potential of PL-based synthetic data generation to address data scarcity in RE. We make both our implementation and generated datasets publicly available to support reproducibility and advancement in the field.",CS,AI_ML,0.85,Extracted from log - paper 939
Seeing the Abstract: Translating the Abstract Language for Vision Language Models,"Natural language goes beyond dryly describing visual content. It contains rich abstract concepts to express feeling, creativity and properties that cannot be directly perceived. Yet, current research in Vision Language Models (VLMs) has not shed light on abstract-oriented language. Our research breaks new ground by uncovering its wide presence and under-estimated value, with extensive analysis. Particularly, we focus our investigation on the fashion domain, a highly-representative field with abstract expressions. By analyzing recent large-scale multimodal fashion datasets, we find that abstract terms have a dominant presence, rivaling the concrete ones, providing novel information, and being useful in the retrieval task. However, a critical challenge emerges: current general-purpose or fashion-specific VLMs are pre-trained with databases that lack sufficient abstract words in their text corpora, thus hindering their ability to effectively represent abstract-oriented language. We propose a training-free and model-agnostic method, Abstract-to-Concrete Translator (ACT), to shift abstract representations towards well-represented concrete ones in the VLM latent space, using pre-trained models and existing multimodal databases. On the text-to-image retrieval task, despite being training-free, ACT outperforms the fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its effectiveness with a strong generalization capability. Moreover, the improvement introduced by ACT is consistent with various VLMs, making it a plug-and-play solution.",CS,AI_ML,0.85,Extracted from log - paper 940
Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover,"This study introduces an innovative crossover operator named Particle Swarm Optimization-inspired Crossover (PSOX), which is specifically developed for real-coded genetic algorithms. Departing from conventional crossover approaches that only exchange information between individuals within the same generation, PSOX uniquely incorporates guidance from both the current global best solution and historical optimal solutions across multiple generations. This novel mechanism enables the algorithm to maintain population diversity while simultaneously accelerating convergence toward promising regions of the search space. The effectiveness of PSOX is rigorously evaluated through comprehensive experiments on 15 benchmark test functions with diverse characteristics, including unimodal, multimodal, and highly complex landscapes. Comparative analysis against five state-of-the-art crossover operators reveals that PSOX consistently delivers superior performance in terms of solution accuracy, algorithmic stability, and convergence speed, especially when combined with an appropriate mutation strategy. Furthermore, the study provides an in-depth investigation of how different mutation rates influence PSOX's performance, yielding practical guidelines for parameter tuning when addressing optimization problems with varying landscape properties.",CS,AI_ML,0.85,Extracted from log - paper 941
DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral,"Acquiring structured data from domain-specific, image-based documents such as scanned reports is crucial for many downstream tasks but remains challenging due to document variability. Many of these documents exist as images rather than as machine-readable text, which requires human annotation to train automated extraction systems. We present DocSpiral, the first Human-in-the-Spiral assistive document annotation platform, designed to address the challenge of extracting structured information from domain-specific, image-based document collections. Our spiral design establishes an iterative cycle in which human annotations train models that progressively require less manual intervention. DocSpiral integrates document format normalization, comprehensive annotation interfaces, evaluation metrics dashboard, and API endpoints for the development of AI / ML models into a unified workflow. Experiments demonstrate that our framework reduces annotation time by at least 41\% while showing consistent performance gains across three iterations during model training. By making this annotation platform freely accessible, we aim to lower barriers to AI/ML models development in document processing, facilitating the adoption of large language models in image-based, document-intensive fields such as geoscience and healthcare. The system is freely available at: https://app.ai4wa.com. The demonstration video is available: https://app.ai4wa.com/docs/docspiral/demo.",CS,AI_ML,0.85,Extracted from log - paper 942
DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations,"Deep learning methods have shown promise in classifying breast cancer histopathology images, but their performance often declines with limited annotated data, a critical challenge in medical imaging due to the high cost and expertise required for annotations.",CS,AI_ML,0.85,Extracted from log - paper 943
"A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case","Large Language Models (LLMs) demonstrate strong potential across a variety of tasks in communications and networking due to their advanced reasoning capabilities. However, because different LLMs have different model structures and are trained using distinct corpora and methods, they may offer varying optimization strategies for the same network issues. Moreover, the limitations of an individual LLM's training data, aggravated by the potential maliciousness of its hosting device, can result in responses with low confidence or even bias. To address these challenges, we propose a blockchain-enabled collaborative framework that connects multiple LLMs into a Trustworthy Multi-LLM Network (MultiLLMN). This architecture enables the cooperative evaluation and selection of the most reliable and high-quality responses to complex network optimization problems. Specifically, we begin by reviewing related work and highlighting the limitations of existing LLMs in collaboration and trust, emphasizing the need for trustworthiness in LLM-based systems. We then introduce the workflow and design of the proposed Trustworthy MultiLLMN framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G communication systems and the difficulty of addressing such threats through traditional modeling techniques, we present FBS defense as a case study to empirically validate the effectiveness of our approach. Finally, we outline promising future research directions in this emerging area.",CS,AI_ML,0.85,Extracted from log - paper 944
A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive,"With the rise of short video platforms in global communication, embedding steganographic data in audio synchronization streams has emerged as a new covert communication method. To address the limitations of traditional techniques in detecting synchronized steganography, this paper proposes a detection and distributed guidance reconstruction model based on short video ""Yupan"" samples released by China's South Sea Fleet on TikTok. The method integrates sliding spectrum feature extraction and intelligent inference mechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is used to extract the main frequency trajectory and construct the synchronization frame detection model (M1), identifying a frame flag ""FFFFFFFFFFFFFFFFFF80"". The subsequent 32-byte payload is decoded by a structured model (M2) to infer distributed guidance commands. Analysis reveals a low-entropy, repetitive byte sequence in the 36 to 45 second audio segment with highly concentrated spectral energy, confirming the presence of synchronization frames. Although plaintext semantics are not restored, the consistency in command field layout suggests features of military communication protocols. The multi-segment splicing model further shows cross-video embedding and centralized decoding capabilities. The proposed framework validates the effectiveness of sliding spectral features for synchronized steganography detection and builds an extensible inference model for covert communication analysis and tactical guidance simulation on open platforms.",CS,AI_ML,0.85,Extracted from log - paper 945
Patterns and Mechanisms of Contrastive Activation Engineering,"Controlling the behavior of Large Language Models (LLMs) remains a significant challenge due to their inherent complexity and opacity. While techniques like fine-tuning can modify model behavior, they typically require extensive computational resources. Recent work has introduced a class of contrastive activation engineering (CAE) techniques as promising approaches for steering LLM outputs through targeted modifications to their internal representations. Applied at inference-time with zero cost, CAE has the potential to introduce a new paradigm of flexible, task-specific LLM behavior tuning. We analyze the performance of CAE in in-distribution, out-of-distribution settings, evaluate drawbacks, and begin to develop comprehensive guidelines for its effective deployment. We find that 1. CAE is only reliably effective when applied to in-distribution contexts. 2. Increasing the number of samples used to generate steering vectors has diminishing returns at around 80 samples. 3. Steering vectors are susceptible to adversarial inputs that reverses the behavior that is steered for. 4. Steering vectors harm the overall model perplexity. 5. Larger models are more resistant to steering-induced degradation.",CS,AI_ML,0.85,Extracted from log - paper 946
seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models,"Current self-supervised algorithms mostly rely on transformations such as data augmentation and masking to learn visual representations. This is achieved by inducing invariance or equivariance with respect to these transformations after encoding two views of an image. This dominant two-view paradigm can limit the flexibility of learned representations for downstream adaptation by creating performance trade-offs between invariance-related tasks such as image classification and more fine-grained equivariance-related tasks. In this work, we introduce \emph{seq-JEPA}, a world modeling paradigm based on joint-embedding predictive architecture that leverages architectural inductive biases to resolve this trade-off. Without requiring an additional equivariance predictor or loss term, seq-JEPA simultaneously learns two architecturally segregated representations: one equivariant to the specified transformations and another invariant to them and suited for tasks such as classification. To do so, our model processes a short sequence of different views (observations) of an input image. Each encoded view is concatenated with embeddings corresponding to the relative transformation (action) producing the next observation in the sequence. A transformer encoder outputs an aggregate representation of this sequence, which is subsequently conditioned on the action leading to the next observation to predict its representation. Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and image classification without sacrificing one for the other. Additionally, our framework excels at tasks that inherently require aggregating a sequence of observations, such as path integration across actions and predictive learning across eye movements.",CS,AI_ML,0.85,Extracted from log - paper 947
RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph,"Comprehending long videos remains a significant challenge for Large Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to hours videos due to their lack of explicit memory and retrieval mechanisms. To address this limitation, we propose RAVU (Retrieval Augmented Video Understanding), a novel framework for video understanding enhanced by retrieval with compositional reasoning over a spatio-temporal graph. We construct a graph representation of the video, capturing both spatial and temporal relationships between entities. This graph serves as a long-term memory, allowing us to track objects and their actions across time. To answer complex queries, we decompose the queries into a sequence of reasoning steps and execute these steps on the graph, retrieving relevant key information. Our approach enables more accurate understanding of long videos, particularly for queries that require multi-hop reasoning and tracking objects across frames. Our approach demonstrate superior performances with limited retrieved frames (5-10) compared with other SOTA methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.",CS,AI_ML,0.85,Extracted from log - paper 948
Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning,"Hindsight relabeling is a powerful tool for overcoming sparsity in goal-conditioned reinforcement learning (GCRL), especially in certain domains such as navigation and locomotion. However, hindsight relabeling can struggle in object-centric domains. For example, suppose that the goal space consists of a robotic arm pushing a particular target block to a goal location. In this case, hindsight relabeling will give high rewards to any trajectory that does not interact with the block. However, these behaviors are only useful when the object is already at the goal -- an extremely rare case in practice. A dataset dominated by these kinds of trajectories can complicate learning and lead to failures. In object-centric domains, one key intuition is that meaningful trajectories are often characterized by object-object interactions such as pushing the block with the gripper. To leverage this intuition, we introduce Hindsight Relabeling using Interactions (HInt), which combines interactions with hindsight relabeling to improve the sample efficiency of downstream RL. However because interactions do not have a consensus statistical definition tractable for downstream GCRL, we propose a definition of interactions based on the concept of null counterfactual: a cause object is interacting with a target object if, in a world where the cause object did not exist, the target object would have different transition dynamics. We leverage this definition to infer interactions in Null Counterfactual Interaction Inference (NCII), which uses a ""nulling'' operation with a learned model to infer interactions. NCII is able to achieve significantly improved interaction inference accuracy in both simple linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.",CS,AI_ML,0.85,Extracted from log - paper 949
CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics,"Neurosymbolic approaches integrating large language models with formal reasoning have recently achieved human-level performance on mathematics competition problems in algebra, geometry and number theory. In comparison, combinatorics remains a challenging domain, characterized by a lack of appropriate benchmarks and theorem libraries. To address this gap, we introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean~4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level, and span over ten combinatorial topics. CombiBench is suitable for testing IMO solving capabilities since it includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its statement contain an images). Furthermore, we provide a comprehensive and standardized evaluation framework, dubbed Fine-Eval (for $\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for formal mathematics. It accommodates not only proof-based problems but also, for the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval as the evaluation method and Kimina Lean Server as the backend, we benchmark several LLMs on CombiBench and observe that their capabilities for formally solving combinatorial problems remain limited. Among all models tested (none of which has been trained for this particular task), Kimina-Prover attains the best results, solving 7 problems (out of 100) under both ``with solution'' and ``without solution'' scenarios. We open source the benchmark dataset alongside with the code of the proposed evaluation method at https://github.com/MoonshotAI/CombiBench/.",CS,AI_ML,0.85,Extracted from log - paper 950
Soft Best-of-n Sampling for Model Alignment,"Best-of-$n$ (BoN) sampling is a practical approach for aligning language model outputs with human preferences without expensive fine-tuning. BoN sampling is performed by generating $n$ responses to a prompt and then selecting the sample that maximizes a reward function. BoN yields high reward values in practice at a distortion cost, as measured by the KL-divergence between the sampled and original distribution. This distortion is coarsely controlled by varying the number of samples: larger $n$ yields a higher reward at a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a generalization of BoN that allows for smooth interpolation between the original distribution and reward-maximizing distribution through a temperature parameter $\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$ sampling converges sharply to the optimal tilted distribution at a rate of $O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete outputs, we analyze an additive reward model that reveals the fundamental limitations of blockwise sampling.",CS,AI_ML,0.85,Extracted from log - paper 951
StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data,"Motion capture (mocap) data often exhibits visually jarring artifacts due to inaccurate sensors and post-processing. Cleaning this corrupted data can require substantial manual effort from human experts, which can be a costly and time-consuming process. Previous data-driven motion cleanup methods offer the promise of automating this cleanup process, but often require in-domain paired corrupted-to-clean training data. Constructing such paired datasets requires access to high-quality, relatively artifact-free motion clips, which often necessitates laborious manual cleanup. In this work, we present StableMotion, a simple yet effective method for training motion cleanup models directly from unpaired corrupted datasets that need cleanup. The core component of our method is the introduction of motion quality indicators, which can be easily annotated through manual labeling or heuristic algorithms and enable training of quality-aware motion generation models on raw motion data with mixed quality. At test time, the model can be prompted to generate high-quality motions using the quality indicators. Our method can be implemented through a simple diffusion-based framework, leading to a unified motion generate-discriminate model, which can be used to both identify and fix corrupted frames. We demonstrate that our proposed method is effective for training motion cleanup models on raw mocap data in production scenarios by applying StableMotion to SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion artifacts. The trained model effectively corrects a wide range of motion artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively. See https://youtu.be/3Y7MMAH02B4 for more results.",CS,AI_ML,0.85,Extracted from log - paper 952
Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo),"We introduce an unsupervised motion-compensated image reconstruction algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging (MRI). We express the image volume corresponding to each specific motion phase as the deformation of a single static image template. The main contribution of the work is the low-rank model for the compact joint representation of the family of diffeomorphisms, parameterized by the motion phases. The diffeomorphism at a specific motion phase is obtained by integrating a parametric velocity field along a path connecting the reference template phase to the motion phase. The velocity field at different phases is represented using a low-rank model. The static template and the low-rank motion model parameters are learned directly from the k-space data in an unsupervised fashion. The more constrained motion model is observed to offer improved recovery compared to current motion-resolved and motion-compensated algorithms for free-breathing 3D cine MRI.",CS,AI_ML,0.85,Extracted from log - paper 953
Holmes: Automated Fact Check with Large Language Models,"The rise of Internet connectivity has accelerated the spread of disinformation, threatening societal trust, decision-making, and national security. Disinformation has evolved from simple text to complex multimodal forms combining images and text, challenging existing detection methods. Traditional deep learning models struggle to capture the complexity of multimodal disinformation. Inspired by advances in AI, this study explores using Large Language Models (LLMs) for automated disinformation detection. The empirical study shows that (1) LLMs alone cannot reliably assess the truthfulness of claims; (2) providing relevant evidence significantly improves their performance; (3) however, LLMs cannot autonomously search for accurate evidence. To address this, we propose Holmes, an end-to-end framework featuring a novel evidence retrieval method that assists LLMs in collecting high-quality evidence. Our approach uses (1) LLM-powered summarization to extract key information from open sources and (2) a new algorithm and metrics to evaluate evidence quality. Holmes enables LLMs to verify claims and generate justifications effectively. Experiments show Holmes achieves 88.3% accuracy on two open-source datasets and 90.2% in real-time verification tasks. Notably, our improved evidence retrieval boosts fact-checking accuracy by 30.8% over existing methods",CS,AI_ML,0.85,Extracted from log - paper 954
VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis,"Real-world machine learning models require rigorous evaluation before deployment, especially in safety-critical domains like autonomous driving and surveillance. The evaluation of machine learning models often focuses on data slices, which are subsets of the data that share a set of characteristics. Data slice finding automatically identifies conditions or data subgroups where models underperform, aiding developers in mitigating performance issues. Despite its popularity and effectiveness, data slicing for vision model validation faces several challenges. First, data slicing often needs additional image metadata or visual concepts, and falls short in certain computer vision tasks, such as object detection. Second, understanding data slices is a labor-intensive and mentally demanding process that heavily relies on the expert's domain knowledge. Third, data slicing lacks a human-in-the-loop solution that allows experts to form hypothesis and test them interactively. To overcome these limitations and better support the machine learning operations lifecycle, we introduce VISLIX, a novel visual analytics framework that employs state-of-the-art foundation models to help domain experts analyze slices in computer vision models. Our approach does not require image metadata or visual concepts, automatically generates natural language insights, and allows users to test data slice hypothesis interactively. We evaluate VISLIX with an expert study and three use cases, that demonstrate the effectiveness of our tool in providing comprehensive insights for validating object detection models.",CS,AI_ML,0.85,Extracted from log - paper 955
"Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE","Oysters are ecologically and commercially important species that require frequent monitoring to track population demographics (e.g. abundance, growth, mortality). Current methods of monitoring oyster reefs often require destructive sampling methods and extensive manual effort. Therefore, they are suboptimal for small-scale or sensitive environments. A recent alternative, the ODYSSEE model, was developed to use deep learning techniques to identify live oysters using video or images taken in the field of oyster reefs to assess abundance. The validity of this model in identifying live oysters on a reef was compared to expert and non-expert annotators. In addition, we identified potential sources of prediction error. Although the model can make inferences significantly faster than expert and non-expert annotators (39.6 s, $2.34 \pm 0.61$ h, $4.50 \pm 1.46$ h, respectively), the model overpredicted the number of live oysters, achieving lower accuracy (63\%) in identifying live oysters compared to experts (74\%) and non-experts (75\%) alike. Image quality was an important factor in determining the accuracy of the model and the annotators. Better quality images improved human accuracy and worsened model accuracy. Although ODYSSEE was not sufficiently accurate, we anticipate that future training on higher-quality images, utilizing additional live imagery, and incorporating additional annotation training classes will greatly improve the model's predictive power based on the results of this analysis. Future research should address methods that improve the detection of living vs. dead oysters.",CS,AI_ML,0.85,Extracted from log - paper 956
"Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation","Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive ""capability signatures"" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI's evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.",CS,AI_ML,0.85,Extracted from log - paper 957
Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering,"This study explores the application of chaos engineering to enhance the robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in production-like environments under real-world conditions. LLM-MAS can potentially improve a wide range of tasks, from answering questions and generating content to automating customer support and improving decision-making processes. However, LLM-MAS in production or preproduction environments can be vulnerable to emergent errors or disruptions, such as hallucinations, agent failures, and agent communication failures. This study proposes a chaos engineering framework to proactively identify such vulnerabilities in LLM-MAS, assess and build resilience against them, and ensure reliable performance in critical applications.",CS,AI_ML,0.85,Extracted from log - paper 958
Latent Adaptive Planner for Dynamic Manipulation,"This paper presents Latent Adaptive Planner (LAP), a novel approach for dynamic nonprehensile manipulation tasks that formulates planning as latent space inference, effectively learned from human demonstration videos. Our method addresses key challenges in visuomotor policy learning through a principled variational replanning framework that maintains temporal consistency while efficiently adapting to environmental changes. LAP employs Bayesian updating in latent space to incrementally refine plans as new observations become available, striking an optimal balance between computational efficiency and real-time adaptability. We bridge the embodiment gap between humans and robots through model-based proportional mapping that regenerates accurate kinematic-dynamic joint states and object positions from human demonstrations. Experimental evaluations across multiple complex manipulation benchmarks demonstrate that LAP achieves state-of-the-art performance, outperforming existing approaches in success rate, trajectory smoothness, and energy efficiency, particularly in dynamic adaptation scenarios. Our approach enables robots to perform complex interactions with human-like adaptability while providing an expandable framework applicable to diverse robotic platforms using the same human demonstration videos.",CS,AI_ML,0.85,Extracted from log - paper 959
BLAB: Brutally Long Audio Bench,"Developing large audio language models (LMs) capable of understanding diverse spoken interactions is essential for accommodating the multimodal nature of human communication and can increase the accessibility of language technologies across different user populations. Recent work on audio LMs has primarily evaluated their performance on short audio segments, typically under 30 seconds, with limited exploration of long-form conversational speech segments that more closely reflect natural user interactions with these models. We introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio benchmark that evaluates audio LMs on localization, duration estimation, emotion, and counting tasks using audio segments averaging 51 minutes in length. BLAB consists of 833+ hours of diverse, full-length audio clips, each paired with human-annotated, text-based natural language questions and answers. Our audio data were collected from permissively licensed sources and underwent a human-assisted filtering process to ensure task compliance. We evaluate six open-source and proprietary audio LMs on BLAB and find that all of them, including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the tasks in BLAB. Our comprehensive analysis reveals key insights into the trade-offs between task difficulty and audio duration. In general, we find that audio LMs struggle with long-form speech, with performance declining as duration increases. They perform poorly on localization, temporal reasoning, counting, and struggle to understand non-phonemic information, relying more on prompts than audio content. BLAB serves as a challenging evaluation framework to develop audio LMs with robust long-form audio understanding capabilities.",CS,AI_ML,0.85,Extracted from log - paper 960
Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text,"LLM evaluation is challenging even the case of base models. In real world deployments, evaluation is further complicated by the interplay of task specific prompts and experiential context. At scale, bias evaluation is often based on short context, fixed choice benchmarks that can be rapidly evaluated, however, these can lose validity when the LLMs' deployed context differs. Large scale human evaluation is often seen as too intractable and costly. Here we present our journey towards developing a semi-automated bias evaluation framework for free text responses that has human insights at its core. We discuss how we developed an operational definition of bias that helped us automate our pipeline and a methodology for classifying bias beyond multiple choice. We additionally comment on how human evaluation helped us uncover problematic templates in a bias benchmark.",CS,AI_ML,0.85,Extracted from log - paper 961
MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning,"Autonomous long-horizon mobile manipulation encompasses a multitude of challenges, including scene dynamics, unexplored areas, and error recovery. Recent works have leveraged foundation models for scene-level robotic reasoning and planning. However, the performance of these methods degrades when dealing with a large number of objects and large-scale environments. To address these limitations, we propose MORE, a novel approach for enhancing the capabilities of language models to solve zero-shot mobile manipulation planning for rearrangement tasks. MORE leverages scene graphs to represent environments, incorporates instance differentiation, and introduces an active filtering scheme that extracts task-relevant subgraphs of object and region instances. These steps yield a bounded planning problem, effectively mitigating hallucinations and improving reliability. Additionally, we introduce several enhancements that enable planning across both indoor and outdoor environments. We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K benchmark, where it becomes the first approach to successfully solve a significant share of the benchmark, outperforming recent foundation model-based approaches. Furthermore, we demonstrate the capabilities of our approach in several complex real-world tasks, mimicking everyday activities. We make the code publicly available at https://more-model.cs.uni-freiburg.de.",CS,AI_ML,0.85,Extracted from log - paper 962
"Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes","Independent learners often struggle with sustaining focus and emotional regulation in unstructured or distracting settings. Although some rely on ambient aids such as music, ASMR, or visual backgrounds to support concentration, these tools are rarely integrated into cohesive, learner-centered systems. Moreover, existing educational technologies focus primarily on content adaptation and feedback, overlooking the emotional and sensory context in which learning takes place. Large language models have demonstrated powerful multimodal capabilities including the ability to generate and adapt text, audio, and visual content. Educational research has yet to fully explore their potential in creating personalized audiovisual learning environments. To address this gap, we introduce an AI-powered system that uses LLMs to generate personalized multisensory study environments. Users select or generate customized visual themes (e.g., abstract vs. realistic, static vs. animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs. novel sounds) to create immersive settings aimed at reducing distraction and enhancing emotional stability. Our primary research question investigates how combinations of personalized audiovisual elements affect learner cognitive load and engagement. Using a mixed-methods design that incorporates biometric measures and performance outcomes, this study evaluates the effectiveness of LLM-driven sensory personalization. The findings aim to advance emotionally responsive educational technologies and extend the application of multimodal LLMs into the sensory dimension of self-directed learning.",CS,AI_ML,0.85,Extracted from log - paper 963
A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts,"Synthetic data sets are used across linguistic domains and NLP tasks, particularly in scenarios where authentic data is limited (or even non-existent). One such domain is that of clinical (healthcare) contexts, where there exist significant and long-standing challenges (e.g., privacy, anonymization, and data governance) which have led to the development of an increasing number of synthetic datasets. One increasingly important category of clinical dataset is that of clinical dialogues which are especially sensitive and difficult to collect, and as such are commonly synthesized.   While such synthetic datasets have been shown to be sufficient in some situations, little theory exists to inform how they may be best used and generalized to new applications. In this paper, we provide an overview of how synthetic datasets are created, evaluated and being used for dialogue related tasks in the medical domain. Additionally, we propose a novel typology for use in classifying types and degrees of data synthesis, to facilitate comparison and evaluation.",CS,AI_ML,0.85,Extracted from log - paper 964
The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI,"Multimodal learning, which integrates diverse data sources such as images, text, and structured data, has proven superior to unimodal counterparts in high-stakes decision-making. However, while performance gains remain the gold standard for evaluating multimodal systems, concerns around bias and robustness are frequently overlooked. In this context, this paper explores two key research questions (RQs): (i) RQ1 examines whether adding a modality con-sistently enhances performance and investigates its role in shaping fairness measures, assessing whether it mitigates or amplifies bias in multimodal models; (ii) RQ2 investigates the impact of missing modalities at inference time, analyzing how multimodal models generalize in terms of both performance and fairness. Our analysis reveals that incorporating new modalities during training consistently enhances the performance of multimodal models, while fairness trends exhibit variability across different evaluation measures and datasets. Additionally, the absence of modalities at inference degrades performance and fairness, raising concerns about its robustness in real-world deployment. We conduct extensive experiments using multimodal healthcare datasets containing images, time series, and structured information to validate our findings.",CS,AI_ML,0.85,Extracted from log - paper 965
Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis,"While Large Language Models (LLMs) achieve remarkable performance through training on massive datasets, they can exhibit concerning behaviors such as verbatim reproduction of training data rather than true generalization. This memorization phenomenon raises significant concerns about data privacy, intellectual property rights, and the reliability of model evaluations. This paper introduces PEARL, a novel approach for detecting memorization in LLMs. PEARL assesses how sensitive an LLM's performance is to input perturbations, enabling memorization detection without requiring access to the model's internals. We investigate how input perturbations affect the consistency of outputs, enabling us to distinguish between true generalization and memorization. Our findings, following extensive experiments on the Pythia open model, provide a robust framework for identifying when the model simply regurgitates learned information. Applied on the GPT 4o models, the PEARL framework not only identified cases of memorization of classic texts from the Bible or common code from HumanEval but also demonstrated that it can provide supporting evidence that some data, such as from the New York Times news articles, were likely part of the training data of a given model.",CS,AI_ML,0.85,Extracted from log - paper 966
Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer,"Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic technique that improves lesion visibility through the administration of an iodinated contrast agent. It acquires both a low-energy image, comparable to standard mammography, and a high-energy image, which are then combined to produce a dual-energy subtracted image highlighting lesion contrast enhancement. While CESM offers superior diagnostic accuracy compared to standard mammography, its use entails higher radiation exposure and potential side effects associated with the contrast medium. To address these limitations, we propose Seg-CycleGAN, a generative deep learning framework for Virtual Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy subtracted images from low-energy images, leveraging lesion segmentation maps to guide the generative process and improve lesion reconstruction. Building upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss terms focused on lesion areas, enhancing the synthesis of diagnostically relevant regions. Experiments on the CESM@UCBM dataset demonstrate that Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while maintaining competitive MSE and VIF. Qualitative evaluations further confirm improved lesion fidelity in the generated images. These results suggest that segmentation-aware generative models offer a viable pathway toward contrast-free CESM alternatives.",CS,AI_ML,0.85,Extracted from log - paper 967
RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale,"We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper",CS,AI_ML,0.85,Extracted from log - paper 968
Generating Narrated Lecture Videos from Slides with Synchronized Highlights,"Turning static slides into engaging video lectures takes considerable time and effort, requiring presenters to record explanations and visually guide their audience through the material. We introduce an end-to-end system designed to automate this process entirely. Given a slide deck, this system synthesizes a video lecture featuring AI-generated narration synchronized precisely with dynamic visual highlights. These highlights automatically draw attention to the specific concept being discussed, much like an effective presenter would. The core technical contribution is a novel highlight alignment module. This module accurately maps spoken phrases to locations on a given slide using diverse strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at selectable granularities (line or word level) and utilizes timestamp-providing Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's effectiveness through a technical evaluation using a manually annotated slide dataset with 1000 samples, finding that LLM-based alignment achieves high location accuracy (F1 > 92%), significantly outperforming simpler methods, especially on complex, math-heavy content. Furthermore, the calculated generation cost averages under $1 per hour of video, offering potential savings of two orders of magnitude compared to conservative estimates of manual production costs. This combination of high accuracy and extremely low cost positions this approach as a practical and scalable tool for transforming static slides into effective, visually-guided video lectures.",CS,AI_ML,0.85,Extracted from log - paper 969
Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach,"Generative AI systems have revolutionized human interaction by enabling natural language-based coding and problem solving. However, the inherent ambiguity of natural language often leads to imprecise instructions, forcing users to iteratively test, correct, and resubmit their prompts. We propose an iterative approach that systematically narrows down these ambiguities through a structured series of clarification questions and alternative solution proposals, illustrated with input/output examples as well. Once every uncertainty is resolved, a final, precise solution is generated. Evaluated on a diverse dataset spanning coding, data analysis, and creative writing, our method demonstrates superior accuracy, competitive resolution times, and higher user satisfaction compared to conventional one-shot solutions, which typically require multiple manual iterations to achieve a correct output.",CS,AI_ML,0.85,Extracted from log - paper 970
The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence,"A key challenge in multi-agent AI is modeling social cooperation under realistic behavioral constraints. Many foundational concepts in economics and ethics such as ""trust"" or ""morality"" are often defined informally, without operational criteria or cognitive grounding, which limits their testability and implementation in artificial agents. Drawing on converging empirical evidence from primate behavior, infant cognition, and economic anthropology, we propose a conceptual framework composed of three cognitively minimal mechanisms: individual recognition, reciprocal credence, and cost return sensitivity. This framework reframes trust as a graded cognitive expectation, providing a simulateable basis for reciprocal exchange in artificial agents, and enabling the bottom-up emergence of scalable cooperation and institutional dynamics.",CS,AI_ML,0.85,Extracted from log - paper 971
The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models,"Automatic program repair (APR) aims to reduce the manual efforts required to identify and fix errors in source code. Before the rise of LLM-based agents, a common strategy was to increase the number of generated patches, sometimes to the thousands, to achieve better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.   We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs - DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.   Our results show that by using only a fraction (<1%) of the fine-tuning dataset, we can achieve improvements of up to 78% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.",CS,AI_ML,0.85,Extracted from log - paper 972
Early Prediction of Sepsis: Feature-Aligned Transfer Learning,"Sepsis is a life threatening medical condition that occurs when the body has an extreme response to infection, leading to widespread inflammation, organ failure, and potentially death. Because sepsis can worsen rapidly, early detection is critical to saving lives. However, current diagnostic methods often identify sepsis only after significant damage has already occurred. Our project aims to address this challenge by developing a machine learning based system to predict sepsis in its early stages, giving healthcare providers more time to intervene.   A major problem with existing models is the wide variability in the patient information or features they use, such as heart rate, temperature, and lab results. This inconsistency makes models difficult to compare and limits their ability to work across different hospitals and settings. To solve this, we propose a method called Feature Aligned Transfer Learning (FATL), which identifies and focuses on the most important and commonly reported features across multiple studies, ensuring the model remains consistent and clinically relevant.   Most existing models are trained on narrow patient groups, leading to population bias. FATL addresses this by combining knowledge from models trained on diverse populations, using a weighted approach that reflects each models contribution. This makes the system more generalizable and effective across different patient demographics and clinical environments. FATL offers a practical and scalable solution for early sepsis detection, particularly in hospitals with limited resources, and has the potential to improve patient outcomes, reduce healthcare costs, and support more equitable healthcare delivery.",CS,AI_ML,0.85,Extracted from log - paper 973
LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery,"Segmentation models can recognize a pre-defined set of objects in images. However, models that can reason over complex user queries that implicitly refer to multiple objects of interest are still in their infancy. Recent advances in reasoning segmentation--generating segmentation masks from complex, implicit query text--demonstrate that vision-language models can operate across an open domain and produce reasonable outputs. However, our experiments show that such models struggle with complex remote-sensing imagery. In this work, we introduce LISAt, a vision-language model designed to describe complex remote-sensing scenes, answer questions about them, and segment objects of interest. We trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES, with 27,615 annotations over 9,205 images, and a multimodal pretraining dataset, PreGRES, containing over 1 million question-answer pairs. LISAt outperforms existing geospatial foundation models such as RS-GPT4V by over 10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 % (gIoU). Our model, datasets, and code are available at https://lisat-bair.github.io/LISAt/",CS,AI_ML,0.85,Extracted from log - paper 974
Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review,"Explainable Artificial Intelligence (XAI) has emerged as a pillar of Trustworthy AI and aims to bring transparency in complex models that are opaque by nature. Despite the benefits of incorporating explanations in models, an urgent need is found in addressing the privacy concerns of providing this additional information to end users. In this article, we conduct a scoping review of existing literature to elicit details on the conflict between privacy and explainability. Using the standard methodology for scoping review, we extracted 57 articles from 1,943 studies published from January 2019 to December 2024. The review addresses 3 research questions to present readers with more understanding of the topic: (1) what are the privacy risks of releasing explanations in AI systems? (2) what current methods have researchers employed to achieve privacy preservation in XAI systems? (3) what constitutes a privacy preserving explanation? Based on the knowledge synthesized from the selected studies, we categorize the privacy risks and preservation methods in XAI and propose the characteristics of privacy preserving explanations to aid researchers and practitioners in understanding the requirements of XAI that is privacy compliant. Lastly, we identify the challenges in balancing privacy with other system desiderata and provide recommendations for achieving privacy preserving XAI. We expect that this review will shed light on the complex relationship of privacy and explainability, both being the fundamental principles of Trustworthy AI.",CS,AI_ML,0.85,Extracted from log - paper 975
Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models,"Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.",CS,AI_ML,0.85,Extracted from log - paper 976
AutoLibra: Agent Metric Induction from Open-Ended Feedback,"Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., ""If you find that the button is disabled, don't click it again"", or ""This agent has too much autonomy to decide what to do on its own"", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: ""coverage"" and ""redundancy"". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.",CS,AI_ML,0.85,Extracted from log - paper 977
Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing,"Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.   This paper aims to address these limitations by introducing a new framework, \textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.   Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.",CS,AI_ML,0.85,Extracted from log - paper 978
Multi-Agent System for Comprehensive Soccer Understanding,"Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.",CS,AI_ML,0.85,Extracted from log - paper 979
Visual Imitation Enables Contextual Humanoid Control,"How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.",CS,AI_ML,0.85,Extracted from log - paper 980
DISARM++: Beyond scanner-free harmonization,"Harmonization of T1-weighted MR images across different scanners is crucial for ensuring consistency in neuroimaging studies. This study introduces a novel approach to direct image harmonization, moving beyond feature standardization to ensure that extracted features remain inherently reliable for downstream analysis. Our method enables image transfer in two ways: (1) mapping images to a scanner-free space for uniform appearance across all scanners, and (2) transforming images into the domain of a specific scanner used in model training, embedding its unique characteristics. Our approach presents strong generalization capability, even for unseen scanners not included in the training phase. We validated our method using MR images from diverse cohorts, including healthy controls, traveling subjects, and individuals with Alzheimer's disease (AD). The model's effectiveness is tested in multiple applications, such as brain age prediction (R2 = 0.60 \pm 0.05), biomarker extraction, AD classification (Test Accuracy = 0.86 \pm 0.03), and diagnosis prediction (AUC = 0.95). In all cases, our harmonization technique outperforms state-of-the-art methods, showing improvements in both reliability and predictive accuracy. Moreover, our approach eliminates the need for extensive preprocessing steps, such as skull-stripping, which can introduce errors by misclassifying brain and non-brain structures. This makes our method particularly suitable for applications that require full-head analysis, including research on head trauma and cranial deformities. Additionally, our harmonization model does not require retraining for new datasets, allowing smooth integration into various neuroimaging workflows. By ensuring scanner-invariant image quality, our approach provides a robust and efficient solution for improving neuroimaging studies across diverse settings. The code is available at this link.",CS,AI_ML,0.85,Extracted from log - paper 981
Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning,"Vision-language models (VLMs) allow to embed texts and images in a shared representation space. However, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. While this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc. no generic and practical methods have so far been proposed to assess it precisely and even reduce it. We therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. Extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. Our code is available at the URL provided in the paper's abstract.",CS,AI_ML,0.85,Extracted from log - paper 982
Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach,"Automating leaf manipulation in agricultural settings faces significant challenges, including the variability of plant morphologies and deformable leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf grasping that combines traditional computer vision with neural networks through self-supervised learning. Our method integrates YOLOv8 for instance segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf representations, which feed into both a geometric feature scoring pipeline and a neural refinement module (GraspPointCNN). The key innovation is our confidence-weighted fusion mechanism that dynamically balances the contribution of each approach based on prediction certainty. Our self-supervised framework uses the geometric pipeline as an expert teacher to automatically generate training data. Experiments demonstrate that our approach achieves an 88.0% success rate in controlled environments and 84.7% in real greenhouse conditions, significantly outperforming both purely geometric (75.3%) and neural (60.2%) methods. This work establishes a new paradigm for agricultural robotics where domain expertise is seamlessly integrated with machine learning capabilities, providing a foundation for fully automated crop monitoring systems.",CS,AI_ML,0.85,Extracted from log - paper 983
Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration,"Multiview point cloud registration plays a crucial role in robotics, automation, and computer vision fields. This paper concentrates on pose graph construction and motion synchronization within multiview registration. Previous methods for pose graph construction often pruned fully connected graphs or constructed sparse graph using global feature aggregated from local descriptors, which may not consistently yield reliable results. To identify dependable pairs for pose graph construction, we design a network model that extracts information from the matching distance between point cloud pairs. For motion synchronization, we propose another neural network model to calculate the absolute pose in a data-driven manner, rather than optimizing inaccurate handcrafted loss functions. Our model takes into account geometric distribution information and employs a modified attention mechanism to facilitate flexible and reliable feature interaction. Experimental results on diverse indoor and outdoor datasets confirm the effectiveness and generalizability of our approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.",CS,AI_ML,0.85,Extracted from log - paper 984
CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting,"Segmenting objects in an environment is a crucial task for autonomous driving and robotics, as it enables a better understanding of the surroundings of each agent. Although camera sensors provide rich visual details, they are vulnerable to adverse weather conditions. In contrast, radar sensors remain robust under such conditions, but often produce sparse and noisy data. Therefore, a promising approach is to fuse information from both sensors. In this work, we propose a novel framework to enhance camera-only baselines by integrating a diffusion model into a camera-radar fusion architecture. We leverage radar point features to create pseudo-masks using the Segment-Anything model, treating the projected radar points as point prompts. Additionally, we propose a noise reduction unit to denoise these pseudo-masks, which are further used to generate inpainted images that complete the missing information in the original images. Our method improves the camera-only segmentation baseline by 2.63% in mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the Waterscenes dataset. This demonstrates the effectiveness of our approach for semantic segmentation using camera-radar fusion under adverse weather conditions.",CS,AI_ML,0.85,Extracted from log - paper 985
Distribution-Conditional Generation: From Class Distribution to Creative Generation,"Text-to-image (T2I) diffusion models are effective at producing semantically aligned images, but their reliance on training data distributions limits their ability to synthesize truly novel, out-of-distribution concepts. Existing methods typically enhance creativity by combining pairs of known concepts, yielding compositions that, while out-of-distribution, remain linguistically describable and bounded within the existing semantic space. Inspired by the soft probabilistic outputs of classifiers on ambiguous inputs, we propose Distribution-Conditional Generation, a novel formulation that models creativity as image synthesis conditioned on class distributions, enabling semantically unconstrained creative generation. Building on this, we propose DisTok, an encoder-decoder framework that maps class distributions into a latent space and decodes them into tokens of creative concept. DisTok maintains a dynamic concept pool and iteratively sampling and fusing concept pairs, enabling the generation of tokens aligned with increasingly complex class distributions. To enforce distributional consistency, latent vectors sampled from a Gaussian prior are decoded into tokens and rendered into images, whose class distributions-predicted by a vision-language model-supervise the alignment between input distributions and the visual semantics of generated tokens. The resulting tokens are added to the concept pool for subsequent composition. Extensive experiments demonstrate that DisTok, by unifying distribution-conditioned fusion and sampling-based synthesis, enables efficient and flexible token-level generation, achieving state-of-the-art performance with superior text-image alignment and human preference scores.",CS,AI_ML,0.85,Extracted from log - paper 986
Towards Smart Point-and-Shoot Photography,"Hundreds of millions of people routinely take photos using their smartphones as point and shoot (PAS) cameras, yet very few would have the photography skills to compose a good shot of a scene. While traditional PAS cameras have built-in functions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. In this paper, we present a first of its kind smart point and shoot (SPAS) system to help users to take good photos. Our SPAS proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. We first constructed a large dataset containing 320K images with camera pose information from 4000 scenes. We then developed an innovative CLIP-based Composition Quality Assessment (CCQA) model to assign pseudo labels to these images. The CCQA introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {bad, poor, fair, good, perfect}. And finally we have developed a camera pose adjustment model (CPAM) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. The two tasks of CPAM make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the CPAM in an end-to-end manner. We will present extensive results to demonstrate the performances of our SPAS system using publicly available image composition datasets.",CS,AI_ML,0.85,Extracted from log - paper 987
Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision,"Video quality assessment (VQA) is essential for quantifying perceptual quality in various video processing workflows, spanning from camera capture systems to over-the-top streaming platforms. While recent supervised VQA models have made substantial progress, the reliance on manually annotated datasets -- a process that is labor-intensive, costly, and difficult to scale up -- has hindered further optimization of their generalization to unseen video content and distortions. To bridge this gap, we introduce a self-supervised learning framework for VQA to learn quality assessment capabilities from large-scale, unlabeled web videos. Our approach leverages a \textbf{learning-to-rank} paradigm to train a large multimodal model (LMM) on video pairs automatically labeled via two manners, including quality pseudo-labeling by existing VQA models and relative quality ranking based on synthetic distortion simulations. Furthermore, we introduce a novel \textbf{iterative self-improvement training strategy}, where the trained model acts an improved annotator to iteratively refine the annotation quality of training data. By training on a dataset $10\times$ larger than the existing VQA benchmarks, our model: (1) achieves zero-shot performance on in-domain VQA benchmarks that matches or surpasses supervised models; (2) demonstrates superior out-of-distribution (OOD) generalization across diverse video content and distortions; and (3) sets a new state-of-the-art when fine-tuned on human-labeled datasets. Extensive experimental results validate the effectiveness of our self-supervised approach in training generalized VQA models. The datasets and code will be publicly released to facilitate future research.",CS,AI_ML,0.85,Extracted from log - paper 988
Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map,"Synthetic dataset generation in Computer Vision, particularly for industrial applications, is still underexplored. Industrial defect segmentation, for instance, requires highly accurate labels, yet acquiring such data is costly and time-consuming. To address this challenge, we propose a novel diffusion-based pipeline for generating high-fidelity industrial datasets with minimal supervision. Our approach conditions the diffusion model on enriched bounding box representations to produce precise segmentation masks, ensuring realistic and accurately localized defect synthesis. Compared to existing layout-conditioned generative methods, our approach improves defect consistency and spatial accuracy. We introduce two quantitative metrics to evaluate the effectiveness of our method and assess its impact on a downstream segmentation task trained on real and synthetic data. Our results demonstrate that diffusion-based synthesis can bridge the gap between artificial and real-world industrial data, fostering more reliable and cost-efficient segmentation models. The code is publicly available at https://github.com/covisionlab/diffusion_labeling.",CS,AI_ML,0.85,Extracted from log - paper 989
PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing,"Remote photoplethysmography (rPPG) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rPPG signals due to their text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative optimization framework that synergizes LLMs with domain-specific rPPG components. Specifically, the Text Prototype Guidance (TPG) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into LLM-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. Finally, rPPG task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. Evaluation on four benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios.",CS,AI_ML,0.85,Extracted from log - paper 990
Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images,"Face anti-spoofing is a critical technology for ensuring the security of face recognition systems. However, its ability to generalize across diverse scenarios remains a significant challenge. In this paper, we attribute the limited generalization ability to two key factors: covariate shift, which arises from external data collection variations, and semantic shift, which results from substantial differences in emerging attack types. To address both challenges, we propose a novel approach for learning unknown spoof prompts, relying solely on real face images from a single source domain. Our method generates textual prompts for real faces and potential unknown spoof attacks by leveraging the general knowledge embedded in vision-language models, thereby enhancing the model's ability to generalize to unseen target domains. Specifically, we introduce a diverse spoof prompt optimization framework to learn effective prompts. This framework constrains unknown spoof prompts within a relaxed prior knowledge space while maximizing their distance from real face images. Moreover, it enforces semantic independence among different spoof prompts to capture a broad range of spoof patterns. Experimental results on nine datasets demonstrate that the learned prompts effectively transfer the knowledge of vision-language models, enabling state-of-the-art generalization ability against diverse unknown attack types across unseen target domains without using any spoof face images.",CS,AI_ML,0.85,Extracted from log - paper 991
Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection,"3D mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3D mask attacks. While most existing methods utilize multimodal features or remote photoplethysmography (rPPG) signals to distinguish between real faces and 3D masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. Detection-related text descriptions offer concise, universal information and are cost-effective to obtain. However, the potential of vision-language multimodal features for 3D mask presentation attack detection remains unexplored. In this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3D mask presentation attack detection. Specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pre-trained vision-language models. Furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. Additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. During training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. Experimental results demonstrate that the proposed method achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets.",CS,AI_ML,0.85,Extracted from log - paper 992
PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model,"Audio-driven human animation technology is widely used in human-computer interaction, and the emergence of diffusion models has further advanced its development. Currently, most methods rely on multi-stage generation and intermediate representations, resulting in long inference time and issues with generation quality in specific foreground regions and audio-motion consistency. These shortcomings are primarily due to the lack of localized fine-grained supervised guidance. To address above challenges, we propose PAHA, an end-to-end audio-driven upper-body human animation framework with diffusion model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss weights based on pose confidence scores, effectively improving visual quality. PCE constructs and trains diffusion-based regional audio-visual classifiers to improve the consistency of motion and co-speech audio. Afterwards, we design two novel inference guidance methods for the foregoing classifiers, Sequential Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality respectively. Additionally, we build CNAS, the first public Chinese News Anchor Speech dataset, to advance research and validation in this field. Extensive experimental results and user studies demonstrate that PAHA significantly outperforms existing methods in audio-motion alignment and video-related evaluations. The codes and CNAS dataset will be released upon acceptance.",CS,AI_ML,0.85,Extracted from log - paper 993
From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction,"Deep learning-based medical image-to-mesh reconstruction has rapidly evolved, enabling the transformation of medical imaging data into three-dimensional mesh models that are critical in computational medicine and in silico trials for advancing our understanding of disease mechanisms, and diagnostic and therapeutic techniques in modern medicine. This survey systematically categorizes existing approaches into four main categories: template models, statistical models, generative models, and implicit models. Each category is analysed in detail, examining their methodological foundations, strengths, limitations, and applicability to different anatomical structures and imaging modalities. We provide an extensive evaluation of these methods across various anatomical applications, from cardiac imaging to neurological studies, supported by quantitative comparisons using standard metrics. Additionally, we compile and analyze major public datasets available for medical mesh reconstruction tasks and discuss commonly used evaluation metrics and loss functions. The survey identifies current challenges in the field, including requirements for topological correctness, geometric accuracy, and multi-modality integration. Finally, we present promising future research directions in this domain. This systematic review aims to serve as a comprehensive reference for researchers and practitioners in medical image analysis and computational medicine.",CS,AI_ML,0.85,Extracted from log - paper 994
Fixed-Length Dense Fingerprint Representation,"Fixed-length fingerprint representations, which map each fingerprint to a compact and fixed-size feature vector, are computationally efficient and well-suited for large-scale matching. However, designing a robust representation that effectively handles diverse fingerprint modalities, pose variations, and noise interference remains a significant challenge. In this work, we propose a fixed-length dense descriptor of fingerprints, and introduce FLARE-a fingerprint matching framework that integrates the Fixed-Length dense descriptor with pose-based Alignment and Robust Enhancement. This fixed-length representation employs a three-dimensional dense descriptor to effectively capture spatial relationships among fingerprint ridge structures, enabling robust and locally discriminative representations. To ensure consistency within this dense feature space, FLARE incorporates pose-based alignment using complementary estimation methods, along with dual enhancement strategies that refine ridge clarity while preserving the original fingerprint modality. The proposed dense descriptor supports fixed-length representation while maintaining spatial correspondence, enabling fast and accurate similarity computation. Extensive experiments demonstrate that FLARE achieves superior performance across rolled, plain, latent, and contactless fingerprints, significantly outperforming existing methods in cross-modality and low-quality scenarios. Further analysis validates the effectiveness of the dense descriptor design, as well as the impact of alignment and enhancement modules on the accuracy of dense descriptor matching. Experimental results highlight the effectiveness and generalizability of FLARE as a unified and scalable solution for robust fingerprint representation and matching. The implementation and code will be publicly available at https://github.com/Yu-Yy/FLARE.",CS,AI_ML,0.85,Extracted from log - paper 995
DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes,"The analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. Current approaches predominantly utilize visual models. However, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. To address this issue we introduce DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. The purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. Extended evaluations on the STAR and AGQA datasets indicate that DyGEnc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. Furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. We hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. Code is available at github.com/linukc/DyGEnc.",CS,AI_ML,0.85,Extracted from log - paper 996
Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning,"Recycling textile fibers is critical to reducing the environmental impact of the textile industry. Hyperspectral near-infrared (NIR) imaging combined with advanced deep learning algorithms offers a promising solution for efficient fiber classification and sorting. In this study, we investigate supervised and unsupervised deep learning models and test their generalization capabilities on different textile structures. We show that optimized convolutional neural networks (CNNs) and autoencoder networks achieve robust generalization under varying conditions. These results highlight the potential of hyperspectral imaging and deep learning to advance sustainable textile recycling through accurate and robust classification.",CS,AI_ML,0.85,Extracted from log - paper 997
Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models,"Backgrounds in images play a major role in contributing to spurious correlations among different data points. Owing to aesthetic preferences of humans capturing the images, datasets can exhibit positional (location of the object within a given frame) and size (region-of-interest to image ratio) biases for different classes. In this paper, we show that these biases can impact how much a model relies on spurious features in the background to make its predictions. To better illustrate our findings, we propose a synthetic dataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images with various backgrounds, object positions, and object sizes. By evaluating the dataset on different pretrained models, we find that most models rely heavily on spurious features in the background when the region-of-interest (ROI) to image ratio is small and the object is far from the center of the image. Moreover, we also show that current methods that aim to mitigate harmful spurious features, do not take into account these factors, hence fail to achieve considerable performance gains for worst-group accuracies when the size and location of core features in an image change.",CS,AI_ML,0.85,Extracted from log - paper 998
Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images,"Text-based pedestrian search (TBPS) in full images aims to locate a target pedestrian in untrimmed images using natural language descriptions. However, in complex scenes with multiple pedestrians, existing methods are limited by uncertainties in detection and matching, leading to degraded performance. To address this, we propose UPD-TBPS, a novel framework comprising three modules: Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts multi-granularity queries to identify potential targets and assigns confidence scores to reduce early-stage uncertainty. PUD leverages visual context decoupling and prototype mining to extract features of the target pedestrian described in the query. It separates and learns pedestrian prototype representations at both the coarse-grained cluster level and the fine-grained individual level, thereby reducing matching uncertainty. ReID evaluates candidates with varying confidence levels, improving detection and retrieval accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the effectiveness of our framework.",CS,AI_ML,0.85,Extracted from log - paper 999
Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment,"The Equine Facial Action Coding System (EquiFACS) enables the systematic annotation of facial movements through distinct Action Units (AUs). It serves as a crucial tool for assessing affective states in horses by identifying subtle facial expressions associated with discomfort. However, the field of horse affective state assessment is constrained by the scarcity of annotated data, as manually labelling facial AUs is both time-consuming and costly. To address this challenge, automated annotation systems are essential for leveraging existing datasets and improving affective states detection tools. In this work, we study different methods for specific ear AU detection and localization from horse videos. We leverage past works on deep learning-based video feature extraction combined with recurrent neural networks for the video classification task, as well as a classic optical flow based approach. We achieve 87.5% classification accuracy of ear movement presence on a public horse video dataset, demonstrating the potential of our approach. We discuss future directions to develop these systems, with the aim of bridging the gap between automated AU detection and practical applications in equine welfare and veterinary diagnostics. Our code will be made publicly available at https://github.com/jmalves5/read-my-ears.",CS,AI_ML,0.85,Extracted from log - paper 1000
Panoramic Out-of-Distribution Segmentation,"Panoramic imaging enables capturing 360{\deg} images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to background clutter and pixel distortions. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.",CS,AI_ML,0.85,Extracted from log - paper 1001
RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT,"Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.",CS,AI_ML,0.85,Extracted from log - paper 1002
Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication,"Cooperative perception, leveraging shared information from multiple vehicles via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous driving to alleviate the limitation of single-vehicle perception. Existing works have explored the effects of V2V communication impairments on perception precision, but they lack generalization to different levels of impairments. In this work, we propose a joint weighting and denoising framework, Coop-WD, to enhance cooperative perception subject to V2V channel impairments. In this framework, the self-supervised contrastive model and the conditional diffusion probabilistic model are adopted hierarchically for vehicle-level and pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is proposed to selectively deactivate denoising to reduce processing overhead. Rician fading, non-stationarity, and time-varying distortion are considered. Simulation results demonstrate that the proposed Coop-WD outperforms conventional benchmarks in all types of channels. Qualitative analysis with visual examples further proves the superiority of our proposed method. The proposed Coop-WD-eco achieves up to 50% reduction in computational cost under severe distortion while maintaining comparable accuracy as channel conditions improve.",CS,AI_ML,0.85,Extracted from log - paper 1003
Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking,"To reduce the reliance on large-scale annotations, self-supervised RGB-T tracking approaches have garnered significant attention. However, the omission of the object region by erroneous pseudo-label or the introduction of background noise affects the efficiency of modality fusion, while pseudo-label noise triggered by similar object noise can further affect the tracking performance. In this paper, we propose GDSTrack, a novel approach that introduces dynamic graph fusion and temporal diffusion to address the above challenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the modalities of neighboring frames, treats them as distractor noise, and leverages the denoising capability of a generative model. Specifically, by constructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the proposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic adjacency matrix to guide graph attention, focusing on and fusing the object's coherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features from neighboring frames as interference, and thus improving robustness against similar-object noise. Extensive experiments conducted on four public RGB-T tracking datasets demonstrate that GDSTrack outperforms the existing state-of-the-art methods. The source code is available at https://github.com/LiShenglana/GDSTrack.",CS,AI_ML,0.85,Extracted from log - paper 1004
MRI motion correction via efficient residual-guided denoising diffusion probabilistic models,"Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly degrade image quality and impair quantitative analysis. Conventional mitigation strategies, such as repeated acquisitions or motion tracking, are costly and workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising diffusion probabilistic model tailored for MRI motion artifact correction. Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in the forward diffusion process, aligning the noise distribution with motion-corrupted data and enabling an efficient four-step reverse diffusion. A U-net backbone enhanced with Swin-Transformer blocks conventional attention layers, improving adaptability across resolutions. Training employs a combined l1+l2 loss, which promotes image sharpness and reduces pixel-level errors. Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic motion simulation framework and on an in-vivo dataset. Comparative analyses were conducted against established methods, including CycleGAN, Pix2pix, and MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and normalized mean squared error (NMSE). Results: The proposed method demonstrated superior performance in removing motion artifacts across all motion severity levels. Res-MoCoDiff consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling time was reduced to 0.37 seconds per batch of two image slices, compared with 101.74 seconds for conventional approaches.",CS,AI_ML,0.85,Extracted from log - paper 1005
UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion,"Background: Brain tumor segmentation has a significant impact on the diagnosis and treatment of brain tumors. Accurate brain tumor segmentation remains challenging due to their irregular shapes, vague boundaries, and high variability. Objective: We propose a brain tumor segmentation method that combines deep learning with prior knowledge derived from a region-growing algorithm. Methods: The proposed method utilizes a multi-scale feature fusion (MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale features and capture global contextual information. To enhance the model's robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout) strategy is employed for uncertainty estimation. Results: Extensive experiments demonstrate that the proposed method achieves superior performance on Brain Tumor Segmentation (BraTS) datasets, significantly outperforming various state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are 89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT) segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019 validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for ET, WT, and TC segmentation, respectively. Ablation studies further confirmed the contribution of each module to segmentation accuracy, indicating that each component played a vital role in overall performance improvement. Conclusion: This study proposed a novel 3D brain tumor segmentation network based on the U-Net architecture. By incorporating the prior knowledge and employing the uncertainty estimation method, the robustness and performance were improved. The code for the proposed method is available at https://github.com/chenzhao2023/UPMAD_Net_BrainSeg.",CS,AI_ML,0.85,Extracted from log - paper 1006
Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD),"Dynamic computed tomography (CT) reconstruction faces significant challenges in addressing motion artifacts, particularly for nonperiodic rapid movements such as cardiac imaging with fast heart rates. Traditional methods struggle with the extreme limited-angle problems inherent in nonperiodic cases. Deep learning methods have improved performance but face generalization challenges. Recent implicit neural representation (INR) techniques show promise through self-supervised deep learning, but have critical limitations: computational inefficiency due to forward-warping modeling, difficulty balancing DVF complexity with anatomical plausibility, and challenges in preserving fine details without additional patient-specific pre-scans. This paper presents a novel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It addresses these challenges through four key contributions: (1) backward-warping deformation that enables direct computation of each dynamic voxel with significantly reduced computational cost, (2) diffeomorphism-based DVF regularization that ensures anatomically plausible deformations while maintaining representational capacity, (3) motion-compensated analytical reconstruction that enhances fine details without requiring additional pre-scans, and (4) dimensional-reduction design for efficient 4D coordinate encoding. Through various simulations and practical studies, including digital and physical phantoms and retrospective patient data, we demonstrate the effectiveness of our approach for nonperiodic dynamic CT reconstruction with enhanced details and reduced motion artifacts. The proposed framework enables more accurate dynamic CT reconstruction with potential clinical applications, such as one-beat cardiac reconstruction, cinematic image sequences for functional imaging, and motion artifact reduction in conventional CT scans.",CS,AI_ML,0.85,Extracted from log - paper 1007
Polar Coordinate-Based 2D Pose Prior with Neural Distance Field,"Human pose capture is essential for sports analysis, enabling precise evaluation of athletes' movements. While deep learning-based human pose estimation (HPE) models from RGB videos have achieved impressive performance on public datasets, their effectiveness in real-world sports scenarios is often hindered by motion blur, occlusions, and domain shifts across different pose representations. Fine-tuning these models can partially alleviate such challenges but typically requires large-scale annotated data and still struggles to generalize across diverse sports environments. To address these limitations, we propose a 2D pose prior-guided refinement approach based on Neural Distance Fields (NDF). Unlike existing approaches that rely solely on angular representations of human poses, we introduce a polar coordinate-based representation that explicitly incorporates joint connection lengths, enabling a more accurate correction of erroneous pose estimations. Additionally, we define a novel non-geodesic distance metric that separates angular and radial discrepancies, which we demonstrate is better suited for polar representations than traditional geodesic distances. To mitigate data scarcity, we develop a gradient-based batch-projection augmentation strategy, which synthesizes realistic pose samples through iterative refinement. Our method is evaluated on a long jump dataset, demonstrating its ability to improve 2D pose estimation across multiple pose representations, making it robust across different domains. Experimental results show that our approach enhances pose plausibility while requiring only limited training data. Code is available at: https://github.com/QGAN2019/polar-NDF.",CS,AI_ML,0.85,Extracted from log - paper 1008
Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks,"The rapid advancement of generative image technology has introduced significant security concerns, particularly in the domain of face generation detection. This paper investigates the vulnerabilities of current AI-generated face detection systems. Our study reveals that while existing detection methods often achieve high accuracy under standard conditions, they exhibit limited robustness against adversarial attacks. To address these challenges, we propose an approach that integrates adversarial training to mitigate the impact of adversarial examples. Furthermore, we utilize diffusion inversion and reconstruction to further enhance detection robustness. Experimental results demonstrate that minor adversarial perturbations can easily bypass existing detection systems, but our method significantly improves the robustness of these systems. Additionally, we provide an in-depth analysis of adversarial and benign examples, offering insights into the intrinsic characteristics of AI-generated content. All associated code will be made publicly available in a dedicated repository to facilitate further research and verification.",CS,AI_ML,0.85,Extracted from log - paper 1009
A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution,"The fusion of low-spatial-resolution hyperspectral images (HSIs) with high-spatial-resolution conventional images (e.g., panchromatic or RGB) has played a significant role in recent advancements in HSI super-resolution. However, this fusion process relies on the availability of precise alignment between image pairs, which is often challenging in real-world scenarios. To mitigate this limitation, we propose a single-image super-resolution model called the Fusion-Guided Inception Network (FGIN). Specifically, we first employ a spectral-spatial fusion module to effectively integrate spectral and spatial information at an early stage. Next, an Inception-like hierarchical feature extraction strategy is used to capture multiscale spatial dependencies, followed by a dedicated multi-scale fusion block. To further enhance reconstruction quality, we incorporate an optimized upsampling module that combines bilinear interpolation with depthwise separable convolutions. Experimental evaluations on two publicly available hyperspectral datasets demonstrate the competitive performance of our method.",CS,AI_ML,0.85,Extracted from log - paper 1010
LiftFeat: 3D Geometry-Aware Local Feature Matching,"Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called \textit{LiftFeat}, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.",CS,AI_ML,0.85,Extracted from log - paper 1011
Mitigating Image Captioning Hallucinations in Vision-Language Models,"Hallucinations in vision-language models (VLMs) hinder reliability and real-world applicability, usually stemming from distribution shifts between pretraining data and test samples. Existing solutions, such as retraining or fine-tuning on additional data, demand significant computational resources and labor-intensive data collection, while ensemble-based methods incur additional costs by introducing auxiliary VLMs. To address these challenges, we propose a novel test-time adaptation framework using reinforcement learning to mitigate hallucinations during inference without retraining or any auxiliary VLMs. By updating only the learnable parameters in the layer normalization of the language model (approximately 0.003% of the model parameters), our method reduces distribution shifts between test samples and pretraining samples. A CLIP-based hallucination evaluation model is proposed to provide dual rewards to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in hallucination rates on LLaVA and InstructBLIP, respectively. Our approach outperforms state-of-the-art baselines with a 68.3% improvement in hallucination mitigation, demonstrating its effectiveness.",CS,AI_ML,0.85,Extracted from log - paper 1012
Enhancing Target-unspecific Tasks through a Features Matrix,"Recent developments in prompt learning of large vision-language models have significantly improved performance in target-specific tasks. However, these prompt optimizing methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge having strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) regularization approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2) the FM significantly showcases its effectiveness in enhancing target-unspecific tasks, achieving state-of-the-art performance.",CS,AI_ML,0.85,Extracted from log - paper 1013
CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection,"Internal defect detection constitutes a critical process in ensuring component quality, for which anomaly detection serves as an effective solution. However, existing anomaly detection datasets predominantly focus on surface defects in visible-light images, lacking publicly available X-ray datasets targeting internal defects in components. To address this gap, we construct the first publicly accessible component X-ray anomaly detection (CXR-AD) dataset, comprising real-world X-ray images. The dataset covers five industrial component categories, including 653 normal samples and 561 defect samples with precise pixel-level mask annotations. We systematically analyze the dataset characteristics and identify three major technical challenges: (1) strong coupling between complex internal structures and defect regions, (2) inherent low contrast and high noise interference in X-ray imaging, and (3) significant variations in defect scales and morphologies. To evaluate dataset complexity, we benchmark three state-of-the-art anomaly detection frameworks (feature-based, reconstruction-based, and zero-shot learning methods). Experimental results demonstrate a 29.78% average performance degradation on CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms in handling internal defect detection tasks. To the best of our knowledge, CXR-AD represents the first publicly available X-ray dataset for component anomaly detection, providing a real-world industrial benchmark to advance algorithm development and enhance precision in internal defect inspection technologies.",CS,AI_ML,0.85,Extracted from log - paper 1014
EOPose : Exemplar-based object reposing using Generalized Pose Correspondences,"Reposing objects in images has a myriad of applications, especially for e-commerce where several variants of product images need to be produced quickly. In this work, we leverage the recent advances in unsupervised keypoint correspondence detection between different object images of the same class to propose an end-to-end framework for generic object reposing. Our method, EOPose, takes a target pose-guidance image as input and uses its keypoint correspondence with the source object image to warp and re-render the latter into the target pose using a novel three-step approach. Unlike generative approaches, our method also preserves the fine-grained details of the object such as its exact colors, textures, and brand marks. We also prepare a new dataset of paired objects based on the Objaverse dataset to train and test our network. EOPose produces high-quality reposing output as evidenced by different image quality metrics (PSNR, SSIM and FID). Besides a description of the method and the dataset, the paper also includes detailed ablation and user studies to indicate the efficacy of the proposed method",CS,AI_ML,0.85,Extracted from log - paper 1015
Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples,"Adversarial examples have revealed the vulnerability of deep learning models and raised serious concerns about information security. The transfer-based attack is a hot topic in black-box attacks that are practical to real-world scenarios where the training datasets, parameters, and structure of the target model are unknown to the attacker. However, few methods consider the particularity of class-specific deep models for fine-grained vision tasks, such as face recognition (FR), giving rise to unsatisfactory attacking performance. In this work, we first investigate what in a face exactly contributes to the embedding learning of FR models and find that both decisive and auxiliary facial features are specific to each FR model, which is quite different from the biological mechanism of human visual system. Accordingly we then propose a novel attack method named Attention-aggregated Attack (AAA) to enhance the transferability of adversarial examples against FR, which is inspired by the attention divergence and aims to destroy the facial features that are critical for the decision-making of other FR models by imitating their attentions on the clean face images. Extensive experiments conducted on various FR models validate the superiority and robust effectiveness of the proposed method over existing methods.",CS,AI_ML,0.85,Extracted from log - paper 1016
Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models,"Introduction: Data from wearable devices collected in free-living settings, and labelled with physical activity behaviours compatible with health research, are essential for both validating existing wearable-based measurement approaches and developing novel machine learning approaches. One common way of obtaining these labels relies on laborious annotation of sequences of images captured by cameras worn by participants through the course of a day. Methods: We compare the performance of three vision language models and two discriminative models on two free-living validation studies with 161 and 111 participants, collected in Oxfordshire, United Kingdom and Sichuan, China, respectively, using the Autographer (OMG Life, defunct) wearable camera. Results: We found that the best open-source vision-language model (VLM) and fine-tuned discriminative model (DM) achieved comparable performance when predicting sedentary behaviour from single images on unseen participants in the Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86, 0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63, 0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53, 0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study, performance fell across all intensity categories, with median Cohen's kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM, and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely available computer vision models could help annotate sedentary behaviour, typically the most prevalent activity of daily living, from wearable camera images within similar populations to seen data, reducing the annotation burden.",CS,AI_ML,0.85,Extracted from log - paper 1017
3D Surface Reconstruction with Enhanced High-Frequency Details,"Neural implicit 3D reconstruction can reproduce shapes without 3D supervision, and it learns the 3D scene through volume rendering methods and neural implicit representations. Current neural surface reconstruction methods tend to randomly sample the entire image, making it difficult to learn high-frequency details on the surface, and thus the reconstruction results tend to be too smooth. We designed a method (FreNeuS) based on high-frequency information to solve the problem of insufficient surface detail. Specifically, FreNeuS uses pixel gradient changes to easily acquire high-frequency regions in an image and uses the obtained high-frequency information to guide surface detail reconstruction. High-frequency information is first used to guide the dynamic sampling of rays, applying different sampling strategies according to variations in high-frequency regions. To further enhance the focus on surface details, we have designed a high-frequency weighting method that constrains the representation of high-frequency details during the reconstruction process. Qualitative and quantitative results show that our method can reconstruct fine surface details and obtain better surface reconstruction quality compared to existing methods. In addition, our method is more applicable and can be generalized to any NeuS-based work.",CS,AI_ML,0.85,Extracted from log - paper 1018
Interpretable Zero-shot Learning with Infinite Class Concepts,"Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images with intermediate class semantics, like human-annotated concepts or class definitions. An emerging alternative leverages Large-scale Language Models (LLMs) to automatically generate class documents. However, these methods often face challenges with transparency in the classification process and may suffer from the notorious hallucination problem in LLMs, resulting in non-visual class semantics. This paper redefines class semantics in ZSL with a focus on transferability and discriminability, introducing a novel framework called Zero-shot Learning with Infinite Class Concepts (InfZSL). Our approach leverages the powerful capabilities of LLMs to dynamically generate an unlimited array of phrase-level class concepts. To address the hallucination challenge, we introduce an entropy-based scoring process that incorporates a ``goodness"" concept selection mechanism, ensuring that only the most transferable and discriminative concepts are selected. Our InfZSL framework not only demonstrates significant improvements on three popular benchmark datasets but also generates highly interpretable, image-grounded concepts. Code will be released upon acceptance.",CS,AI_ML,0.85,Extracted from log - paper 1019
GUAVA: Generalizable Upper Body 3D Gaussian Avatar,"Reconstructing a high-quality, animatable 3D human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3D human avatar reconstruction typically requires multi-view or monocular videos and training on individual IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's expressiveness, these methods often focus on body motion but struggle with facial expressions. To address these challenges, we first introduce an expressive human model (EHM) to enhance facial expression capabilities and develop an accurate tracking method. Based on this template model, we propose GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar reconstruction. We leverage inverse texture mapping and projection sampling techniques to infer Ubody (upper-body) Gaussians from a single image. The rendered images are refined through a neural refiner. Experimental results demonstrate that GUAVA significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.",CS,AI_ML,0.85,Extracted from log - paper 1020
A Vision-Language Model for Focal Liver Lesion Classification,"Accurate classification of focal liver lesions is crucial for diagnosis and treatment in hepatology. However, traditional supervised deep learning models depend on large-scale annotated datasets, which are often limited in medical imaging. Recently, Vision-Language models (VLMs) such as Contrastive Language-Image Pre-training model (CLIP) has been applied to image classifications. Compared to the conventional convolutional neural network (CNN), which classifiers image based on visual information only, VLM leverages multimodal learning with text and images, allowing it to learn effectively even with a limited amount of labeled data. Inspired by CLIP, we pro-pose a Liver-VLM, a model specifically designed for focal liver lesions (FLLs) classification. First, Liver-VLM incorporates class information into the text encoder without introducing additional inference overhead. Second, by calculating the pairwise cosine similarities between image and text embeddings and optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively aligns image features with class-level text features. Experimental results on MPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the standard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve (AUC). Further analysis shows that using a lightweight ResNet18 backbone enhances classification performance, particularly under data-constrained conditions.",CS,AI_ML,0.85,Extracted from log - paper 1021
From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection,"In recent years, language-guided open-world aerial object detection has gained significant attention due to its better alignment with real-world application needs. However, due to limited datasets, most existing language-guided methods primarily focus on vocabulary, which fails to meet the demands of more fine-grained open-world detection. To address this limitation, we propose constructing a large-scale language-guided open-set aerial detection dataset, encompassing three levels of language guidance: from words to phrases, and ultimately to sentences. Centered around an open-source large vision-language model and integrating image-operation-based preprocessing with BERT-based postprocessing, we present the OS-W2S Label Engine, an automatic annotation pipeline capable of handling diverse scene annotations for aerial images. Using this label engine, we expand existing aerial detection datasets with rich textual annotations and construct a novel benchmark dataset, called Multi-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of current remote sensing grounding data and enabling effective open-set aerial detection. Specifically, MI-OAD contains 163,023 images and 2 million image-caption pairs, approximately 40 times larger than comparable datasets. We also employ state-of-the-art open-set methods from the natural image domain, trained on our proposed dataset, to validate the model's open-set detection capabilities. For instance, when trained on our dataset, Grounding DINO achieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs under zero-shot transfer conditions. Both the dataset and the label engine will be released publicly.",CS,AI_ML,0.85,Extracted from log - paper 1022
FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing,"The task of scene text editing is to modify or add texts on images while maintaining the fidelity of newly generated text and visual coherence with the background. Recent works based on latent diffusion models (LDM) show improved text editing results, yet still face challenges and often generate inaccurate or unrecognizable characters, especially for non-Latin ones (\eg, Chinese), which have complex glyph structures. To address these issues, we present FLUX-Text, a simple and advanced multilingual scene text editing framework based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning, considering both visual and textual modalities. To retain the original generative capabilities of FLUX-Fill while enhancing its understanding and generation of glyphs, we propose lightweight glyph and text embedding modules. Owning to the lightweight design, FLUX-Text is trained only with $100K$ training examples compared to current popular methods trained with 2.9M ones. With no bells and whistles, our method achieves state-of-the-art performance on text editing tasks. Qualitative and quantitative experiments on the public datasets demonstrate that our method surpasses previous works in text fidelity.",CS,AI_ML,0.85,Extracted from log - paper 1023
Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning,"Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.",CS,AI_ML,0.85,Extracted from log - paper 1024
3D Gaussian Splatting Data Compression with Mixture of Priors,"3D Gaussian Splatting (3DGS) data compression is crucial for enabling efficient storage and transmission in 3D scene modeling. However, its development remains limited due to inadequate entropy models and suboptimal quantization strategies for both lossless and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. In this work, we propose a novel Mixture of Priors (MoP) strategy to simultaneously address these two challenges. Specifically, inspired by the Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior information through multiple lightweight MLPs to generate diverse prior features, which are subsequently integrated into the MoP feature via a gating mechanism. To enhance lossless compression, the resulting MoP feature is utilized as a hyperprior to improve conditional entropy modeling. Meanwhile, for lossy compression, we employ the MoP feature as guidance information in an element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine Quantization (C2FQ) strategy with a predefined quantization step value. Specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the MoP feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. Extensive experiments demonstrate that our proposed 3DGS data compression framework achieves state-of-the-art performance across multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&Temples.",CS,AI_ML,0.85,Extracted from log - paper 1025
3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation,"Semantic segmentation of 3D LiDAR point clouds, essential for autonomous driving and infrastructure management, is best achieved by supervised learning, which demands extensive annotated datasets and faces the problem of domain shifts. We introduce a new 3D semantic segmentation pipeline that leverages aligned scenes and state-of-the-art 2D segmentation methods, avoiding the need for direct 3D annotation or reliance on additional modalities such as camera images at inference time. Our approach generates 2D views from LiDAR scans colored by sensor intensity and applies 2D semantic segmentation to these views using a camera-domain pretrained model. The segmented 2D outputs are then back-projected onto the 3D points, with a simple voting-based estimator that merges the labels associated to each 3D point. Our main contribution is a global pipeline for 3D semantic segmentation requiring no prior 3D annotation and not other modality for inference, which can be used for pseudo-label generation. We conduct a thorough ablation study and demonstrate the potential of the generated pseudo-labels for the Unsupervised Domain Adaptation task.",CS,AI_ML,0.85,Extracted from log - paper 1026
Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification,"Visible-infrared person re-identification (VIReID) provides a solution for ReID tasks in 24-hour scenarios; however, significant challenges persist in achieving satisfactory performance due to the substantial discrepancies between visible (VIS) and infrared (IR) modalities. Existing methods inadequately leverage information from different modalities, primarily focusing on digging distinguishing features from modality-shared information while neglecting modality-specific details. To fully utilize differentiated minutiae, we propose a Base-Detail Feature Learning Framework (BDLF) that enhances the learning of both base and detail knowledge, thereby capitalizing on both modality-shared and modality-specific information. Specifically, the proposed BDLF mines detail and base features through a lossless detail feature extraction module and a complementary base embedding generation mechanism, respectively, supported by a novel correlation restriction method that ensures the features gained by BDLF enrich both detail and base knowledge across VIS and IR features. Comprehensive experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the effectiveness of BDLF.",CS,AI_ML,0.85,Extracted from log - paper 1027
OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction,"The safe operation of autonomous vehicles (AVs) is highly dependent on their understanding of the surroundings. For this, the task of 3D semantic occupancy prediction divides the space around the sensors into voxels, and labels each voxel with both occupancy and semantic information. Recent perception models have used multisensor fusion to perform this task. However, existing multisensor fusion-based approaches focus mainly on using sensor information in the Cartesian coordinate system. This ignores the distribution of the sensor readings, leading to a loss of fine-grained details and performance degradation. In this paper, we propose OccCylindrical that merges and refines the different modality features under cylindrical coordinates. Our method preserves more fine-grained geometry detail that leads to better performance. Extensive experiments conducted on the nuScenes dataset, including challenging rainy and nighttime scenarios, confirm our approach's effectiveness and state-of-the-art performance. The code will be available at: https://github.com/DanielMing123/OccCylindrical",CS,AI_ML,0.85,Extracted from log - paper 1028
DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor,"Video Quality Assessment (VQA) aims to evaluate video quality based on perceptual distortions and human preferences. Despite the promising performance of existing methods using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), they often struggle to align closely with human perceptions, particularly in diverse real-world scenarios. This challenge is exacerbated by the limited scale and diversity of available datasets. To address this limitation, we introduce a novel VQA framework, DiffVQA, which harnesses the robust generalization capabilities of diffusion models pre-trained on extensive datasets. Our framework adapts these models to reconstruct identical input frames through a control module. The adapted diffusion model is then used to extract semantic and distortion features from a resizing branch and a cropping branch, respectively. To enhance the model's ability to handle long-term temporal dynamics, a parallel Mamba module is introduced, which extracts temporal coherence augmented features that are merged with the diffusion features to predict the final score. Experiments across multiple datasets demonstrate DiffVQA's superior performance on intra-dataset evaluations and its exceptional generalization across datasets. These results confirm that leveraging a diffusion model as a feature extractor can offer enhanced VQA performance compared to CNN and ViT backbones.",CS,AI_ML,0.85,Extracted from log - paper 1029
PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs,"Convolutional neural networks (CNNs) are crucial for computer vision tasks on resource-constrained devices. Quantization effectively compresses these models, reducing storage size and energy cost. However, in modern depthwise-separable architectures, the computational cost is distributed unevenly across its components, with pointwise operations being the most expensive. By applying a general quantization scheme to this imbalanced cost distribution, existing quantization approaches fail to fully exploit potential efficiency gains. To this end, we introduce PROM, a straightforward approach for quantizing modern depthwise-separable convolutional networks by selectively using two distinct bit-widths. Specifically, pointwise convolutions are quantized to ternary weights, while the remaining modules use 8-bit weights, which is achieved through a simple quantization-aware training procedure. Additionally, by quantizing activations to 8-bit, our method transforms pointwise convolutions with ternary weights into int8 additions, which enjoy broad support across hardware platforms and effectively eliminates the need for expensive multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost by more than an order of magnitude (23.9x) and its storage size by 2.7x compared to the float16 baseline while retaining similar classification performance on ImageNet. Our method advances the Pareto frontier for energy consumption vs. top-1 accuracy for quantized convolutional models on ImageNet. PROM addresses the challenges of quantizing depthwise-separable convolutional networks to both ternary and 8-bit weights, offering a simple way to reduce energy cost and storage size.",CS,AI_ML,0.85,Extracted from log - paper 1030
Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data,"Hyperspectral images (HSIs) capture rich spectral signatures that reveal vital material properties, offering broad applicability across various domains. However, the scarcity of labeled HSI data limits the full potential of deep learning, especially for transformer-based architectures that require large-scale training. To address this constraint, we propose Spatial-Frequency Masked Image Modeling (SFMIM), a self-supervised pretraining strategy for hyperspectral data that utilizes the large portion of unlabeled data. Our method introduces a novel dual-domain masking mechanism that operates in both spatial and frequency domains. The input HSI cube is initially divided into non-overlapping patches along the spatial dimension, with each patch comprising the entire spectrum of its corresponding spatial location. In spatial masking, we randomly mask selected patches and train the model to reconstruct the masked inputs using the visible patches. Concurrently, in frequency masking, we remove portions of the frequency components of the input spectra and predict the missing frequencies. By learning to reconstruct these masked components, the transformer-based encoder captures higher-order spectral-spatial correlations. We evaluate our approach on three publicly available HSI classification benchmarks and demonstrate that it achieves state-of-the-art performance. Notably, our model shows rapid convergence during fine-tuning, highlighting the efficiency of our pretraining strategy.",CS,AI_ML,0.85,Extracted from log - paper 1031
PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models,"Advanced diffusion models have made notable progress in text-to-image compositional generation. However, it is still a challenge for existing models to achieve text-image alignment when confronted with complex text prompts. In this work, we highlight two factors that affect this alignment: the quality of the randomly initialized noise and the reliability of the generated controlling mask. We then propose PiCo (Pick-and-Control), a novel training-free approach with two key components to tackle these two factors. First, we develop a noise selection module to assess the quality of the random noise and determine whether the noise is suitable for the target text. A fast sampling strategy is utilized to ensure efficiency in the noise selection stage. Second, we introduce a referring mask module to generate pixel-level masks and to precisely modulate the cross-attention maps. The referring mask is applied to the standard diffusion process to guide the reasonable interaction between text and image features. Extensive experiments have been conducted to verify the effectiveness of PiCo in liberating users from the tedious process of random generation and in enhancing the text-image alignment for diverse text descriptions.",CS,AI_ML,0.85,Extracted from log - paper 1032
CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization,"The inherent synchronization between a speaker's lip movements, voice, and the underlying linguistic content offers a rich source of information for improving speech processing tasks, especially in challenging conditions where traditional audio-only systems falter. We introduce CoGenAV, a powerful and data-efficient model designed to learn versatile audio-visual representations applicable across a wide range of speech and audio-visual tasks. CoGenAV is trained by optimizing a dual objective derived from natural audio-visual synchrony, contrastive feature alignment and generative text prediction, using only 223 hours of labeled data from the LRS2 dataset. This contrastive-generative synchronization strategy effectively captures fundamental cross-modal correlations. We showcase the effectiveness and versatility of the learned CoGenAV representations on multiple benchmarks. When utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these representations contribute to achieving a state-of-the-art Word Error Rate (WER) of 1.27. They also enable strong performance in Visual Speech Recognition (VSR) with a WER of 22.0 on LRS2, and significantly improve performance in noisy environments by over 70%. Furthermore, CoGenAV representations benefit speech reconstruction tasks, boosting performance in Speech Enhancement and Separation, and achieve competitive results in audio-visual synchronization tasks like Active Speaker Detection (ASD). Our model will be open-sourced to facilitate further development and collaboration within both academia and industry.",CS,AI_ML,0.85,Extracted from log - paper 1033
Interactive Instance Annotation with Siamese Networks,"Annotating instance masks is time-consuming and labor-intensive. A promising solution is to predict contours using a deep learning model and then allow users to refine them. However, most existing methods focus on in-domain scenarios, limiting their effectiveness for cross-domain annotation tasks. In this paper, we propose SiamAnno, a framework inspired by the use of Siamese networks in object tracking. SiamAnno leverages one-shot learning to annotate previously unseen objects by taking a bounding box as input and predicting object boundaries, which can then be adjusted by annotators. Trained on one dataset and tested on another without fine-tuning, SiamAnno achieves state-of-the-art (SOTA) performance across multiple datasets, demonstrating its ability to handle domain and environment shifts in cross-domain tasks. We also provide more comprehensive results compared to previous work, establishing a strong baseline for future research. To our knowledge, SiamAnno is the first model to explore Siamese architecture for instance annotation.",CS,AI_ML,0.85,Extracted from log - paper 1034
Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets,"Instruction-Action (IA) data pairs are valuable for training robotic systems, especially autonomous vehicles (AVs), but having humans manually annotate this data is costly and time-inefficient. This paper explores the potential of using mobile application Global Positioning System (GPS) references and Natural Language Processing (NLP) to automatically generate large volumes of IA commands and responses without having a human generate or retroactively tag the data. In our pilot data collection, by driving to various destinations and collecting voice instructions from GPS applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. We provide details on our completely automated data collection prototype system, ADVLAT-Engine. We characterize collected GPS voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. Through research and exploration into the automation of IA data pairs using GPS references, the potential to increase the speed and volume at which high-quality IA datasets are created, while minimizing cost, can pave the way for robust vision-language-action (VLA) models to serve tasks in vision-language navigation (VLN) and human-interactive autonomous systems.",CS,AI_ML,0.85,Extracted from log - paper 1035
Robust Fairness Vision-Language Learning for Medical Image Analysis,"The advent of Vision-Language Models (VLMs) in medical image analysis has the potential to help process multimodal inputs and increase performance over traditional inference methods. However, when considering the domain in which these models will be implemented, fairness and robustness are important to ensure the model stays true for any patient. In this paper, we introduce a framework for ensuring robustness and fairness of VLM models. This framework modifies the loss function at training by identifying and adjusting faulty image-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing Sinkhorn distance to ensure the loss distributions of protected groups do not deviate from the total loss. Experimental testing of our framework shows up to a 8.6\% improvement when looking at equity-scaled AUC.",CS,AI_ML,0.85,Extracted from log - paper 1036
Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control,"Visual defect detection in industrial glass manufacturing remains a critical challenge due to the low frequency of defective products, leading to imbalanced datasets that limit the performance of deep learning models and computer vision systems. This paper presents a novel approach using Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic defective glass product images for data augmentation, effectively addressing class imbalance issues in manufacturing quality control and automated visual inspection. The methodology significantly enhances image classification performance of standard CNN architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting anomalies by increasing the minority class representation. Experimental results demonstrate substantial improvements in key machine learning metrics, particularly in recall for defective samples across all tested deep neural network architectures while maintaining perfect precision. The most dramatic improvement was observed in ResNet50V2's overall classification accuracy, which increased from 78 percent to 93 percent when trained with the augmented data. This work provides a scalable, cost-effective approach to enhancing automated defect detection in glass manufacturing that can potentially be extended to other industrial quality assurance systems and industries with similar class imbalance challenges.",CS,AI_ML,0.85,Extracted from log - paper 1037
STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis,"We propose a multimodal spatiotemporal graph neural network (STG) framework to predict colorectal cancer liver metastasis (CRLM) progression. Current clinical models do not effectively integrate the tumor's spatial heterogeneity, dynamic evolution, and complex multimodal data relationships, limiting their predictive accuracy. Our STG framework combines preoperative CT imaging and clinical data into a heterogeneous graph structure, enabling joint modeling of tumor distribution and temporal evolution through spatial topology and cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal neighborhood information and leverages supervised and contrastive learning strategies to enhance the model's ability to capture temporal features and improve robustness. A lightweight version of the model reduces parameter count by 78.55%, maintaining near-state-of-the-art performance. The model jointly optimizes recurrence risk regression and survival analysis tasks, with contrastive loss improving feature representational discriminability and cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a time-adjacent accuracy of 85% and a mean absolute error of 1.1005, significantly outperforming existing methods. The innovative heterogeneous graph construction and spatiotemporal decoupling mechanism effectively uncover the associations between dynamic tumor microenvironment changes and prognosis, providing reliable quantitative support for personalized treatment decisions.",CS,AI_ML,0.85,Extracted from log - paper 1038
TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion,"Video frame interpolation (VFI) that leverages the bio-inspired event cameras as guidance has recently shown better performance and memory efficiency than the frame-based methods, thanks to the event cameras' advantages, such as high temporal resolution. A hurdle for event-based VFI is how to effectively deal with non-linear motion, caused by the dynamic changes in motion direction and speed within the scene. Existing methods either use events to estimate sparse optical flow or fuse events with image features to estimate dense optical flow. Unfortunately, motion errors often degrade the VFI quality as the continuous motion cues from events do not align with the dense spatial information of images in the temporal dimension. In this paper, we find that object motion is continuous in space, tracking local regions over continuous time enables more accurate identification of spatiotemporal feature correlations. In light of this, we propose a novel continuous point tracking-based VFI framework, named TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation (SARS) module to divide the scene into similar patches. Then, a Continuous Trajectory guided Motion Estimation (CTME) module is proposed to track the continuous motion trajectory of each patch through events. Finally, intermediate frames at any given time are generated through global motion optimization and frame refinement. Moreover, we collect a real-world dataset that features fast non-linear motion. Extensive experiments show that our method outperforms prior arts in both motion estimation and frame interpolation quality.",CS,AI_ML,0.85,Extracted from log - paper 1039
Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation,"Accurate MRI-to-CT translation promises the integration of complementary imaging information without the need for additional imaging sessions. Given the practical challenges associated with acquiring paired MRI and CT scans, the development of robust methods capable of leveraging unpaired datasets is essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT translation methods, which predominantly rely on cycle consistency and contrastive learning frameworks, frequently encounter challenges in accurately translating anatomical features that are highly discernible on CT but less distinguishable on MRI, such as bone structures. This limitation renders these approaches less suitable for applications in radiation therapy, where precise bone representation is essential for accurate treatment planning. To address this challenge, we propose a path- and bone-contour regularized approach for unpaired MRI-to-CT translation. In our method, MRI and CT images are projected to a shared latent space, where the MRI-to-CT mapping is modeled as a continuous flow governed by neural ordinary differential equations. The optimal mapping is obtained by minimizing the transition path length of the flow. To enhance the accuracy of translated bone structures, we introduce a trainable neural network to generate bone contours from MRI and implement mechanisms to directly and indirectly encourage the model to focus on bone contours and their adjacent regions. Evaluations conducted on three datasets demonstrate that our method outperforms existing unpaired MRI-to-CT translation approaches, achieving lower overall error rates. Moreover, in a downstream bone segmentation task, our approach exhibits superior performance in preserving the fidelity of bone structures. Our code is available at: https://github.com/kennysyp/PaBoT.",CS,AI_ML,0.85,Extracted from log - paper 1040
Image Recognition with Online Lightweight Vision Transformer: A Survey,"The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: https://github.com/ajxklo/Lightweight-VIT",CS,AI_ML,0.85,Extracted from log - paper 1041
Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability,"The diffusion models, in early stages focus on constructing basic image structures, while the refined details, including local features and textures, are generated in later stages. Thus the same network layers are forced to learn both structural and textural information simultaneously, significantly differing from the traditional deep learning architectures (e.g., ResNet or GANs) which captures or generates the image semantic information at different layers. This difference inspires us to explore the time-wise diffusion models. We initially investigate the key contributions of the U-Net parameters to the denoising process and identify that properly zeroing out certain parameters (including large parameters) contributes to denoising, substantially improving the generation quality on the fly. Capitalizing on this discovery, we propose a simple yet effective method-termed ``MaskUNet''- that enhances generation quality with negligible parameter numbers. Our method fully leverages timestep- and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer two fine-tuning strategies: a training-based approach and a training-free approach, including tailored networks and optimization functions. In zero-shot inference on the COCO dataset, MaskUNet achieves the best FID score and further demonstrates its effectiveness in downstream task evaluations. Project page: https://gudaochangsheng.github.io/MaskUnet-Page/",CS,AI_ML,0.85,Extracted from log - paper 1042
Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera,"Forest inventories rely on accurate measurements of the diameter at breast height (DBH) for ecological monitoring, resource management, and carbon accounting. While LiDAR-based techniques can achieve centimeter-level precision, they are cost-prohibitive and operationally complex. We present a low-cost alternative that only needs a consumer-grade 360 video camera. Our semi-automated pipeline comprises of (i) a dense point cloud reconstruction using Structure from Motion (SfM) photogrammetry software called Agisoft Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based technique to estimate cross section shape and DBH. We introduce an interactive visualization tool for inspecting segmented trees and their estimated DBH. On 61 acquisitions of 43 trees under a variety of conditions, our method attains median absolute relative errors of 5-9% with respect to ""ground-truth"" manual measurements. This is only 2-4% higher than LiDAR-based estimates, while employing a single 360 camera that costs orders of magnitude less, requires minimal setup, and is widely available.",CS,AI_ML,0.85,Extracted from log - paper 1043
Sim2Real Transfer for Vision-Based Grasp Verification,"The verification of successful grasps is a crucial aspect of robot manipulation, particularly when handling deformable objects. Traditional methods relying on force and tactile sensors often struggle with deformable and non-rigid objects. In this work, we present a vision-based approach for grasp verification to determine whether the robotic gripper has successfully grasped an object. Our method employs a two-stage architecture; first YOLO-based object detection model to detect and locate the robot's gripper and then a ResNet-based classifier determines the presence of an object. To address the limitations of real-world data capture, we introduce HSR-GraspSynth, a synthetic dataset designed to simulate diverse grasping scenarios. Furthermore, we explore the use of Visual Question Answering capabilities as a zero-shot baseline to which we compare our model. Experimental results demonstrate that our approach achieves high accuracy in real-world environments, with potential for integration into grasping pipelines. Code and datasets are publicly available at https://github.com/pauamargant/HSR-GraspSynth .",CS,AI_ML,0.85,Extracted from log - paper 1044
An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices,"Continuous monitoring of behavior and physiology via wearable devices offers a novel, objective method for the early detection of worsening depression and anxiety. In this study, we present an explainable anomaly detection framework that identifies clinically meaningful increases in symptom severity using consumer-grade wearable data. Leveraging data from 2,023 participants with defined healthy baselines, our LSTM autoencoder model learned normal health patterns of sleep duration, step count, and resting heart rate. Anomalies were flagged when self-reported depression or anxiety scores increased by >=5 points (a threshold considered clinically significant). The model achieved an adjusted F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393 symptom-worsening episodes across 341 participants, with higher performance observed for episodes involving concurrent depression and anxiety escalation (F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 = 0.85). Model interpretability was supported by SHAP-based analysis, which identified resting heart rate as the most influential feature in 71.4 percentage of detected anomalies, followed by physical activity and sleep. Together, our findings highlight the potential of explainable anomaly detection to enable personalized, scalable, and proactive mental health monitoring in real-world settings.",CS,AI_ML,0.85,Extracted from log - paper 1045
Dual Prompting for Diverse Count-level PET Denoising,"The to-be-denoised positron emission tomography (PET) volumes are inherent with diverse count levels, which imposes challenges for a unified model to tackle varied cases. In this work, we resort to the recently flourished prompt learning to achieve generalizable PET denoising with different count levels. Specifically, we propose dual prompts to guide the PET denoising in a divide-and-conquer manner, i.e., an explicitly count-level prompt to provide the specific prior information and an implicitly general denoising prompt to encode the essential PET denoising knowledge. Then, a novel prompt fusion module is developed to unify the heterogeneous prompts, followed by a prompt-feature interaction module to inject prompts into the features. The prompts are able to dynamically guide the noise-conditioned denoising process. Therefore, we are able to efficiently train a unified denoising model for various count levels, and deploy it to different cases with personalized prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly selected 13-22\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies. It shows our dual prompting can largely improve the performance with informed count-level and outperform the count-conditional model.",CS,AI_ML,0.85,Extracted from log - paper 1046
GIF: Generative Inspiration for Face Recognition at Scale,"Aiming to reduce the computational cost of Softmax in massive label space of Face Recognition (FR) benchmarks, recent studies estimate the output using a subset of identities. Although promising, the association between the computation cost and the number of identities in the dataset remains linear only with a reduced ratio. A shared characteristic among available FR methods is the employment of atomic scalar labels during training. Consequently, the input to label matching is through a dot product between the feature vector of the input and the Softmax centroids. Inspired by generative modeling, we present a simple yet effective method that substitutes scalar labels with structured identity code, i.e., a sequence of integers. Specifically, we propose a tokenization scheme that transforms atomic scalar labels into structured identity codes. Then, we train an FR backbone to predict the code for each input instead of its scalar label. As a result, the associated computational cost becomes logarithmic w.r.t. number of identities. We demonstrate the benefits of the proposed method by conducting experiments. In particular, our method outperforms its competitors by 1.52%, and 0.6% at TAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the association between computational cost and the number of identities from linear to logarithmic. See code at https://github.com/msed-Ebrahimi/GIF",CS,AI_ML,0.85,Extracted from log - paper 1047
NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results,"This paper presents an overview of the NTIRE 2025 Challenge on UGC Video Enhancement. The challenge constructed a set of 150 user-generated content videos without reference ground truth, which suffer from real-world degradations such as noise, blur, faded colors, compression artifacts, etc. The goal of the participants was to develop an algorithm capable of improving the visual quality of such videos. Given the widespread use of UGC on short-form video platforms, this task holds substantial practical importance. The evaluation was based on subjective quality assessment in crowdsourcing, obtaining votes from over 8000 assessors. The challenge attracted more than 25 teams submitting solutions, 7 of which passed the final phase with source code verification. The outcomes may provide insights into the state-of-the-art in UGC video enhancement and highlight emerging trends and effective strategies in this evolving research area. All data, including the processed videos and subjective comparison votes and scores, is made publicly available at https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.",CS,AI_ML,0.85,Extracted from log - paper 1048
Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking,"Spatial Transcriptomics is a groundbreaking technology that integrates histology images with spatially resolved gene expression profiles. Among the various Spatial Transcriptomics techniques available, Visium has emerged as the most widely adopted. However, its accessibility is limited by high costs, the need for specialized expertise, and slow clinical integration. Additionally, gene capture inefficiencies lead to significant dropout, corrupting acquired data. To address these challenges, the deep learning community has explored the gene expression prediction task directly from histology images. Yet, inconsistencies in datasets, preprocessing, and training protocols hinder fair comparisons between models. To bridge this gap, we introduce SpaRED, a systematically curated database comprising 26 public datasets, providing a standardized resource for model evaluation. We further propose SpaCKLE, a state-of-the-art transformer-based gene expression completion model that reduces mean squared error by over 82.5% compared to existing approaches. Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE substantially improves the results across all the gene expression prediction models. Altogether, our contributions constitute the most comprehensive benchmark of gene expression prediction from histology images to date and a stepping stone for future research on Spatial Transcriptomics.",CS,AI_ML,0.85,Extracted from log - paper 1049
Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation,"Adversarial attacks have been fairly explored for computer vision and vision-language models. However, the avenue of adversarial attack for the vision language segmentation models (VLSMs) is still under-explored, especially for medical image analysis.   Thus, we have investigated the robustness of VLSMs against adversarial attacks for 2D medical images with different modalities with radiology, photography, and endoscopy. The main idea of this project was to assess the robustness of the fine-tuned VLSMs specially in the medical domain setting to address the high risk scenario.   First, we have fine-tuned pre-trained VLSMs for medical image segmentation with adapters.   Then, we have employed adversarial attacks -- projected gradient descent (PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to determine its robustness against adversaries.   We have reported models' performance decline to analyze the adversaries' impact.   The results exhibit significant drops in the DSC and IoU scores after the introduction of these adversaries. Furthermore, we also explored universal perturbation but were not able to find for the medical images.   \footnote{https://github.com/anjilab/secure-private-ai}",CS,AI_ML,0.85,Extracted from log - paper 1050
Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images,"Neural compression methods are gaining popularity due to their superior rate-distortion performance over traditional methods, even at extremely low bitrates below 0.1 bpp. As deep learning architectures, these models are prone to bias during the training process, potentially leading to unfair outcomes for individuals in different groups. In this paper, we present a general, structured, scalable framework for evaluating bias in neural image compression models. Using this framework, we investigate racial bias in neural compression algorithms by analyzing nine popular models and their variants. Through this investigation, we first demonstrate that traditional distortion metrics are ineffective in capturing bias in neural compression models. Next, we highlight that racial bias is present in all neural compression models and can be captured by examining facial phenotype degradation in image reconstructions. We then examine the relationship between bias and realism in the decoded images and demonstrate a trade-off across models. Finally, we show that utilizing a racially balanced training set can reduce bias but is not a sufficient bias mitigation strategy. We additionally show the bias can be attributed to compression model bias and classification model bias. We believe that this work is a first step towards evaluating and eliminating bias in neural image compression models.",CS,AI_ML,0.85,Extracted from log - paper 1051
Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation,"Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research.",CS,AI_ML,0.85,Extracted from log - paper 1052
R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning,"Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.",CS,AI_ML,0.85,Extracted from log - paper 1053
WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch,"LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\%, surpassing the performance of the best proprietary model.",CS,AI_ML,0.85,Extracted from log - paper 1054
NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation,"We present our system submission for SemEval 2025 Task 5, which focuses on cross-lingual subject classification in the English and German academic domains. Our approach leverages bilingual data during training, employing negative sampling and a margin-based retrieval objective. We demonstrate that a dimension-as-token self-attention mechanism designed with significantly reduced internal dimensions can effectively encode sentence embeddings for subject retrieval. In quantitative evaluation, our system achieved an average recall rate of 32.24% in the general quantitative setting (all subjects), 43.16% and 31.53% of the general qualitative evaluation methods with minimal GPU usage, highlighting their competitive performance. Our results demonstrate that our approach is effective in capturing relevant subject information under resource constraints, although there is still room for improvement.",CS,AI_ML,0.85,Extracted from log - paper 1055
IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages,"The rapid progress in question-answering (QA) systems has predominantly benefited high-resource languages, leaving Indic languages largely underrepresented despite their vast native speaker base. In this paper, we present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset covering nine major Indic languages, systematically derived from the SQuAD dataset. Building on previous work with MahaSQuAD for Marathi, our approach adapts and extends translation techniques to maintain high linguistic fidelity and accurate answer-span alignment across diverse languages. IndicSQuAD comprises extensive training, validation, and test sets for each language, providing a robust foundation for model development. We evaluate baseline performances using language-specific monolingual BERT models and the multilingual MuRIL-BERT. The results indicate some challenges inherent in low-resource settings. Moreover, our experiments suggest potential directions for future work, including expanding to additional languages, developing domain-specific datasets, and incorporating multimodal data. The dataset and models are publicly shared at https://github.com/l3cube-pune/indic-nlp",CS,AI_ML,0.85,Extracted from log - paper 1056
Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure,"We explore the potential of ChatGPT (3.5-turbo and 4) to generate conversations focused on self-care strategies for African-American heart failure patients -- a domain with limited specialized datasets. To simulate patient-health educator dialogues, we employed four prompting strategies: domain, African American Vernacular English (AAVE), Social Determinants of Health (SDOH), and SDOH-informed reasoning. Conversations were generated across key self-care domains of food, exercise, and fluid intake, with varying turn lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as age, gender, neighborhood, and socioeconomic status. Our findings show that effective prompt design is essential. While incorporating SDOH and reasoning improves dialogue quality, ChatGPT still lacks the empathy and engagement needed for meaningful healthcare communication.",CS,AI_ML,0.85,Extracted from log - paper 1057
Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval,"Current sparse neural information retrieval (IR) methods, and to a lesser extent more traditional models such as BM25, do not take into account the document collection and the complex interplay between different term weights when representing a single document. In this paper, we show how the Rational Speech Acts (RSA), a linguistics framework used to minimize the number of features to be communicated when identifying an object in a set, can be adapted to the IR case -- and in particular to the high number of potential features (here, tokens). RSA dynamically modulates token-document interactions by considering the influence of other documents in the dataset, better contrasting document representations. Experiments show that incorporating RSA consistently improves multiple sparse retrieval models and achieves state-of-the-art performance on out-of-domain datasets from the BEIR benchmark. https://github.com/arthur-75/Rational-Retrieval-Acts",CS,AI_ML,0.85,Extracted from log - paper 1058
Say It Another Way: A Framework for User-Grounded Paraphrasing,"Small changes in how a prompt is worded can lead to meaningful differences in the behavior of large language models (LLMs), raising concerns about the stability and reliability of their evaluations. While prior work has explored simple formatting changes, these rarely capture the kinds of natural variation seen in real-world language use. We propose a controlled paraphrasing framework based on a taxonomy of minimal linguistic transformations to systematically generate natural prompt variations. Using the BBQ dataset, we validate our method with both human annotations and automated checks, then use it to study how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our analysis shows that even subtle prompt modifications can lead to substantial changes in model behavior. These results highlight the need for robust, paraphrase-aware evaluation protocols.",CS,AI_ML,0.85,Extracted from log - paper 1059
Faster MoE LLM Inference for Extremely Large Models,"Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually becoming the mainstream approach for ultra-large-scale models. Existing optimization efforts for MoE models have focused primarily on coarse-grained MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE models are gaining popularity, yet research on them remains limited. Therefore, we want to discuss the efficiency dynamic under different service loads. Additionally, fine-grained models allow deployers to reduce the number of routed experts, both activated counts and total counts, raising the question of how this reduction affects the trade-off between MoE efficiency and performance. Our findings indicate that while deploying MoE models presents greater challenges, it also offers significant optimization opportunities. Reducing the number of activated experts can lead to substantial efficiency improvements in certain scenarios, with only minor performance degradation. Reducing the total number of experts provides limited efficiency gains but results in severe performance degradation. Our method can increase throughput by at least 10\% without any performance degradation. Overall, we conclude that MoE inference optimization remains an area with substantial potential for exploration and improvement.",CS,AI_ML,0.85,Extracted from log - paper 1060
BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models,"In this paper, we present a new form of backdoor attack against Large Language Models (LLMs): lingual-backdoor attacks. The key novelty of lingual-backdoor attacks is that the language itself serves as the trigger to hijack the infected LLMs to generate inflammatory speech. They enable the precise targeting of a specific language-speaking group, exacerbating racial discrimination by malicious entities. We first implement a baseline lingual-backdoor attack, which is carried out by poisoning a set of training data for specific downstream tasks through translation into the trigger language. However, this baseline attack suffers from poor task generalization and is impractical in real-world settings. To address this challenge, we design BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any downstream tasks within the chat LLMs, regardless of the specific questions of these tasks. We design a new approach using PPL-constrained Greedy Coordinate Gradient-based Search (PGCG) based adversarial training to expand the decision boundary of lingual-backdoor, thereby enhancing the generalization ability of lingual-backdoor across various tasks. We perform extensive experiments to validate the effectiveness of our proposed attacks. Specifically, the baseline attack achieves an ASR of over 90% on the specified tasks. However, its ASR reaches only 37.61% across six tasks in the task-agnostic scenario. In contrast, BadLingual brings up to 37.35% improvement over the baseline. Our study sheds light on a new perspective of vulnerabilities in LLMs with multilingual capabilities and is expected to promote future research on the potential defenses to enhance the LLMs' robustness",CS,AI_ML,0.85,Extracted from log - paper 1061
Sentence Embeddings as an intermediate target in end-to-end summarisation,"Current neural network-based methods to the problem of document summarisation struggle when applied to datasets containing large inputs. In this paper we propose a new approach to the challenge of content-selection when dealing with end-to-end summarisation of user reviews of accommodations. We show that by combining an extractive approach with externally pre-trained sentence level embeddings in an addition to an abstractive summarisation model we can outperform existing methods when this is applied to the task of summarising a large input dataset. We also prove that predicting sentence level embedding of a summary increases the quality of an end-to-end system for loosely aligned source to target corpora, than compared to commonly predicting probability distributions of sentence selection.",CS,AI_ML,0.85,Extracted from log - paper 1062
Evaluation of LLMs on Long-tail Entity Linking in Historical Documents,"Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP) applications, enabling the disambiguation of entity mentions by linking them to their corresponding entries in a reference knowledge base (KB). Thanks to their deep contextual understanding capabilities, LLMs offer a new perspective to tackle EL, promising better results than traditional methods. Despite the impressive generalization capabilities of LLMs, linking less popular, long-tail entities remains challenging as these entities are often underrepresented in training data and knowledge bases. Furthermore, the long-tail EL task is an understudied problem, and limited studies address it with LLMs. In the present work, we assess the performance of two popular LLMs, GPT and LLama3, in a long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated benchmark of sentences from domain-specific historical texts, we quantitatively compare the performance of LLMs in identifying and linking entities to their corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity Linking and Relation Extraction framework. Our preliminary experiments reveal that LLMs perform encouragingly well in long-tail EL, indicating that this technology can be a valuable adjunct in filling the gap between head and long-tail EL.",CS,AI_ML,0.85,Extracted from log - paper 1063
Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models,"Recent advances in large language models have demonstrated that Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning capabilities to non-reasoning models. However, models fine-tuned with this approach inherit the ""overthinking"" problem from teacher models, producing verbose and redundant reasoning chains during inference. To address this challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought \textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning (\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their short counterparts obtained through structure-preserved rewriting. Our experiments demonstrate that models trained using the LS-Mixture SFT method, compared to those trained with direct SFT, achieved an average accuracy improvement of 2.3\% across various benchmarks while substantially reducing model response length by approximately 47.61\%. This work offers an approach to endow non-reasoning models with reasoning capabilities through supervised fine-tuning while avoiding the inherent overthinking problems inherited from teacher models, thereby enabling efficient reasoning in the fine-tuned models.",CS,AI_ML,0.85,Extracted from log - paper 1064
Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis,"Explainable disease diagnosis, which leverages patient information (e.g., signs and symptoms) and computational models to generate probable diagnoses and reasonings, offers clear clinical values. However, when clinical notes encompass insufficient evidence for a definite diagnosis, such as the absence of definitive symptoms, diagnostic uncertainty usually arises, increasing the risk of misdiagnosis and adverse outcomes. Although explicitly identifying and explaining diagnostic uncertainties is essential for trustworthy diagnostic systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an uncertainty-aware large language model (LLM) created by fine-tuning open-source LLMs with diagnostic criteria. We formalized the task and assembled richly annotated datasets that capture varying degrees of diagnostic ambiguity. Evaluating ConfiDx on real-world datasets demonstrated that it excelled in identifying diagnostic uncertainties, achieving superior diagnostic performance, and generating trustworthy explanations for diagnoses and uncertainties. To our knowledge, this is the first study to jointly address diagnostic uncertainty recognition and explanation, substantially enhancing the reliability of automatic diagnostic systems.",CS,AI_ML,0.85,Extracted from log - paper 1065
Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation,"Mamba's theoretical infinite-context potential is limited in practice when sequences far exceed training lengths. This work explores unlocking Mamba's long-context memory ability by a simple-yet-effective method, Recall with Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a teacher model. Specifically, RwR prepends these summarization as CoT prompts during fine-tuning, teaching Mamba to actively recall and reason over long contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's long-context performance against comparable Transformer/hybrid baselines under similar pretraining conditions, while preserving short-context capabilities, all without architectural changes.",CS,AI_ML,0.85,Extracted from log - paper 1066
Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback,"Large language models (LLMs) have shown promise in providing scalable mental health support, while evaluating their counseling capability remains crucial to ensure both efficacy and safety. Existing evaluations are limited by the static assessment that focuses on knowledge tests, the single perspective that centers on user experience, and the open-loop framework that lacks actionable feedback. To address these issues, we propose {\Psi}-Arena, an interactive framework for comprehensive assessment and optimization of LLM-based counselors, featuring three key characteristics: (1) Realistic arena interactions that simulate real-world counseling through multi-stage dialogues with psychologically profiled NPC clients, (2) Tripartite evaluation that integrates assessments from the client, counselor, and supervisor perspectives, and (3) Closed-loop optimization that iteratively improves LLM counselors using diagnostic feedback. Experiments across eight state-of-the-art LLMs show significant performance variations in different real-world scenarios and evaluation perspectives. Moreover, reflection-based optimization results in up to a 141% improvement in counseling performance. We hope PsychoArena provides a foundational resource for advancing reliable and human-aligned LLM applications in mental healthcare.",CS,AI_ML,0.85,Extracted from log - paper 1067
SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation,"While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments.",CS,AI_ML,0.85,Extracted from log - paper 1068
"Survey of Abstract Meaning Representation: Then, Now, Future","This paper presents a survey of Abstract Meaning Representation (AMR), a semantic representation framework that captures the meaning of sentences through a graph-based structure. AMR represents sentences as rooted, directed acyclic graphs, where nodes correspond to concepts and edges denote relationships, effectively encoding the meaning of complex sentences. This survey investigates AMR and its extensions, focusing on AMR capabilities. It then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by showing traditional, current, and possible futures approaches. It also reviews various applications of AMR including text generation, text classification, and information extraction and information seeking. By analyzing recent developments and challenges in the field, this survey provides insights into future directions for research and the potential impact of AMR on enhancing machine understanding of human language.",CS,AI_ML,0.85,Extracted from log - paper 1069
Improving Model Alignment Through Collective Intelligence of Open-Source LLMS,"Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback, which necessitates high-quality human-labeled data. Constructing such datasets is often expensive and hard to scale, and may face potential limitations on diversity and generalization. To address these challenges, we introduce Mixture of Agents Alignment (MoAA), that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning and preference optimization, leading to improved performance compared to using a single model alone to generate alignment data (e.g. using GPT-4o alone). Evaluation results show that our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising direction for model alignment through this new scalable and diverse synthetic data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement pipeline, where models finetuned on MoA-generated data surpass their own initial capabilities, providing evidence that our approach can push the frontier of open-source LLMs without reliance on stronger external supervision. Data and code will be released.",CS,AI_ML,0.85,Extracted from log - paper 1070
Teaching Models to Understand (but not Generate) High-risk Data,"Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.",CS,AI_ML,0.85,Extracted from log - paper 1071
Radio: Rate-Distortion Optimization for Large Language Model Compression,"In recent years, the compression of large language models (LLMs) has emerged as a key problem in facilitating LLM deployment on resource-limited devices, reducing compute costs, and mitigating the environmental footprint due to large-scale AI infrastructure. Here, we establish the foundations of LLM quantization from a rate-distortion theory perspective and propose a quantization technique based on simple rate-distortion optimization. Our technique scales to models containing hundreds of billions of weight parameters and offers users the flexibility to compress models, post-training, to a model size or accuracy specified by the user.",CS,AI_ML,0.85,Extracted from log - paper 1072
"UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output","Hallucinations pose a significant challenge for large language models when answering knowledge-intensive queries. As LLMs become more widely adopted, it is crucial not only to detect if hallucinations occur but also to pinpoint exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM: Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes, is a recent effort in this direction. This paper describes the UCSC system submission to the shared Mu-SHROOM task. We introduce a framework that first retrieves relevant context, next identifies false content from the answer, and finally maps them back to spans in the LLM output. The process is further enhanced by automatically optimizing prompts. Our system achieves the highest overall performance, ranking #1 in average position across all languages. We release our code and experiment results.",CS,AI_ML,0.85,Extracted from log - paper 1073
Logits-Constrained Framework with RoBERTa for Ancient Chinese NER,"This paper presents a Logits-Constrained (LC) framework for Ancient Chinese Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our two-stage model integrates GujiRoBERTa for contextual encoding and a differentiable decoding mechanism to enforce valid BMES label transitions. Experiments demonstrate that LC improves performance over traditional CRF and BiLSTM-based approaches, especially in high-label or large-data settings. We also propose a model selection criterion balancing label complexity and dataset size, providing practical guidance for real-world Ancient Chinese NLP tasks.",CS,AI_ML,0.85,Extracted from log - paper 1074
AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation,"Chest X-rays (CXRs) are the most frequently performed imaging examinations in clinical settings. Recent advancements in Large Multimodal Models (LMMs) have enabled automated CXR interpretation, enhancing diagnostic accuracy and efficiency. However, despite their strong visual understanding, current Medical LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level understanding and interaction, and (2) Limited accuracy and interpretability due to single-step reasoning. In this paper, we empower MLMMs with anatomy-centric reasoning capabilities to enhance their interactivity and explainability. Specifically, we first propose an Anatomical Ontology-Guided Reasoning (AOR) framework, which centers on cross-modal region-level information to facilitate multi-step reasoning. Next, under the guidance of expert physicians, we develop AOR-Instruction, a large instruction dataset for MLMMs training. Our experiments demonstrate AOR's superior performance in both VQA and report generation tasks.",CS,AI_ML,0.85,Extracted from log - paper 1075
ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations,"We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.",CS,AI_ML,0.85,Extracted from log - paper 1076
When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger,"We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal formal model showing that once an AI agent feeds its own outputs back as inputs and crosses an explicit information-integration threshold, its internal complexity will grow without bound under our assumptions. The framework unifies earlier ideas on self-prompting large language models, G\""odelian self-reference, and AutoML, yet remains implementation-agnostic. The model furthermore scales naturally to interacting swarms of agents, hinting at super-linear effects once communication among instances is permitted. For safety reasons, we omit system-specific implementation details and release only a brief, model-agnostic toy prototype in Appendix C.",CS,AI_ML,0.85,Extracted from log - paper 1077
"Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models","Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount.",CS,AI_ML,0.85,Extracted from log - paper 1078
Using Knowledge Graphs to harvest datasets for efficient CLIP model training,"Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.",CS,AI_ML,0.85,Extracted from log - paper 1079
Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play,"A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.",CS,AI_ML,0.85,Extracted from log - paper 1080
Predicting Movie Hits Before They Happen with LLMs,"Addressing the cold-start issue in content recommendation remains a critical ongoing challenge. In this work, we focus on tackling the cold-start problem for movies on a large entertainment platform. Our primary goal is to forecast the popularity of cold-start movies using Large Language Models (LLMs) leveraging movie metadata. This method could be integrated into retrieval systems within the personalization pipeline or could be adopted as a tool for editorial teams to ensure fair promotion of potentially overlooked movies that may be missed by traditional or algorithmic solutions. Our study validates the effectiveness of this approach compared to established baselines and those we developed.",CS,AI_ML,0.85,Extracted from log - paper 1081
fastabx: A library for efficient computation of ABX discriminability,"We introduce fastabx, a high-performance Python library for building ABX discrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at https://github.com/bootphon/fastabx.",CS,AI_ML,0.85,Extracted from log - paper 1082
Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models,"Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.",CS,AI_ML,0.85,Extracted from log - paper 1083
A Survey on Progress in LLM Alignment from the Perspective of Reward Design,"The alignment of large language models (LLMs) with human values and intentions represents a core challenge in current AI research, where reward mechanism design has become a critical factor in shaping model behavior. This study conducts a comprehensive investigation of reward mechanisms in LLM alignment through a systematic theoretical framework, categorizing their development into three key phases: (1) feedback (diagnosis), (2) reward design (prescription), and (3) optimization (treatment). Through a four-dimensional analysis encompassing construction basis, format, expression, and granularity, this research establishes a systematic classification framework that reveals evolutionary trends in reward modeling. The field of LLM alignment faces several persistent challenges, while recent advances in reward design are driving significant paradigm shifts. Notable developments include the transition from reinforcement learning-based frameworks to novel optimization paradigms, as well as enhanced capabilities to address complex alignment scenarios involving multimodal integration and concurrent task coordination. Finally, this survey outlines promising future research directions for LLM alignment through innovative reward design strategies.",CS,AI_ML,0.85,Extracted from log - paper 1084
Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset,"Proper names in Arabic Wikipedia are frequently undiacritized, creating ambiguity in pronunciation and interpretation, especially for transliterated named entities of foreign origin. While transliteration and diacritization have been well-studied separately in Arabic NLP,their intersection remains underexplored. In this paper, we introduce a new manually diacritized dataset of Arabic proper names of various origins with their English Wikipedia equivalent glosses, and present the challenges and guidelines we followed to create it. We benchmark GPT-4o on the task of recovering full diacritization given the undiacritized Arabic and English forms, and analyze its performance. Achieving 73% accuracy, our results underscore both the difficulty of the task and the need for improved models and resources. We release our dataset to facilitate further research on Arabic Wikipedia proper name diacritization.",CS,AI_ML,0.85,Extracted from log - paper 1085
Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning,"Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design.",CS,AI_ML,0.85,Extracted from log - paper 1086
LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis,"Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.",CS,AI_ML,0.85,Extracted from log - paper 1087
Automatic Proficiency Assessment in L2 English Learners,"Second language proficiency (L2) in English is usually perceptually evaluated by English teachers or expert evaluators, with the inherent intra- and inter-rater variability. This paper explores deep learning techniques for comprehensive L2 proficiency assessment, addressing both the speech signal and its correspondent transcription. We analyze spoken proficiency classification prediction using diverse architectures, including 2D CNN, frequency-based CNN, ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based proficiency assessment by fine-tuning a BERT language model within resource constraints. Finally, we tackle the complex task of spontaneous dialogue assessment, managing long-form audio and speaker interactions through separate applications of wav2vec 2.0 and BERT models. Results from experiments on EFCamDat and ANGLISH datasets and a private dataset highlight the potential of deep learning, especially the pretrained wav2vec 2.0 model, for robust automated L2 proficiency evaluation.",CS,AI_ML,0.85,Extracted from log - paper 1088
Ensemble Kalman filter for uncertainty in human language comprehension,"Artificial neural networks (ANNs) are widely used in modeling sentence processing but often exhibit deterministic behavior, contrasting with human sentence comprehension, which manages uncertainty during ambiguous or unexpected inputs. This is exemplified by reversal anomalies-sentences with unexpected role reversals that challenge syntax and semantics-highlighting the limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model. To address these limitations, we propose a Bayesian framework for sentence comprehension, applying an extension of the ensemble Kalman filter (EnKF) for Bayesian inference to quantify uncertainty. By framing language comprehension as a Bayesian inverse problem, this approach enhances the SG model's ability to reflect human sentence processing with respect to the representation of uncertainty. Numerical experiments and comparisons with maximum likelihood estimation (MLE) demonstrate that Bayesian methods improve uncertainty representation, enabling the model to better approximate human cognitive processing when dealing with linguistic ambiguities.",CS,AI_ML,0.85,Extracted from log - paper 1089
EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning,"Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.",CS,AI_ML,0.85,Extracted from log - paper 1090
Bielik v3 Small: Technical Report,"We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.",CS,AI_ML,0.85,Extracted from log - paper 1091
Bemba Speech Translation: Exploring a Low-Resource African Language,"This paper describes our system submission to the International Conference on Spoken Language Translation (IWSLT 2025), low-resource languages track, namely for Bemba-to-English speech translation. We built cascaded speech translation systems based on Whisper and NLLB-200, and employed data augmentation techniques, such as back-translation. We investigate the effect of using synthetic data and discuss our experimental setup.",CS,AI_ML,0.85,Extracted from log - paper 1092
Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda,"In this paper,we explore the application of Back translation (BT) as a semi-supervised technique to enhance Neural Machine Translation(NMT) models for the English-Luganda language pair, specifically addressing the challenges faced by low-resource languages. The purpose of our study is to demonstrate how BT can mitigate the scarcity of bilingual data by generating synthetic data from monolingual corpora. Our methodology involves developing custom NMT models using both publicly available and web-crawled data, and applying Iterative and Incremental Back translation techniques. We strategically select datasets for incremental back translation across multiple small datasets, which is a novel element of our approach. The results of our study show significant improvements, with translation performance for the English-Luganda pair exceeding previous benchmarks by more than 10 BLEU score units across all translation directions. Additionally, our evaluation incorporates comprehensive assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced understanding of translation quality. The conclusion drawn from our research confirms the efficacy of BT when strategically curated datasets are utilized, establishing new performance benchmarks and demonstrating the potential of BT in enhancing NMT models for low-resource languages.",CS,AI_ML,0.85,Extracted from log - paper 1093
Incentivizing Inclusive Contributions in Model Sharing Markets,"While data plays a crucial role in training contemporary AI models, it is acknowledged that valuable public data will be exhausted in a few years, directing the world's attention towards the massive decentralized private data. However, the privacy-sensitive nature of raw data and lack of incentive mechanism prevent these valuable data from being fully exploited. Addressing these challenges, this paper proposes inclusive and incentivized personalized federated learning (iPFL), which incentivizes data holders with diverse purposes to collaboratively train personalized models without revealing raw data. iPFL constructs a model-sharing market by solving a graph-based training optimization and incorporates an incentive mechanism based on game theory principles. Theoretical analysis shows that iPFL adheres to two key incentive properties: individual rationality and truthfulness. Empirical studies on eleven AI tasks (e.g., large language models' instruction-following tasks) demonstrate that iPFL consistently achieves the highest economic utility, and better or comparable model performance compared to baseline methods. We anticipate that our iPFL can serve as a valuable technique for boosting future AI models on decentralized private data while making everyone satisfied.",CS,AI_ML,0.85,Extracted from log - paper 1094
Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs,"One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work.",CS,AI_ML,0.85,Extracted from log - paper 1095
Bielik 11B v2 Technical Report,"We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.",CS,AI_ML,0.85,Extracted from log - paper 1096
Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL,"Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.",CS,AI_ML,0.85,Extracted from log - paper 1097
RM-R1: Reward Modeling as Reasoning,"Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.",CS,AI_ML,0.85,Extracted from log - paper 1098
JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings,"Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \textbf{J}oint \textbf{T}ensor representation modulus constraint and \textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence \textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.",CS,AI_ML,0.85,Extracted from log - paper 1099
SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning,"Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.",CS,AI_ML,0.85,Extracted from log - paper 1100
Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering,"The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.",CS,AI_ML,0.85,Extracted from log - paper 1101
Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques,"Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.",CS,AI_ML,0.85,Extracted from log - paper 1102
Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition,"Sign language recognition (SLR) faces fundamental challenges in creating accurate annotations due to the inherent complexity of simultaneous manual and non-manual signals. To the best of our knowledge, this is the first work to integrate generative large language models (LLMs) into SLR tasks. We propose a novel Generative Sign-description Prompts Multi-positive Contrastive learning (GSP-MC) method that leverages retrieval-augmented generation (RAG) with domain-specific LLMs, incorporating multi-step prompt engineering and expert-validated sign language corpora to produce precise multipart descriptions. The GSP-MC method also employs a dual-encoder architecture to bidirectionally align hierarchical skeleton features with multiple text descriptions (global, synonym, and part level) through probabilistic matching. Our approach combines global and part-level losses, optimizing KL divergence to ensure robust alignment across all relevant text-skeleton pairs while capturing both sign-level semantics and detailed part dynamics. Experiments demonstrate state-of-the-art performance against existing methods on the Chinese SLR500 (reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's cross-lingual effectiveness highlight its potential for developing inclusive communication technologies.",CS,AI_ML,0.85,Extracted from log - paper 1103
Demystifying optimized prompts in language models,"Modern language models (LMs) are not robust to out-of-distribution inputs. Machine generated (``optimized'') prompts can be used to modulate LM outputs and induce specific behaviors while appearing completely uninterpretable. In this work, we investigate the composition of optimized prompts, as well as the mechanisms by which LMs parse and build predictions from optimized prompts. We find that optimized prompts primarily consist of punctuation and noun tokens which are more rare in the training data. Internally, optimized prompts are clearly distinguishable from natural language counterparts based on sparse subsets of the model's activations. Across various families of instruction-tuned models, optimized prompts follow a similar path in how their representations form through the network.",CS,AI_ML,0.85,Extracted from log - paper 1104
Parameter-Efficient Transformer Embeddings,"Embedding layers in transformer-based NLP models typically account for the largest share of model parameters, scaling with vocabulary size but not yielding performance gains proportional to scale. We propose an alternative approach in which token embedding vectors are first generated deterministically, directly from the token IDs using a Fourier expansion of their normalized values, followed by a lightweight multilayer perceptron (MLP) that captures higher-order interactions. We train standard transformers and our architecture on natural language inference tasks (SNLI and MNLI), and evaluate zero-shot performance on sentence textual similarity (STS-B). Our results demonstrate that the proposed method achieves competitive performance using significantly fewer parameters, trains faster, and operates effectively without the need for dropout. This proof-of-concept study highlights the potential for scalable, memory-efficient language models and motivates further large-scale experimentation based on our findings.",CS,AI_ML,0.85,Extracted from log - paper 1105
Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models,"Commercial Large Language Models (LLMs) have recently incorporated memory features to deliver personalised responses. This memory retains details such as user demographics and individual characteristics, allowing LLMs to adjust their behaviour based on personal information. However, the impact of integrating personalised information into the context has not been thoroughly assessed, leading to questions about its influence on LLM behaviour. Personalisation can be challenging, particularly with sensitive topics. In this paper, we examine various state-of-the-art LLMs to understand their behaviour in different personalisation scenarios, specifically focusing on hate speech. We prompt the models to assume country-specific personas and use different languages for hate speech detection. Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area. To mitigate these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate speech classifications made with and without country or language-specific context. The refined models demonstrate improved performance in both personalised contexts and when no context is provided.",CS,AI_ML,0.85,Extracted from log - paper 1106
SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation,"Evaluating text summarization quality remains a critical challenge in Natural Language Processing. Current approaches face a trade-off between performance and interpretability. We present SEval-Ex, a framework that bridges this gap by decomposing summarization evaluation into atomic statements, enabling both high performance and explainability. SEval-Ex employs a two-stage pipeline: first extracting atomic statements from text source and summary using LLM, then a matching between generated statements. Unlike existing approaches that provide only summary-level scores, our method generates detailed evidence for its decisions through statement-level alignments. Experiments on the SummEval benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with 0.580 correlation on consistency with human consistency judgments, surpassing GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our framework shows robustness against hallucination.",CS,AI_ML,0.85,Extracted from log - paper 1107
Interpretable Emergent Language Using Inter-Agent Transformers,"This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.",CS,AI_ML,0.85,Extracted from log - paper 1108
DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units,"Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.",CS,AI_ML,0.85,Extracted from log - paper 1109
Exploring new Approaches for Information Retrieval through Natural Language Processing,"This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.",CS,AI_ML,0.85,Extracted from log - paper 1110
Measuring Hong Kong Massive Multi-Task Language Understanding,"Multilingual understanding is crucial for the cross-cultural applicability of Large Language Models (LLMs). However, evaluation benchmarks designed for Hong Kong's unique linguistic landscape, which combines Traditional Chinese script with Cantonese as the spoken form and its cultural context, remain underdeveloped. To address this gap, we introduce HKMMLU, a multi-task language understanding benchmark that evaluates Hong Kong's linguistic competence and socio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions across 66 subjects, organized into four categories: Science, Technology, Engineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To evaluate the multilingual understanding ability of LLMs, 90,550 Mandarin-Cantonese translation tasks were additionally included. We conduct comprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs of varying sizes on HKMMLU. The results show that the best-performing model, DeepSeek-V3, struggles to achieve an accuracy of 75\%, significantly lower than that of MMLU and CMMLU. This performance gap highlights the need to improve LLMs' capabilities in Hong Kong-specific language and knowledge domains. Furthermore, we investigate how question language, model size, prompting strategies, and question and reasoning token lengths affect model performance. We anticipate that HKMMLU will significantly advance the development of LLMs in multilingual and cross-cultural contexts, thereby enabling broader and more impactful applications.",CS,AI_ML,0.85,Extracted from log - paper 1111
"Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization","As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate ``scaling effects'' - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks.",CS,AI_ML,0.85,Extracted from log - paper 1112
A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking,"Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG) by determining how source materials are segmented before indexing. Despite evidence that Large Language Models (LLMs) are sensitive to the layout and structure of retrieved data, there is currently no framework to analyze the impact of different chunking methods. In this paper, we introduce a novel methodology that defines essential characteristics of the chunking process at three levels: intrinsic passage properties, extrinsic passage properties, and passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a domain-agnostic, automatic evaluation metric that quantifies and aggregates these characteristics. Our empirical evaluations across seven domains demonstrate that the HOPE metric correlates significantly (p > 0.13) with various RAG performance indicators, revealing contrasts between the importance of extrinsic and intrinsic properties of passages. Semantic independence between passages proves essential for system performance with a performance gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On the contrary, traditional assumptions about maintaining concept unity within passages show minimal impact. These findings provide actionable insights for optimizing chunking strategies, thus improving RAG system design to produce more factually correct responses.",CS,AI_ML,0.85,Extracted from log - paper 1113
Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use,"This paper presents a domain-specific implementation of Retrieval-Augmented Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law. Motivated by the increasing prevalence of DMCA takedowns and the lack of accessible legal support for content creators, we propose a structured approach that combines semantic search with legal knowledge graphs and court citation networks to improve retrieval quality and reasoning reliability. Our prototype models legal precedents at the statutory factor level (e.g., purpose, nature, amount, market effect) and incorporates citation-weighted graph representations to prioritize doctrinally authoritative sources. We use Chain-of-Thought reasoning and interleaved retrieval steps to better emulate legal reasoning. Preliminary testing suggests this method improves doctrinal relevance in the retrieval process, laying groundwork for future evaluation and deployment of LLM-based legal assistance tools.",CS,AI_ML,0.85,Extracted from log - paper 1114
Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents,"Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach.",CS,AI_ML,0.85,Extracted from log - paper 1115
QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach,"Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering ""Write Once, Run Anywhere"" of tensor programs an open question.   We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically translating tensor programs across DLS via both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via pre-defined meta-prompts for program transformation. During each program transformation, efficient symbolic program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high performance, we propose a hierarchical auto-tuning approach to systematically explore both the parameters and sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average, and the performance of translated programs achieves up to 2.0x over vendor-provided manually-optimized libraries. As a result, the programming productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor programs.",CS,AI_ML,0.85,Extracted from log - paper 1116
Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study,"Despite significant advances in long-context reasoning by large language models (LLMs), primarily through Online Reinforcement Learning (RL) methods, these approaches incur substantial computational costs and complexity. In contrast, simpler and more economical Offline RL methods remain underexplored. To address this gap, we investigate the effectiveness of Offline RL methods, specifically Direct Preference Optimization (DPO) and its length-desensitized variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive experiments across multiple reasoning benchmarks demonstrate that these simpler Offline RL methods substantially improve model performance, achieving an average enhancement of 3.3\%, with a particularly notable increase of 10.1\% on the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity to output length, emphasizing that increasing reasoning length should align with semantic richness, as indiscriminate lengthening may adversely affect model performance. We provide comprehensive descriptions of our data processing and training methodologies, offering empirical evidence and practical insights for developing more cost-effective Offline RL approaches.",CS,AI_ML,0.85,Extracted from log - paper 1117
Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data,"Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}",CS,AI_ML,0.85,Extracted from log - paper 1118
Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading,"When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.",CS,AI_ML,0.85,Extracted from log - paper 1119
LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications,"Solving non-convex resource allocation problems poses significant challenges in wireless communication systems, often beyond the capability of traditional optimization techniques. To address this issue, we propose LLM-OptiRA, the first framework that leverages large language models (LLMs) to automatically detect and transform non-convex components into solvable forms, enabling fully automated resolution of non-convex resource allocation problems in wireless communication systems. LLM-OptiRA not only simplifies problem-solving by reducing reliance on expert knowledge, but also integrates error correction and feasibility validation mechanisms to ensure robustness. Experimental results show that LLM-OptiRA achieves an execution rate of 96% and a success rate of 80% on GPT-4, significantly outperforming baseline approaches in complex optimization tasks across diverse scenarios.",CS,AI_ML,0.85,Extracted from log - paper 1120
LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning,"Evaluating the quality of slide-based multimedia instruction is challenging. Existing methods like manual assessment, reference-based metrics, and large language model evaluators face limitations in scalability, context capture, or bias. In this paper, we introduce LecEval, an automated metric grounded in Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal knowledge acquisition in slide-based learning. LecEval assesses effectiveness using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset of over 2,000 slides from more than 50 online course videos, annotated with fine-grained human ratings across these rubrics. A model trained on this dataset demonstrates superior accuracy and adaptability compared to existing metrics, bridging the gap between automated and human assessments. We release our dataset and toolkits at https://github.com/JoylimJY/LecEval.",CS,AI_ML,0.85,Extracted from log - paper 1121
What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction,"The notion of language modeling has gradually shifted in recent years from a distribution over finite-length strings to general-purpose prediction models for textual inputs and outputs, following appropriate alignment phases. This paper analyzes the distinction between distribution estimation and response prediction in the context of LLMs, and their often conflicting goals. We examine the training phases of LLMs, which include pretraining, in-context learning, and preference tuning, and also the common use cases for their output probabilities, which include completion probabilities and explicit probabilities as output. We argue that the different settings lead to three distinct intended output distributions. We demonstrate that NLP works often assume that these distributions should be similar, which leads to misinterpretations of their experimental findings. Our work sets firmer formal foundations for the interpretation of LLMs, which will inform ongoing work on the interpretation and use of LLMs' induced distributions.",CS,AI_ML,0.85,Extracted from log - paper 1122
An overview of artificial intelligence in computer-assisted language learning,"Computer-assisted language learning -- CALL -- is an established research field. We review how artificial intelligence can be applied to support language learning and teaching. The need for intelligent agents that assist language learners and teachers is increasing: the human teacher's time is a scarce and costly resource, which does not scale with growing demand. Further factors contribute to the need for CALL: pandemics and increasing demand for distance learning, migration of large populations, the need for sustainable and affordable support for learning, etc. CALL systems are made up of many components that perform various functions, and AI is applied to many different aspects in CALL, corresponding to their own expansive research areas. Most of what we find in the research literature and in practical use are prototypes or partial implementations -- systems that perform some aspects of the overall desired functionality. Complete solutions -- most of them commercial -- are few, because they require massive resources. Recent advances in AI should result in improvements in CALL, yet there is a lack of surveys that focus on AI in the context of this research field. This paper aims to present a perspective on the AI methods that can be employed for language learning from a position of a developer of a CALL system. We also aim to connect work from different disciplines, to build bridges for interdisciplinary work.",CS,AI_ML,0.85,Extracted from log - paper 1123
Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs,"Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. Upon publishing, we will also opensource our model signal on the entire C4 dataset. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance.",CS,AI_ML,0.85,Extracted from log - paper 1124
LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load,"Information on the web, such as scientific publications and Wikipedia, often surpasses users' reading level. To help address this, we used a self-refinement approach to develop a LLM capability for minimally lossy text simplification. To validate our approach, we conducted a randomized study involving 4563 participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical scientific articles), biology, law, finance, literature/philosophy, and aerospace/computer science. Participants were randomized to viewing original or simplified texts in a subject area, and answered multiple-choice questions (MCQs) that tested their comprehension of the text. The participants were also asked to provide qualitative feedback such as task difficulty. Our results indicate that participants who read the simplified text answered more MCQs correctly than their counterparts who read the original text (3.9% absolute increase, p<0.05). This gain was most striking with PubMed (14.6%), while more moderate gains were observed for finance (5.5%), aerospace/computer science (3.8%) domains, and legal (3.5%). Notably, the results were robust to whether participants could refer back to the text while answering MCQs. The absolute accuracy decreased by up to ~9% for both original and simplified setups where participants could not refer back to the text, but the ~4% overall improvement persisted. Finally, participants' self-reported perceived ease based on a simplified NASA Task Load Index was greater for those who read the simplified text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study, involving an order of magnitude more participants than prior works, demonstrates the potential of LLMs to make complex information easier to understand. Our work aims to enable a broader audience to better learn and make use of expert knowledge available on the web, improving information accessibility.",CS,AI_ML,0.85,Extracted from log - paper 1125
Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview,"Large Language Models (LLMs) have become integral to daily life, widely adopted in communication, decision-making, and information retrieval, raising critical questions about how these systems implicitly form and express socio-cognitive attitudes or ""worldviews"". While existing research extensively addresses demographic and ethical biases, broader dimensions-such as attitudes toward authority, equality, autonomy, and fate-remain under-explored. In this paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework grounded in Cultural Theory, operationalizing four canonical worldviews (Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable sub-dimensions. Using SWT, we empirically identify distinct and interpretable cognitive profiles across 28 diverse LLMs. Further, inspired by Social Referencing Theory, we experimentally demonstrate that explicit social cues systematically shape these cognitive attitudes, revealing both general response patterns and nuanced model-specific variations. Our findings enhance the interpretability of LLMs by revealing implicit socio-cognitive biases and their responsiveness to social feedback, thus guiding the development of more transparent and socially responsible language technologies.",CS,AI_ML,0.85,Extracted from log - paper 1126
A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models,"Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations.",CS,AI_ML,0.85,Extracted from log - paper 1127
Explainability by design: an experimental analysis of the legal coding process,"Behind a set of rules in Deontic Defeasible Logic, there is a mapping process of normative background fragments. This process goes from text to rules and implicitly encompasses an explanation of the coded fragments.   In this paper we deliver a methodology for \textit{legal coding} that starts with a fragment and goes onto a set of Deontic Defeasible Logic rules, involving a set of \textit{scenarios} to test the correctness of the coded fragments. The methodology is illustrated by the coding process of an example text. We then show the results of a series of experiments conducted with humans encoding a variety of normative backgrounds and corresponding cases in which we have measured the efforts made in the coding process, as related to some measurable features. To process these examples, a recently developed technology, Houdini, that allows reasoning in Deontic Defeasible Logic, has been employed.   Finally we provide a technique to forecast time required in coding, that depends on factors such as knowledge of the legal domain, knowledge of the coding processes, length of the text, and a measure of \textit{depth} that refers to the length of the paths of legal references.",CS,AI_ML,0.85,Extracted from log - paper 1128
CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation,"Automated evidence-based misinformation detection systems, which evaluate the veracity of short claims against evidence, lack comprehensive analysis of their adversarial vulnerabilities. Existing black-box text-based adversarial attacks are ill-suited for evidence-based misinformation detection systems, as these attacks primarily focus on token-level substitutions involving gradient or logit-based optimization strategies, which are incapable of fooling the multi-component nature of these detection systems. These systems incorporate both retrieval and claim-evidence comparison modules, which requires attacks to break the retrieval of evidence and/or the comparison module so that it draws incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach that employs a two-agent system, a Prompt Optimization Agent and an Attacker Agent, to create adversarial claim rewritings that manipulate evidence retrieval and mislead claim-evidence comparison, effectively bypassing the system without altering the meaning of the claim. The Attacker Agent produces semantically equivalent rewrites that attempt to mislead detectors, while the Prompt Optimization Agent analyzes failed attack attempts and refines the prompt of the Attacker to guide subsequent rewrites. This enables larger structural and stylistic transformations of the text rather than token-level substitutions, adapting the magnitude of changes based on previous outcomes. Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on binary model decisions to guide its rewriting process, eliminating the need for classifier logits or extensive querying. We evaluate CAMOUFLAGE on four systems, including two recent academic systems and two real-world APIs, with an average attack success rate of 46.92\% while preserving textual coherence and semantic equivalence to the original claims.",CS,AI_ML,0.85,Extracted from log - paper 1129
Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams,"We present a framework for large-scale sentiment and topic analysis of Twitter discourse. Our pipeline begins with targeted data collection using conflict-specific keywords, followed by automated sentiment labeling via multiple pre-trained models to improve annotation robustness. We examine the relationship between sentiment and contextual features such as timestamp, geolocation, and lexical content. To identify latent themes, we apply Latent Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and metadata attributes. Finally, we develop an interactive visualization interface to support exploration of sentiment trends and topic distributions across time and regions. This work contributes a scalable methodology for social media analysis in dynamic geopolitical contexts.",CS,AI_ML,0.85,Extracted from log - paper 1130
"Humans can learn to detect AI-generated texts, or at least learn when they can't","This study investigates whether individuals can learn to accurately discriminate between human-written and AI-produced texts when provided with immediate feedback, and if they can use this feedback to recalibrate their self-perceived competence. We also explore the specific criteria individuals rely upon when making these decisions, focusing on textual style and perceived readability.   We used GPT-4o to generate several hundred texts across various genres and text types comparable to Koditex, a multi-register corpus of human-written texts. We then presented randomized text pairs to 255 Czech native speakers who identified which text was human-written and which was AI-generated. Participants were randomly assigned to two conditions: one receiving immediate feedback after each trial, the other receiving no feedback until experiment completion. We recorded accuracy in identification, confidence levels, response times, and judgments about text readability along with demographic data and participants' engagement with AI technologies prior to the experiment.   Participants receiving immediate feedback showed significant improvement in accuracy and confidence calibration. Participants initially held incorrect assumptions about AI-generated text features, including expectations about stylistic rigidity and readability. Notably, without feedback, participants made the most errors precisely when feeling most confident -- an issue largely resolved among the feedback group.   The ability to differentiate between human and AI-generated texts can be effectively learned through targeted training with explicit feedback, which helps correct misconceptions about AI stylistic features and readability, as well as potential other variables that were not explored, while facilitating more accurate self-assessment. This finding might be particularly important in educational contexts.",CS,AI_ML,0.85,Extracted from log - paper 1131
Positional Attention for Efficient BERT-Based Named Entity Recognition,"This paper presents a framework for Named Entity Recognition (NER) leveraging the Bidirectional Encoder Representations from Transformers (BERT) model in natural language processing (NLP). NER is a fundamental task in NLP with broad applicability across downstream applications. While BERT has established itself as a state-of-the-art model for entity recognition, fine-tuning it from scratch for each new application is computationally expensive and time-consuming. To address this, we propose a cost-efficient approach that integrates positional attention mechanisms into the entity recognition process and enables effective customization using pre-trained parameters. The framework is evaluated on a Kaggle dataset derived from the Groningen Meaning Bank corpus and achieves strong performance with fewer training epochs. This work contributes to the field by offering a practical solution for reducing the training cost of BERT-based NER systems while maintaining high accuracy.",CS,AI_ML,0.85,Extracted from log - paper 1132
Intra-Layer Recurrence in Transformers for Language Modeling,"Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures.",CS,AI_ML,0.85,Extracted from log - paper 1133
$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge,"Humans and intelligent animals can effortlessly internalize new information (""news"") and accurately extract the implications for performing downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the news is explicitly given as context, fine-tuning remains challenging for the models to consolidate learning in weights. In this paper, we introduce $\textit{New News}$, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. We first demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our news dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications and Self-QAs -- designed to distill the knowledge from the model with context into the weights of the model without the context, which we term $\textit{System-2 Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news. Furthermore, we discover the $\textit{contexual shadowing effect}$, where training with the news $\textit{in context}$ followed by its rephrases or QAs degrade learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.",CS,AI_ML,0.85,Extracted from log - paper 1134
Accelerating Large Language Model Reasoning via Speculative Search,"Tree-search-based reasoning methods have significantly enhanced the reasoning capability of large language models (LLMs) by facilitating the exploration of multiple intermediate reasoning steps, i.e., thoughts. However, these methods suffer from substantial inference latency, as they have to generate numerous reasoning thoughts, severely limiting LLM applicability. To address this challenge, we propose a novel Speculative Search (SpecSearch) framework that significantly accelerates LLM reasoning by optimizing thought generation. Specifically, SpecSearch utilizes a small model to strategically collaborate with a large model at both thought and token levels, efficiently generating high-quality reasoning thoughts. The major pillar of SpecSearch is a novel quality-preserving rejection mechanism, which effectively filters out thoughts whose quality falls below that of the large model's outputs. Moreover, we show that SpecSearch preserves comparable reasoning quality to the large model. Experiments on both the Qwen and Llama models demonstrate that SpecSearch significantly outperforms state-of-the-art approaches, achieving up to 2.12$\times$ speedup with comparable reasoning quality.",CS,AI_ML,0.85,Extracted from log - paper 1135
Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis,"The increasing sophistication of AI-generated texts highlights the urgent need for accurate and transparent detection tools, especially in educational settings, where verifying authorship is essential. Existing literature has demonstrated that the application of stylometric features with machine learning classifiers can yield excellent results. Building on this foundation, this study proposes a comprehensive framework that integrates stylometric analysis with psycholinguistic theories, offering a clear and interpretable approach to distinguishing between AI-generated and human-written texts. This research specifically maps 31 distinct stylometric features to cognitive processes such as lexical retrieval, discourse planning, cognitive load management, and metacognitive self-monitoring. In doing so, it highlights the unique psycholinguistic patterns found in human writing. Through the intersection of computational linguistics and cognitive science, this framework contributes to the development of reliable tools aimed at preserving academic integrity in the era of generative AI.",CS,AI_ML,0.85,Extracted from log - paper 1136
Differential Privacy for Network Assortativity,"The analysis of network assortativity is of great importance for understanding the structural characteristics of and dynamics upon networks. Often, network assortativity is quantified using the assortativity coefficient that is defined based on the Pearson correlation coefficient between vertex degrees. It is well known that a network may contain sensitive information, such as the number of friends of an individual in a social network (which is abstracted as the degree of vertex.). So, the computation of the assortativity coefficient leads to privacy leakage, which increases the urgent need for privacy-preserving protocol. However, there has been no scheme addressing the concern above.   To bridge this gap, in this work, we are the first to propose approaches based on differential privacy (DP for short). Specifically, we design three DP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The first two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are designed for settings where each individual only knows his/her direct friends. In contrast, the third algorithm, based on Decentralized DP (DDP), targets scenarios where each individual has a broader view, i.e., also knowing his/her friends' friends. Theoretically, we prove that each algorithm enables an unbiased estimation of the assortativity coefficient of the network. We further evaluate the performance of the proposed algorithms using mean squared error (MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by $Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three algorithms have different assumptions, so each has its applicability scenario. Lastly, we conduct extensive numerical simulations, which demonstrate that the presented approaches are adequate to achieve the estimation of network assortativity under the demand for privacy protection.",CS,AI_ML,0.85,Extracted from log - paper 1137
Empc: Effective Path Prioritization for Symbolic Execution with Path Cover,"Symbolic execution is a powerful program analysis technique that can formally reason the correctness of program behaviors and detect software bugs. It can systematically explore the execution paths of the tested program. But it suffers from an inherent limitation: path explosion. Path explosion occurs when symbolic execution encounters an overwhelming number (exponential to the program size) of paths that need to be symbolically reasoned. It severely impacts the scalability and performance of symbolic execution. To tackle this problem, previous works leverage various heuristics to prioritize paths for symbolic execution. They rank the exponential number of paths using static rules or heuristics and explore the paths with the highest rank. However, in practice, these works often fail to generalize to diverse programs. In this work, we propose a novel and effective path prioritization technique with path cover, named Empc. Our key insight is that not all paths need to be symbolically reasoned. Unlike traditional path prioritization, our approach leverages a small subset of paths as a minimum path cover (MPC) that can cover all code regions of the tested programs. To encourage diversity in path prioritization, we compute multiple MPCs. We then guide the search for symbolic execution on the small number of paths inside multiple MPCs rather than the exponential number of paths. We implement our technique Empc based on KLEE. We conduct a comprehensive evaluation of Empc to investigate its performance in code coverage, bug findings, and runtime overhead. The evaluation shows that Empc can cover 19.6% more basic blocks than KLEE's best search strategy and 24.4% more lines compared to the state-of-the-art work cgs. Empc also finds 24 more security violations than KLEE's best search strategy. Meanwhile, Empc can significantly reduce the memory usage of KLEE by up to 93.5%.",CS,AI_ML,0.85,Extracted from log - paper 1138
SKALD: Scalable K-Anonymisation for Large Datasets,"Data privacy and anonymisation are critical concerns in today's data-driven society, particularly when handling personal and sensitive user data. Regulatory frameworks worldwide recommend privacy-preserving protocols such as k-anonymisation to de-identify releases of tabular data. Available hardware resources provide an upper bound on the maximum size of dataset that can be processed at a time. Large datasets with sizes exceeding this upper bound must be broken up into smaller data chunks for processing. In these cases, standard k-anonymisation tools such as ARX can only operate on a per-chunk basis. This paper proposes SKALD, a novel algorithm for performing k-anonymisation on large datasets with limited RAM. Our SKALD algorithm offers multi-fold performance improvement over standard k-anonymisation methods by extracting and combining sufficient statistics from each chunk during processing to ensure successful k-anonymisation while providing better utility.",CS,AI_ML,0.85,Extracted from log - paper 1139
Mitigating Backdoor Triggered and Targeted Data Poisoning Attacks in Voice Authentication Systems,"Voice authentication systems remain susceptible to two major threats: backdoor triggered attacks and targeted data poisoning attacks. This dual vulnerability is critical because conventional solutions typically address each threat type separately, leaving systems exposed to adversaries who can exploit both attacks simultaneously. We propose a unified defense framework that effectively addresses both BTA and TDPA. Our framework integrates a frequency focused detection mechanism that flags covert pitch boosting and sound masking backdoor attacks in near real time, followed by a convolutional neural network that addresses TDPA. This dual layered defense approach utilizes multidimensional acoustic features to isolate anomalous signals without requiring costly model retraining. In particular, our PBSM detection mechanism can seamlessly integrate into existing voice authentication pipelines and scale effectively for large scale deployments. Experimental results on benchmark datasets and their compression with the state of the art algorithm demonstrate that our PBSM detection mechanism outperforms the state of the art. Our framework reduces attack success rates to as low as five to fifteen percent while maintaining a recall rate of up to ninety five percent in recognizing TDPA.",CS,AI_ML,0.85,Extracted from log - paper 1140
Directed Greybox Fuzzing via Large Language Model,"Directed greybox fuzzing (DGF) focuses on efficiently reaching specific program locations or triggering particular behaviors, making it essential for tasks like vulnerability detection and crash reproduction. However, existing methods often suffer from path explosion and randomness in input mutation, leading to inefficiencies in exploring and exploiting target paths. In this paper, we propose HGFuzzer, an automatic framework that leverages the large language model (LLM) to address these challenges. HGFuzzer transforms path constraint problems into targeted code generation tasks, systematically generating test harnesses and reachable inputs to reduce unnecessary exploration paths significantly. Additionally, we implement custom mutators designed specifically for target functions, minimizing randomness and improving the precision of directed fuzzing. We evaluated HGFuzzer on 20 real-world vulnerabilities, successfully triggering 17, including 11 within the first minute, achieving a speedup of at least 24.8x compared to state-of-the-art directed fuzzers. Furthermore, HGFuzzer discovered 9 previously unknown vulnerabilities, all of which were assigned CVE IDs, demonstrating the effectiveness of our approach in identifying real-world vulnerabilities.",CS,AI_ML,0.85,Extracted from log - paper 1141
Elevating Cyber Threat Intelligence against Disinformation Campaigns with LLM-based Concept Extraction and the FakeCTI Dataset,"The swift spread of fake news and disinformation campaigns poses a significant threat to public trust, political stability, and cybersecurity. Traditional Cyber Threat Intelligence (CTI) approaches, which rely on low-level indicators such as domain names and social media handles, are easily evaded by adversaries who frequently modify their online infrastructure. To address these limitations, we introduce a novel CTI framework that focuses on high-level, semantic indicators derived from recurrent narratives and relationships of disinformation campaigns. Our approach extracts structured CTI indicators from unstructured disinformation content, capturing key entities and their contextual dependencies within fake news using Large Language Models (LLMs). We further introduce FakeCTI, the first dataset that systematically links fake news to disinformation campaigns and threat actors. To evaluate the effectiveness of our CTI framework, we analyze multiple fake news attribution techniques, spanning from traditional Natural Language Processing (NLP) to fine-tuned LLMs. This work shifts the focus from low-level artifacts to persistent conceptual structures, establishing a scalable and adaptive approach to tracking and countering disinformation campaigns.",CS,AI_ML,0.85,Extracted from log - paper 1142
A Chaos Driven Metric for Backdoor Attack Detection,"The advancement and adoption of Artificial Intelligence (AI) models across diverse domains have transformed the way we interact with technology. However, it is essential to recognize that while AI models have introduced remarkable advancements, they also present inherent challenges such as their vulnerability to adversarial attacks. The current work proposes a novel defense mechanism against one of the most significant attack vectors of AI models - the backdoor attack via data poisoning of training datasets. In this defense technique, an integrated approach that combines chaos theory with manifold learning is proposed. A novel metric - Precision Matrix Dependency Score (PDS) that is based on the conditional variance of Neurochaos features is formulated. The PDS metric has been successfully evaluated to distinguish poisoned samples from non-poisoned samples across diverse datasets.",CS,AI_ML,0.85,Extracted from log - paper 1143
Bridging Expertise Gaps: The Role of LLMs in Human-AI Collaboration for Cybersecurity,"This study investigates whether large language models (LLMs) can function as intelligent collaborators to bridge expertise gaps in cybersecurity decision-making. We examine two representative tasks-phishing email detection and intrusion detection-that differ in data modality, cognitive complexity, and user familiarity. Through a controlled mixed-methods user study, n = 58 (phishing, n = 34; intrusion, n = 24), we find that human-AI collaboration improves task performance,reducing false positives in phishing detection and false negatives in intrusion detection. A learning effect is also observed when participants transition from collaboration to independent work, suggesting that LLMs can support long-term skill development. Our qualitative analysis shows that interaction dynamics-such as LLM definitiveness, explanation style, and tone-influence user trust, prompting strategies, and decision revision. Users engaged in more analytic questioning and showed greater reliance on LLM feedback in high-complexity settings. These results provide design guidance for building interpretable, adaptive, and trustworthy human-AI teaming systems, and demonstrate that LLMs can meaningfully support non-experts in reasoning through complex cybersecurity problems.",CS,AI_ML,0.85,Extracted from log - paper 1144
An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground Integrated Networks,"Recently emerged 6G space-air-ground integrated networks (SAGINs), which integrate satellites, aerial networks, and terrestrial communications, offer ubiquitous coverage for various mobile applications. However, the highly dynamic, open, and heterogeneous nature of SAGINs poses severe security issues. Forming a defense line of SAGINs suffers from two preliminary challenges: 1) accurately understanding massive unstructured multi-dimensional threat information to generate defense strategies against various malicious attacks, 2) rapidly adapting to potential unknown threats to yield more effective security strategies. To tackle the above two challenges, we propose a novel security framework for SAGINs based on Large Language Models (LLMs), which consists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG leverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent mechanisms to analyze massive unstructured multi-dimensional threat data and generate comprehensive security strategies, thus addressing the first challenge. Our proposed 6G-INST relies on a novel self-evolving method to automatically update LLM-6GNG, enabling it to accommodate unknown threats under dynamic communication environments, thereby addressing the second challenge. Additionally, we prototype the proposed framework with ns-3, OpenAirInterface (OAI), and software-defined radio (SDR). Experiments on three benchmarks demonstrate the effectiveness of our framework. The results show that our framework produces highly accurate security strategies that remain robust against a variety of unknown attacks. We will release our code to contribute to the community.",CS,AI_ML,0.85,Extracted from log - paper 1145
Towards Effective Identification of Attack Techniques in Cyber Threat Intelligence Reports using Large Language Models,"This work evaluates the performance of Cyber Threat Intelligence (CTI) extraction methods in identifying attack techniques from threat reports available on the web using the MITRE ATT&CK framework. We analyse four configurations utilising state-of-the-art tools, including the Threat Report ATT&CK Mapper (TRAM) and open-source Large Language Models (LLMs) such as Llama2. Our findings reveal significant challenges, including class imbalance, overfitting, and domain-specific complexity, which impede accurate technique extraction. To mitigate these issues, we propose a novel two-step pipeline: first, an LLM summarises the reports, and second, a retrained SciBERT model processes a rebalanced dataset augmented with LLM-generated data. This approach achieves an improvement in F1-scores compared to baseline models, with several attack techniques surpassing an F1-score of 0.90. Our contributions enhance the efficiency of web-based CTI systems and support collaborative cybersecurity operations in an interconnected digital landscape, paving the way for future research on integrating human-AI collaboration platforms.",CS,AI_ML,0.85,Extracted from log - paper 1146
Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems,"Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable to adversarial attacks. It is crucial for an IDS to learn to recognize adversarial examples before malicious entities exploit them. In this paper, we generated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We validate the generalization and scalability of the adversarial samples to tackle a broad range of real attacks on Industrial Control Systems (ICS). We evaluated the impact by assessing multiple attacks generated using the proposed method. The model trained with adversarial samples detected attacks with 95% accuracy on real-world attack data not used during training. The study was conducted using an operational secure water treatment (SWaT) testbed.",CS,AI_ML,0.85,Extracted from log - paper 1147
Towards a standardized methodology and dataset for evaluating LLM-based digital forensic timeline analysis,"Large language models (LLMs) have seen widespread adoption in many domains including digital forensics. While prior research has largely centered on case studies and examples demonstrating how LLMs can assist forensic investigations, deeper explorations remain limited, i.e., a standardized approach for precise performance evaluations is lacking. Inspired by the NIST Computer Forensic Tool Testing Program, this paper proposes a standardized methodology to quantitatively evaluate the application of LLMs for digital forensic tasks, specifically in timeline analysis. The paper describes the components of the methodology, including the dataset, timeline generation, and ground truth development. Additionally, the paper recommends using BLEU and ROUGE metrics for the quantitative evaluation of LLMs through case studies or tasks involving timeline analysis. Experimental results using ChatGPT demonstrate that the proposed methodology can effectively evaluate LLM-based forensic timeline analysis. Finally, we discuss the limitations of applying LLMs to forensic timeline analysis.",CS,AI_ML,0.85,Extracted from log - paper 1148
Acoustic Side-Channel Attacks on a Computer Mouse,"Acoustic Side-Channel Attacks (ASCAs) extract sensitive information by using audio emitted from a computing devices and their peripherals. Attacks targeting keyboards are popular and have been explored in the literature. However, similar attacks targeting other human interface peripherals, such as computer mice, are under-explored. To this end, this paper considers security leakage via acoustic signals emanating from normal mouse usage. We first confirm feasibility of such attacks by showing a proof-of-concept attack that classifies four mouse movements with 97% accuracy in a controlled environment. We then evolve the attack towards discerning twelve unique mouse movements using a smartphone to record the experiment. Using Machine Learning (ML) techniques, the model is trained on an experiment with six participants to be generalizable and discern among twelve movements with 94% accuracy. In addition, we experiment with an attack that detects a user action of closing a full-screen window on a laptop. Achieving an accuracy of 91%, this experiment highlights exploiting audio leakage from computer mouse movements in a realistic scenario.",CS,AI_ML,0.85,Extracted from log - paper 1149
SoK: Stealing Cars Since Remote Keyless Entry Introduction and How to Defend From It,"Remote Keyless Entry (RKE) systems have been the target of thieves since their introduction in automotive industry. Robberies targeting vehicles and their remote entry systems are booming again without a significant advancement from the industrial sector being able to protect against them. Researchers and attackers continuously play cat and mouse to implement new methodologies to exploit weaknesses and defense strategies for RKEs. In this fragment, different attacks and defenses have been discussed in research and industry without proper bridging. In this paper, we provide a Systematization Of Knowledge (SOK) on RKE and Passive Keyless Entry and Start (PKES), focusing on their history and current situation, ranging from legacy systems to modern web-based ones. We provide insight into vehicle manufacturers' technologies and attacks and defense mechanisms involving them. To the best of our knowledge, this is the first comprehensive SOK on RKE systems, and we address specific research questions to understand the evolution and security status of such systems. By identifying the weaknesses RKE still faces, we provide future directions for security researchers and companies to find viable solutions to address old attacks, such as Relay and RollJam, as well as new ones, like API vulnerabilities.",CS,AI_ML,0.85,Extracted from log - paper 1150
Antifragility of RIS-assisted Communication Systems under Jamming Attacks,"Antifragility of communication systems is defined as measure of benefits gained from the adverse events and variability of its environment. In this paper, we introduce the notion of antifragility in Reconfigurable Intelligent Surface (RIS) assisted communication systems affected by a jamming attack. We analyzed the antifragility of the two hop systems, where the wireless path contains source node, RIS, destination node, and a eavesdropping/jamming node. We propose and analyze the antifragility performance for several jamming models, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude shifting. Our paper shows that antifragility throughput can indeed be achieved under certain power thresholds and for various jamming models. In particular, high jamming power combined with low baseline data rates yields an antifragile gain factor of approximately five times. The results confirm that reconfigurable intelligent surfaces, when coupled with an antifragile design philosophy, can convert hostile interference from a liability into a throughput gain.",CS,AI_ML,0.85,Extracted from log - paper 1151
Attestable builds: compiling verifiable binaries on untrusted systems using trusted execution environments,"In this paper we present attestable builds, a new paradigm to provide strong source-to-binary correspondence in software artifacts. We tackle the challenge of opaque build pipelines that disconnect the trust between source code, which can be understood and audited, and the final binary artifact, which is difficult to inspect. Our system uses modern trusted execution environments (TEEs) and sandboxed build containers to provide strong guarantees that a given artifact was correctly built from a specific source code snapshot. As such it complements existing approaches like reproducible builds which typically require time-intensive modifications to existing build configurations and dependencies, and require independent parties to continuously build and verify artifacts. In comparison, an attestable build requires only minimal changes to an existing project, and offers nearly instantaneous verification of the correspondence between a given binary and the source code and build pipeline used to construct it. We evaluate it by building open-source software libraries - focusing on projects which are important to the trust chain and those which have proven difficult to be built deterministically. Overall, the overhead (42 seconds start-up latency and 14% increase in build duration) is small in comparison to the overall build time. Importantly, our prototype builds even complex projects such as LLVM Clang without requiring any modifications to their source code and build scripts. Finally, we formally model and verify the attestable build design to demonstrate its security against well-resourced adversaries.",CS,AI_ML,0.85,Extracted from log - paper 1152
Unveiling the Landscape of LLM Deployment in the Wild: An Empirical Study,"Background: Large language models (LLMs) are increasingly deployed via open-source and commercial frameworks, enabling individuals and organizations to self-host advanced AI capabilities. However, insecure defaults and misconfigurations often expose LLM services to the public Internet, posing significant security and system engineering risks. Aims: This study aims to unveil the current landscape of public-facing LLM deployments in the wild through a large-scale empirical study, focusing on service prevalence, exposure characteristics, systemic vulnerabilities, and associated risks. Method: We conducted an Internet-wide measurement to identify public-facing LLM deployments across 15 frameworks, discovering 320,102 services. We extracted 158 unique API endpoints, grouped into 12 functional categories based on capabilities and security risks. We further analyzed configurations, authentication practices, and geographic distributions, revealing deployment trends and systemic issues in real-world LLM system engineering. Results: Our study shows that public LLM deployments are rapidly growing but often insecure. Among all endpoints, we observe widespread use of insecure protocols, poor TLS configurations, and unauthenticated access to critical operations. Security risks, including model disclosure, system leakage, and unauthorized access, are pervasive, highlighting the need for secure-by-default frameworks and stronger deployment practices. Conclusions: Public-facing LLM deployments suffer from widespread security and configuration flaws, exposing services to misuse, model theft, resource hijacking, and remote exploitation. Strengthening default security, deployment practices, and operational standards is critical for the growing self-hosted LLM ecosystem.",CS,AI_ML,0.85,Extracted from log - paper 1153
An Efficient Hybrid Key Exchange Mechanism,"We present \textsc{CHOKE}, a novel code-based hybrid key-encapsulation mechanism (KEM) designed to securely and efficiently transmit multiple session keys simultaneously. By encoding $n$ independent session keys with an individually secure linear code and encapsulating each resulting coded symbol using a separate KEM, \textsc{CHOKE} achieves computational individual security -- each key remains secure as long as at least one underlying KEM remains unbroken. Compared to traditional serial or combiner-based hybrid schemes, \textsc{CHOKE} reduces computational and communication costs by an $n$-fold factor. Furthermore, we show that the communication cost of our construction is optimal under the requirement that each KEM must be used at least once.",CS,AI_ML,0.85,Extracted from log - paper 1154
Dynamic Graph-based Fingerprinting of In-browser Cryptomining,"The decentralized and unregulated nature of cryptocurrencies, combined with their monetary value, has made them a vehicle for various illicit activities. One such activity is cryptojacking, an attack that uses stolen computing resources to mine cryptocurrencies without consent for profit. In-browser cryptojacking malware exploits high-performance web technologies like WebAssembly to mine cryptocurrencies directly within the browser without file downloads. Although existing methods for cryptomining detection report high accuracy and low overhead, they are often susceptible to various forms of obfuscation, and due to the limited variety of cryptomining scripts in the wild, standard code obfuscation methods present a natural and appealing solution to avoid detection. To address these limitations, we propose using instruction-level data-flow graphs to detect cryptomining behavior. Data-flow graphs offer detailed structural insights into a program's computations, making them suitable for characterizing proof-of-work algorithms, but they can be difficult to analyze due to their large size and susceptibility to noise and fragmentation under obfuscation. We present two techniques to simplify and compare data-flow graphs: (1) a graph simplification algorithm to reduce the computational burden of processing large and granular data-flow graphs while preserving local substructures; and (2) a subgraph similarity measure, the n-fragment inclusion score, based on fragment inclusion that is robust against noise and obfuscation. Using data-flow graphs as computation fingerprints, our detection framework PoT (Proof-of-Theft) was able to achieve high detection accuracy against standard obfuscations, outperforming existing detection methods. Moreover, PoT uses generic data-flow properties that can be applied to other platforms more susceptible to cryptojacking such as servers and data centers.",CS,AI_ML,0.85,Extracted from log - paper 1155
Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation,"Rust is a promising programming language that focuses on concurrency, usability, and security. It is used in production code by major industry players and got recommended by government bodies. Rust provides strong security guarantees achieved by design utilizing the concepts of ownership and borrowing. However, Rust allows programmers to write unsafe code which is not subject to the strict Rust security policy. Empirical studies show that security issues in practice always involve code written in unsafe Rust.   In this paper, we present the first approach that utilizes selective code coverage feedback to focus the fuzzing efforts on unsafe Rust code. Our approach significantly improves the efficiency when fuzzing Rust programs and does not require additional computational resources while fuzz testing the target. To quantify the impact of partial code instrumentation, we implement our approach by extending the capabilities of the Rust compiler toolchain. We present an automated approach to detect unsafe and safe code components to decide which parts of the program a fuzzer should focus on when running a fuzzing campaign to find vulnerabilities in Rust programs. Our approach is fully compatible with existing fuzzing implementations and does not require complex manual work, thus retaining the existing high usability standard. Focusing on unsafe code, our implementation allows us to generate inputs that trigger more unsafe code locations with statistical significance and therefore is able to detect potential vulnerabilities in a shorter time span while imposing no performance overhead during fuzzing itself.",CS,AI_ML,0.85,Extracted from log - paper 1156
Encrypted Federated Search Using Homomorphic Encryption,"The sharing of information between agencies is effective in dealing with cross-jurisdictional criminal activities; however, such sharing is often restricted due to concerns about data privacy, ownership, and compliance. Towards this end, this work has introduced a privacy-preserving federated search system that allows law enforcement agencies to conduct queries on encrypted criminal databases by utilizing Homomorphic Encryption (HE). The key innovation here is the ability to execute encrypted queries across distributed databases, without the decryption of the data, thus preserving end-to-end confidentiality. In essence, this approach meets stringent privacy requirements in the interests of national security and regulatory compliance. The system incorporates the CKKS and BFV scheme embedded within TenSEAL, with each agency holding its key pair in a centralized key management table. In this federated search, encrypted queries are computed on the server side, and only authorized clients can decrypt the computed results. The matching of agencies is flexible for working in real-time while at the same time being secure and scalable while preserving control over data and the integrity of the process. Experimental results demonstrate the model. This paper also provide the implementation code and other details.",CS,AI_ML,0.85,Extracted from log - paper 1157
"Moneros Decentralized P2P Exchanges: Functionality, Adoption, and Privacy Risks","Privacy-focused cryptocurrencies like Monero remain popular, despite increasing regulatory scrutiny that has led to their delisting from major centralized exchanges. The latter also explains the recent popularity of decentralized exchanges (DEXs) with no centralized ownership structures. These platforms typically leverage peer-to-peer (P2P) networks, promising secure and anonymous asset trading. However, questions of liability remain, and the academic literature lacks comprehensive insights into the functionality, trading activity, and privacy claims of these P2P platforms. In this paper, we provide an early systematization of the current landscape of decentralized peer-to-peer exchanges within the Monero ecosystem. We examine several recently developed DEX platforms, analyzing their popularity, functionality, architectural choices, and potential weaknesses. We further identify and report on a privacy vulnerability in the recently popularized Haveno exchange, demonstrating that certain Haveno trades could be detected, allowing transactions to be linked across the Monero and Bitcoin blockchains. We hope that our findings can nourish the discussion in the research community about more secure designs, and provide insights for regulators.",CS,AI_ML,0.85,Extracted from log - paper 1158
Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models,"Email spam detection is a critical task in modern communication systems, essential for maintaining productivity, security, and user experience. Traditional machine learning and deep learning approaches, while effective in static settings, face significant limitations in adapting to evolving spam tactics, addressing class imbalance, and managing data scarcity. These challenges necessitate innovative approaches that reduce dependency on extensive labeled datasets and frequent retraining. This study investigates the effectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced Natural Language Processing (NLP) techniques such as BERT for email spam detection. By employing BERT to preprocess and extract critical information from email content, and FLAN-T5 to classify emails in a Zero-Shot framework, the proposed approach aims to address the limitations of traditional spam detection systems. The integration of FLAN-T5 and BERT enables robust spam detection without relying on extensive labeled datasets or frequent retraining, making it highly adaptable to unseen spam patterns and adversarial environments. This research highlights the potential of leveraging zero-shot learning and NLPs for scalable and efficient spam detection, providing insights into their capability to address the dynamic and challenging nature of spam detection tasks.",CS,AI_ML,0.85,Extracted from log - paper 1159
A Slicing-Based Approach for Detecting and Patching Vulnerable Code Clones,"Code cloning is a common practice in software development, but it poses significant security risks by propagating vulnerabilities across cloned segments. To address this challenge, we introduce srcVul, a scalable, precise detection approach that combines program slicing with Locality-Sensitive Hashing to identify vulnerable code clones and recommend patches. srcVul builds a database of vulnerability-related slices by analyzing known vulnerable programs and their corresponding patches, indexing each slice's unique structural characteristics as a vulnerability slicing vector. During clone detection, srcVul efficiently matches slicing vectors from target programs with those in the database, recommending patches upon identifying similarities. Our evaluation of srcVul against three state-of-the-art vulnerable clone detectors demonstrates its accuracy, efficiency, and scalability, achieving 91% precision and 75% recall on established vulnerability databases and open-source repositories. These results highlight srcVul's effectiveness in detecting complex vulnerability patterns across diverse codebases.",CS,AI_ML,0.85,Extracted from log - paper 1160
An End-to-End Model For Logits Based Large Language Models Watermarking,"The rise of LLMs has increased concerns over source tracing and copyright protection for AIGC, highlighting the need for advanced detection technologies. Passive detection methods usually face high false positives, while active watermarking techniques using logits or sampling manipulation offer more effective protection. Existing LLM watermarking methods, though effective on unaltered content, suffer significant performance drops when the text is modified and could introduce biases that degrade LLM performance in downstream tasks. These methods fail to achieve an optimal tradeoff between text quality and robustness, particularly due to the lack of end-to-end optimization of the encoder and decoder. In this paper, we introduce a novel end-to-end logits perturbation method for watermarking LLM-generated text. By jointly optimization, our approach achieves a better balance between quality and robustness. To address non-differentiable operations in the end-to-end training pipeline, we introduce an online prompting technique that leverages the on-the-fly LLM as a differentiable surrogate. Our method achieves superior robustness, outperforming distortion-free methods by 37-39% under paraphrasing and 17.2% on average, while maintaining text quality on par with these distortion-free methods in terms of text perplexity and downstream tasks. Our method can be easily generalized to different LLMs.",CS,AI_ML,0.85,Extracted from log - paper 1161
Performance Analysis and Deployment Considerations of Post-Quantum Cryptography for Consumer Electronics,"Quantum computing threatens the security foundations of consumer electronics (CE). Preparing the diverse CE ecosystem, particularly resource-constrained devices, for the post-quantum era requires quantitative understanding of quantum-resistant cryptography (PQC) performance. This paper presents a comprehensive cross-platform performance analysis of leading PQC Key Encapsulation Mechanisms (KEMs) and digital signatures (NIST standards/candidates) compared against classical RSA/ECC. We evaluated execution time, communication costs (key/signature sizes), and memory footprint indicators on high-performance (macOS/M4, Ubuntu/x86) and constrained platforms (Raspberry Pi 4/ARM). Our quantitative results reveal lattice-based schemes, notably NIST standards ML-KEM (Kyber) and ML-DSA (Dilithium), provide a strong balance of computational efficiency and moderate communication/storage overhead, making them highly suitable for many CE applications. In contrast, code-based Classic McEliece imposes significant key size challenges, while hash-based SPHINCS+ offers high security assurance but demands large signature sizes impacting bandwidth and storage. Based on empirical data across platforms and security levels, we provide specific deployment recommendations tailored to different CE scenarios (e.g., wearables, smart home hubs, mobile devices), offering guidance for manufacturers navigating the PQC transition.",CS,AI_ML,0.85,Extracted from log - paper 1162
Risk Assessment and Threat Modeling for safe autonomous driving technology,"This research paper delves into the field of autonomous vehicle technology, examining the vulnerabilities inherent in each component of these transformative vehicles. Autonomous vehicles (AVs) are revolutionizing transportation by seamlessly integrating advanced functionalities such as sensing, perception, planning, decision-making, and control. However, their reliance on interconnected systems and external communication interfaces renders them susceptible to cybersecurity threats.   This research endeavors to develop a comprehensive threat model for AV systems, employing OWASP Threat Dragon and the STRIDE framework. This model categorizes threats into Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service (DoS), and Elevation of Privilege.   A systematic risk assessment is conducted to evaluate vulnerabilities across various AV components, including perception modules, planning systems, control units, and communication interfaces.",CS,AI_ML,0.85,Extracted from log - paper 1163
Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees,"A decision tree is an easy-to-understand tool that has been widely used for classification tasks. On the one hand, due to privacy concerns, there has been an urgent need to create privacy-preserving classifiers that conceal the user's input from the classifier. On the other hand, with the rise of cloud computing, data owners are keen to reduce risk by outsourcing their model, but want security guarantees that third parties cannot steal their decision tree model. To address these issues, Joye and Salehi introduced a theoretical protocol that efficiently evaluates decision trees while maintaining privacy by leveraging their comparison protocol that is resistant to timing attacks. However, their approach was not only inefficient but also prone to side-channel attacks. Therefore, in this paper, we propose a new decision tree inference protocol in which the model is shared and evaluated among multiple entities. We partition our decision tree model by each level to be stored in a new entity we refer to as a ""level-site."" Utilizing this approach, we were able to gain improved average run time for classifier evaluation for a non-complete tree, while also having strong mitigations against side-channel attacks.",CS,AI_ML,0.85,Extracted from log - paper 1164
Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents,"Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts.",CS,AI_ML,0.85,Extracted from log - paper 1165
Triple-identity Authentication: The Future of Secure Access,"In a typical authentication process, the local system verifies the user's identity using a stored hash value generated by a cross-system hash algorithm. This article shifts the research focus from traditional password encryption to the establishment of gatekeeping mechanisms for effective interactions between a system and the outside world. Here, we propose a triple-identity authentication system to achieve this goal. Specifically, this local system opens the inner structure of its hash algorithm to all user credentials, including the login name, login password, and authentication password. When a login credential is entered, the local system hashes it and then creates a unique identifier using intermediate hash elements randomly selected from the open algorithm. Importantly, this locally generated unique identifier (rather than the stored hash produced by the open algorithm) is utilized to verify the user's combined identity, which is generated by combining the entered credential with the International Mobile Equipment Identity and the International Mobile Subscriber Identity. The verification process is implemented at each interaction point: the login name field, the login password field, and the server's authentication point. Thus, within the context of this triple-identity authentication system, we establish a robust gatekeeping mechanism for system interactions, ultimately providing a level of security that is equivalent to multi-factor authentication.",CS,AI_ML,0.85,Extracted from log - paper 1166
A Survey on Privacy Risks and Protection in Large Language Models,"Although Large Language Models (LLMs) have become increasingly integral to diverse applications, their capabilities raise significant privacy concerns. This survey offers a comprehensive overview of privacy risks associated with LLMs and examines current solutions to mitigate these challenges. First, we analyze privacy leakage and attacks in LLMs, focusing on how these models unintentionally expose sensitive information through techniques such as model inversion, training data extraction, and membership inference. We investigate the mechanisms of privacy leakage, including the unauthorized extraction of training data and the potential exploitation of these vulnerabilities by malicious actors. Next, we review existing privacy protection against such risks, such as inference detection, federated learning, backdoor mitigation, and confidential computing, and assess their effectiveness in preventing privacy leakage. Furthermore, we highlight key practical challenges and propose future research directions to develop secure and privacy-preserving LLMs, emphasizing privacy risk assessment, secure knowledge transfer between models, and interdisciplinary frameworks for privacy governance. Ultimately, this survey aims to establish a roadmap for addressing escalating privacy challenges in the LLMs domain.",CS,AI_ML,0.85,Extracted from log - paper 1167
"UK Finfluencers: Exploring Content, Reach, and Responsibility","The rise of social media financial influencers (finfluencers) has significantly transformed the personal finance landscape, making financial advice and insights more accessible to a broader and younger audience. By leveraging digital platforms, these influencers have contributed to the democratization of financial literacy. However, the line between education and promotion is often blurred, as many finfluencers lack formal financial qualifications, raising concerns about the accuracy and reliability of the information they share. This study investigates the patterns and behaviours of finfluencers in the UK on TikTok, focusing not on individual actions but on broader trends and the interactions between influencers and their followers. The aim is to identify common engagement patterns and propose guidelines that can help protect the public from potential financial harm. Specifically, the paper contributes a detailed analysis of finfluencer content categorization, sentiment trends, and the prevalence and role of disclaimers, offering empirical insights that inform recommendations for safer and more transparent financial communication on social media.",CS,AI_ML,0.85,Extracted from log - paper 1168
Towards Trustworthy Federated Learning with Untrusted Participants,"Resilience against malicious parties and data privacy are essential for trustworthy distributed learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of workers shares a randomness seed unknown to others. In a setting where malicious workers may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, leveraging shared randomness between workers. We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted. Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server.",CS,AI_ML,0.85,Extracted from log - paper 1169
An Approach for Handling Missing Attribute Values in Attribute-Based Access Control Policy Mining,"Attribute-Based Access Control (ABAC) enables highly expressive and flexible access decisions by considering a wide range of contextual attributes. ABAC policies use logical expressions that combine these attributes, allowing for precise and context-aware control. Algorithms that mine ABAC policies from legacy access control systems can significantly reduce the costs associated with migrating to ABAC. However, a major challenge in this process is handling incomplete entity information, where some attribute values are missing.   This paper introduces an approach that enhances the policy mining process by predicting or inferring missing attribute values. This is accomplished by employing a contextual clustering technique that groups entities according to their known attributes, which are then used to analyze and refine authorization decisions. By effectively managing incomplete data, our approach provides security administrators with a valuable tool to improve their attribute data and ensure a smoother, more efficient transition to ABAC.",CS,AI_ML,0.85,Extracted from log - paper 1170
PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework,"Federated Learning (FL) enables collaborative model training while preserving data privacy, but its classical cryptographic underpinnings are vulnerable to quantum attacks. This vulnerability is particularly critical in sensitive domains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure Blockchain-based Federated Learning), a framework integrating post-quantum cryptography (PQC) with blockchain verification to secure FL against quantum adversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly Dilithium) signatures to authenticate model updates and leverage optimized smart contracts for decentralized validation. Extensive evaluations on diverse datasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient cryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms) with a fixed signature size of 3309 Bytes. Blockchain integration incurs a manageable overhead, with average transaction times around 4.8 s and gas usage per update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the cryptographic overhead relative to transaction time remains minimal (around 0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the bottleneck in blockchain-based FL. The system maintains competitive model accuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with round times showing sublinear growth with increasing client numbers. Our open-source implementation and reproducible benchmarks validate the feasibility of deploying long-term, quantum-resistant security in practical FL systems.",CS,AI_ML,0.85,Extracted from log - paper 1171
M-ary Precomputation-Based Accelerated Scalar Multiplication Algorithms for Enhanced Elliptic Curve Cryptography,"Efficient scalar multiplication is critical for enhancing the performance of elliptic curve cryptography (ECC), especially in applications requiring large-scale or real-time cryptographic operations. This paper proposes an M-ary precomputation-based scalar multiplication algorithm, aiming to optimize both computational efficiency and memory usage. The method reduces the time complexity from $\Theta(Q \log p)$ to $\Theta\left(\frac{Q \log p}{\log Q}\right)$ and achieves a memory complexity of $\Theta\left(\frac{Q \log p}{\log^2 Q}\right)$. Experiments on ElGamal encryption and NS3-based communication simulations validate its effectiveness. On secp256k1, the proposed method achieves up to a 59\% reduction in encryption time and 30\% memory savings. In network simulations, the binary-optimized variant reduces communication time by 22.1\% on secp384r1 and simulation time by 25.4\% on secp521r1. The results demonstrate the scalability, efficiency, and practical applicability of the proposed algorithm. The source code will be publicly released upon acceptance.",CS,AI_ML,0.85,Extracted from log - paper 1172
Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp,"The Open Radio Access Network (O-RAN) architecture is revolutionizing cellular networks with its open, multi-vendor design and AI-driven management, aiming to enhance flexibility and reduce costs. Although it has many advantages, O-RAN is not threat-free. While previous studies have mainly examined vulnerabilities arising from O-RAN's intelligent components, this paper is the first to focus on the security challenges and vulnerabilities introduced by transitioning from single-operator to multi-operator RAN architectures. This shift increases the risk of untrusted third-party operators managing different parts of the network. To explore these vulnerabilities and their potential mitigation, we developed an open-access testbed environment that integrates a wireless network simulator with the official O-RAN Software Community (OSC) RAN intelligent component (RIC) cluster. This environment enables realistic, live data collection and serves as a platform for demonstrating APATE (adversarial perturbation against traffic efficiency), an evasion attack in which a malicious cell manipulates its reported key performance indicators (KPIs) and deceives the O-RAN traffic steering to gain unfair allocations of user equipment (UE). To ensure that O-RAN's legitimate activity continues, we introduce MARRS (monitoring adversarial RAN reports), a detection framework based on a long-short term memory (LSTM) autoencoder (AE) that learns contextual features across the network to monitor malicious telemetry (also demonstrated in our testbed). Our evaluation showed that by executing APATE, an attacker can obtain a 248.5% greater UE allocation than it was supposed to in a benign scenario. In addition, the MARRS detection method was also shown to successfully classify malicious cell activity, achieving accuracy of 99.2% and an F1 score of 0.978.",CS,AI_ML,0.85,Extracted from log - paper 1173
Backdoor Attacks Against Patch-based Mixture of Experts,"As Deep Neural Networks (DNNs) continue to require larger amounts of data and computational power, Mixture of Experts (MoE) models have become a popular choice to reduce computational complexity. This popularity increases the importance of considering the security of MoE architectures. Unfortunately, the security of models using a MoE architecture has not yet gained much attention compared to other DNN models. In this work, we investigate the vulnerability of patch-based MoE (pMoE) models for image classification against backdoor attacks. We examine multiple trigger generation methods and Fine-Pruning as a defense. To better understand a pMoE model's vulnerability to backdoor attacks, we investigate which factors affect the model's patch selection. Our work shows that pMoE models are highly susceptible to backdoor attacks. More precisely, we achieve high attack success rates of up to 100% with visible triggers and a 2% poisoning rate, whilst only having a clean accuracy drop of 1.0%. Additionally, we show that pruning itself is ineffective as a defense but that fine-tuning can remove the backdoor almost completely. Our results show that fine-tuning the model for five epochs reduces the attack success rate to 2.1% whilst sacrificing 1.4% accuracy.",CS,AI_ML,0.85,Extracted from log - paper 1174
Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning,"The widespread adoption of Artificial Intelligence (AI) has been driven by significant advances in intelligent system research. However, this progress has raised concerns about data privacy, leading to a growing awareness of the need for privacy-preserving AI. In response, there has been a seismic shift in interest towards the leading paradigm for training Machine Learning (ML) models on decentralized data silos while maintaining data privacy, Federated Learning (FL). This research paper presents a comprehensive performance analysis of a cutting-edge approach to personalize ML model while preserving privacy achieved through Privacy Preserving Machine Learning with the innovative framework of Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns about data privacy, this study evaluates the effectiveness of PPMLFPL addressing the critical balance between personalized model refinement and maintaining the confidentiality of individual user data. According to our analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated personalized learning settings is strongly suggested. The results offer valuable insights creating it a promising scope for future advancements in the field of privacy-conscious data-driven technologies.",CS,AI_ML,0.85,Extracted from log - paper 1175
Energy-Efficient NTT Sampler for Kyber Benchmarked on FPGA,"Kyber is a lattice-based key encapsulation mechanism selected for standardization by the NIST Post-Quantum Cryptography (PQC) project. A critical component of Kyber's key generation process is the sampling of matrix elements from a uniform distribution over the ring Rq . This step is one of the most computationally intensive tasks in the scheme, significantly impacting performance in low-power embedded systems such as Internet of Things (IoT), wearable devices, wireless sensor networks (WSNs), smart cards, TPMs (Trusted Platform Modules), etc. Existing approaches to this sampling, notably conventional SampleNTT and Parse-SPDM3, rely on rejection sampling. Both algorithms require a large number of random bytes, which needs at least three SHAKE-128 squeezing steps per polynomial. As a result, it causes significant amount of latency and energy. In this work, we propose a novel and efficient sampling algorithm, namely Modified SampleNTT, which substantially educes the average number of bits required from SHAKE-128 to generate elements in Rq - achieving approximately a 33% reduction compared to conventional SampleNTT. Modified SampleNTT achieves 99.16% success in generating a complete polynomial using only two SHAKE-128 squeezes, outperforming both state-of-the-art methods, which never succeed in two squeezes of SHAKE-128. Furthermore, our algorithm maintains the same average rejection rate as existing techniques and passes all standard statistical tests for randomness quality. FPGA implementation on Artix-7 demonstrates a 33.14% reduction in energy, 33.32% lower latency, and 0.28% fewer slices compared to SampleNTT. Our results confirm that Modified SampleNTT is an efficient and practical alternative for uniform polynomial sampling in PQC schemes such as Kyber, especially for low-power security processors.",CS,AI_ML,0.85,Extracted from log - paper 1176
Unified Steganography via Implicit Neural Representation,"Digital steganography is the practice of concealing for encrypted data transmission. Typically, steganography methods embed secret data into cover data to create stega data that incorporates hidden secret data. However, steganography techniques often require designing specific frameworks for each data type, which restricts their generalizability. In this paper, we present U-INR, a novel method for steganography via Implicit Neural Representation (INR). Rather than using the specific framework for each data format, we directly use the neurons of the INR network to represent the secret data and cover data across different data types. To achieve this idea, a private key is shared between the data sender and receivers. Such a private key can be used to determine the position of secret data in INR networks. To effectively leverage this key, we further introduce a key-based selection strategy that can be used to determine the position within the INRs for data storage. Comprehensive experiments across multiple data types, including images, videos, audio, and SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR, emphasizing its potential for improving data security and privacy in various applications.",CS,AI_ML,0.85,Extracted from log - paper 1177
Allocation of Heterogeneous Resources in General Lotto Games,"The allocation of resources plays an important role in the completion of system objectives and tasks, especially in the presence of strategic adversaries. Optimal allocation strategies are becoming increasingly more complex, given that multiple heterogeneous types of resources are at a system planner's disposal. In this paper, we focus on deriving optimal strategies for the allocation of heterogeneous resources in a well-known competitive resource allocation model known as the General Lotto game. In standard formulations, outcomes are determined solely by the players' allocation strategies of a common, single type of resource across multiple contests. In particular, a player wins a contest if it sends more resources than the opponent. Here, we propose a multi-resource extension where the winner of a contest is now determined not only by the amount of resources allocated, but also by the composition of resource types that are allocated. We completely characterize the equilibrium payoffs and strategies for two distinct formulations. The first consists of a weakest-link/best-shot winning rule, and the second considers a winning rule based on a weighted linear combination of the allocated resources. We then consider a scenario where the resource types are costly to purchase, and derive the players' equilibrium investments in each of the resource types.",CS,AI_ML,0.85,Extracted from log - paper 1178
HoneyBee: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning,"As vector databases gain traction in enterprise applications, robust access control has become critical to safeguard sensitive data. Access control in these systems is often implemented through hybrid vector queries, which combine nearest neighbor search on vector data with relational predicates based on user permissions. However, existing approaches face significant trade-offs: creating dedicated indexes for each user minimizes query latency but introduces excessive storage redundancy, while building a single index and applying access control after vector search reduces storage overhead but suffers from poor recall and increased query latency. This paper introduces HoneyBee, a dynamic partitioning framework that bridges the gap between these approaches by leveraging the structure of Role-Based Access Control (RBAC) policies. RBAC, widely adopted in enterprise settings, groups users into roles and assigns permissions to those roles, creating a natural ""thin waist"" in the permission structure that is ideal for partitioning decisions. Specifically, HoneyBee produces overlapping partitions where vectors can be strategically replicated across different partitions to reduce query latency while controlling storage overhead. By introducing analytical models for the performance and recall of the vector search, HoneyBee formulates the partitioning strategy as a constrained optimization problem to dynamically balance storage, query efficiency, and recall. Evaluations on RBAC workloads demonstrate that HoneyBee reduces storage redundancy compared to role partitioning and achieves up to 6x faster query speeds than row-level security (RLS) with only 1.4x storage increase, offering a practical middle ground for secure and efficient vector search.",CS,AI_ML,0.85,Extracted from log - paper 1179
Disassembly as Weighted Interval Scheduling with Learned Weights,"Disassembly is the first step of a variety of binary analysis and transformation techniques, such as reverse engineering, or binary rewriting. Recent disassembly approaches consist of three phases: an exploration phase, that overapproximates the binary's code; an analysis phase, that assigns weights to candidate instructions or basic blocks; and a conflict resolution phase, that downselects the final set of instructions. We present a disassembly algorithm that generalizes this pattern for a wide range of architectures, namely x86, x64, arm32, and aarch64. Our algorithm presents a novel conflict resolution method that reduces disassembly to weighted interval scheduling.",CS,AI_ML,0.85,Extracted from log - paper 1180
The DCR Delusion: Measuring the Privacy Risk of Synthetic Data,"Synthetic data has become an increasingly popular way to share data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, practitioners and researchers often rely on simpler proxy metrics such as Distance to Closest Record (DCR). These metrics estimate privacy by measuring the similarity between the training data and generated synthetic data. This similarity is also compared against that between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, it passes the test and is considered private. In this work we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models such as Baynet and CTGAN and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue DCR and other distance-based metrics to be flawed by design and show a example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous.",CS,AI_ML,0.85,Extracted from log - paper 1181
Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security,"The increasing density of modern DRAM has heightened its vulnerability to Rowhammer attacks, which induce bit flips by repeatedly accessing specific memory rows. This paper presents an analysis of bit flip patterns generated by advanced Rowhammer techniques that bypass existing hardware defenses. First, we investigate the phenomenon of adjacent bit flips--where two or more physically neighboring bits are corrupted simultaneously--and demonstrate they occur with significantly higher frequency than previously documented. We also show that if multiple bits flip within a byte, they are more likely to be adjacent than randomly distributed: for example, if 4 bits flip within a byte, there is an 87% chance that they are all adjacent. We also demonstrate that bit flips within a row will naturally cluster together likely due to the underlying physics of the attack. We then investigate two fault injection attacks enabled by multiple adjacent or nearby bit flips. First, we show how these correlated flips enable efficient cryptographic signature correction attacks, successfully recovering ECDSA private keys from OpenSSL implementations where single-bit approaches would be unfeasible. Second, we introduce a targeted attack against large language models by exploiting Rowhammer-induced corruptions in tokenizer dictionaries of GGUF model files. This attack effectively rewrites safety instructions in system prompts by swapping safety-critical tokens with benign alternatives, circumventing model guardrails while maintaining normal functionality in other contexts. Our experimental results across multiple DRAM configurations reveal that current memory protection schemes are inadequate against these sophisticated attack vectors, which can achieve their objectives with precise, minimal modifications rather than random corruption.",CS,AI_ML,0.85,Extracted from log - paper 1182
"Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration","The rapid digitalization of communication systems has elevated Interactive Voice Response (IVR) technologies to become critical interfaces for customer engagement. With Artificial Intelligence (AI) now driving these platforms, ensuring secure, compliant, and ethically designed development practices is more imperative than ever. AI-powered IVRs leverage Natural Language Processing (NLP) and Machine Learning (ML) to personalize interactions, automate service delivery, and optimize user experiences. However, these innovations expose systems to heightened risks, including data privacy breaches, AI decision opacity, and model security vulnerabilities. This paper analyzes the evolution of IVRs from static code-based designs to adaptive AI-driven systems, presenting a cybersecurity-centric perspective. We propose a practical governance framework that embeds agile security principles, compliance with global data legislation, and user-centric ethics. Emphasizing privacy-by-design, adaptive risk modeling, and transparency, the paper argues that ethical AI integration is not a feature but a strategic imperative. Through this multidimensional lens, we highlight how modern IVRs can transition from communication tools to intelligent, secure, and accountable digital frontlines-resilient against emerging threats and aligned with societal expectations.",CS,AI_ML,0.85,Extracted from log - paper 1183
VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models,"The rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. Existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. In this work, we introduce VIDSTAMP, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. By fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal perceptual impact. Leveraging architectural components such as 3D convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. VIDSTAMP embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log P-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. Code: Code: \url{https://github.com/SPIN-UMass/VidStamp}",CS,AI_ML,0.85,Extracted from log - paper 1184
Machine Learning for Cyber-Attack Identification from Traffic Flows,"This paper presents our simulation of cyber-attacks and detection strategies on the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual machines and the OPNSense firewall, along with traffic dynamics from SUMO and exploitation via the Metasploit framework. We try to answer the research questions: are we able to identify cyber attacks by only analyzing traffic flow patterns. In this research, the cyber attacks are focused particularly when lights are randomly turned all green or red at busy intersections by adversarial attackers. Despite challenges stemming from imbalanced data and overlapping traffic patterns, our best model shows 85\% accuracy when detecting intrusions purely using traffic flow statistics. Key indicators for successful detection included occupancy, jam length, and halting durations.",CS,AI_ML,0.85,Extracted from log - paper 1185
Explainable Machine Learning for Cyberattack Identification from Traffic Flows,"The increasing automation of traffic management systems has made them prime targets for cyberattacks, disrupting urban mobility and public safety. Traditional network-layer defenses are often inaccessible to transportation agencies, necessitating a machine learning-based approach that relies solely on traffic flow data. In this study, we simulate cyberattacks in a semi-realistic environment, using a virtualized traffic network to analyze disruption patterns. We develop a deep learning-based anomaly detection system, demonstrating that Longest Stop Duration and Total Jam Distance are key indicators of compromised signals. To enhance interpretability, we apply Explainable AI (XAI) techniques, identifying critical decision factors and diagnosing misclassification errors. Our analysis reveals two primary challenges: transitional data inconsistencies, where mislabeled recovery-phase traffic misleads the model, and model limitations, where stealth attacks in low-traffic conditions evade detection. This work enhances AI-driven traffic security, improving both detection accuracy and trustworthiness in smart transportation systems.",CS,AI_ML,0.85,Extracted from log - paper 1186
LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps,"Given a text, can we determine whether it was generated by a large language model (LLM) or by a human? A widely studied approach to this problem is watermarking. We propose an undetectable and elementary watermarking scheme in the closed setting. Also, in the harder open setting, where the adversary has access to most of the model, we propose an unremovable watermarking scheme.",CS,AI_ML,0.85,Extracted from log - paper 1187
"Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability","While machine learning has significantly advanced Network Intrusion Detection Systems (NIDS), particularly within IoT environments where devices generate large volumes of data and are increasingly susceptible to cyber threats, these models remain vulnerable to adversarial attacks. Our research reveals a critical flaw in existing adversarial attack methodologies: the frequent violation of domain-specific constraints, such as numerical and categorical limits, inherent to IoT and network traffic. This leads to up to 80.3% of adversarial examples being invalid, significantly overstating real-world vulnerabilities. These invalid examples, though effective in fooling models, do not represent feasible attacks within practical IoT deployments. Consequently, relying on these results can mislead resource allocation for defense, inflating the perceived susceptibility of IoT-enabled NIDS models to adversarial manipulation. Furthermore, we demonstrate that simpler surrogate models like Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate, we analyze the transferability of adversarial severity to other ML/DL models commonly used in IoT contexts. This work underscores the importance of considering both domain constraints and model architecture when evaluating and designing robust ML/DL models for security-critical IoT and network applications.",CS,AI_ML,0.85,Extracted from log - paper 1188
Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams,"Local Differential Privacy (LDP) enables massive data collection and analysis while protecting end users' privacy against untrusted aggregators. It has been applied to various data types (e.g., categorical, numerical, and graph data) and application settings (e.g., static and streaming). Recent findings indicate that LDP protocols can be easily disrupted by poisoning or manipulation attacks, which leverage injected/corrupted fake users to send crafted data conforming to the LDP reports. However, current attacks primarily target static protocols, neglecting the security of LDP protocols in the streaming settings. Our research fills the gap by developing novel fine-grained manipulation attacks to LDP protocols for data streams. By reviewing the attack surfaces in existing algorithms, We introduce a unified attack framework with composable modules, which can manipulate the LDP estimated stream toward a target stream. Our attack framework can adapt to state-of-the-art streaming LDP algorithms with different analytic tasks (e.g., frequency and mean) and LDP models (event-level, user-level, w-event level). We validate our attacks theoretically and through extensive experiments on real-world datasets, and finally explore a possible defense mechanism for mitigating these attacks.",CS,AI_ML,0.85,Extracted from log - paper 1189
Watermark Overwriting Attack on StegaStamp algorithm,"This paper presents an attack method on the StegaStamp watermarking algorithm that completely removes watermarks from an image with minimal quality loss, developed as part of the NeurIPS ""Erasing the invisible"" competition.",CS,AI_ML,0.85,Extracted from log - paper 1190
Shuffling Cards When You Are of Very Little Brain: Low Memory Generation of Permutations,"How can we generate a permutation of the numbers $1$ through $n$ so that it is hard to guess the next element given the history so far? The twist is that the generator of the permutation (the ``Dealer"") has limited memory, while the ``Guesser"" has unlimited memory. With unbounded memory (actually $n$ bits suffice), the Dealer can generate a truly random permutation where~$\ln n$ is the expected number of correct guesses.   Our main results are tight bounds for the relationship between the guessing probability and the memory required to generate the permutation. We suggest a method for the Dealer that requires~$m$ bits of storage, constant time for each turn and makes any Guesser pick correctly only $O(n/m+\log m)$ cards in expectation. The method does not require any secrecy from the dealer, i.e. it is ``open book"" or ``whitebox"". On the other hand, we show that this bound is the best possible, even for Dealers with secret memory: For any $m$-bit Dealer there is a (computationally powerful) guesser that makes $\Omega(n/m+\log m)$ correct guesses in expectation.   We also give an $O(n)$ bit memory Dealer that generates perfectly random permutations and operates in constant time per turn.",CS,AI_ML,0.85,Extracted from log - paper 1191
PHSafe: Disclosure Avoidance for the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC),"This article describes the disclosure avoidance algorithm that the U.S. Census Bureau used to protect the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC). The tabulations contain statistics of counts of U.S. persons living in certain types of households, including averages. The article describes the PHSafe algorithm, which is based on adding noise drawn from a discrete Gaussian distribution to the statistics of interest. We prove that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy. We then describe how the algorithm was implemented on Tumult Analytics and briefly outline the parameterization and tuning of the algorithm.",CS,AI_ML,0.85,Extracted from log - paper 1192
SafeTab-H: Disclosure Avoidance for the 2020 Census Detailed Demographic and Housing Characteristics File B (Detailed DHC-B),"This article describes SafeTab-H, a disclosure avoidance algorithm applied to the release of the U.S. Census Bureau's Detailed Demographic and Housing Characteristics File B (Detailed DHC-B) as part of the 2020 Census. The tabulations contain household statistics about household type and tenure iterated by the householder's detailed race, ethnicity, or American Indian and Alaska Native tribe and village at varying levels of geography. We describe the algorithmic strategy which is based on adding noise from a discrete Gaussian distribution and show that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy. We discuss how the implementation of the SafeTab-H codebase relies on the Tumult Analytics privacy library. We also describe the theoretical expected error properties of the algorithm and explore various aspects of its parameter tuning.",CS,AI_ML,0.85,Extracted from log - paper 1193
SafeTab-P: Disclosure Avoidance for the 2020 Census Detailed Demographic and Housing Characteristics File A (Detailed DHC-A),"This article describes the disclosure avoidance algorithm that the U.S. Census Bureau used to protect the Detailed Demographic and Housing Characteristics File A (Detailed DHC-A) of the 2020 Census. The tabulations contain statistics (counts) of demographic characteristics of the entire population of the United States, crossed with detailed races and ethnicities at varying levels of geography. The article describes the SafeTab-P algorithm, which is based on adding noise drawn to statistics of interest from a discrete Gaussian distribution. A key innovation in SafeTab-P is the ability to adaptively choose how many statistics and at what granularity to release them, depending on the size of a population group. We prove that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy (zCDP). We then describe how the algorithm was implemented on Tumult Analytics and briefly outline the parameterization and tuning of the algorithm.",CS,AI_ML,0.85,Extracted from log - paper 1194
Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks,"Hierarchical Federated Learning (HFL) has recently emerged as a promising solution for intelligent decision-making in vehicular networks, helping to address challenges such as limited communication resources, high vehicle mobility, and data heterogeneity. However, HFL remains vulnerable to adversarial and unreliable vehicles, whose misleading updates can significantly compromise the integrity and convergence of the global model. To address these challenges, we propose a novel defense framework that integrates dynamic vehicle selection with robust anomaly detection within a cluster-based HFL architecture, specifically designed to counter Gaussian noise and gradient ascent attacks. The framework performs a comprehensive reliability assessment for each vehicle by evaluating historical accuracy, contribution frequency, and anomaly records. Anomaly detection combines Z-score and cosine similarity analyses on model updates to identify both statistical outliers and directional deviations in model updates. To further refine detection, an adaptive thresholding mechanism is incorporated into the cosine similarity metric, dynamically adjusting the threshold based on the historical accuracy of each vehicle to enforce stricter standards for consistently high-performing vehicles. In addition, a weighted gradient averaging mechanism is implemented, which assigns higher weights to gradient updates from more trustworthy vehicles. To defend against coordinated attacks, a cross-cluster consistency check is applied to identify collaborative attacks in which multiple compromised clusters coordinate misleading updates. Together, these mechanisms form a multi-level defense strategy to filter out malicious contributions effectively. Simulation results show that the proposed algorithm significantly reduces convergence time compared to benchmark methods across both 1-hop and 3-hop topologies.",CS,AI_ML,0.85,Extracted from log - paper 1195
"LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures","As large language models (LLMs) continue to evolve, it is critical to assess the security threats and vulnerabilities that may arise both during their training phase and after models have been deployed. This survey seeks to define and categorize the various attacks targeting LLMs, distinguishing between those that occur during the training phase and those that affect already trained models. A thorough analysis of these attacks is presented, alongside an exploration of defense mechanisms designed to mitigate such threats. Defenses are classified into two primary categories: prevention-based and detection-based defenses. Furthermore, our survey summarizes possible attacks and their corresponding defense strategies. It also provides an evaluation of the effectiveness of the known defense mechanisms for the different security threats. Our survey aims to offer a structured framework for securing LLMs, while also identifying areas that require further research to improve and strengthen defenses against emerging security challenges.",CS,AI_ML,0.85,Extracted from log - paper 1196
Active Sybil Attack and Efficient Defense Strategy in IPFS DHT,"The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P) storage that relies on Kademlia, a Distributed Hash Table (DHT) structure commonly used in P2P systems for its proved scalability. However, DHTs are known to be vulnerable to Sybil attacks, in which a single entity controls multiple malicious nodes. Recent studies have shown that IPFS is affected by a passive content eclipse attack, leveraging Sybils, in which adversarial nodes hide received indexed information from other peers, making the content appear unavailable. Fortunately, the latest mitigation strategy coupling an attack detection based on statistical tests and a wider publication strategy upon detection was able to circumvent it.   In this work, we present a new active attack, with malicious nodes responding with semantically correct but intentionally false data, exploiting both an optimized placement of Sybils to stay below the detection threshold and an early trigger of the content discovery termination in Kubo, the main IPFS implementation. Our attack achieves to completely eclipse content on the latest Kubo release. When evaluated against the most recent known mitigation, it successfully denies access to the target content in approximately 80\% of lookup attempts.   To address this vulnerability, we propose a new mitigation called SR-DHT-Store, which enables efficient, Sybil-resistant content publication without relying on attack detection but instead on a systematic and precise use of region-based queries, defined by a dynamically computed XOR distance to the target ID. SR-DHT-Store can be combined with other defense mechanisms resulting in a defense strategy that completely mitigates both passive and active Sybil attacks at a lower overhead, while allowing an incremental deployment.",CS,AI_ML,0.85,Extracted from log - paper 1197
Poster: Machine Learning for Vulnerability Detection as Target Oracle in Automated Fuzz Driver Generation,"In vulnerability detection, machine learning has been used as an effective static analysis technique, although it suffers from a significant rate of false positives. Contextually, in vulnerability discovery, fuzzing has been used as an effective dynamic analysis technique, although it requires manually writing fuzz drivers. Fuzz drivers usually target a limited subset of functions in a library that must be chosen according to certain criteria, e.g., the depth of a function, the number of paths. These criteria are verified by components called target oracles. In this work, we propose an automated fuzz driver generation workflow composed of: (1) identifying a likely vulnerable function by leveraging a machine learning for vulnerability detection model as a target oracle, (2) automatically generating fuzz drivers, (3) fuzzing the target function to find bugs which could confirm the vulnerability inferred by the target oracle. We show our method on an existing vulnerability in libgd, with a plan for large-scale evaluation.",CS,AI_ML,0.85,Extracted from log - paper 1198
A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories,"Recent advancements in large language models (LLMs) have spurred the development of diverse AI applications from code generation and video editing to text generation; however, AI supply chains such as Hugging Face, which host pretrained models and their associated configuration files contributed by the public, face significant security challenges; in particular, configuration files originally intended to set up models by specifying parameters and initial settings can be exploited to execute unauthorized code, yet research has largely overlooked their security compared to that of the models themselves; in this work, we present the first comprehensive study of malicious configurations on Hugging Face, identifying three attack scenarios (file, website, and repository operations) that expose inherent risks; to address these threats, we introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in the context of their associated runtime code and critical libraries, effectively detecting suspicious elements with low false positive rates and high accuracy; our extensive evaluation uncovers thousands of suspicious repositories and configuration files, underscoring the urgent need for enhanced security validation in AI model hosting platforms.",CS,AI_ML,0.85,Extracted from log - paper 1199
Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation,"Large Language Models (LLMs) have demonstrated remarkable capabilities in code-related tasks, raising concerns about their potential for automated exploit generation (AEG). This paper presents the first systematic study on LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical proficiency. To mitigate dataset bias, we introduce a benchmark with refactored versions of five software security labs. Additionally, we design an LLM-based attacker to systematically prompt LLMs for exploit generation. Our experiments reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to uncensored models, while Llama3 is the most resistant. However, no model successfully generates exploits for refactored labs, though GPT-4o's minimal errors highlight the potential for LLM-driven AEG advancements.",CS,AI_ML,0.85,Extracted from log - paper 1200
Capability-Based Multi-Tenant Access Management in Crowdsourced Drone Services,"We propose a capability-based access control method that leverages OAuth 2.0 and Verifiable Credentials (VCs) to share resources in crowdsourced drone services. VCs securely encode claims about entities, offering flexibility. However, standardized protocols for VCs are lacking, limiting their adoption. To address this, we integrate VCs into OAuth 2.0, creating a novel access token. This token encapsulates VCs using JSON Web Tokens (JWT) and employs JWT-based methods for proof of possession. Our method streamlines VC verification with JSON Web Signatures (JWS) requires only minor adjustments to current OAuth 2.0 systems. Furthermore, in order to increase security and efficiency in multi-tenant environments, we provide a novel protocol for VC creation that makes use of the OAuth 2.0 client credentials grant. Using VCs as access tokens enhances OAuth 2.0, supporting long-term use and efficient data management. This system aids bushfire management authorities by ensuring high availability, enhanced privacy, and improved data portability. It supports multi-tenancy, allowing drone operators to control data access policies in a decentralized environment.",CS,AI_ML,0.85,Extracted from log - paper 1201
Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory,"Security vulnerabilities in Windows Active Directory (AD) systems are typically modeled using an attack graph and hardening AD systems involves an iterative workflow: security teams propose an edge to remove, and IT operations teams manually review these fixes before implementing the removal. As verification requires significant manual effort, we formulate an Adaptive Path Removal Problem to minimize the number of steps in this iterative removal process. In our model, a wizard proposes an attack path in each step and presents it as a set of multiple-choice options to the IT admin. The IT admin then selects one edge from the proposed set to remove. This process continues until the target $t$ is disconnected from source $s$ or the number of proposed paths reaches $B$. The model aims to optimize the human effort by minimizing the expected number of interactions between the IT admin and the security wizard. We first prove that the problem is $\mathcal{\#P}$-hard. We then propose a set of solutions including an exact algorithm, an approximate algorithm, and several scalable heuristics. Our best heuristic, called DPR, can operate effectively on larger-scale graphs compared to the exact algorithm and consistently outperforms the approximate algorithm across all graphs. We verify the effectiveness of our algorithms on several synthetic AD graphs and an AD attack graph collected from a real organization.",CS,AI_ML,0.85,Extracted from log - paper 1202
Quantum Support Vector Regression for Robust Anomaly Detection,"Anomaly Detection (AD) is critical in data analysis, particularly within the domain of IT security. In recent years, Machine Learning (ML) algorithms have emerged as a powerful tool for AD in large-scale data. In this study, we explore the potential of quantum ML approaches, specifically quantum kernel methods, for the application to robust AD. We build upon previous work on Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a comprehensive benchmark on IBM quantum hardware using eleven datasets. Our results demonstrate that QSVR achieves strong classification performance and even outperforms the noiseless simulation on two of these datasets. Moreover, we investigate the influence of - in the NISQ-era inevitable - quantum noise on the performance of the QSVR. Our findings reveal that the model exhibits robustness to depolarizing, phase damping, phase flip, and bit flip noise, while amplitude damping and miscalibration noise prove to be more disruptive. Finally, we explore the domain of Quantum Adversarial Machine Learning and demonstrate that QSVR is highly vulnerable to adversarial attacks and that noise does not improve the adversarial robustness of the model.",CS,AI_ML,0.85,Extracted from log - paper 1203
A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts,"Generating high-quality steganographic text is a fundamental challenge in the field of generative linguistic steganography. This challenge arises primarily from two aspects: firstly, the capabilities of existing models in text generation are limited; secondly, embedding algorithms fail to effectively mitigate the negative impacts of sensitive information's properties, such as semantic content or randomness. Specifically, to ensure that the recipient can accurately extract hidden information, embedding algorithms often have to consider selecting candidate words with relatively low probabilities. This phenomenon leads to a decrease in the number of high-probability candidate words and an increase in low-probability candidate words, thereby compromising the semantic coherence and logical fluency of the steganographic text and diminishing the overall quality of the generated steganographic material. To address this issue, this paper proposes a novel embedding algorithm, character-based diffusion embedding algorithm (CDEA). Unlike existing embedding algorithms that strive to eliminate the impact of sensitive information's properties on the generation process, CDEA leverages sensitive information's properties. It enhances the selection frequency of high-probability candidate words in the candidate pool based on general statistical properties at the character level and grouping methods based on power-law distributions, while reducing the selection frequency of low-probability candidate words in the candidate pool. Furthermore, to ensure the effective transformation of sensitive information in long sequences, we also introduce the XLNet model. Experimental results demonstrate that the combination of CDEA and XLNet significantly improves the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility.",CS,AI_ML,0.85,Extracted from log - paper 1204
Attack and defense techniques in large language models: A survey and new perspectives,"Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.",CS,AI_ML,0.85,Extracted from log - paper 1205
Preserving Privacy and Utility in LLM-Based Product Recommendations,"Large Language Model (LLM)-based recommendation systems leverage powerful language models to generate personalized suggestions by processing user interactions and preferences. Unlike traditional recommendation systems that rely on structured data and collaborative filtering, LLM-based models process textual and contextual information, often using cloud-based infrastructure. This raises privacy concerns, as user data is transmitted to remote servers, increasing the risk of exposure and reducing control over personal information. To address this, we propose a hybrid privacy-preserving recommendation framework which separates sensitive from nonsensitive data and only shares the latter with the cloud to harness LLM-powered recommendations. To restore lost recommendations related to obfuscated sensitive data, we design a de-obfuscation module that reconstructs sensitive recommendations locally. Experiments on real-world e-commerce datasets show that our framework achieves almost the same recommendation utility with a system which shares all data with an LLM, while preserving privacy to a large extend. Compared to obfuscation-only techniques, our approach improves HR@10 scores and category distribution alignment, offering a better balance between privacy and recommendation quality. Furthermore, our method runs efficiently on consumer-grade hardware, making privacy-aware LLM-based recommendation systems practical for real-world use.",CS,AI_ML,0.85,Extracted from log - paper 1206
Addressing Noise and Stochasticity in Fraud Detection for Service Networks,"Fraud detection is crucial in social service networks to maintain user trust and improve service network security. Existing spectral graph-based methods address this challenge by leveraging different graph filters to capture signals with different frequencies in service networks. However, most graph filter-based methods struggle with deriving clean and discriminative graph signals. On the one hand, they overlook the noise in the information propagation process, resulting in degradation of filtering ability. On the other hand, they fail to discriminate the frequency-specific characteristics of graph signals, leading to distortion of signals fusion. To address these issues, we develop a novel spectral graph network based on information bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB splits the original graph into homophilic and heterophilic subgraphs to better capture the signals at different frequencies. For the first limitation, SGNN-IB applies information bottleneck theory to extract key characteristics of encoded representations. For the second limitation, SGNN-IB introduces prototype learning to implement signal fusion, preserving the frequency-specific characteristics of signals. Extensive experiments on three real-world datasets demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.",CS,AI_ML,0.85,Extracted from log - paper 1207
Non-Adaptive Cryptanalytic Time-Space Lower Bounds via a Shearer-like Inequality for Permutations,"The power of adaptivity in algorithms has been intensively studied in diverse areas of theoretical computer science. In this paper, we obtain a number of sharp lower bound results which show that adaptivity provides a significant extra power in cryptanalytic time-space tradeoffs with (possibly unlimited) preprocessing time.   Most notably, we consider the discrete logarithm (DLOG) problem in a generic group of $N$ elements. The classical `baby-step giant-step' algorithm for the problem has time complexity $T=O(\sqrt{N})$, uses $O(\sqrt{N})$ bits of space (up to logarithmic factors in $N$) and achieves constant success probability.   We examine a generalized setting where an algorithm obtains an advice string of $S$ bits and is allowed to make $T$ arbitrary non-adaptive queries that depend on the advice string (but not on the challenge group element).   We show that in this setting, the $T=O(\sqrt{N})$ online time complexity of the baby-step giant-step algorithm cannot be improved, unless the advice string is more than $\Omega(\sqrt{N})$ bits long. This lies in stark contrast with the classical adaptive Pollard's rho algorithm for DLOG, which can exploit preprocessing to obtain the tradeoff curve $ST^2=O(N)$. We obtain similar sharp lower bounds for several other cryptanalytic problems.   To obtain our results, we present a new model that allows analyzing non-adaptive preprocessing algorithms for a wide array of search and decision problems in a unified way. Since previous proof techniques inherently cannot distinguish between adaptive and non-adaptive algorithms for the problems in our model, they cannot be used to obtain our results. Consequently, our proof uses a variant of Shearer's lemma for this setting, due to Barthe, Cordero-Erausquin, Ledoux, and Maurey (2011). This seems to be the first time a variant of Shearer's lemma for permutations is used in an algorithmic context.",CS,AI_ML,0.85,Extracted from log - paper 1208
Balancing Security and Liquidity: A Time-Weighted Snapshot Framework for DAO Governance Voting,"As new project upgrading the blockchain industry, novel forms of attack challenges developers to rethink about the design of their innovations. In the growth stage of the development, Decentralized Autonomous Organizations (DAO) introduces different approaches in managing fund through voting in governance tokens. However, relying on tokens as a weight for voting introduces opportunities for hackers to manipulate voting results through flash loan, allowing malicious proposals - fund withdrawal from DAO to hacker's wallet - to execute through the smart contract. In this research, we learned different defense mechanism against the flash loan attack, and their weakness in accessibility that compromise the security of different blockchain projects. Based on our observation, we propose a new defensing structure and apply it with cases.",CS,AI_ML,0.85,Extracted from log - paper 1209
Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting,"While supervised deep neural networks (DNNs) have proven effective for device authentication via radio frequency (RF) fingerprinting, they are hindered by domain shift issues and the scarcity of labeled data. The success of large language models has led to increased interest in unsupervised pre-trained models (PTMs), which offer better generalization and do not require labeled datasets, potentially addressing the issues mentioned above. However, the inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently explored. In this paper, we thoroughly investigate data-free backdoor attacks on such PTMs in RF fingerprinting, focusing on a practical scenario where attackers lack access to downstream data, label information, and training processes. To realize the backdoor attack, we carefully design a set of triggers and predefined output representations (PORs) for the PTMs. By mapping triggers and PORs through backdoor training, we can implant backdoor behaviors into the PTMs, thereby introducing vulnerabilities across different downstream RF fingerprinting tasks without requiring prior knowledge. Extensive experiments demonstrate the wide applicability of our proposed attack to various input domains, protocols, and PTMs. Furthermore, we explore potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack.",CS,AI_ML,0.85,Extracted from log - paper 1210
Duality on the Thermodynamics of the Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Scheme,"This study investigates a duality approach to information leak detection in the generalized Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme. While previous work by Chamon and Kish sampled voltages at zero-current instances, this research explores sampling currents at zero-voltage crossings. The objective is to determine if this dual approach can reveal information leaks in non-equilibrium KLJN systems. Results indicate that the duality method successfully detects information leaks, further supporting the necessity of thermal equilibrium for unconditional security in KLJN systems.",CS,AI_ML,0.85,Extracted from log - paper 1211
TherMod Communication: Low Power or Hot Air?,"The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme leverages statistical physics to enable secure communication with zero average power flow in a wired channel. While the original KLJN scheme requires significant power for operation, a recent wireless modification, TherMod, proposed by Basar claims a ""low power"" implementation. This paper critically examines this claim. We explain that the additional components inherent in Basar's wireless adaptation substantially increase power consumption, rendering the ""low power"" assertion inappropriate. Furthermore, we clarify that the security claims of the original KLJN scheme do not directly translate to this wireless adaptation, implying significant security breach. Finally, the scheme looks identical one of the stealth communicators from 2005, which was shown not to be secure.",CS,AI_ML,0.85,Extracted from log - paper 1212
OET: Optimization-based prompt injection Evaluation Toolkit,"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, enabling their widespread adoption across various domains. However, their susceptibility to prompt injection attacks poses significant security risks, as adversarial inputs can manipulate model behavior and override intended instructions. Despite numerous defense strategies, a standardized framework to rigorously evaluate their effectiveness, especially under adaptive adversarial scenarios, is lacking. To address this gap, we introduce OET, an optimization-based evaluation toolkit that systematically benchmarks prompt injection attacks and defenses across diverse datasets using an adaptive testing framework. Our toolkit features a modular workflow that facilitates adversarial string generation, dynamic attack execution, and comprehensive result analysis, offering a unified platform for assessing adversarial robustness. Crucially, the adaptive testing framework leverages optimization methods with both white-box and black-box access to generate worst-case adversarial examples, thereby enabling strict red-teaming evaluations. Extensive experiments underscore the limitations of current defense mechanisms, with some models remaining susceptible even after implementing security enhancements.",CS,AI_ML,0.85,Extracted from log - paper 1213
From Texts to Shields: Convergence of Large Language Models and Cybersecurity,"This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 1214
Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models,"Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches.   A significant challenge is the massive size of LLMs, which, by nature of their compute intensive operation, quickly evicts embedding vectors from the cache. We address this by balancing the number of tokens monitored against the amount of information leaked. Monitoring more tokens increases potential vocabulary leakage but raises the chance of missing cache hits due to eviction; monitoring fewer tokens improves detection reliability but limits vocabulary coverage.   Through extensive experimentation, we demonstrate the feasibility of leaking tokens from LLMs via cache side-channels. Our findings reveal a new vulnerability in LLM deployments, highlighting that even sophisticated models are susceptible to traditional side-channel attacks. We discuss the implications for privacy and security in LLM-serving infrastructures and suggest considerations for mitigating such threats. For proof of concept we consider two concrete attack scenarios: Our experiments show that an attacker can recover as much as 80%-90% of a high entropy API key with single shot monitoring. As for English text we can reach a 40% recovery rate with a single shot. We should note that the rate highly depends on the monitored token set and these rates can be improved by targeting more specialized output domains.",CS,AI_ML,0.85,Extracted from log - paper 1215
Enhancing the Cloud Security through Topic Modelling,"Protecting cloud applications is crucial in an age where security constantly threatens the digital world. The inevitable cyber-attacks throughout the CI/CD pipeline make cloud security innovations necessary. This research is motivated by applying Natural Language Processing (NLP) methodologies, such as Topic Modelling, to analyse cloud security data and predict future attacks. This research aims to use topic modelling, specifically Latent Dirichlet Allocation (LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and PLSA, security-related text data, such as reports, logs, and other relevant documents, will be analysed and sorted into relevant topics (such as phishing or encryption). These algorithms may apply through Python using the Gensim framework. The topics shall be utilised to detect vulnerabilities within relevant CI/CD pipeline records or log data. This application of Topic Modelling anticipates providing a new form of vulnerability detection, improving overall security throughout the CI/CD pipeline.",CS,AI_ML,0.85,Extracted from log - paper 1216
Auditing without Leaks Despite Curiosity,"\textit{Auditing} data accesses helps preserve privacy and ensures accountability by allowing one to determine who accessed (potentially sensitive) information. A prior formal definition of register auditability was based on the values returned by read operations, \emph{without accounting for cases where a reader might learn a value without explicitly reading it or gain knowledge of data access without being an auditor}.   This paper introduces a refined definition of auditability that focuses on when a read operation is \emph{effective}, rather than relying on its completion and return of a value. Furthermore, we formally specify the constraints that \textit{prevent readers from learning values they did not explicitly read or from auditing other readers' accesses.}   Our primary algorithmic contribution is a wait-free implementation of a \emph{multi-writer, multi-reader register} that tracks effective reads while preventing unauthorized audits. The key challenge is ensuring that a read is auditable as soon as it becomes effective, which we achieve by combining value access and access logging into a single atomic operation. Another challenge is recording accesses without exposing them to readers, which we address using a simple encryption technique (one-time pad).   We extend this implementation to an \emph{auditable max register} that tracks the largest value ever written. The implementation deals with the additional challenge posed by the max register semantics, which allows readers to learn prior values without reading them.   The max register, in turn, serves as the foundation for implementing an \emph{auditable snapshot} object and, more generally, \emph{versioned types}. These extensions maintain the strengthened notion of auditability, appropriately adapted from multi-writer, multi-reader registers.",CS,AI_ML,0.85,Extracted from log - paper 1217
Key exchange protocol based on circulant matrix action over congruence-simple semiring,"We present a new key exchange protocol based on circulant matrices acting on matrices over a congruence-simple semiring. We describe how to compute matrices with the necessary properties for the implementation of the protocol. Additionally, we provide an analysis of its computational cost and its security against known attacks.",CS,AI_ML,0.85,Extracted from log - paper 1218
RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks,"Network attackers have increasingly resorted to proxy chains, VPNs, and anonymity networks to conceal their activities. To tackle this issue, past research has explored the applicability of traffic correlation techniques to perform attack attribution, i.e., to identify an attacker's true network location. However, current traffic correlation approaches rely on well-provisioned and centralized systems that ingest flows from multiple network probes to compute correlation scores. Unfortunately, this makes correlation efforts scale poorly for large high-speed networks.   In this paper, we propose RevealNet, a decentralized framework for attack attribution that orchestrates a fleet of P4-programmable switches to perform traffic correlation. RevealNet builds on a set of correlation primitives inspired by prior work on computing and comparing flow sketches -- compact summaries of flows' key characteristics -- to enable efficient, distributed, in-network traffic correlation. Our evaluation suggests that RevealNet achieves comparable accuracy to centralized attack attribution systems while significantly reducing both the computational complexity and bandwidth overheads imposed by correlation tasks.",CS,AI_ML,0.85,Extracted from log - paper 1219
A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks,"The security of image data in the Internet of Things (IoT) and edge networks is crucial due to the increasing deployment of intelligent systems for real-time decision-making. Traditional encryption algorithms such as AES and RSA are computationally expensive for resource-constrained IoT devices and ineffective for large-volume image data, leading to inefficiencies in privacy-preserving distributed learning applications. To address these concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic Chain Permutation and Confusion mechanisms to enhance security while maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS, which extracts and reorganizes pixels based on high and low edge intensity features for correlation disruption; (2) Chaotic Chain Permutation, which employs a logistic chaotic map with SHA-256-based dynamically updated keys for block-wise permutation; and (3) Chaotic chain Confusion, which utilises dynamically generated chaotic seed matrices for bitwise XOR operations. Extensive security and performance evaluations demonstrate that the proposed scheme significantly reduces pixel correlation -- almost zero, achieves high entropy values close to 8, and resists differential cryptographic attacks. The optimum design of the proposed scheme makes it suitable for real-time deployment in resource-constrained environments.",CS,AI_ML,0.85,Extracted from log - paper 1220
Notes on Univariate Sumcheck,These notes describe an adaptation of the multivariate sumcheck protocol to univariate polynomials interpolated over roots of unity.,CS,AI_ML,0.85,Extracted from log - paper 1221
Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks,"This article describes the process of creating a script and conducting an analytical study of a dataset using the DeepMIMO emulator. An advertorial attack was carried out using the FGSM method to maximize the gradient. A comparison is made of the effectiveness of binary classifiers in the task of detecting distorted data. The dynamics of changes in the quality indicators of the regression model were analyzed in conditions without adversarial attacks, during an adversarial attack and when the distorted data was isolated. It is shown that an adversarial FGSM attack with gradient maximization leads to an increase in the value of the MSE metric by 33% and a decrease in the R2 indicator by 10% on average. The LightGBM binary classifier effectively identifies data with adversarial anomalies with 98% accuracy. Regression machine learning models are susceptible to adversarial attacks, but rapid analysis of network traffic and data transmitted over the network makes it possible to identify malicious activity",CS,AI_ML,0.85,Extracted from log - paper 1222
Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services,"Due to the increasing number of tasks that are solved on remote servers, identifying and classifying traffic is an important task to reduce the load on the server. There are various methods for classifying traffic. This paper discusses machine learning models for solving this problem. However, such ML models are also subject to attacks that affect the classification result of network traffic. To protect models, we proposed a solution based on an autoencoder",CS,AI_ML,0.85,Extracted from log - paper 1223
"Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management","This paper proposes a decentralized, blockchain-based system for the publication of Common Vulnerabilities and Exposures (CVEs), aiming to mitigate the limitations of the current centralized model primarily overseen by MITRE. The proposed architecture leverages a permissioned blockchain, wherein only authenticated CVE Numbering Authorities (CNAs) are authorized to submit entries. This ensures controlled write access while preserving public transparency. By incorporating smart contracts, the system supports key features such as embargoed disclosures and decentralized governance. We evaluate the proposed model in comparison with existing practices, highlighting its advantages in transparency, trust decentralization, and auditability. A prototype implementation using Hyperledger Fabric is presented to demonstrate the feasibility of the approach, along with a discussion of its implications for the future of vulnerability disclosure.",CS,AI_ML,0.85,Extracted from log - paper 1224
HoneyWin: High-Interaction Windows Honeypot in Enterprise Environment,"Windows operating systems (OS) are ubiquitous in enterprise Information Technology (IT) and operational technology (OT) environments. Due to their widespread adoption and known vulnerabilities, they are often the primary targets of malware and ransomware attacks. With 93% of the ransomware targeting Windows-based systems, there is an urgent need for advanced defensive mechanisms to detect, analyze, and mitigate threats effectively. In this paper, we propose HoneyWin a high-interaction Windows honeypot that mimics an enterprise IT environment. The HoneyWin consists of three Windows 11 endpoints and an enterprise-grade gateway provisioned with comprehensive network traffic capturing, host-based logging, deceptive tokens, endpoint security and real-time alerts capabilities. The HoneyWin has been deployed live in the wild for 34 days and receives more than 5.79 million unsolicited connections, 1.24 million login attempts, 5 and 354 successful logins via remote desktop protocol (RDP) and secure shell (SSH) respectively. The adversary interacted with the deceptive token in one of the RDP sessions and exploited the public-facing endpoint to initiate the Simple Mail Transfer Protocol (SMTP) brute-force bot attack via SSH sessions. The adversary successfully harvested 1,250 SMTP credentials after attempting 151,179 credentials during the attack.",CS,AI_ML,0.85,Extracted from log - paper 1225
Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication,"Secure and reliable communications are crucial for Intelligent Transportation Systems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key role in enabling mobility-enhancing and safety-critical services. Current V2I authentication relies on credential-based methods over wireless Non-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation and proximity attacks. To mitigate these risks, we propose a unified Multi-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS cryptographic credentials with a Line-of-Sight (LOS) visual channel. Our approach leverages a challenge-response security paradigm: the infrastructure issues challenges and the vehicle's headlights respond by flashing a structured sequence containing encoded security data. Deep learning models on the infrastructure side then decode the embedded information to authenticate the vehicle. Real-world experimental evaluations demonstrate high test accuracy, reaching an average of 95% and 96.6%, respectively, under various lighting, weather, speed, and distance conditions. Additionally, we conducted extensive experiments on three state-of-the-art deep learning models, including detailed ablation studies for decoding the flashing sequence. Our results indicate that the optimal architecture employs a dual-channel design, enabling simultaneous decoding of the flashing sequence and extraction of vehicle spatial and locational features for robust authentication.",CS,AI_ML,0.85,Extracted from log - paper 1226
PatchFuzz: Patch Fuzzing for JavaScript Engines,"Patch fuzzing is a technique aimed at identifying vulnerabilities that arise from newly patched code. While researchers have made efforts to apply patch fuzzing to testing JavaScript engines with considerable success, these efforts have been limited to using ordinary test cases or publicly available vulnerability PoCs (Proof of Concepts) as seeds, and the sustainability of these approaches is hindered by the challenges associated with automating the PoC collection. To address these limitations, we propose an end-to-end sustainable approach for JavaScript engine patch fuzzing, named PatchFuzz. It automates the collection of PoCs of a broader range of historical vulnerabilities and leverages both the PoCs and their corresponding patches to uncover new vulnerabilities more effectively. PatchFuzz starts by recognizing git commits which intend to fix security bugs. Subsequently, it extracts and processes PoCs from these commits to form the seeds for fuzzing, while utilizing code revisions to focus limited fuzzing resources on the more vulnerable code areas through selective instrumentation. The mutation strategy of PatchFuzz is also optimized to maximize the potential of the PoCs. Experimental results demonstrate the effectiveness of PatchFuzz. Notably, 54 bugs across six popular JavaScript engines have been exposed and a total of $62,500 bounties has been received.",CS,AI_ML,0.85,Extracted from log - paper 1227
Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation,"The sharing of external data has become a strong demand of financial institutions, but the privacy issue has led to the difficulty of interconnecting different platforms and the low degree of data openness. To effectively solve the privacy problem of financial data in trans-border flow and sharing, to ensure that the data is available but not visible, to realize the joint portrait of all kinds of heterogeneous data of business organizations in different industries, we propose a Heterogeneous Federated Graph Neural Network (HFGNN) approach. In this method, the distribution of heterogeneous business data of trans-border organizations is taken as subgraphs, and the sharing and circulation process among subgraphs is constructed as a statistically heterogeneous global graph through a central server. Each subgraph learns the corresponding personalized service model through local training to select and update the relevant subset of subgraphs with aggregated parameters, and effectively separates and combines topological and feature information among subgraphs. Finally, our simulation experimental results show that the proposed method has higher accuracy performance and faster convergence speed than existing methods.",CS,AI_ML,0.85,Extracted from log - paper 1228
LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems,"The increasing complexity and scale of the Internet of Things (IoT) have made security a critical concern. This paper presents a novel Large Language Model (LLM)-based framework for comprehensive threat detection and prevention in IoT environments. The system integrates lightweight LLMs fine-tuned on IoT-specific datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated, context-aware mitigation strategies optimized for resource-constrained devices. A modular Docker-based deployment enables scalable and reproducible evaluation across diverse network conditions. Experimental results in simulated IoT environments demonstrate significant improvements in detection accuracy, response latency, and resource efficiency over traditional security methods. The proposed framework highlights the potential of LLM-driven, autonomous security solutions for future IoT ecosystems.",CS,AI_ML,0.85,Extracted from log - paper 1229
Moral Testing of Autonomous Driving Systems,"Autonomous Driving System (ADS) testing plays a crucial role in their development, with the current focus primarily on functional and safety testing. However, evaluating the non-functional morality of ADSs, particularly their decision-making capabilities in unavoidable collision scenarios, is equally important to ensure the systems' trustworthiness and public acceptance. Unfortunately, testing ADS morality is nearly impossible due to the absence of universal moral principles. To address this challenge, this paper first extracts a set of moral meta-principles derived from existing moral experiments and well-established social science theories, aiming to capture widely recognized and common-sense moral values for ADSs. These meta-principles are then formalized as quantitative moral metamorphic relations, which act as the test oracle. Furthermore, we propose a metamorphic testing framework to systematically identify potential moral issues. Finally, we illustrate the implementation of the framework and present typical violation cases using the VIRES VTD simulator and its built-in ADS.",CS,AI_ML,0.85,Extracted from log - paper 1230
ATRAF-driven IMRaD Methodology: Tradeoff and Risk Analysis of Software Architectures Across Abstraction Levels,"Software architecture research relies on key architectural artifacts -- Software Architectures, Reference Architectures, and Architectural Frameworks -- that underpin the design and analysis of complex systems. Evaluating these artifacts is essential to assess tradeoffs and risks affecting quality attributes such as performance, modifiability, and security. Although methodologies like the Architecture Tradeoff Analysis Method (ATAM) support software architecture evaluation, their industrial focus misaligns with the IMRaD (Introduction, Methods, Results, Discussion) format prevalent in academic research, impeding transparency and reproducibility. Our prior work introduced the Architecture Tradeoff and Risk Analysis Framework (ATRAF), extending ATAM through three methods -- ATRAM, RATRAM, and AFTRAM, addressing all abstraction levels, using a unified, iterative four-phase spiral model. These phases -- Scenario and Requirements Gathering, Architectural Views and Scenario Realization, Attribute-Specific Analyses, and Sensitivity, Tradeoff, and Risk Analysis -- ensure traceability and coherence. This paper presents the ATRAF-driven IMRaD Methodology, a concise method to align ATRAF's phases with IMRaD sections. This methodology enhances the rigor, transparency, and accessibility of software architecture research, enabling systematic reporting of complex evaluations.",CS,AI_ML,0.85,Extracted from log - paper 1231
Qimax: Efficient quantum simulation via GPU-accelerated extended stabilizer formalism,"Simulating Clifford and near-Clifford circuits using the extended stabilizer formalism has become increasingly popular, particularly in quantum error correction. Compared to the state-vector approach, the extended stabilizer formalism can solve the same problems with fewer computational resources, as it operates on stabilizers rather than full state vectors. Most existing studies on near-Clifford circuits focus on balancing the trade-off between the number of ancilla qubits and simulation accuracy, often overlooking performance considerations. Furthermore, in the presence of high-rank stabilizers, performance is limited by the sequential property of the stabilizer formalism. In this work, we introduce a parallelized version of the extended stabilizer formalism, enabling efficient execution on multi-core devices such as GPU. Experimental results demonstrate that, in certain scenarios, our Python-based implementation outperforms state-of-the-art simulators such as Qiskit and Pennylane.",CS,AI_ML,0.85,Extracted from log - paper 1232
Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis,"The field of deep learning has witnessed significant breakthroughs, spanning various applications, and fundamentally transforming current software capabilities. However, alongside these advancements, there have been increasing concerns about reproducing the results of these deep learning methods. This is significant because reproducibility is the foundation of reliability and validity in software development, particularly in the rapidly evolving domain of deep learning. The difficulty of reproducibility may arise due to several reasons, including having differences from the original execution environment, incompatible software libraries, proprietary data and source code, lack of transparency, and the stochastic nature in some software. A study conducted by the Nature journal reveals that more than 70% of researchers failed to reproduce other researchers experiments and over 50% failed to reproduce their own experiments. Irreproducibility of deep learning poses significant challenges for researchers and practitioners. To address these concerns, this paper presents a systematic approach at analyzing and improving the reproducibility of deep learning models by demonstrating these guidelines using a case study. We illustrate the patterns and anti-patterns involved with these guidelines for improving the reproducibility of deep learning models. These guidelines encompass establishing a methodology to replicate the original software environment, implementing end-to-end training and testing algorithms, disclosing architectural designs, and enhancing transparency in data processing and training pipelines. We also conduct a sensitivity analysis to understand the model performance across diverse conditions. By implementing these strategies, we aim to bridge the gap between research and practice, so that innovations in deep learning can be effectively reproduced and deployed within software.",CS,AI_ML,0.85,Extracted from log - paper 1233
An Empirical Study on the Impact of Gender Diversity on Code Quality in AI Systems,"The rapid advancement of AI systems necessitates high-quality, sustainable code to ensure reliability and mitigate risks such as bias and technical debt. However, the underrepresentation of women in software engineering raises concerns about homogeneity in AI development. Studying gender diversity in AI systems is crucial, as diverse perspectives are essential for improving system robustness, reducing bias, and enhancing overall code quality. While prior research has demonstrated the benefits of diversity in general software teams, its specific impact on the code quality of AI systems remains unexplored. This study addresses this gap by examining how gender diversity within AI teams influences project popularity, code quality, and individual contributions. Our study makes three key contributions. First, we analyzed the relationship between team diversity and repository popularity, revealing that diverse AI repositories not only differ significantly from non-diverse ones but also achieve higher popularity and greater community engagement. Second, we explored the effect of diversity on the overall code quality of AI systems and found that diverse repositories tend to have superior code quality compared to non-diverse ones. Finally, our analysis of individual contributions revealed that although female contributors contribute to a smaller proportion of the total code, their contributions demonstrate consistently higher quality than those of their male counterparts. These findings highlight the need to remove barriers to female participation in AI development, as greater diversity can improve the overall quality of AI systems.",CS,AI_ML,0.85,Extracted from log - paper 1234
Testing SSD Firmware with State Data-Aware Fuzzing: Accelerating Coverage in Nondeterministic I/O Environments,"Solid-State Drive (SSD) firmware manages complex internal states, including flash memory maintenance. Due to nondeterministic I/O operations, traditional testing methods struggle to rapidly achieve coverage of firmware code areas that require extensive I/O accumulation. To address this challenge, we propose a state data-aware fuzzing approach that leverages SSD firmware's internal state to guide input generation under nondeterministic I/O conditions and accelerate coverage discovery. Our experiments with an open-source SSD firmware emulator show that the proposed method achieves the same firmware test coverage as a state-of-the-art coverage-based fuzzer (AFL++) while requiring approximately 67% fewer commands, without reducing the number of crashes or hangs detected. Moreover, we extend our experiments by incorporating various I/O commands beyond basic write/read operations to reflect real user scenarios, and we confirm that our strategy remains effective even for multiple types of I/O tests. We further validate the effectiveness of state data-aware fuzzing for firmware testing under I/O environments and suggest that this approach can be extended to other storage firmware or threshold-based embedded systems in the future.",CS,AI_ML,0.85,Extracted from log - paper 1235
Can We Recycle Our Old Models? An Empirical Evaluation of Model Selection Mechanisms for AIOps Solutions,"AIOps (Artificial Intelligence for IT Operations) solutions leverage the tremendous amount of data produced during the operation of large-scale systems and machine learning models to assist software practitioners in their system operations. Existing AIOps solutions usually maintain AIOps models against concept drift through periodical retraining, despite leaving a pile of discarded historical models that may perform well on specific future data. Other prior works propose dynamically selecting models for prediction tasks from a set of candidate models to optimize the model performance. However, there is no prior work in the AIOps area that assesses the use of model selection mechanisms on historical models to improve model performance or robustness. To fill the gap, we evaluate several model selection mechanisms by assessing their capabilities in selecting the optimal AIOps models that were built in the past to make predictions for the target data. We performed a case study on three large-scale public operation datasets: two trace datasets from the cloud computing platforms of Google and Alibaba, and one disk stats dataset from the BackBlaze cloud storage data center. We observe that the model selection mechnisms utilizing temporal adjacency tend to have a better performance and can prevail the periodical retraining approach. Our findings also highlight a performance gap between existing model selection mechnisms and the theoretical upper bound which may motivate future researchers and practitioners in investigating more efficient and effective model selection mechanisms that fit in the context of AIOps.",CS,AI_ML,0.85,Extracted from log - paper 1236
A Unifying Framework to Enable Artificial Intelligence in High Performance Computing Workflows,"Current trends point to a future where large-scale scientific applications are tightly-coupled HPC/AI hybrids. Hence, we urgently need to invest in creating a seamless, scalable framework where HPC and AI/ML can efficiently work together and adapt to novel hardware and vendor libraries without starting from scratch every few years. The current ecosystem and sparsely-connected community are not sufficient to tackle these challenges, and we require a breakthrough catalyst for science similar to what PyTorch enabled for AI.",CS,AI_ML,0.85,Extracted from log - paper 1237
Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for Automated Patch Correctness Assessment,"Automated program repair (APR) aims to automatically repair program errors without human intervention, and recent years have witnessed a growing interest on this research topic. While much progress has been made and techniques originating from different disciplines have been proposed, APR techniques generally suffer from the patch overfitting issue, i.e., the generated patches are not genuinely correct despite they pass the employed tests. To alleviate this issue, many research efforts have been devoted for automated patch correctness assessment (APCA). In particular, with the emergence of large language model (LLM) technology, researchers have employed LLM to assess the patch correctness and have obtained the state-of-the-art performance. The literature on APCA has demonstrated the importance of capturing patch semantic and explicitly considering certain code attributes in predicting patch correctness. However, existing LLM-based methods typically treat code as token sequences and ignore the inherent formal structure for code, making it difficult to capture the deep patch semantics. Moreover, these LLM-based methods also do not explicitly account for enough code attributes. To overcome these drawbacks, we in this paper design a novel patch graph representation named attributed patch semantic graph (APSG), which adequately captures the patch semantic and explicitly reflects important patch attributes. To effectively use graph information in APSG, we accordingly propose a new parameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA. Extensive evaluations have been conducted to evaluate our method, and the results show that compared to the state-of-the-art methods, our method improves accuracy and F1 score by 2.3% to 6.6% and 1.8% to 6.1% respectively.",CS,AI_ML,0.85,Extracted from log - paper 1238
SynQ: An Embedded DSL for Synchronous System Design with Quantitative Types,"System design automation aims to manage the design of embedded systems with ever-increasing complexity. To the success of system design automation, there is still a lack of systematic and formal design process because an entire design process, from a system's specification to its implementation, has to deal with inherent concerns about the systems' different aspects and, consequently, inherent semantic gaps. These gaps make it hard for a design process to be traceable or transparent. Particularly, guaranteeing the correctness of produced implementations becomes the main challenge for a system design process.   SynQ (Synchronous system design with Quantitative types) is an embedded domain specification language (EDSL) targeting the design of systems obeying the perfect synchrony hypothesis. SynQ is based on a component-based design framework and, by design, facilitates semantic coherency by leveraging the quantitative type theory (QTT) and language embedding. SynQ enables a semantically coherent design process, including formal specification and verification, modelling, simulation and code generation. This paper presents SynQ and its underlying formalism and demonstrates its features and potential for semantically coherent system design through a case study.",CS,AI_ML,0.85,Extracted from log - paper 1239
Automating Automotive Software Development: A Synergy of Generative AI and Formal Methods,"As the automotive industry shifts its focus toward software-defined vehicles, the need for faster and reliable software development continues to grow. However, traditional methods show their limitations. The rise of Generative Artificial Intelligence (GenAI), particularly Large Language Models (LLMs), introduces new opportunities to automate automotive software development tasks such as requirement analysis and code generation. However, due to the complexity of automotive systems, where software components must interact with each other seamlessly, challenges remain in software integration and system-level validation. In this paper, we propose to combine GenAI with model-driven engineering to automate automotive software development. Our approach uses LLMs to convert free-text requirements into event chain descriptions and to generate platform-independent software components that realize the required functionality. At the same time, formal models are created based on event chain descriptions to support system validation and the generation of integration code for integrating generated software components in the whole vehicle system through middleware. This approach increases development automation while enabling formal analysis to improve system reliability. As a proof of concept, we used GPT-4o to implement our method and tested it in the CARLA simulation environment with ROS2 middleware. We evaluated the system in a simple Autonomous Emergency Braking scenario.",CS,AI_ML,0.85,Extracted from log - paper 1240
Beyond the model: Key differentiators in large language models and multi-agent services,"With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it has become evident that large language models (LLMs) are no longer the sole defining factor in generative AI. As many now operate at comparable levels of capability, the real race is not about having the biggest model but optimizing the surrounding ecosystem, including data quality and management, computational efficiency, latency, and evaluation frameworks. This review article delves into these critical differentiators that ensure modern AI services are efficient and profitable.",CS,AI_ML,0.85,Extracted from log - paper 1241
"Running a Data Integration Lab in the Context of the EHRI Project: Challenges, Lessons Learnt and Future Directions","Historical study of the Holocaust is commonly hampered by the dispersed and fragmented nature of important archival sources relating to this event. The EHRI project set out to mitigate this problem by building a trans-national network of archives, researchers, and digital practitioners, and one of its main outcomes was the creation of the EHRI Portal, a ""virtual observatory"" that gathers in one centralised platform descriptions of Holocaust-related archival sources from around the world. In order to build the Portal a strong data identification and integration effort was required, culminating in the project's third phase with the creation of the EHRI-3 data integration lab. The focus of the lab was to lower the bar to participation in the EHRI Portal by providing support to institutions in conforming their archival metadata with that required for integration, ultimately opening the process up to smaller institutions (and even so-called ""micro-archives"") without the necessary resources to undertake this process themselves. In this paper we present our experiences from running the data integration lab and discuss some of the challenges (both of a technical and social nature), how we tried to overcome them, and the overall lessons learnt. We envisage this work as an archetype upon which other practitioners seeking to pursue similar data integration activities can build their own efforts.",CS,AI_ML,0.85,Extracted from log - paper 1242
Towards Effective Issue Assignment using Online Machine Learning,"Efficient issue assignment in software development relates to faster resolution time, resources optimization, and reduced development effort. To this end, numerous systems have been developed to automate issue assignment, including AI and machine learning approaches. Most of them, however, often solely focus on a posteriori analyses of textual features (e.g. issue titles, descriptions), disregarding the temporal characteristics of software development. Thus, they fail to adapt as projects and teams evolve, such cases of team evolution, or project phase shifts (e.g. from development to maintenance). To incorporate such cases in the issue assignment process, we propose an Online Machine Learning methodology that adapts to the evolving characteristics of software projects. Our system processes issues as a data stream, dynamically learning from new data and adjusting in real time to changes in team composition and project requirements. We incorporate metadata such as issue descriptions, components and labels and leverage adaptive drift detection mechanisms to identify when model re-evaluation is necessary. Upon assessing our methodology on a set of software projects, we conclude that it can be effective on issue assignment, while meeting the evolving needs of software teams.",CS,AI_ML,0.85,Extracted from log - paper 1243
LAMeD: LLM-generated Annotations for Memory Leak Detection,"Static analysis tools are widely used to detect software bugs and vulnerabilities but often struggle with scalability and efficiency in complex codebases. Traditional approaches rely on manually crafted annotations -- labeling functions as sources or sinks -- to track data flows, e.g., ensuring that allocated memory is eventually freed, and code analysis tools such as CodeQL, Infer, or Cooddy can use function specifications, but manual annotation is laborious and error-prone, especially for large or third-party libraries. We present LAMeD (LLM-generated Annotations for Memory leak Detection), a novel approach that leverages large language models (LLMs) to automatically generate function-specific annotations. When integrated with analyzers such as Cooddy, LAMeD significantly improves memory leak detection and reduces path explosion. We also suggest directions for extending LAMeD to broader code analysis.",CS,AI_ML,0.85,Extracted from log - paper 1244
RouthSearch: Inferring PID Parameter Specification for Flight Control Program by Coordinate Search,"Flight control programs use PID control modules with user-configurable Proportional (P), Integral (I), and Derivative (D) parameters to manage UAV flying behaviors. Users can adjust these PID parameters during flight. However, flight control programs lack sufficient safety checks on user-provided PID parameters, leading to a severe UAV vulnerability - the input validation bug. This occurs when a user misconfigures PID parameters, causing dangerous states like deviation from the expected path, loss of control, or crash.   Prior works use random testing like fuzzing, but these are not effective in the three-dimensional search space of PID parameters. The expensive dynamic execution of UAV tests further hinders random testing performance.   We address PID parameter misconfiguration by combining the Routh-Hurwitz stability criterion with coordinate search, introducing RouthSearch. Instead of ad-hoc identification, RouthSearch principledly determines valid ranges for three-dimensional PID parameters. We first leverage the Routh-Hurwitz Criterion to identify a theoretical PID parameter boundary, then refine it using efficient coordinate search. The determined valid range can filter misconfigured PID parameters from users during flight and help discover logical bugs in flight control programs.   We evaluated RouthSearch across eight flight modes in PX4 and Ardupilot. Results show RouthSearch determines valid ranges with 92.0% accuracy compared to ground truth. RouthSearch discovers 3,853 PID misconfigurations within 48 hours, while the STOA work PGFuzz discovers only 449 sets, significantly outperforming prior works by 8.58 times. Our method also helped detect three bugs in ArduPilot and PX4.",CS,AI_ML,0.85,Extracted from log - paper 1245
An Empirical Study on the Performance and Energy Usage of Compiled Python Code,"Python is a popular programming language known for its ease of learning and extensive libraries. However, concerns about performance and energy consumption have led to the development of compilers to enhance Python code efficiency. Despite the proven benefits of existing compilers on the efficiency of Python code, there is limited analysis comparing their performance and energy efficiency, particularly considering code characteristics and factors like CPU frequency and core count. Our study investigates how compilation impacts the performance and energy consumption of Python code, using seven benchmarks compiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython, Pyston-lite, and the experimental Python 3.13 version, compared to CPython. The benchmarks are single-threaded and executed on an NUC and a server, measuring energy usage, execution time, memory usage, and Last-Level Cache (LLC) miss rates at a fixed frequency and on a single core. The results show that compilation can significantly enhance execution time, energy and memory usage, with Codon, PyPy, and Numba achieving over 90\% speed and energy improvements. Nuitka optimizes memory usage consistently on both testbeds. The impact of compilation on LLC miss rate is not clear since it varies considerably across benchmarks for each compiler. Our study is important for researchers and practitioners focused on improving Python code performance and energy efficiency. We outline future research directions, such as exploring caching effects on energy usage. Our findings help practitioners choose the best compiler based on their efficiency benefits and accessibility.",CS,AI_ML,0.85,Extracted from log - paper 1246
Regulating Algorithmic Management: A Multi-Stakeholder Study of Challenges in Aligning Software and the Law for Workplace Scheduling,"The impacts of algorithmic management (AM) on worker well-being have led to increasing calls to regulate AM practices to prevent further worker harms. Yet existing work in aligning software with the law reduces compliance to just one piece of the entire process of regulating AM -- which involves rule operationalization, software use, and enforcement. We interviewed key stakeholders involved in enforcing or complying with workplace scheduling law -- regulators, advocates, defense attorneys, scheduling managers, and workers ($N = 38$). Based on their beliefs and experiences, we describe how scheduling software affects beliefs about and compliance with workplace scheduling law. In so doing, we discuss the challenges and opportunities in designing software as a tool for regulating AM.",CS,AI_ML,0.85,Extracted from log - paper 1247
Refining Fuzzed Crashing Inputs for Better Fault Diagnosis,"We present DiffMin, a technique that refines a fuzzed crashing input to gain greater similarities to given passing inputs to help developers analyze the crashing input to identify the failure-inducing condition and locate buggy code for debugging. DiffMin iteratively applies edit actions to transform a fuzzed input while preserving the crash behavior. Our pilot study with the Magma benchmark demonstrates that DiffMin effectively minimizes the differences between crashing and passing inputs while enhancing the accuracy of spectrum-based fault localization, highlighting its potential as a valuable pre-debugging step after greybox fuzzing.",CS,AI_ML,0.85,Extracted from log - paper 1248
A Path Less Traveled: Reimagining Software Engineering Automation via a Neurosymbolic Paradigm,"The emergence of Large Code Models (LCMs) has transformed software engineering (SE) automation, driving significant advancements in tasks such as code generation, source code documentation, code review, and bug fixing. However, these advancements come with trade-offs: achieving high performance often entails exponential computational costs, reduced interpretability, and an increasing dependence on data-intensive models with hundreds of billions of parameters. In this paper, we propose Neurosymbolic Software Engineering, in short NSE, as a promising paradigm combining neural learning with symbolic (rule-based) reasoning, while strategically introducing a controlled source of chaos to simulate the complex dynamics of real-world software systems. This hybrid methodology aims to enhance efficiency, reliability, and transparency in AI-driven software engineering while introducing controlled randomness to adapt to evolving requirements, unpredictable system behaviors, and non-deterministic execution environments. By redefining the core principles of AI-driven software engineering automation, NSE lays the groundwork for solutions that are more adaptable, transparent, and closely aligned with the evolving demands of modern software development practices.",CS,AI_ML,0.85,Extracted from log - paper 1249
On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles,"Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible.",CS,AI_ML,0.85,Extracted from log - paper 1250
Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes,"While large language models (LLMs) are increasingly used for generating parallel scientific code, most current efforts emphasize functional correctness, often overlooking performance and energy considerations. In this work, we propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel code on a target parallel system for a given parallel code as input. Through a multi-stage, iterative pipeline process, LASSI-EE achieved an average energy reduction of 47% across 85% of the 20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the broader potential of LLMs, not only for generating correct code but also for enabling energy-aware programming. We also address key insights and limitations within the framework, offering valuable guidance for future improvements.",CS,AI_ML,0.85,Extracted from log - paper 1251
Proceedings of the First International Workshop on Autonomous Systems Quality Assurance and Prediction with Digital Twins,"This volume contains the proceedings of the First International Workshop on Autonomous Systems Quality Assurance and Prediction with Digital Twins (ASQAP 2025), which was held in Hamilton, Canada, on May 4th, 2025, as a satellite event of ETAPS 2025. The aim of ASQAP 2025 is to gather experts from academia and industry to explore the potential of digital twin technology in supporting quality assurance in autonomous systems, including concepts such as specification, verification, validation, testing, analysis, and many others.",CS,AI_ML,0.85,Extracted from log - paper 1252
Large Language Models are overconfident and amplify human bias,"Large language models (LLMs) are revolutionizing every aspect of society. They are increasingly used in problem-solving tasks to substitute human assessment and reasoning. LLMs are trained on what humans write and thus prone to learn human biases. One of the most widespread human biases is overconfidence. We examine whether LLMs inherit this bias. We automatically construct reasoning problems with known ground truths, and prompt LLMs to assess the confidence in their answers, closely following similar protocols in human experiments. We find that all five LLMs we study are overconfident: they overestimate the probability that their answer is correct between 20% and 60%. Humans have accuracy similar to the more advanced LLMs, but far lower overconfidence. Although humans and LLMs are similarly biased in questions which they are certain they answered correctly, a key difference emerges between them: LLM bias increases sharply relative to humans if they become less sure that their answers are correct. We also show that LLM input has ambiguous effects on human decision making: LLM input leads to an increase in the accuracy, but it more than doubles the extent of overconfidence in the answers.",CS,AI_ML,0.85,Extracted from log - paper 1253
"Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency","The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.",CS,AI_ML,0.85,Extracted from log - paper 1254
Requirements-Based Test Generation: A Comprehensive Survey,"As an important way of assuring software quality, software testing generates and executes test cases to identify software failures. Many strategies have been proposed to guide test-case generation, such as source-code-based approaches and methods based on bug reports. Requirements-based test generation (RBTG) constructs test cases based on specified requirements, aligning with user needs and expectations, without requiring access to the source code. Since its introduction in 1994, there have been many contributions to the development of RBTG, including various approaches, implementations, tools, assessment and evaluation methods, and applications. This paper provides a comprehensive survey on RBTG, categorizing requirement types, classifying approaches, investigating types of test cases, summarizing available tools, and analyzing experimental evaluations. This paper also summarizes the domains and industrial applications of RBTG, and discusses some open research challenges and potential future work.",CS,AI_ML,0.85,Extracted from log - paper 1255
Testing Database Systems with Large Language Model Synthesized Fragments,"Various automated testing approaches have been proposed for Database Management Systems (DBMSs). Many such approaches generate pairs of equivalent queries to identify bugs that cause DBMSs to compute incorrect results, and have found hundreds of bugs in mature, widely used DBMSs. Most of these approaches are based on manually written SQL generators; however, their bug-finding capabilities remain constrained by the limited set of SQL features supported by the generators. In this work, we propose ShQveL, an approach that augments existing SQL test-case generators by leveraging Large Language Models (LLMs) to synthesize SQL fragments. Our key idea is to systematically incorporate SQL features gained through automated interactions with LLMs into the SQL generators, increasing the features covered while efficiently generating test cases. Specifically, ShQveL uses SQL sketches -- SQL statements with incomplete code segments that LLMs fill -- to integrate LLM-generated content into the generator. We evaluated ShQveL on 5 DBMSs and discovered 55 unique and previously unknown bugs, 50 of which were promptly fixed after our reports.",CS,AI_ML,0.85,Extracted from log - paper 1256
Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach,"UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study.",CS,AI_ML,0.85,Extracted from log - paper 1257
One Documentation Does Not Fit All: Case Study of TensorFlow Documentation,"Software documentation guides the proper use of tools or services. With the rapid growth of machine learning libraries, individuals from various fields are incorporating machine learning into their workflows through programming. However, many of these users lack software engineering experience, affecting the usability of the documentation. Traditionally, software developers have created documentation primarily for their peers, making it challenging for others to interpret and effectively use these resources. Moreover, no study has specifically focused on machine learning software documentation or analyzing the backgrounds of developers who rely on such documentation, highlighting a critical gap in understanding how to make these resources more accessible. This study examined customization trends in TensorFlow tutorials and compared these artifacts to analyze content and design differences. We also analyzed Stack Overflow questions related to TensorFlow documentation to understand the types of questions and the backgrounds of the developers asking them. Further, we developed two taxonomies based on the nature and triggers of the questions for machine learning software. Our findings showed no significant differences in the content or the nature of the questions across different tutorials. Our results show that 24.9% of the questions concern errors and exceptions, while 64.3% relate to inadequate and non-generalizable examples in the documentation. Despite efforts to create customized documentation, our analysis indicates that current TensorFlow documentation does not effectively support its target users.",CS,AI_ML,0.85,Extracted from log - paper 1258
Site Reliability Engineering (SRE) and Observations on SRE Process to Make Tasks Easier,"This paper explores Site Reliability Engineering (SRE), a modern approach to maintaining scalable and reliable software systems. It presents observations on how structured SRE processes improve operational efficiency, reduce system downtime, and simplify maintenance. Drawing from real-world implementations, the study outlines key techniques in automation, monitoring, incident management, and deployment strategies. The work also highlights how these practices can be tailored to different environments, offering practical insights for engineers aiming to improve service reliability.",CS,AI_ML,0.85,Extracted from log - paper 1259
ImageR: Enhancing Bug Report Clarity by Screenshots,"In issue-tracking systems, incorporating screenshots significantly enhances the clarity of bug reports, facilitating more efficient communication and expediting issue resolution. However, determining when and what type of visual content to include remains challenging, as not all attachments effectively contribute to problem-solving; studies indicate that 22.5% of images in issue reports fail to aid in resolving the reported issues. To address this, we introduce ImageR, an AI model and tool that analyzes issue reports to assess the potential benefits of including screenshots and recommends the most pertinent types when appropriate. By proactively suggesting relevant visuals, ImageR aims to make issue reports clearer, more informative, and time-efficient. We have curated and publicly shared a dataset comprising 6,235 Bugzilla issues, each meticulously labeled with the type of image attachment, providing a valuable resource for benchmarking and advancing research in image processing within developer communication contexts. To evaluate ImageR, we conducted empirical experiments on a subset of these reports from various Mozilla projects. The tool achieved an F1-score of 0.76 in determining when images are needed, with 75% of users finding its recommendations highly valuable. By minimizing the back-and-forth communication often needed to obtain suitable screenshots, ImageR streamlines the bug reporting process. Furthermore, it guides users in selecting the most effective visual documentation from ten established categories, potentially reducing resolution times and improving the quality of bug documentation. ImageR is open-source, inviting further use and improvement by the community. The labeled dataset offers a rare resource for benchmarking and exploring image processing in the context of developer communication.",CS,AI_ML,0.85,Extracted from log - paper 1260
Certus: A domain specific language for confidence assessment in assurance cases,"Assurance cases (ACs) are prepared to argue that a system has satisfied critical quality attributes. Many methods exist to assess confidence in ACs, including quantitative methods that represent confidence numerically. While quantitative methods are attractive in principle, existing methods suffer from issues related to interpretation, subjectivity, scalability, dialectic reasoning, and trustworthiness, which have limited their adoption. This paper introduces Certus, a domain specific language for quantitative confidence assessment. In Certus, users describe their confidence with fuzzy sets, which allow them to represent their judgment using vague, but linguistically meaningful terminology. Certus includes syntax to specify confidence propagation using expressions that can be easily inspected by users. To demonstrate the concept of the language, Certus is applied to a worked example from the automotive domain.",CS,AI_ML,0.85,Extracted from log - paper 1261
OODTE: A Differential Testing Engine for the ONNX Optimizer,"With $700$ stars on GitHub and part of the official ONNX repository, the ONNX Optimizer consists of the standard method to apply graph-based optimizations on ONNX models. However, its ability to preserve model accuracy across optimizations, has not been rigorously explored. We propose OODTE, a utility to automatically and thoroughly assess the correctness of the ONNX Optimizer. OODTE follows a simple, yet effective differential testing and evaluation approach that can be easily adopted to other compiler optimizers. In particular, OODTE utilizes a number of ONNX models, then optimizes them and executes both the original and the optimized variants across a user-defined set of inputs, while automatically logging any issues with the optimization process. Finally, for successfully optimized models, OODTE compares the results, and, if any accuracy deviations are observed, it iteratively repeats the process for each pass of the ONNX Optimizer, to localize the root cause of the differences observed. Using OODTE, we sourced well-known $130$ models from the official ONNX Model Hub, used for a wide variety of tasks (classification, object detection, semantic segmentation, text summarization, question and answering, sentiment analysis) from the official ONNX model hub. We detected 15 issues, 14 of which were previously unknown, associated with optimizer crashes and accuracy deviations. We also observed $9.2$% of all model instances presenting issues leading into the crash of the optimizer, or the generation of an invalid model while using the primary optimizer strategies. In addition, $30$% of the classification models presented accuracy differences across the original and the optimized model variants, while $16.6$% of semantic segmentation and object detection models are also affected, at least to a limited extent.",CS,AI_ML,0.85,Extracted from log - paper 1262
LogDB: Multivariate Log-based Failure Diagnosis for Distributed Databases (Extended from MultiLog),"Distributed databases, as the core infrastructure software for internet applications, play a critical role in modern cloud services. However, existing distributed databases frequently experience system failures and performance degradation, often leading to significant economic losses. Log data, naturally generated within systems, can effectively reflect internal system states. In practice, operators often manually inspect logs to monitor system behavior and diagnose anomalies, a process that is labor-intensive and costly. Although various log-based failure diagnosis methods have been proposed, they are generally not tailored for database systems and fail to fully exploit the internal characteristics and distributed nature of these systems. To address this gap, we propose LogDB, a log-based failure diagnosis method specifically designed for distributed databases. LogDB extracts and compresses log features at each database node and then aggregates these features at the master node to diagnose cluster-wide anomalies. Experiments conducted on the open-source distributed database system Apache IoTDB demonstrate that LogDB achieves robust failure diagnosis performance across different workloads and a variety of anomaly types.",CS,AI_ML,0.85,Extracted from log - paper 1263
A Defect Taxonomy for Infrastructure as Code: A Replication Study,"Background: As Infrastructure as Code (IaC) becomes standard practice, ensuring the reliability of IaC scripts is essential. Defect taxonomies are valuable tools for this, offering a common language for issues and enabling systematic tracking. A significant prior study developed such a taxonomy, but based it exclusively on the declarative language Puppet. It remained unknown whether this taxonomy applies to programming language-based IaC (PL-IaC) tools like Pulumi, Terraform CDK, and AWS CDK. Aim: We replicated this foundational work to assess the generalizability of the taxonomy across a broader and more diverse landscape. Method: We performed qualitative analysis on 3,364 defect-related commits from 285 open-source PL-IaC repositories (PIPr dataset) to derive a PL-IaC-specific defect taxonomy. We then enhanced the ACID tool, originally developed for the prior study, to automatically classify and analyze defect distributions across an expanded dataset-447 open-source repositories and 94 proprietary projects from VTEX (e-commerce) and Nubank (financial). Results: Our research confirmed the same eight defect categories identified in the original study, with idempotency and security defects appearing infrequently but persistently across projects. Configuration Data defects maintain high frequency in both open-source and proprietary codebases. Conclusions: Our replication supports the generalizability of the original taxonomy, suggesting IaC development challenges surpass organizational boundaries. Configuration Data defects emerge as a persistent high-frequency problem, while idempotency and security defects remain important concerns despite lower frequency. These patterns appear consistent across open-source and proprietary projects, indicating they are fundamental to the IaC paradigm itself, transcending specific tools or project types.",CS,AI_ML,0.85,Extracted from log - paper 1264
Overcoming Obstacles: Challenges of Gender Inequality in Undergraduate ICT Programs,"Context: Gender inequality is a widely discussed issue across various sectors, including Information Technology and Communication (ICT). In Brazil, women represent less than 18% of ICT students in higher education. Prior studies highlight gender-related barriers that discourage women from staying in ICT. However, they provide limited insights into their perceptions as undergraduate students and the factors influencing their participation and confidence. Goal: This study explores the perceptions of women undergraduate students in ICT regarding gender inequality. Method: A survey of 402 women from 18 Brazilian states enrolled in ICT courses was conducted using a mixed-method approach, combining quantitative and qualitative analyses. Results: Women students reported experiencing discriminatory practices from peers and professors, both inside and outside the classroom. Gender stereotypes were found to undermine their self-confidence and self-esteem, occasionally leading to course discontinuation. Conclusions: Factors such as lack of representation, inappropriate jokes, isolation, mistrust, and difficulty being heard contribute to harmful outcomes, including reduced participation and reluctance to take leadership roles. Addressing these issues is essential to creating a safe and respectful learning environment for all students.",CS,AI_ML,0.85,Extracted from log - paper 1265
Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments,"Safety critical software assessment requires robust assessment against complex regulatory frameworks, a process traditionally limited by manual evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning (DRAFT), a novel approach that enhances the capabilities of a large language model (LLM) for safety-critical compliance assessment. DRAFT builds upon existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel fine-tuning framework that accommodates our dual-retrieval architecture, which simultaneously accesses both software documentation and applicable reference standards. To fine-tune DRAFT, we develop a semi-automated dataset generation methodology that incorporates variable numbers of relevant documents with meaningful distractors, closely mirroring real-world assessment scenarios. Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over the baseline model, with qualitative improvements in evidence handling, response structure, and domain-specific reasoning. DRAFT represents a practical approach to improving compliance assessment systems while maintaining the transparency and evidence-based reasoning essential in regulatory domains.",CS,AI_ML,0.85,Extracted from log - paper 1266
BiGSCoder: State Space Model for Code Understanding,"We present BiGSCoder, a novel encoder-only bidirectional state-space model (SSM) featuring a gated architecture, pre-trained for code understanding on a code dataset using masked language modeling. Our work aims to systematically evaluate SSMs' capabilities in coding tasks compared to traditional transformer architectures; BiGSCoder is built for this purpose. Through comprehensive experiments across diverse pre-training configurations and code understanding benchmarks, we demonstrate that BiGSCoder outperforms transformer-based models, despite utilizing simpler pre-training strategies and much less training data. Our results indicate that BiGSCoder can serve as a more sample-efficient alternative to conventional transformer models. Furthermore, our study shows that SSMs perform better without positional embeddings and can effectively extrapolate to longer sequences during fine-tuning.",CS,AI_ML,0.85,Extracted from log - paper 1267
Micro-Patterns in Solidity Code,"Solidity is the predominant programming language for blockchain-based smart contracts, and its characteristics pose significant challenges for code analysis and maintenance. Traditional software analysis approaches, while effective for conventional programming languages, often fail to address Solidity-specific features such as gas optimization and security constraints.   This paper introduces micro-patterns - recurring, small-scale design structures that capture key behavioral and structural peculiarities specific to a language - for Solidity language and demonstrates their value in understanding smart contract development practices. We identified 18 distinct micro-patterns organized in five categories (Security, Functional, Optimization, Interaction, and Feedback), detailing their characteristics to enable automated detection.   To validate this proposal, we analyzed a dataset of 23258 smart contracts from five popular blockchains (Ethereum, Polygon, Arbitrum, Fantom and Optimism). Our analysis reveals widespread adoption of micro-patterns, with 99% of contracts implementing at least one pattern and an average of 2.76 patterns per contract. The Storage Saver pattern showed the highest adoption (84.62% mean coverage), while security patterns demonstrated platform-specific adoption rates. Statistical analysis revealed significant platform-specific differences in pattern adoption, particularly in Borrower, Implementer, and Storage Optimization patterns.",CS,AI_ML,0.85,Extracted from log - paper 1268
Design for a Digital Twin in Clinical Patient Care,"Digital Twins hold great potential to personalize clinical patient care, provided the concept is translated to meet specific requirements dictated by established clinical workflows. We present a generalizable Digital Twin design combining knowledge graphs and ensemble learning to reflect the entire patient's clinical journey and assist clinicians in their decision-making. Such Digital Twins can be predictive, modular, evolving, informed, interpretable and explainable with applications ranging from oncology to epidemiology.",CS,AI_ML,0.85,Extracted from log - paper 1269
An instrument to measure factors that constitute the socio-technical context of testing experience,"We consider testing a cooperative and social practice that is shaped by the tools developers use, the tests they write, and their mindsets and human needs. This work is one part of a project that explores the human- and socio-technical context of testing through the lens of those interwoven elements: test suite and tools as technical infrastructure and collaborative factors and motivation as mindset. Drawing on empirical observations of previous work, this survey examines how these factors relate to each other. We want to understand which combination of factors can help developers strive and make the most of their ambitions to leverage the potential that software testing practices have. In this report, we construct a survey instrument to measure the factors that constitute the socio-technical context of testing experience. In addition, we state our hypotheses about how these factors impact testing experience and explain the considerations and process that led to the construction of the survey questions.",CS,AI_ML,0.85,Extracted from log - paper 1270
Automatic techniques for issue report classification: A systematic mapping study,"Several studies have evaluated automatic techniques for classifying software issue reports to assist practitioners in effectively assigning relevant resources based on the type of issue. Currently, no comprehensive overview of this area has been published. A comprehensive overview will help identify future research directions and provide an extensive collection of potentially relevant existing solutions. This study aims to provide a comprehensive overview of the use of automatic techniques to classify issue reports. We conducted a systematic mapping study and identified 46 studies on the topic.   The study results indicate that the existing literature applies various techniques for classifying issue reports, including traditional machine learning and deep learning-based techniques and more advanced large language models. Furthermore, we observe that these studies (a) lack the involvement of practitioners, (b) do not consider other potentially relevant adoption factors beyond prediction accuracy, such as the explainability, scalability, and generalizability of the techniques, and (c) mainly rely on archival data from open-source repositories only. Therefore, future research should focus on real industrial evaluations, consider other potentially relevant adoption factors, and actively involve practitioners.",CS,AI_ML,0.85,Extracted from log - paper 1271
CppSATD: A Reusable Self-Admitted Technical Debt Dataset in C++,"In software development, technical debt (TD) refers to suboptimal implementation choices made by the developers to meet urgent deadlines and limited resources, posing challenges for future maintenance. Self-Admitted Technical Debt (SATD) is a sub-type of TD, representing specific TD instances ``openly admitted'' by the developers and often expressed through source code comments. Previous research on SATD has focused predominantly on the Java programming language, revealing a significant gap in cross-language SATD. Such a narrow focus limits the generalizability of existing findings as well as SATD detection techniques across multiple programming languages. Our work addresses such limitation by introducing CppSATD, a dedicated C++ SATD dataset, comprising over 531,000 annotated comments and their source code contexts. Our dataset can serve as a foundation for future studies that aim to develop SATD detection methods in C++, generalize the existing findings to other languages, or contribute novel insights to cross-language SATD research.",CS,AI_ML,0.85,Extracted from log - paper 1272
Evaluating the Impact of Data Cleaning on the Quality of Generated Pull Request Descriptions,"Pull Requests (PRs) are central to collaborative coding, summarizing code changes for reviewers. However, many PR descriptions are incomplete, uninformative, or have out-of-context content, compromising developer workflows and hindering AI-based generation models trained on commit messages and original descriptions as ""ground truth."" This study examines the prevalence of ""noisy"" PRs and evaluates their impact on state-of-the-art description generation models. To do so, we propose four cleaning heuristics to filter noise from an initial dataset of 169K+ PRs drawn from 513 GitHub repositories. We train four models-BART, T5, PRSummarizer, and iTAPE-on both raw and cleaned datasets. Performance is measured via ROUGE-1, ROUGE-2, and ROUGE-L metrics, alongside a manual evaluation to assess description quality improvements from a human perspective. Cleaning the dataset yields significant gains: average F1 improvements of 8.6% (ROUGE-1), 8.7% (ROUGE-2), and 8.5% (ROUGE-L). Manual assessment confirms higher readability and relevance in descriptions generated by the best-performing model, BART when trained on cleaned data. Dataset refinement markedly enhances PR description generation, offering a foundation for more accurate AI-driven tools and guidelines to assist developers in crafting high-quality PR descriptions.",CS,AI_ML,0.85,Extracted from log - paper 1273
Towards an Interpretable Analysis for Estimating the Resolution Time of Software Issues,"Lately, software development has become a predominantly online process, as more teams host and monitor their projects remotely. Sophisticated approaches employ issue tracking systems like Jira, predicting the time required to resolve issues and effectively assigning and prioritizing project tasks. Several methods have been developed to address this challenge, widely known as bug-fix time prediction, yet they exhibit significant limitations. Most consider only textual issue data and/or use techniques that overlook the semantics and metadata of issues (e.g., priority or assignee expertise). Many also fail to distinguish actual development effort from administrative delays, including assignment and review phases, leading to estimates that do not reflect the true effort needed. In this work, we build an issue monitoring system that extracts the actual effort required to fix issues on a per-project basis. Our approach employs topic modeling to capture issue semantics and leverages metadata (components, labels, priority, issue type, assignees) for interpretable resolution time analysis. Final predictions are generated by an aggregated model, enabling contributors to make informed decisions. Evaluation across multiple projects shows the system can effectively estimate resolution time and provide valuable insights.",CS,AI_ML,0.85,Extracted from log - paper 1274
Detecting the Root Cause Code Lines in Bug-Fixing Commits by Heterogeneous Graph Learning,"With the continuous growth in the scale and complexity of software systems, defect remediation has become increasingly difficult and costly. Automated defect prediction tools can proactively identify software changes prone to defects within software projects, thereby enhancing software development efficiency. However, existing work in heterogeneous and complex software projects continues to face challenges, such as struggling with heterogeneous commit structures and ignoring cross-line dependencies in code changes, which ultimately reduce the accuracy of defect identification. To address these challenges, we propose an approach called RC_Detector. RC_Detector comprises three main components: the bug-fixing graph construction component, the code semantic aggregation component, and the cross-line semantic retention component. The bug-fixing graph construction component identifies the code syntax structures and program dependencies within bug-fixing commits and transforms them into heterogeneous graph formats by converting the source code into vector representations. The code semantic aggregation component adapts to heterogeneous data by using heterogeneous attention to learn the hidden semantic representation of target code lines. The cross-line semantic retention component regulates propagated semantic information by using attenuation and reinforcement gates derived from old and new code semantic representations, effectively preserving cross-line semantic relationships. Extensive experiments were conducted to evaluate the performance of our model by collecting data from 87 open-source projects, including 675 bug-fixing commits. The experimental results demonstrate that our model outperforms state-of-the-art approaches, achieving significant improvements of 83.15%,96.83%,78.71%,74.15%,54.14%,91.66%,91.66%, and 34.82% in MFR, respectively, compared with the state-of-the-art approaches.",CS,AI_ML,0.85,Extracted from log - paper 1275
Identifying Root Cause of bugs by Capturing Changed Code Lines with Relational Graph Neural Networks,"The Just-In-Time defect prediction model helps development teams improve software quality and efficiency by assessing whether code changes submitted by developers are likely to introduce defects in real-time, allowing timely identification of potential issues during the commit stage. However, two main challenges exist in current work due to the reality that all deleted and added lines in bug-fixing commits may be related to the root cause of the introduced bug: 1) lack of effective integration of heterogeneous graph information, and 2) lack of semantic relationships between changed code lines. To address these challenges, we propose a method called RC-Detection, which utilizes relational graph convolutional network to capture the semantic relationships between changed code lines. RC-Detection is used to detect root-cause deletion lines in changed code lines, thereby identifying the root cause of introduced bugs in bug-fixing commits. To evaluate the effectiveness of RC-Detection, we used three datasets that contain high-quality bug-fixing and bug-introducing commits. Extensive experiments were conducted to evaluate the performance of our model by collecting data from 87 open-source projects, including 675 bug-fix commits. The experimental results show that, compared to the most advanced root cause detection methods, RC-Detection improved Recall@1, Recall@2, Recall@3, and MFR by at 4.107%, 5.113%, 4.289%, and 24.536%, respectively.",CS,AI_ML,0.85,Extracted from log - paper 1276
A SCADE Model Verification Method Based on B-Model Transformation,"Due to the limitations of SCADE models in expressing and verifying abstract specifications in safety-critical systems, this study proposes a formal verification framework based on the B-Method. By establishing a semantic equivalence transformation mechanism from SCADE models to B models, a hierarchical mapping rule set is constructed, covering type systems, control flow structures, and state machines. This effectively addresses key technical challenges such as loop-equivalent transformation proof for high-order operators and modeling of temporal logic storage structures. The proposed method innovatively leverages the abstraction capabilities of B-Method in set theory and first-order logic, overcoming the constraints of native verification tools of SCADE in complex specification descriptions. It successfully verifies abstract specifications that are difficult to model directly in SCADE. Experimental results show that the transformed B models achieve a higher defect detection rate and improved verification efficiency in the ProB verification environment compared to the native verifier of SCADE, significantly enhancing the formal verification capability of safety-critical systems. This study provides a cross-model verification paradigm for embedded control systems in avionics, rail transportation, and other domains, demonstrating substantial engineering application value.",CS,AI_ML,0.85,Extracted from log - paper 1277
Aggregating empirical evidence from data strategy studies: a case on model quantization,"Background: As empirical software engineering evolves, more studies adopt data strategies$-$approaches that investigate digital artifacts such as models, source code, or system logs rather than relying on human subjects. Synthesizing results from such studies introduces new methodological challenges.   Aims: This study assesses the effects of model quantization on correctness and resource efficiency in deep learning (DL) systems. Additionally, it explores the methodological implications of aggregating evidence from empirical studies that adopt data strategies.   Method: We conducted a research synthesis of six primary studies that empirically evaluate model quantization. We applied the Structured Synthesis Method (SSM) to aggregate the findings, which combines qualitative and quantitative evidence through diagrammatic modeling. A total of 19 evidence models were extracted and aggregated.   Results: The aggregated evidence indicates that model quantization weakly negatively affects correctness metrics while consistently improving resource efficiency metrics, including storage size, inference latency, and GPU energy consumption$-$a manageable trade-off for many DL deployment contexts. Evidence across quantization techniques remains fragmented, underscoring the need for more focused empirical studies per technique.   Conclusions: Model quantization offers substantial efficiency benefits with minor trade-offs in correctness, making it a suitable optimization strategy for resource-constrained environments. This study also demonstrates the feasibility of using SSM to synthesize findings from data strategy-based research.",CS,AI_ML,0.85,Extracted from log - paper 1278
"The Architecture Tradeoff and Risk Analysis Framework (ATRAF): A Unified Approach for Evaluating Software Architectures, Reference Architectures, and Architectural Frameworks","Modern software systems are guided by hierarchical architectural concepts -- software architectures, reference architectures, and architectural frameworks -- each operating at a distinct level of abstraction. These artifacts promote reuse, scalability, and consistency, but also embed tradeoffs that shape critical quality attributes such as modifiability, performance, and security. Existing evaluation methods, such as the Architecture Tradeoff Analysis Method (ATAM), focus on system-specific architectures and are not designed to address the broader generality and variability of higher-level architectural forms. To close this gap, we introduce the Architecture Tradeoff and Risk Analysis Framework (ATRAF) -- a unified, scenario-driven framework for evaluating tradeoffs and risks across architectural levels. ATRAF encompasses three methods: the Architecture Tradeoff and Risk Analysis Method (ATRAM), extending ATAM with enhanced risk identification for concrete systems; the Reference Architecture Tradeoff and Risk Analysis Method (RATRAM), adapting ATRAM to the evaluation of domain-level reference architectures; and the Architectural Framework Tradeoff and Risk Analysis Method (AFTRAM), supporting the evaluation of architectural frameworks that guide entire system families. All three methods follow an iterative spiral process that enables the identification of sensitivities, tradeoffs, and risks while supporting continuous refinement of architectural artifacts. We demonstrate ATRAF through progressively abstracted examples derived from the Remote Temperature Sensor (RTS) case, originally introduced in the ATAM literature. ATRAF equips architects, reference modelers, and framework designers with a practical, systematic approach for analyzing design alternatives and managing quality attribute tradeoffs early in the lifecycle and across all levels of architectural abstraction.",CS,AI_ML,0.85,Extracted from log - paper 1279
From Requirements to Test Cases: An NLP-Based Approach for High-Performance ECU Test Case Automation,"Automating test case specification generation is vital for improving the efficiency and accuracy of software testing, particularly in complex systems like high-performance Electronic Control Units (ECUs). This study investigates the use of Natural Language Processing (NLP) techniques, including Rule-Based Information Extraction and Named Entity Recognition (NER), to transform natural language requirements into structured test case specifications. A dataset of 400 feature element documents from the Polarion tool was used to evaluate both approaches for extracting key elements such as signal names and values. The results reveal that the Rule-Based method outperforms the NER method, achieving 95% accuracy for more straightforward requirements with single signals, while the NER method, leveraging SVM and other machine learning algorithms, achieved 77.3% accuracy but struggled with complex scenarios. Statistical analysis confirmed that the Rule-Based approach significantly enhances efficiency and accuracy compared to manual methods. This research highlights the potential of NLP-driven automation in improving quality assurance, reducing manual effort, and expediting test case generation, with future work focused on refining NER and hybrid models to handle greater complexity.",CS,AI_ML,0.85,Extracted from log - paper 1280
LLMPrism: Black-box Performance Diagnosis for Production LLM Training Platforms,"Large Language Models (LLMs) have brought about revolutionary changes in diverse fields, rendering LLM training of utmost importance for modern enterprises. To meet this demand, multi-tenant large-scale LLM training platforms have been built to offer LLM training services. Nevertheless, due to the complexity and synchronous nature of LLM training process, performance issues occur frequently and can result in substantial resource wastage. The limited visibility from the perspective of platform providers impedes existing profiling methods and poses challenges to the monitoring and diagnosis of the performance of LLM training jobs. For the first time, this paper proposes the utilization of underlying network flow data to reconstruct the training timelines of jobs based on the distinct characteristics in the LLM training procedure. We design LLMPrism, the first black-box performance diagnosis system for LLM training platforms. By progressively recognizing LLM training jobs, identifying their parallelism strategies, and reconstructing the training timelines, LLMPrism achieves non-intrusive, lightweight, and continuous monitoring of LLM training systems. Leveraging this monitoring capability, it further effectively diagnoses potential performance issues. Since Oct. 2024, LLMPrism has been deployed on our large-scale production Platform-X, in which the evaluations and deployment experiences demonstrate that LLMPrism can achieve accurate timeline reconstruction with an error within 0.3% and effectively diagnose various performance issues.",CS,AI_ML,0.85,Extracted from log - paper 1281
When Deep Learning Meets Information Retrieval-based Bug Localization: A Survey,"Bug localization is a crucial aspect of software maintenance, running through the entire software lifecycle. Information retrieval-based bug localization (IRBL) identifies buggy code based on bug reports, expediting the bug resolution process for developers. Recent years have witnessed significant achievements in IRBL, propelled by the widespread adoption of deep learning (DL). To provide a comprehensive overview of the current state of the art and delve into key issues, we conduct a survey encompassing 61 IRBL studies leveraging DL. We summarize best practices in each phase of the IRBL workflow, undertake a meta-analysis of prior studies, and suggest future research directions. This exploration aims to guide further advancements in the field, fostering a deeper understanding and refining practices for effective bug localization. Our study suggests that the integration of DL in IRBL enhances the model's capacity to extract semantic and syntactic information from both bug reports and source code, addressing issues such as lexical gaps, neglect of code structure information, and cold-start problems. Future research avenues for IRBL encompass exploring diversity in programming languages, adopting fine-grained granularity, and focusing on real-world applications. Most importantly, although some studies have started using large language models for IRBL, there is still a need for more in-depth exploration and thorough investigation in this area.",CS,AI_ML,0.85,Extracted from log - paper 1282
An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding,"Binary code analysis plays a pivotal role in the field of software security and is widely used in tasks such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, reverse engineers face significant challenges in understanding binary code due to the lack of intuitive semantic information. Although traditional reverse tools can convert binary code into C-like pseudo code, the lack of code comments and symbolic information such as function names still makes code understanding difficult. In recent years, two groups of techniques have shown promising prospects: (1) Deep learning-based techniques have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This has left participants wondering about the capabilities of LLMs in binary code understanding. To this end, this work proposes a benchmark to evaluate the effectiveness of LLMs in real-world reverse engineering scenarios, which covers two key binary code understanding tasks, i.e., function name recovery and binary code summarization. To more comprehensively evaluate, we include binaries with multiple target architectures as well as different optimization options. We gain valuable insights into the capabilities and limitations through extensive empirical studies of popular LLMs using our benchmark. Our evaluations reveal that existing LLMs can understand binary code to a certain extent, thereby improving the efficiency of binary code analysis. Our results highlight the great potential of the LLMs in advancing the field of binary code understanding, and provide new directions for binary code analysis techniques.",CS,AI_ML,0.85,Extracted from log - paper 1283
SWE-smith: Scaling Data for Software Engineering Agents,"Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.",CS,AI_ML,0.85,Extracted from log - paper 1284
"CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation","Real world development demands code that is readable, extensible, and testable by organizing the implementation into modular components and iteratively reuse pre-implemented code. We term this iterative, multi-turn process codeflow and introduce CodeFlowBench, the first benchmark designed for comprehensively evaluating LLMs' ability to perform codeflow, namely to implement new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously updated via an automated pipeline that decomposes each problem into a series of function-level subproblems based on its dependency tree and each subproblem is paired with unit tests. We further propose a novel evaluation framework with tasks and metrics tailored to multi-turn code reuse to assess model performance. In experiments across various LLMs under both multi-turn and single-turn patterns. We observe models' poor performance on CodeFlowBench, with a substantial performance drop in the iterative codeflow scenario. For instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8% in single-turn pattern. Further analysis shows that different models excel at different dependency depths, yet all struggle to correctly solve structurally complex problems, highlighting challenges for current LLMs to serve as code generation tools when performing codeflow. Overall, CodeFlowBench offers a comprehensive benchmark and new insights into LLM capabilities for multi-turn, iterative code generation, guiding future advances in code generation tasks.",CS,AI_ML,0.85,Extracted from log - paper 1285
Using quantum annealing to generate test cases for cyber-physical systems,"Quantum computing has emerged as a powerful tool to efficiently solve computational challenges, particularly in simulation and optimisation. However, hardware limitations prevent quantum computers from achieving the full theoretical potential. Among the quantum algorithms, quantum annealing is a prime candidate to solve optimisation problems. This makes it a natural candidate for search-based software testing in the Cyber-Physical Systems (CPS) domain, which demands effective test cases due to their safety-critical nature. This work explores the use of quantum annealing to enhance test case generation for CPS through a mutation-based approach. We encode test case mutation as a binary optimisation problem, and use quantum annealing to identify and target critical regions of the test cases for improvement. Our approach mechanises this process into an algorithm that uses D-Wave's quantum annealer to find the solution. As a main contribution, we offer insights into how quantum annealing can advance software testing methodologies by empirically evaluating the correlation between problem size, hardware limitations, and the effectiveness of the results. Moreover, we compare the proposed method against state-of-the-art classical optimisation algorithms, targeting efficiency (time to generate test cases) and effectiveness (fault detection rates). Results indicate that quantum annealing enables faster test case generation while achieving comparable fault detection performance to state-of-the-art alternatives.",CS,AI_ML,0.85,Extracted from log - paper 1286
Canonicalization for Unreproducible Builds in Java,"The increasing complexity of software supply chains and the rise of supply chain attacks have elevated concerns around software integrity. Users and stakeholders face significant challenges in validating that a given software artifact corresponds to its declared source. Reproducible Builds address this challenge by ensuring that independently performed builds from identical source code produce identical binaries. However, achieving reproducibility at scale remains difficult, especially in Java, due to a range of non-deterministic factors and caveats in the build process. In this work, we focus on reproducibility in Java-based software, archetypal of enterprise applications. We introduce a conceptual framework for reproducible builds, we analyze a large dataset from Reproducible Central, and we develop a novel taxonomy of six root causes of unreproducibility. We study actionable mitigations: artifact and bytecode canonicalization using OSS-Rebuild and jNorm respectively. Finally, we present Chains-Rebuild, a tool that raises reproducibility success from 9.48% to 26.89% on 12,283 unreproducible artifacts. To sum up, our contributions are the first large-scale taxonomy of build unreproducibility causes in Java, a publicly available dataset of unreproducible builds, and Chains-Rebuild, a canonicalization tool for mitigating unreproducible builds in Java.",CS,AI_ML,0.85,Extracted from log - paper 1287
A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense,"With the rapid advancement of blockchain technology, smart contracts have enabled the implementation of increasingly complex functionalities. However, ensuring the security of smart contracts remains a persistent challenge across the stages of development, compilation, and execution. Vulnerabilities within smart contracts not only undermine the security of individual applications but also pose significant risks to the broader blockchain ecosystem, as demonstrated by the growing frequency of attacks since 2016, resulting in substantial financial losses. This paper provides a comprehensive analysis of key security risks in Ethereum smart contracts, specifically those written in Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two prevalent and critical vulnerability types (reentrancy and integer overflow) by examining their underlying mechanisms, replicating attack scenarios, and assessing effective countermeasures.",CS,AI_ML,0.85,Extracted from log - paper 1288
Identifying Critical Dependencies in Large-Scale Continuous Software Engineering,"Continuous Software Engineering (CSE) is widely adopted in the industry, integrating practices such as Continuous Integration and Continuous Deployment (CI/CD). Beyond technical aspects, CSE also encompasses business activities like continuous planning, budgeting, and operational processes. Coordinating these activities in large-scale product development involves multiple stakeholders, increasing complexity. This study aims to address this complexity by identifying and analyzing critical dependencies in large-scale CSE. Based on 17 semi-structured interviews conducted at two Nordic fintech companies, our preliminary findings indicate that dependencies between software teams and support functions, as well as between software teams and external entities, are the primary sources of delays and bottlenecks. As a next step, we plan to further refine our understanding of critical dependencies in large-scale CSE and explore coordination mechanisms that can better support software development teams in managing these challenges.",CS,AI_ML,0.85,Extracted from log - paper 1289
A Test Suite for Efficient Robustness Evaluation of Face Recognition Systems,"Face recognition is a widely used authentication technology in practice, where robustness is required. It is thus essential to have an efficient and easy-to-use method for evaluating the robustness of (possibly third-party) trained face recognition systems. Existing approaches to evaluating the robustness of face recognition systems are either based on empirical evaluation (e.g., measuring attacking success rate using state-of-the-art attacking methods) or formal analysis (e.g., measuring the Lipschitz constant). While the former demands significant user efforts and expertise, the latter is extremely time-consuming. In pursuit of a comprehensive, efficient, easy-to-use and scalable estimation of the robustness of face recognition systems, we take an old-school alternative approach and introduce RobFace, i.e., evaluation using an optimised test suite. It contains transferable adversarial face images that are designed to comprehensively evaluate a face recognition system's robustness along a variety of dimensions. RobFace is system-agnostic and still consistent with system-specific empirical evaluation or formal analysis. We support this claim through extensive experimental results with various perturbations on multiple face recognition systems. To our knowledge, RobFace is the first system-agnostic robustness estimation test suite.",CS,AI_ML,0.85,Extracted from log - paper 1290
On the Encapsulation of Medical Imaging AI Algorithms,"In the context of collaborative AI research and development projects, it would be ideal to have self-contained encapsulated algorithms that can be easily shared between different parties, executed and validated on data at different sites, or trained in a federated manner. In practice, all of this is possible but greatly complicated, because human supervision and expert knowledge is needed to set up the execution of algorithms based on their documentation, possibly implicit assumptions, and knowledge about the execution environment and data involved.   We derive and formulate a range of detailed requirements from the above goal and from specific use cases, focusing on medical imaging AI algorithms. Furthermore, we refer to a number of existing APIs and implementations and review which aspects each of them addresses, which problems are still open, and which public standards and ontologies may be relevant. Our contribution is a comprehensive collection of aspects that have not yet been addressed in their entirety by any single solution.   Working towards the formulated goals should lead to more sustainable algorithm ecosystems and relates to the FAIR principles for research data, where this paper focuses on interoperability and (re)usability of medical imaging AI algorithms.",CS,AI_ML,0.85,Extracted from log - paper 1291
Assessing LLM code generation quality through path planning tasks,"As LLM-generated code grows in popularity, more evaluation is needed to assess the risks of using such tools, especially for safety-critical applications such as path planning. Existing coding benchmarks are insufficient as they do not reflect the context and complexity of safety-critical applications. To this end, we assessed six LLMs' abilities to generate the code for three different path-planning algorithms and tested them on three maps of various difficulties. Our results suggest that LLM-generated code presents serious hazards for path planning applications and should not be applied in safety-critical contexts without rigorous testing.",CS,AI_ML,0.85,Extracted from log - paper 1292
Automated Test Generation from Program Documentation Encoded in Code Comments,"Documenting the functionality of software units with code comments, e.g., Javadoc comments, is a common programmer best-practice in software engineering. This paper introduces a novel test generation technique that exploits the code-comment documentation constructively. We originally address those behaviors as test objectives, which we pursue in search-based fashion. We deliver test cases with names and oracles properly contextualized on the target behaviors. Our experiments against a benchmark of 118 Java classes indicate that the proposed approach successfully tests many software behaviors that may remain untested with coverage-driven test generation approaches, and distinctively detects unknown failures.",CS,AI_ML,0.85,Extracted from log - paper 1293
OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification,"We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyu-hkust/OSVBench.",CS,AI_ML,0.85,Extracted from log - paper 1294
The Development of Reflective Practice on a Work-Based Software Engineering Program: A Longitudinal Study,"This study examines the development of reflective practice among students on a four-year work-based Software Engineering program. Using two established models of reflection - Boud et al.'s Model of Reflective Process and Bain et al.'s 5R Framework for Reflection - we analyse a series of reflective assignments submitted by students over four years. Our longitudinal analysis reveals clear trends in how students' reflective abilities evolve over the course of the program. We find that more sophisticated forms of reflection, such as integration of knowledge, appropriation of skills, and reconstruction of practice, increase markedly in prevalence in later years. The complementary nature of workplace experience and university study is highlighted in students' reflections, demonstrating a key benefit of the work-based learning approach. By the final year, all students demonstrate the ability to reconstruct their experiences to inform future practice. Our findings provide insight into how reflective practice develops in Software Engineering education and suggest potential value in incorporating more structured reflection into traditional degree programs. The study also reveals instances of meta-reflection, where students reflect on the value of reflection itself, indicating a deep engagement with the reflective process. While acknowledging limitations, this work offers a unique longitudinal perspective on the development of reflective practice in work-based Software Engineering education.",CS,AI_ML,0.85,Extracted from log - paper 1295
An Empirical Study on the Capability of LLMs in Decomposing Bug Reports,"Background: Bug reports are essential to the software development life cycle. They help developers track and resolve issues, but are often difficult to process due to their complexity, which can delay resolution and affect software quality. Aims: This study investigates whether large language models (LLMs) can assist developers in automatically decomposing complex bug reports into smaller, self-contained units, making them easier to understand and address. Method: We conducted an empirical study on 127 resolved privacy-related bug reports collected from Apache Jira. We evaluated ChatGPT and DeepSeek using different prompting strategies. We first tested both LLMs with zero-shot prompts, then applied improved prompts with demonstrations (using few-shot prompting) to measure their abilities in bug decomposition. Results: Our findings show that LLMs are capable of decomposing bug reports, but their overall performance still requires further improvement and strongly depends on the quality of the prompts. With zero-shot prompts, both studied LLMs (ChatGPT and DeepSeek) performed poorly. After prompt tuning, ChatGPT's true decomposition rate increased by 140\% and DeepSeek's by 163.64\%. Conclusions: LLMs show potential in helping developers analyze and decompose complex bug reports, but they still need improvement in terms of accuracy and bug understanding.",CS,AI_ML,0.85,Extracted from log - paper 1296
MANILA: A Low-Code Application to Benchmark Machine Learning Models and Fairness-Enhancing Methods,"This paper presents MANILA, a web-based low-code application to benchmark machine learning models and fairness-enhancing methods and select the one achieving the best fairness and effectiveness trade-off. It is grounded on an Extended Feature Model that models a general fairness benchmarking workflow as a Software Product Line. The constraints defined among the features guide users in creating experiments that do not lead to execution errors. We describe the architecture and implementation of MANILA and evaluate it in terms of expressiveness and correctness.",CS,AI_ML,0.85,Extracted from log - paper 1297
A Systematic Literature Review of Parameter-Efficient Fine-Tuning for Large Code Models,"The rise of Artificial Intelligence (AI)-and particularly Large Language Models (LLMs) for code-has reshaped Software Engineering (SE) by enabling the automation of tasks such as code generation, bug detection, and repair. However, these models require significant computational resources for training and fine-tuning, posing challenges for real-world adoption in resource-constrained environments. To address this, the research community has increasingly turned to Parameter-Efficient Fine-Tuning (PEFT)-a class of techniques that enables the adaptation of large models by updating only a small subset of parameters, rather than the entire model. In this Systematic Literature Review (SLR), we examine the growing application of PEFT techniques-across a wide range of software engineering tasks. We analyze how these methods are used to optimize various deep learning (DL) architectures, focusing on their impact on both performance and efficiency. Our study synthesizes findings from 27 peer-reviewed papers, identifying patterns in configuration strategies and adaptation trade-offs. The outcome of this review is a comprehensive taxonomy that categorizes PEFT usage by task type, distinguishing between generative (e.g., Code Summarization) and non-generative (e.g., Code Clone Detection) scenarios. Our findings aim to inform future research and guide the practical deployment of PEFT in sustainable, AI-powered software development. Our artifacts are publicly available at https://github.com/alvi75/SLR-PEFT",CS,AI_ML,0.85,Extracted from log - paper 1298
LELANTE: LEveraging LLM for Automated ANdroid TEsting,"Given natural language test case description for an Android application, existing testing approaches require developers to manually write scripts using tools such as Appium and Espresso to execute the corresponding test case. This process is labor-intensive and demands significant effort to maintain as UI interfaces evolve throughout development. In this work, we introduce LELANTE, a novel framework that utilizes large language models (LLMs) to automate test case execution without requiring pre-written scripts. LELANTE interprets natural language test case descriptions, iteratively generate action plans, and perform the actions directly on the Android screen using its GUI. LELANTE employs a screen refinement process to enhance LLM interpretability, constructs a structured prompt for LLMs, and implements an action generation mechanism based on chain-of-thought reasoning of LLMs. To further reduce computational cost and enhance scalability, LELANTE utilizes model distillation using a foundational LLM. In experiments across 390 test cases spanning 10 popular Android applications, LELANTE achieved a 73% test execution success rate. Our results demonstrate that LLMs can effectively bridge the gap between natural language test case description and automated execution, making mobile testing more scalable and adaptable.",CS,AI_ML,0.85,Extracted from log - paper 1299
"Secure Coding with AI, From Creation to Inspection","While prior studies have explored security in code generated by ChatGPT and other Large Language Models, they were conducted in controlled experimental settings and did not use code generated or provided from actual developer interactions. This paper not only examines the security of code generated by ChatGPT based on real developer interactions, curated in the DevGPT dataset, but also assesses ChatGPT's capability to find and fix these vulnerabilities. We analysed 1,586 C, C++, and C# code snippets using static scanners, which detected potential issues in 124 files. After manual analysis, we selected 26 files with 32 confirmed vulnerabilities for further investigation.   We submitted these files to ChatGPT via the OpenAI API, asking it to detect security issues, identify the corresponding Common Weakness Enumeration numbers, and propose fixes. The responses and modified code were manually reviewed and re-scanned for vulnerabilities. ChatGPT successfully detected 18 out of 32 security issues and resolved 17 issues but failed to recognize or fix the remainder. Interestingly, only 10 vulnerabilities were resulted from the user prompts, while 22 were introduced by ChatGPT itself.   We highlight for developers that code generated by ChatGPT is more likely to contain vulnerabilities compared to their own code. Furthermore, at times ChatGPT reports incorrect information with apparent confidence, which may mislead less experienced developers. Our findings confirm previous studies in demonstrating that ChatGPT is not sufficiently reliable for generating secure code nor identifying all vulnerabilities, highlighting the continuing importance of static scanners and manual review.",CS,AI_ML,0.85,Extracted from log - paper 1300
Unlocking User-oriented Pages: Intention-driven Black-box Scanner for Real-world Web Applications,"Black-box scanners have played a significant role in detecting vulnerabilities for web applications. A key focus in current black-box scanning is increasing test coverage (i.e., accessing more web pages). However, since many web applications are user-oriented, some deep pages can only be accessed through complex user interactions, which are difficult to reach by existing black-box scanners. To fill this gap, a key insight is that web pages contain a wealth of semantic information that can aid in understanding potential user intention. Based on this insight, we propose Hoyen, a black-box scanner that uses the Large Language Model to predict user intention and provide guidance for expanding the scanning scope. Hoyen has been rigorously evaluated on 12 popular open-source web applications and compared with 6 representative tools. The results demonstrate that Hoyen performs a comprehensive exploration of web applications, expanding the attack surface while achieving about 2x than the coverage of other scanners on average, with high request accuracy. Furthermore, Hoyen detected over 90% of its requests towards the core functionality of the application, detecting more vulnerabilities than other scanners, including unique vulnerabilities in well-known web applications. Our data/code is available at https://hoyen.tjunsl.com/",CS,AI_ML,0.85,Extracted from log - paper 1301
"Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges","Recent technical breakthroughs in large language models (LLMs) have enabled them to fluently generate source code. Software developers often leverage both general-purpose and code-specialized LLMs to revise existing code or even generate a whole function from scratch. These capabilities are also beneficial in no-code or low-code contexts, in which one can write programs without a technical background. However, due to their internal design, LLMs are prone to generating hallucinations, which are incorrect, nonsensical, and not justifiable information but difficult to identify its presence. This problem also occurs when generating source code. Once hallucinated code is produced, it is often challenging for users to identify and fix it, especially when such hallucinations can be identified under specific execution paths. As a result, the hallucinated code may remain unnoticed within the codebase. This survey investigates recent studies and techniques relevant to hallucinations generated by CodeLLMs. We categorize the types of hallucinations in the code generated by CodeLLMs, review existing benchmarks and mitigation strategies, and identify open challenges. Based on these findings, this survey outlines further research directions in the detection and removal of hallucinations produced by CodeLLMs.",CS,AI_ML,0.85,Extracted from log - paper 1302
Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces,"Adaptive User Interfaces (AUI) play a crucial role in modern software applications by dynamically adjusting interface elements to accommodate users' diverse and evolving needs. However, existing adaptation strategies often lack real-time responsiveness. Reinforcement Learning (RL) has emerged as a promising approach for addressing complex, sequential adaptation challenges, enabling adaptive systems to learn optimal policies based on previous adaptation experiences. Although RL has been applied to AUIs,integrating RL agents effectively within user interactions remains a challenge.   In this paper, we enhance a RL-based Adaptive User Interface adaption framework by incorporating personalized human feedback directly into the leaning process. Unlike prior approaches that rely on a single pre-trained RL model, our approach trains a unique RL agent for each user, allowing individuals to actively shape their personal RL agent's policy, potentially leading to more personalized and responsive UI adaptations. To evaluate this approach, we conducted an empirical study to assess the impact of integrating human feedback into the RL-based Adaptive User Interface adaption framework and its effect on User Experience (UX). The study involved 33 participants interacting with AUIs incorporating human feedback and non-adaptive user interfaces in two domains: an e-learning platform and a trip-planning application. The results suggest that incorporating human feedback into RL-driven adaptations significantly enhances UX, offering promising directions for advancing adaptive capabilities and user-centered design in AUIs.",CS,AI_ML,0.85,Extracted from log - paper 1303
Using LLMs in Generating Design Rationale for Software Architecture Decisions,"Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. Based on the results, we further discussed the pros and cons of the three prompting strategies and the strengths and limitations of the DR generated by LLMs.",CS,AI_ML,0.85,Extracted from log - paper 1304
"Understanding Large Language Model Supply Chain: Structure, Domain, and Vulnerabilities","Large Language Models (LLMs) have revolutionized artificial intelligence (AI), driving breakthroughs in natural language understanding, text generation, and autonomous systems. However, the rapid growth of LLMs presents significant challenges in the security and reliability of the Large Language Model Supply Chain (LLMSC), a complex network of open-source components, libraries, and tools essential for LLM development and deployment. Despite its critical importance, the LLMSC remains underexplored, particularly regarding its structural characteristics, domain composition, and security vulnerabilities. To address this gap, we conduct the first empirical study of the LLMSC, analyzing a curated dataset of open-source packages from PyPI and NPM across 14 functional domains. We construct a directed dependency graph comprising 15,725 nodes, 10,402 edges, and 180 unique vulnerabilities to investigate the structural characteristics of the LLMSC and analyze how security risks propagate through its dependency network. Our findings reveal that the LLMSC exhibits a ``locally dense, globally sparse'' topology, with 79.7% of dependency trees containing fewer than 5 nodes, while a few large trees dominate the ecosystem, accounting for 77.66% of all nodes. The graph is characterized by high-degree hubs, with the top 5 most connected nodes averaging 1,282 dependents each. Security analysis shows that critical vulnerabilities propagate to an average of 142.1 nodes at the second layer of dependency trees and peak at 237.8 affected nodes at the third layer. Notably, cascading risks are concentrated in critical hub nodes such as transformers, which directly or indirectly affect over 1,300 downstream packages. These findings provide quantitative insights into the structural and security dynamics of the LLMSC and emphasize the need for targeted mitigation strategies to enhance ecosystem resilience.",CS,AI_ML,0.85,Extracted from log - paper 1305
Identifying Uncertainty in Self-Adaptive Robotics with Large Language Models,"Future self-adaptive robots are expected to operate in highly dynamic environments while effectively managing uncertainties. However, identifying the sources and impacts of uncertainties in such robotic systems and defining appropriate mitigation strategies is challenging due to the inherent complexity of self-adaptive robots and the lack of comprehensive knowledge about the various factors influencing uncertainty. Hence, practitioners often rely on intuition and past experiences from similar systems to address uncertainties. In this article, we evaluate the potential of large language models (LLMs) in enabling a systematic and automated approach to identify uncertainties in self-adaptive robotics throughout the software engineering lifecycle. For this evaluation, we analyzed 10 advanced LLMs with varying capabilities across four industrial-sized robotics case studies, gathering the practitioners' perspectives on the LLM-generated responses related to uncertainties. Results showed that practitioners agreed with 63-88% of the LLM responses and expressed strong interest in the practicality of LLMs for this purpose.",CS,AI_ML,0.85,Extracted from log - paper 1306
CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation,"Large language models (LLMs) play a crucial role in software engineering, excelling in tasks like code generation and maintenance. However, existing benchmarks are often narrow in scope, focusing on a specific task and lack a comprehensive evaluation framework that reflects real-world applications. To address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark), designed to evaluate LLMs across four critical dimensions: code understanding, code generation, code modification, and code review. These dimensions capture essential developer needs, ensuring a more systematic and representative evaluation. CoCo-Bench includes multiple programming languages and varying task difficulties, with rigorous manual review to ensure data quality and accuracy. Empirical results show that CoCo-Bench aligns with existing benchmarks while uncovering significant variations in model performance, effectively highlighting strengths and weaknesses. By offering a holistic and objective evaluation, CoCo-Bench provides valuable insights to guide future research and technological advancements in code-oriented LLMs, establishing a reliable benchmark for the field.",CS,AI_ML,0.85,Extracted from log - paper 1307
ComplexVCoder: An LLM-Driven Framework for Systematic Generation of Complex Verilog Code,"Recent advances have demonstrated the promising capabilities of large language models (LLMs) in generating register-transfer level (RTL) code, such as Verilog. However, existing LLM-based frameworks still face significant challenges in accurately handling the complexity of real-world RTL designs, particularly those that are large-scale and involve multi-level module instantiations. To address this issue, we present ComplexVCoder, an open-source LLM-driven framework that enhances both the generation quality and efficiency of complex Verilog code. Specifically, we introduce a two-stage generation mechanism, which leverages an intermediate representation to enable a more accurate and structured transition from natural language descriptions to intricate Verilog designs. In addition, we introduce a rule-based alignment method and a domain-specific retrieval-augmented generation (RAG) to further improve the correctness of the synthesized code by incorporating relevant design knowledge during generation. To evaluate our approach, we construct a comprehensive dataset comprising 55 complex Verilog designs derived from real-world implementations. We also release an open-source benchmark suite for systematically assessing the quality of auto-generated RTL code together with the ComplexVCoder framework. Experimental results show that ComplexVCoder outperforms SOTA frameworks such as CodeV and RTLCoder by 14.6% and 22.2%, respectively, in terms of function correctness on complex Verilog benchmarks. Furthermore, ComplexVcoder achieves comparable generation performances in terms of functionality correctness using a lightweight 32B model (Qwen2.5), rivaling larger-scale models such as GPT-3.5 and DeepSeek-V3.",CS,AI_ML,0.85,Extracted from log - paper 1308
Seeking Specifications: The Case for Neuro-Symbolic Specification Synthesis,"This work is concerned with the generation of formal specifications from code, using Large Language Models (LLMs) in combination with symbolic methods. Concretely, in our study, the programming language is C, the specification language is ACSL, and the LLM is Deepseek-R1. In this context, we address two research directions, namely the specification of intent vs. implementation on the one hand, and the combination of symbolic analyses with LLMs on the other hand. For the first, we investigate how the absence or presence of bugs in the code impacts the generated specifications, as well as whether and how a user can direct the LLM to specify intent or implementation, respectively. For the second, we investigate the impact of results from symbolic analyses on the specifications generated by the LLM. The LLM prompts are augmented with outputs from two formal methods tools in the Frama-C ecosystem, Pathcrawler and EVA. We demonstrate how the addition of symbolic analysis to the workflow impacts the quality of annotations.",CS,AI_ML,0.85,Extracted from log - paper 1309
Challenges of Requirements Communication and Digital Assets Verification in Infrastructure Projects,"Background: Poor communication of requirements between clients and suppliers contributes to project overruns,in both software and infrastructure projects. Existing literature offers limited insights into the communication challenges at this interface. Aim: Our research aim to explore the processes and associated challenges with requirements activities that include client-supplier interaction and communication. Method: we study requirements validation, communication, and digital asset verification processes through two case studies in the road and railway sectors, involving interviews with ten experts across three companies. Results: We identify 13 challenges, along with their causes and consequences, and suggest solution areas from existing literature. Conclusion: Interestingly, the challenges in infrastructure projects mirror those found in software engineering, highlighting a need for further research to validate potential solutions.",CS,AI_ML,0.85,Extracted from log - paper 1310
Taxonomic Trace Links: Rethinking Traceability and its Benefits,"Traceability greatly supports knowledge-intensive tasks, e.g., coverage check and impact analysis. Despite its clear benefits, the \emph{practical} implementation of traceability poses significant challenges, leading to a reduced focus on the creation and maintenance of trace links. We propose a new approach -- Taxonomic Trace Links (TTL) -- which rethinks traceability and its benefits. With TTL, trace links are created indirectly through a domain-specific taxonomy, a simplified version of a domain model. TTL has the potential to address key traceability challenges, such as the granularity of trace links, the lack of a common data structure among software development artifacts, and unclear responsibility for traceability. We explain how TTL addresses these challenges and perform an initial validation with practitioners. We identified six challenges associated with TTL implementation that need to be addressed. Finally, we propose a research roadmap to further develop and evaluate the technical solution of TTL. TTL appears to be particularly feasible in practice where a domain taxonomy is already established",CS,AI_ML,0.85,Extracted from log - paper 1311
Sleeping Giants -- Activating Dormant Java Deserialization Gadget Chains through Stealthy Code Changes,"Java deserialization gadget chains are a well-researched critical software weakness. The vast majority of known gadget chains rely on gadgets from software dependencies. Furthermore, it has been shown that small code changes in dependencies have enabled these gadget chains. This makes gadget chain detection a purely reactive endeavor. Even if one dependency's deployment pipeline employs gadget chain detection, a gadget chain can still result from gadgets in other dependencies. In this work, we assess how likely small code changes are to enable a gadget chain. These changes could either be accidental or intentional as part of a supply chain attack. Specifically, we show that class serializability is a strongly fluctuating property over a dependency's evolution. Then, we investigate three change patterns by which an attacker could stealthily introduce gadgets into a dependency. We apply these patterns to 533 dependencies and run three state-of-the-art gadget chain detectors both on the original and the modified dependencies. The tools detect that applying the modification patterns can activate/inject gadget chains in 26.08% of the dependencies we selected. Finally, we verify the newly detected chains. As such, we identify dormant gadget chains in 53 dependencies that could be added through minor code modifications. This both shows that Java deserialization gadget chains are a broad liability to software and proves dormant gadget chains as a lucrative supply chain attack vector.",CS,AI_ML,0.85,Extracted from log - paper 1312
ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement,"In supercomputing, efficient and optimized code generation is essential to leverage high-performance systems effectively. We propose Agentic Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate, robust, and efficient code generation, completion, and translation. ARCS integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT) reasoning to systematically break down and iteratively refine complex programming tasks. An agent-based RAG mechanism retrieves relevant code snippets, while real-time execution feedback drives the synthesis of candidate solutions. This process is formalized as a state-action search tree optimization, balancing code correctness with editing efficiency. Evaluations on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly outperforms traditional prompting methods in translation and generation quality. By enabling scalable and precise code synthesis, ARCS offers transformative potential for automating and optimizing code development in supercomputing applications, enhancing computational resource utilization.",CS,AI_ML,0.85,Extracted from log - paper 1313
CrashFixer: A crash resolution agent for the Linux kernel,"Code large language models (LLMs) have shown impressive capabilities on a multitude of software engineering tasks. In particular, they have demonstrated remarkable utility in the task of code repair. However, common benchmarks used to evaluate the performance of code LLMs are often limited to small-scale settings. In this work, we build upon kGym, which shares a benchmark for system-level Linux kernel bugs and a platform to run experiments on the Linux kernel.   This paper introduces CrashFixer, the first LLM-based software repair agent that is applicable to Linux kernel bugs. Inspired by the typical workflow of a kernel developer, we identify the key capabilities an expert developer leverages to resolve a kernel crash. Using this as our guide, we revisit the kGym platform and identify key system improvements needed to practically run LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of code). We implement these changes by extending kGym to create an improved platform - called kGymSuite, which will be open-sourced. Finally, the paper presents an evaluation of various repair strategies for such complex kernel bugs and showcases the value of explicitly generating a hypothesis before attempting to fix bugs in complex systems such as the Linux kernel. We also evaluated CrashFixer's capabilities on still open bugs, and found at least two patch suggestions considered plausible to resolve the reported bug.",CS,AI_ML,0.85,Extracted from log - paper 1314
Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs,"Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.",CS,AI_ML,0.85,Extracted from log - paper 1315
An Empirical Study on Common Defects in Modern Web Browsers Using Knowledge Embedding in GPT-4o,"Technology is advancing at an unprecedented pace. With the advent of cutting-edge technologies, keeping up with rapid changes are becoming increasingly challenging. In addition to that, increasing dependencies on the cloud technologies have imposed enormous pressure on modern web browsers leading to adapting new technologies faster and making them more susceptible to defects/bugs. Although, many studies have explored browser bugs, a comparative study among the modern browsers generalizing the bug categories and their nature was still lacking. To fill this gap, we undertook an empirical investigation aimed at gaining insights into the prevalent bugs in Google Chromium and Mozilla Firefox as the representatives of modern web browsers. We used GPT-4.o to identify the defect (bugs) categories and analyze the clusters of the most commonly appeared bugs in the two prominent web browsers. Additionally, we compared our LLM based bug categorization with the traditional NLP based approach using TF-IDF and K-Means clustering. We found that although Google Chromium and Firefox have evolved together since almost around the same time (2006-2008), Firefox suffers from high number of bugs having extremely high defect-prone components compared to Chromium. This exploratory study offers valuable insights on the browser bugs and defect-prone components to the developers, enabling them to craft web browsers and web-applications with enhanced resilience and reduced errors.",CS,AI_ML,0.85,Extracted from log - paper 1316
Automated Unit Test Case Generation: A Systematic Literature Review,"Software is omnipresent within all factors of society. It is thus important to ensure that software are well tested to mitigate bad user experiences as well as the potential for severe financial and human losses. Software testing is however expensive and absorbs valuable time and resources. As a result, the field of automated software testing has grown of interest to researchers in past decades. In our review of present and past research papers, we have identified an information gap in the areas of improvement for the Genetic Algorithm and Particle Swarm Optimisation. A gap in knowledge in the current challenges that face automated testing has also been identified. We therefore present this systematic literature review in an effort to consolidate existing knowledge in regards to the evolutionary approaches as well as their improvements and resulting limitations. These improvements include hybrid algorithm combinations as well as interoperability with mutation testing and neural networks. We will also explore the main test criterion that are used in these algorithms alongside the challenges currently faced in the field related to readability, mocking and more.",CS,AI_ML,0.85,Extracted from log - paper 1317
SoK: Enhancing Privacy-Preserving Software Development from a Developers' Perspective,"In software development, privacy preservation has become essential with the rise of privacy concerns and regulations such as GDPR and CCPA. While several tools, guidelines, methods, methodologies, and frameworks have been proposed to support developers embedding privacy into software applications, most of them are proofs-of-concept without empirical evaluations, making their practical applicability uncertain. These solutions should be evaluated for different types of scenarios (e.g., industry settings such as rapid software development environments, teams with different privacy knowledge, etc.) to determine what their limitations are in various industry settings and what changes are required to refine current solutions before putting them into industry and developing new developer-supporting approaches. For that, a thorough review of empirically evaluated current solutions will be very effective. However, the existing secondary studies that examine the available developer support provide broad overviews but do not specifically analyze empirically evaluated solutions and their limitations. Therefore, this Systematic Literature Review (SLR) aims to identify and analyze empirically validated solutions that are designed to help developers in privacy-preserving software development. The findings will provide valuable insights for researchers to improve current privacy-preserving solutions and for practitioners looking for effective and validated solutions to embed privacy into software development.",CS,AI_ML,0.85,Extracted from log - paper 1318
"A survey on large language model (LLM) security and privacy:The Good, The Bad, and The Ugly","Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.",CS,AI_ML,0.85,Extracted from log - paper 1319
Detect Anything 3D in the Wild,"Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which mitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data. DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at DetAny3D project page.",CS,AI_ML,0.85,Extracted from log - paper 1320
Survey of clustering algorithms,"Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.",CS,AI_ML,0.85,Extracted from log - paper 1321
Understanding egocentric activities,"We present a method to analyze daily activities, such as meal preparation, using video from an egocentric camera. Our method performs inference about activities, actions, hands, and objects. Daily activities are a challenging domain for activity recognition which are well-suited to an egocentric approach. In contrast to previous activity recognition methods, our approach does not require pre-trained detectors for objects and hands. Instead we demonstrate the ability to learn a hierarchical model of an activity by exploiting the consistent appearance of objects, hands, and actions that results from the egocentric context. We show that joint modeling of activities, actions, and objects leads to superior performance in comparison to the case where they are considered independently. We introduce a novel representation of actions based on object-hand interactions and experimentally demonstrate the superior performance of our representation in comparison to standard activity representations such as bag of words.",CS,AI_ML,0.85,Extracted from log - paper 1322
High-performance Implementation of Elliptic Curve Cryptography Using Vector Instructions,"Elliptic curve cryptosystems are considered an efficient alternative to conventional systems such as DSA and RSA. Recently, Montgomery and Edwards elliptic curves have been used to implement cryptosystems. In particular, the elliptic curves Curve25519 and Curve448 were used for instantiating Diffie-Hellman protocols named X25519 and X448. Mapping these curves to twisted Edwards curves allowed deriving two new signature instances, called Ed25519 and Ed448, of the Edwards Digital Signature Algorithm. In this work, we focus on the secure and efficient software implementation of these algorithms using SIMD parallel processing. We present software techniques that target the Intel AVX2 vector instruction set for accelerating prime field arithmetic and elliptic curve operations. Our contributions result in a high-performance software library for AVX2-ready processors. For example, our library computes digital signatures 19% (for Ed25519) and 29% (for Ed448) faster than previous optimized implementations. Also, our library improves by 10% and 20% the execution time of X25519 and X448, respectively.",CS,AI_ML,0.85,Extracted from log - paper 1323
Federated Learning in Mobile Edge Networks: A Comprehensive Survey,"In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL.",CS,AI_ML,0.85,Extracted from log - paper 1324
On Extractive and Abstractive Neural Document Summarization with Transformer Language Models,"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work.",CS,AI_ML,0.85,Extracted from log - paper 1325
STGSN — A Spatial–Temporal Graph Neural Network framework for time-evolving social networks,"Social Network Analysis (SNA) has been a popular field of research since the early 1990s. Law enforcement agencies have been utilizing it as a tool for intelligence gathering and criminal investigation for decades. However, the graph nature of social networks makes it highly restricted to intelligence analysis tasks, such as role prediction (node classification), social relation inference (link prediction), and criminal group discovery (community detection), etc. In the past few years, many studies have focused on Graph Neural Network (GNN), which utilizes deep learning methods to solve graph-related problems. However, we have rarely seen GNNs tackle time-evolving social network problems, especially in the criminology field. The existing studies have commonly over-looked the temporal-evolution characteristics of social networks. In this paper, we propose a graph neural network framework, namely Spatial-Temporal Graph Social Network (STGSN), which models social networks from both spatial and temporal perspectives. Using a novel approach, we leverage the temporal attention mechanism to capture social networks’ temporal features. We design a method analyzing temporal attention distribution to improve the interpretation ability of our method. In the end, we conduct extensive experiments on six public datasets to prove our methods’ effectiveness.",CS,AI_ML,0.85,Extracted from log - paper 1326
On the capability of static code analysis to detect security vulnerabilities,"Context: Static analysis of source code is a scalable method for discovery of software faults and security vulnerabilities. Techniques for static code analysis have matured in the last decade and many tools have been developed to support automatic detection.Objective: This research work is focused on empirical evaluation of the ability of static code analysis tools to detect security vulnerabilities with an objective to better understand their strengths and shortcomings.Method: We conducted an experiment which consisted of using the benchmarking test suite Juliet to evaluate three widely used commercial tools for static code analysis. Using design of experiments approach to conduct the analysis and evaluation and including statistical testing of the results are unique characteristics of this work. In addition to the controlled experiment, the empirical evaluation included case studies based on three open source programs.Results: Our experiment showed that 27% of C/C++ vulnerabilities and 11% of Java vulnerabilities were missed by all three tools. Some vulnerabilities were detected by only one or combination of two tools; 41% of C/C++ and 21% of Java vulnerabilities were detected by all three tools. More importantly, static code analysis tools did not show statistically significant difference in their ability to detect security vulnerabilities for both C/C++ and Java. Interestingly, all tools had median and mean of the per CWE recall values and overall recall across all CWEs close to or below 50%, which indicates comparable or worse performance than random guessing. While for C/C++ vulnerabilities one of the tools had better performance in terms of probability of false alarm than the other two tools, there was no statistically significant difference among tools’ probability of false alarm for Java test cases.Conclusions: Despite recent advances in methods for static code analysis, the state-of-the-art tools are not very effective in detecting security vulnerabilities.",CS,AI_ML,0.85,Extracted from log - paper 1327
Self-Supervised Representation Learning From Videos for Facial Action Unit Detection,"In this paper, we aim to learn discriminative representation for facial action unit (AU) detection from large amount of videos without manual annotations. Inspired by the fact that facial actions are the movements of facial muscles, we depict the movements as the transformation between two face images in different frames and use it as the self-supervisory signal to learn the representations. However, under the uncontrolled condition, the transformation is caused by both facial actions and head motions. To remove the influence by head motions, we propose a Twin-Cycle Autoencoder (TCAE) that can disentangle the facial action related movements and the head motion related ones. Specifically, TCAE is trained to respectively change the facial actions and head poses of the source face to those of the target face. Our experiments validate TCAE's capability of decoupling the movements. Experimental results also demonstrate that the learned representation is discriminative for AU detection, where TCAE outperforms or is comparable with the state-of-the-art self-supervised learning methods and supervised AU detection methods.",CS,AI_ML,0.85,Extracted from log - paper 1328
Auto-Keras: An Efficient Neural Architecture Search System,"Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at the auto keras website. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.",CS,AI_ML,0.85,Extracted from log - paper 1329
Graph theoretic techniques for pruning data and their applications,"In pattern recognition tasks, we usually do not pay much attention to the arbitrarily chosen training set of a pattern classifier beforehand. This correspondence proposes several methods for pruning data sets based upon graph theory in order to alleviate redundancy in the original data set while retaining the original data structure as far as possible. The proposed methods are applied to the training sets for pattern recognition by a multilayered perceptron neural network (MLP-NN) and the locations of the centroids of a radial basis function neural network (RBF-NN). The advantage of the proposed graph theoretic methods is that they do not require any calculation for the statistical distributions of the clusters. The experimental results in comparison both with the k-means clustering and with the learning vector quantization (LVQ) methods show that the proposed methods give encouraging performance in terms of computation for data classification tasks.",CS,AI_ML,0.85,Extracted from log - paper 1330
Secure multi-party computation problems and their applications: a review and open problems,"The growth of the Internet has triggered tremendous opportunities for cooperative computation, where people are jointly conducting computation tasks based on the private inputs they each supplies. These computations could occur between mutually untrusted parties, or even between competitors. For example, customers might send to a remote database queries that contain private information; two competing financial organizations might jointly invest in a project that must satisfy both organizations' private and valuable constraints, and so on. Today, to conduct such computations, one entity must usually know the inputs from all the participants; however if nobody can be trusted enough to know all the inputs, privacy will become a primary concern.This problem is referred to as Secure Multi-party Computation Problem (SMC) in the literature. Research in the SMC area has been focusing on only a limited set of specific SMC problems, while privacy concerned cooperative computations call for SMC studies in a variety of computation domains. Before we can study the problems, we need to identify and define the specific SMC problems for those computation domains. We have developed a framework to facilitate this problem-discovery task. Based on our framework, we have identified and defined a number of new SMC problems for a spectrum of computation domains. Those problems include privacy-preserving database query, privacy-preserving scientific computations, privacy-preserving intrusion detection, privacy-preserving statistical analysis, privacy-preserving geometric computations, and privacy-preserving data mining.The goal of this paper is not only to present our results, but also to serve as a guideline so other people can identify useful SMC problems in their own computation domains.",CS,AI_ML,0.85,Extracted from log - paper 1331
A simple differentiable programming language,"Automatic differentiation plays a prominent role in scientific computing and in modern machine learning, often in the context of powerful programming systems. The relation of the various embodiments of automatic differentiation to the mathematical notion of derivative is not always entirely clear---discrepancies can arise, sometimes inadvertently. In order to study automatic differentiation in such programming contexts, we define a small but expressive programming language that includes a construct for reverse-mode differentiation. We give operational and denotational semantics for this language. The operational semantics employs popular implementation techniques, while the denotational semantics employs notions of differentiation familiar from real analysis. We establish that these semantics coincide.",CS,AI_ML,0.85,Extracted from log - paper 1332
Succinct Dynamic Data Structures,"We develop succinct data structures to represent (i) a sequence of values to support partial sum and select queries and update(changing values) and (ii) a dynamic array consisting of a sequence of elements which supports insertion, deletion and access of an element at any given index.For the partial sums problem on n non-negative integers of k bits each, we support update operations in O(b) time and sum in O(logb n) time, for any parameter b, lgn/lglgn ≤ b ≤ n∈ for any fixed positive ∈ < 1. The space used is kn+o(kn) bits and the time bounds are optimal. When b = lgn/lglgn or k = 1 (i.e., when we are dealing with a bit-vector), we can also support the select operation in the same time as the sum operation, but the update time becomes amortised. For the dynamic array problem, we give two structures both using o(n) bits of extra space where n is the number of elements in the array: one supports lookup in constant worst case time and updates in O(n ε) worst case time, and the other supports all operations in O(lgn/lglgn) amortized time. The time bound of both these structures are optimal.",CS,AI_ML,0.85,Extracted from log - paper 1333
Is neuro-symbolic AI meeting its promises in natural language processing? A structured review,"Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis.",CS,AI_ML,0.85,Extracted from log - paper 1334
Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection,"Graph-based models have been widely used to fraud detection tasks. Owing to the development of Graph Neural Networks~(GNNs), recent works have proposed many GNN-based fraud detectors based on either homogeneous or heterogeneous graphs. These works leverage existing GNNs and aggregate the neighborhood information to learn the node embeddings, which relies on the assumption that the neighbors share similar context, features, and relations. However, the inconsistency problem incurred by fraudsters is hardly investigated, i.e., the context inconsistency, feature inconsistency, and relation inconsistency. In this paper, we introduce these inconsistencies and design a new GNN framework, GraphConsis, to tackle the inconsistency problem: (1) for the context inconsistency, we propose to combine the context embeddings with node features; (2) for the feature inconsistency, we design a consistency score to filter the inconsistent neighbors and generate corresponding sampling probability; (3) for the relation inconsistency, we learn the relation attention weights associated with the sampled nodes. Empirical analysis on four datasets demonstrates that the inconsistency problem is critical in fraud detection tasks. Extensive experiments show the effectiveness of GraphConsis. We also released a GNN-based fraud detection toolbox with implementations of SOTA models. The code is available at the github repo.",CS,AI_ML,0.85,Extracted from log - paper 1335
Analysis of Problems and Prospects of Implementation of Post-Quantum Cryptographic Algorithms ,"The paper provides an overview and analysis of the current state, problems, and prospects of post-quantum cryptography. Considered the status of the PostQuantum Cryptography Standardization Process. Organizations like the National Institute of Standards and Technology (NIST) are actively working on standardizing post-quantum cryptography. Evaluation rounds have been conducted, thoroughly analyzing numerous candidate post-quantum algorithms to select the most efficient and secure ones. Identified main categories of postquantum cryptographic algorithms. Described key size of post-quantum cryptographic algorithms. Open-source libraries like Open Quantum Safe (OQS) have been developed, offering implementations of various post-quantum algorithms. These libraries enable researchers, developers, and engineers to utilize and test post-quantum algorithms in various applications. There is growing awareness of the need to prepare for the post-quantum computing era. Many companies, organizations, and governments are exploring the implications of quantum computing for their infrastructure and data security and considering the adoption of post-quantum cryptographic solutions.",CS,AI_ML,0.85,Extracted from log - paper 1336
Multiple fault localization of software programs: A systematic literature review,"Multiple fault localization (MFL) is the act of identifying the locations of multiple faults (more than one fault) in a faulty software program. This is known to be more complicated, tedious, and costly in comparison to the traditional practice of presuming that a software contains a single fault. Due to the increasing interest in MFL by the research community, a broad spectrum of MFL debugging approaches and solutions have been proposed and developed. The aim of this study is to systematically review existing research on MFL in the software fault localization (SFL) domain. This study also aims to identify, categorize, and synthesize relevant studies in the research domain. Consequently, using an evidence-based systematic methodology, we identified 55 studies relevant to four research questions. The methodology provides a systematic selection and evaluation process with rigorous and repeatable evidence-based studies selection process. The result of the systematic review shows that research on MFL is gaining momentum with stable growth in the last 5 years. Three prominent MFL debugging approaches were identified, i.e. One-bug-at-a-time debugging approach (OBA), parallel debugging approach, and multiple-bug-at-a-time debugging approach (MBA), with OBA debugging approach being utilized the most. The study concludes with some identified research challenges and suggestions for future research. Although MFL is becoming of grave concern, existing solutions in the field are less mature. Studies utilizing real faults in their experiments are scarce. Concrete solutions to reduce MFL debugging time and cost by adopting an approach such as MBA debugging approach are also less, which require more attention from the research community.",CS,AI_ML,0.85,Extracted from log - paper 1337
Data-driven contextual modeling for 3D scene understanding,"The recent development of fast depth map fusion technique enables the realtime, detailed scene reconstruction using commodity depth camera, making the indoor scene understanding more possible than ever. To address the specific challenges in object analysis at subscene level, this work proposes a data-driven approach to modeling contextual information covering both intra-object part relations and inter-object object layouts. Our method combines the detection of https://www.sciencedirect.com/topics/computer-science/individual-object and object groups within the same framework, enabling contextual analysis without knowing the objects in the scene a priori. The key idea is that while contextual information could benefit the detection of either individual objects or object groups, both can contribute to object extraction when objects are unknown.Our method starts with a robust segmentation and partitions a subscene into segments, each of which represents either an independent object or a part of some object. A set of classifiers are trained for both individual objects and object groups, using a database of 3D scene models. We employ the multiple kernel learning (MKL) to learn per-category optimized classifiers for objects and object groups. Finally, we perform a graph matching to extract objects using the classifiers, thus grouping the segments into either an object or an object group. The output is an object-level labeled segmentation of the input subscene. Experiments demonstrate that the unified contextual analysis framework achieves robust object detection and recognition over cluttered subscenes.",CS,AI_ML,0.85,Extracted from log - paper 1338
Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,"Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. Weshowthat our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.",CS,AI_ML,0.85,Extracted from log - paper 1339
A Survey on Bias and Fairness in Machine Learning,"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",CS,AI_ML,0.85,Extracted from log - paper 1340
Multi-agent Path Planning and Network Flow,"This paper connects multi-agent path planning on graphs (roadmaps) to network flow problems, showing that the former can be reduced to the latter, therefore enabling the application of combinatorial network flow algorithms, as well as general linear program techniques, tomulti-agent path planning problems on graphs. Exploiting this connection, we show that when the goals are permutation invariant, the problem always has a feasible solution path set with a longest finish time of no more than n + V - 1 steps, in which n is the number of agents and V is the number of vertices of the underlying graph.We then give a complete algorithm that finds such a solution in O(nVE) time, with E being the number of edges of the graph. Taking a further step, we study time and distance optimality of the feasible solutions, show that they have a pairwise Pareto optimal structure, and again provide efficient algorithms for optimizing two of these practical objectives.",CS,AI_ML,0.85,Extracted from log - paper 1341
Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey,"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.",CS,AI_ML,0.85,Extracted from log - paper 1342
A Three-Stage Quantum Cryptography Protocol,"We present a three-stage quantum cryptographic protocol based on public key cryptography in which each party uses its own secret key. Unlike the BB84 protocol, where the qubits are transmitted in only one direction and classical information exchanged thereafter, the communication in the proposed protocol remains quantum in each stage. A related system of key distribution is also described.",CS,AI_ML,0.85,Extracted from log - paper 1343
Practical Private Computation and Zero-Knowledge Tools for Privacy-Preserving Distributed Data Mining,"In this paper we explore private computation built on vector addition and its applications in privacy-preserving data mining. Vector addition is a surprisingly general tool for implementing many algorithms prevalent in distributed data mining. Examples include linear algorithms like voting and summation, as well as non-linear algorithms such as SVD, PCA, k-means, ID3, machine learning algorithms based on Expectation Maximization (EM), etc., and all algorithms in the statistical query model [27]. The non-linear algorithms aggregate data only in certain steps, such as conjugate gradient, which are linear in the data. We introduce a new and highly efficient VSS (Verifiable Secret-Sharing) protocol in a special but widely-applicable model that allows secret-shared arithmetic operations in such aggregation steps to be done over small fields (e.g. 32 or 64 bits). There are two major advantages: (1) in this framework private arithmetic operations have the same cost as normal arithmetic and (2) the scheme admits extremely efficient zero-knowledge (ZK) protocols for verifying properties of user data. As a concrete example, we present a very efficient zero-knowledge method based on random projection for verification that uses a linear number of inexpensive small field operations, and only a logarithmic number of large-field (1024 bits or more) cryptographic operations. Our implementation shows that the approach can achieve orders of magnitude reduction in running time over standard techniques (from hours to seconds) for large scale problems. The ZK tools provide efficient mechanisms for dealing with actively cheating users, a realistic threat in distributed data mining which has been lacking practical solutions.",CS,AI_ML,0.85,Extracted from log - paper 1344
Machine Learning in Compiler Optimization,"In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.",CS,AI_ML,0.85,Extracted from log - paper 1345
"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers","Mirroring the success of masked language models, vision-and-language counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family - LXMERT - finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT's image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.",CS,AI_ML,0.85,Extracted from log - paper 1346
A Survey on Causal Inference,"Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.",CS,AI_ML,0.85,Extracted from log - paper 1347
RLTCP: A reinforcement learning approach to prioritizing automated user interface tests,"User interface testing validates the correctness of an application through visual cues and interactive events emitted in real-world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for full execution. This paper describes a novel test prioritization method called RLTCP whose goal is to maximize the number of test faults detected while reducing the amount of test. We define a weighted coverage graph to model the underlying association among test cases for the user interface testing. Our method combines Reinforcement Learning (RL) and the coverage graph to prioritize test cases. While RL is found to be suitable for rapidly changing projects with abundant historical data, the coverage graph considers in-depth the event-based aspects of user interface testing and provides a fine-grained level at which the RL system can gain more insights into individual test cases. We experiment and assess the proposed method using nine data sets obtained from two mature web applications, finding that the method outperforms the six, including the state-of-the-art, methods. The use of both reinforcement learning and the underlying structure of user interface tests modeled via the coverage has the potential to improve the performance of test prioritization methods. Our study also shows the benefit of using the coverage graph to gain insights into test cases, their relationship and execution history.",CS,AI_ML,0.85,Extracted from log - paper 1348
Formal Verification of Smart Contracts: Short Paper,"Ethereum is a framework for cryptocurrencies which uses blockchain technology to provide an open global computing platform, called the Ethereum Virtual Machine (EVM). EVM executes bytecode on a simple stack machine. Programmers do not usually write EVM code; instead, they can program in a JavaScript-like language, called Solidity, that compiles to bytecode. Since the main purpose of EVM is to execute smart contracts that manage and transfer digital assets (called Ether), security is of paramount importance. However, writing secure smart contracts can be extremely difficult: due to the openness of Ethereum, both programs and pseudonymous users can call into the public methods of other programs, leading to potentially dangerous compositions of trusted and untrusted code. This risk was recently illustrated by an attack on TheDAO contract that exploited subtle details of the EVM semantics to transfer roughly $50M worth of Ether into the control of an attacker. In this paper, we outline a framework to analyze and verify both the runtime safety and the functional correctness of Ethereum contracts by translation to F*, a functional programming language aimed at program verification.",CS,AI_ML,0.85,Extracted from log - paper 1349
Combining Graph-Based Learning With Automated Data Collection for Code Vulnerability Detection,"This paper presents FUNDED (Flow-sensitive vUl-Nerability coDE Detection), a novel learning framework for building vulnerability detection models. Funded leverages the advances in graph neural networks (GNNs) to develop a novel graph-based learning method to capture and reason about the program’s control, data, and call dependencies. Unlike prior work that treats the program as a sequential sequence or an untyped graph, Funded learns and operates on a graph representation of the program source code, in which individual statements are connected to other statements through relational edges. By capturing the program syntax, semantics and flows, Funded finds better code representation for the downstream software vulnerability detection task. To provide sufficient training data to build an effective deep learning model, we combine probabilistic learning and statistical assessments to automatically gather high-quality training samples from open-source projects. This provides many real-life vulnerable code training samples to complement the limited vulnerable code samples available in standard vulnerability databases. We apply Funded to identify software vulnerabilities at the function level from program source code. We evaluate Funded on large real-world datasets with programs written in C, Java, Swift and Php, and compare it against six state-of-the-art code vulnerability detection models. Experimental results show that Funded significantly outperforms alternative approaches across evaluation settings.",CS,AI_ML,0.85,Extracted from log - paper 1350
Symbolic planning and control using game theory and grammatical inference,"A system can accomplish an objective specified in temporal logic while interacting with an unknown, dynamic but rule-governed environment, by employing grammatical inference and adapting its plan of action on-line. The purposeful interaction of the system with its unknown environment can be described by a deterministic two-player zero-sum game. Using special new product operations, the whole game can be expressed with a factored, modular representation. This representation not only offers computational benefits but also isolates the unknown behavior of the dynamic environment in a particular subsystem, which then becomes the target of learning. As the fidelity of the identified environment model increases, the strategy synthesized based on the learned hypothesis converges in finite time to the one that satisfies the task specification.",CS,AI_ML,0.85,Extracted from log - paper 1351
Energy-Efficient Algorithms,"We initiate the systematic study of the energy complexity of algorithms (in addition to time and space complexity) based on Landauer's Principle in physics, which gives a lower bound on the amount of energy a system must dissipate if it destroys information. We propose energy-aware variations of three standard models of computation: circuit RAM, word RAM, and transdichotomous RAM. On top of these models, we build familiar high-level primitives such as control logic, memory allocation, and garbage collection with zero energy complexity and only constant-factor overheads in space and time complexity, enabling simple expression of energy-efficient algorithms. We analyze several classic algorithms in our models and develop low-energy variations: comparison sort, insertion sort, counting sort, breadth-first search, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL trees, binary heaps, and dynamic arrays. We explore the time/space/energy trade-off and develop several general techniques for analyzing algorithms and reducing their energy complexity. These results lay a theoretical foundation for a new field of semi-reversible computing and provide a new framework for the investigation of algorithms.",CS,AI_ML,0.85,Extracted from log - paper 1352
A Fairness-aware Incentive Scheme for Federated Learning,"In federated learning (FL), data owners ""share"" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.",CS,AI_ML,0.85,Extracted from log - paper 1353
A Self-Regulated Learning Approach to Educational Recommender Design,"The field of education has the potential to better facilitate student learning by employing educational recommenders that adapt the learning process to the needs of individual learners. While existing research has shown promise and explores a variety of types of educational recommenders, there is currently a lack of research that ties educational theory to the design of these systems. The theory considered here, self-regulated learning, focuses on putting students in control of their learning and is appropriate in situations where learning is autonomous. This research proposes a design science approach to investigate a theoretical base of self-regulated learning (SRL) for a knowledge-based recommender design framework. Existing research on knowledge-based recommender design with an inclusion of an ontology component will guide development of this artifact. Anticipated results include the formation of design principles to inform the creation of SRL guided recommenders for both practical applications and future research.",CS,AI_ML,0.85,Extracted from log - paper 1354
AI for Connectivism Learning - Undergraduate Students’ Experiences of ChatGPT in Advanced Programming Courses,"Advanced programming skills are required for computing courses on merging topics, and students often struggle to develop these skills to solve complex problems. To address this challenge, faculty members provide additional lectures, practice sessions, and educational technology tools. This paper discusses the challenges faced by computer science students in developing advanced programming skills and explores the use of AI chatbots, specifically ChatGPT, as a support tool for learning. We study the engagement and effectiveness of ChatGPT in helping students learn advanced programming skills using two engagement learning frameworks (CIE and MELT) for evaluation. The study involves designing a computing lab exercise (design and code questions) for students to complete using ChatGPT and collecting data through surveys. The findings provide initial evidence that ChatGPT can be an effective tool for supporting student learning in advanced programming courses.",CS,AI_ML,0.85,Extracted from log - paper 1355
AI for Social Good (AI4SG) Education in an Undergraduate Introductory MIS Course,"We describe a pedagogical study in which we designed and implemented a module for undergraduate Management Information Systems (MIS) students, aimed at preparing them for an increasingly AI-impacted business environment and society. We developed a learning module that taps their inherent motivation to make a meaningful difference, challenging them to ideate applications of AI for social good (AI4SG), focused specifically on sustainability. We piloted the module in an existing introductory MIS course, first establishing a range of fundamental AI capabilities through hands-on demos and study cases. Then, with instructor guidance, the student teams, working in a social entrepreneurship ""start-up"" context, identified sustainability challenges impacting their own communities and worked together to propose and pitch AI-powered solutions. The results suggest that students find this approach deepened their understanding of sustainability issues in their communities, improved their knowledge of how AI could address social issues, and improved their confidence in their ability to innovate.",CS,AI_ML,0.85,Extracted from log - paper 1356
Technology-Enabled Peer Assessment and Feedback: A Design Science Research Study,"Peer assessment and feedback are important pedagogical practices that can improve student learning, engagement, and satisfaction. However, they also pose challenges such as bias, reliability, and validity. Technology-enabled peer assessment and feedback (TEPAF) systems can address some of these challenges by providing tools and features that support the process. This paper presents a design science research (DSR) study that aims to develop and evaluate a TEPAF system for higher education. The study follows the DSR methodology and uses a mixed-methods approach to collect and analyze data from students and instructors. The paper contributes to the literature by providing design principles and implications for TEPAF systems and practices.",CS,AI_ML,0.85,Extracted from log - paper 1357
Understanding the Impact of Digital Transformation on Teaching and Learning in Higher Education,"Digital transformation (DT) has been a significant force in reshaping higher education, especially in the wake of the COVID-19 pandemic. This study investigates the impact of DT on teaching and learning practices in higher education institutions (HEIs). Using a qualitative case study approach, the research explores the experiences of faculty members and students with DT initiatives. The findings highlight the benefits and challenges of DT, including increased flexibility, accessibility, and innovation, as well as issues related to digital divide, resistance to change, and the need for digital literacy. The study offers recommendations for HEIs to effectively implement DT strategies and enhance teaching and learning outcomes.",CS,AI_ML,0.85,Extracted from log - paper 1358
Bridging the Gap: An Interview Study on Challenges in Software Testing and Educational Needs,"As information systems become increasingly complex, software testing is being recognized as a key element in this process. However, organizations continue to encounter challenges in effectively implementing and executing testing procedures, highlighting the persisting difficulty arising from a lack of knowledge and awareness. Thus, it is crucial to understand the challenges related to software testing to point out educational needs in software testing. To gain a comprehensive understanding of the current challenges related to software testing, this study conducted an empirical cross-sectional interview study with 23 experts. Additionally, a literature review was conducted to investigate if there are any discrepancies or similarities between the current challenges identified in academic research and industry, emphasizing relevant educational needs in software testing. The software testing life cycle was utilized as a framework to systematically present the results. Overall, this study provides insights into the challenges of software testing and current educational needs.",CS,AI_ML,0.85,Extracted from log - paper 1359
Enhancing Student Engagement in Online Learning Environments: The Role of Gamification,"Online learning environments have become increasingly prevalent in higher education, but maintaining student engagement remains a challenge. Gamification, the use of game elements in non-game contexts, has been proposed as a strategy to enhance engagement and motivation. This paper examines the effectiveness of gamification in online learning environments through a review of existing literature and a case study of a gamified online course. The results indicate that gamification can improve student engagement, participation, and satisfaction, but its impact on learning outcomes is mixed. The paper discusses the implications for educators and provides recommendations for designing effective gamified online learning experiences.",CS,AI_ML,0.85,Extracted from log - paper 1360
Determinants of Gamification Effectiveness: Gamification Affordances and Coping Responses in the Context of Gamified ERP Training,"Gamified training is often used to facilitate the adoption of complex information systems in organizations. However, little research has focused on the factors that influence the effective use of gamified training. This study investigates the determinants of effective use of the enterprise resource planning simulation game, ERPsim. The study proposes that ERPsim affordances, such as collaboration and competition, affect coping responses, including task-oriented, emotion-oriented, and avoidance, which in turn impact the effective use of the game. The data were collected from 255 graduate students enrolled in an ERP course at a public university in the United States. The results show that collaboration affordance significantly affects all three coping responses, while competition affordance only affects task-oriented coping. Additionally, task-, and emotion-oriented coping are found to influence the effective use of ERPsim, but avoidance coping does not. The study highlights the importance of affordances and coping responses in gamified training design and implementation towards effective use.",CS,AI_ML,0.85,Extracted from log - paper 1361
Developing AI Literacy of Management Students using Problem and Project based Learning,"Artificial Intelligence (AI) tools are becoming everyday tool for business of all sizes. Business and management students who have good understanding of AI concepts, tools, technologies, and ethics will have an advantage in contributing to the integration of AI technologies in business. The objective of this study is to examine the effectiveness of using problem-based learning (PBL) and project-based learning (PjBL) to teach AI literacy to master's level management students in a university setting. A module on AI literacy was developed using PBL and PjBL, and a quantitative approach was employed to assess its effectiveness by measuring students' experiences and perceived AI literacy level before and after the module. The results indicate that both PBL and PjBL approaches are effective in developing AI literacy, with 69% of students reporting an increase in their AI literacy level after completing the module. The study also offers recommendations for future AI literacy education in business schools.",CS,AI_ML,0.85,Extracted from log - paper 1362
Do Asynchronous Courses Work? Comparisons of Student Performance in a Multimodal Undergraduate Database Course,"Advances in digital learning technologies and connectivity tools coupled with the COVID-19 pandemic have led to increased online course offerings. As online courses gain popularity, it becomes more important to assess their efficacy in assuring student learning. In the domain of Information Systems (IS) pedagogy, the teaching of database courses is under-researched. This study employs quantitative methods to evaluate differences in academic performance between students in an on-campus modality compared to an online asynchronous offering of the same introductory undergraduate database course. In particular, we compare mean scores across the performance evaluation categories of attendance, quizzes, assignments, final exam, and cumulative final scores. Results indicate the raw mean scores for the on-campus section exceeded the online section across every category, but only assignment and attendance averages were significantly higher for the on-campus section when bootstrapped across 1000 samples. These results inform remedial measures to improve student engagement in these categories.",CS,AI_ML,0.85,Extracted from log - paper 1363
Effects of Business Simulation Games on IS students’ Resilience: Instructors’ Perspectives,"While resilience is acknowledged as a complex construct and one that is difficult to assess, universities are recognising its importance. They are beginning to invest in research and services aimed at building student resilience. However, there is limited research into the levels of, and contributors to, student resilience. This study explored the effects of an experiential learning tool, business simulation games (BSGs), on the development of IS students' academic resilience. The researcher interviewed instructors conducting ERPsim labs to find out their views regarding the impact of BSGs on three aspects of the Resilience at University (RAU) scale: academic buoyancy, personal competence and social competency. Findings from the instructors' interviews demonstrated that Business simulations help students build resilience by offering a safe environment where they can fail, learn, and try again. Simulations may educate students on managing stress and pressure, remaining focused on their objectives, and having a good attitude.",CS,AI_ML,0.85,Extracted from log - paper 1364
Establishing a Gamified Programming Practice Environment with Immediate Feedback,"Feedback serves a key role of closing the gap between students’ current understanding and desired learning. Immediate feedback helps students identify and correct misconceptions, motivates them to acquire knowledge, and increases efficacy and motivation to learn. However, although traditional teaching methods provide feedback to students, students cannot get answers to problems immediately under the condition of many students taking courses, which interrupts their learning enthusiasm. Although teachers use gamification mechanisms and elements to improve students' learning motivation, in gamified learning without immediate feedback, the game elements have little help to students’ motivation. With the guidance of self-determination theory, this project changes the traditional way of handing in practice assignments and develops a highly motivating SQL programming practice exercise that is not limited by time and space, allowing exploration and trial-and-error process surroundings. The system completely records the content of each activity process and problem-solving so as to provide teachers with after-the-fact review and serve as a reference for classroom teaching. Taking the immediate feedback automated assignment correction system platform as the core, we encourage students to challenge themselves through the gamified self-element (goal setting, experiences, level, and badge) and cooperate or compete with peers through social elements (leaderboard, achievement visibility, likes and shares). We provide students with an engaging but meaningful learning experience in gamified scenarios. This project analyzes a large number of students’ learning process data recorded in the system. With the support of rich data, it shows the details of the students’ process of completing the challenge tasks like a magnifying glass, and provides a dynamic lab to verify the causal relationship between the students’ learning process and learning outcomes from the behavioral level (rather than the attitude level). From the perspective of learning gamification, this project explores how gamification elements in the learning process affect students’ learning engagement and learning outcomes. The results of this project help to gain further understanding of learning gamification.",CS,AI_ML,0.85,Extracted from log - paper 1365
Explain AI-Based Essay Scorings without XAI - Empirical Investigation of an User-Centered UI Design for AI-Based AES Systems,"The lack of understandability of AI-based decisions is increasingly posing trust-related and regulatory problems. This also applies to the educational sector, where AI is a central element of modern automated essay scoring (AES) systems. However, current research on explainable AI primarily focuses on complex technical approaches. These explanations usually show a lack of understandability by the actual users, who often have no knowledge of AI. Based on an experiment with 245 students at a German university, we were able to show that even the basic principles of user interface design can improve understandability and hereby trustworthiness. Thus, the use of visual elements promotes understandability even when only little information is provided. Especially when providing further AI-specific information on the scoring of AES systems, however, it must be considered that in combination with visual elements an information congruency can be observed, leading to a cognitive overload in the worst case.",CS,AI_ML,0.85,Extracted from log - paper 1366
Exploring the Perception of a Chatbot's Language Style in a Learning Situation,"In this Wizard-of-Oz experiment, we assess the impact of language style (dominant vs. submissive) on students’ perception of a chatbot in a learning context. 38 probands were involved in this within-subject experiment while learning about Digital Literacy in a guided text-based conversation via Slack, in which they supposedly interacted with a chatbot. We quantitatively measured constructs in a follow-up survey and discussed implications with the probands and 14 other students from a bachelor’s course on digital transformation. Results show that the dominant language style significantly negatively affected learners’ enjoyment during the interaction, but did not reveal a significant influence on perceived competence, empathy, identification, trust, or usefulness between the two contrasted language styles. Our qualitative results indicate that language style preference depends on learning context, interaction time, and learner personality. We reflect on future research needed to build upon these initial findings.",CS,AI_ML,0.85,Extracted from log - paper 1367
The Effect of Learning Analytics Dashboards on Student Motivation and Performance,"Learning analytics dashboards (LADs) provide students with visualizations of their learning progress and performance. This study investigates the effect of LADs on student motivation and academic performance in an undergraduate course. Using a quasi-experimental design, the research compares students who had access to LADs with those who did not. The results show that LADs can enhance students' motivation, self-regulation, and academic achievement. The paper discusses the implications for the design and implementation of LADs in educational settings.",CS,AI_ML,0.85,Extracted from log - paper 1368
Grade-O-Matic: A Tool to Test and Grade Programs,"To improve learning in higher education, the primary focus should be on engaging students in a process that best enhances their learning--a process that includes feedback on the effectiveness of their learning efforts. If that is true of most subjects of higher education, it is doubly so for teaching programming in the college classroom. Learning programming and related topics requires experience and personal commitment. It requires effort, often prolonged effort, over time. Unfortunately, students (and business students in particular) regularly seek ways to minimize their investment of time and maximize their grade outcome. The only recourse for MIS/IS instructors is, then, to create graded programming assignments with a variety and quantity sufficient to coerce students into investing the time necessary for learning. However, grading student programs is a labor-intensive responsibility for faculty members. It is tedious and error prone. Grading programs consistently and in a timely manner is often a challenge for faculty members. Delays in graded feedback are detrimental to student learning. This paper presents an innovative tool called Grade-O-Matic. It can be used by both students and instructors to grade programs quickly and in large quantities. By using the Grade-O-Matic tool, faculty members can assign more graded programming assignments, and students can get immediate feedback on their work before turning it in to be graded. This paper describes the Grade-O-Matic tool, and its creation, from its inception to its current state using terminology of the Design Science Research Methodology (DSRM) [Peffers, et. al., 2007]. The distinctives of the tool, and how it is different from similar efforts, are noted. The paper concludes with anecdotal evidence to support the claim that the tool, and the immediacy it affords, supports student learning. Also noted are generalized principles that may support future efforts.",CS,AI_ML,0.85,Extracted from log - paper 1369
How Today’s AI Content Generators Outperform Average Novice Students in Information Systems Exams,"The public availability of sophisticated AI content generators based on large-scale language models achieved enormous media attention in recent months. AI content generators are expected to have a major impact on future information systems developments as well as private and business life. Particularly in the field of education, the question arises as to how these innovations in AI technology will impact teaching, learning, and examination. With a specific focus on examinations in the information systems discipline, we show in this research-in-progress paper, based on an experimental field study, how a today’s AI content generator performs in solving a fundamental information systems exam. Based on our preliminary results, we found that the quality varies across different exam question types. With this research project, we offer start points and initial guidance on how educators can deal with the availability of AI content generators.",CS,AI_ML,0.85,Extracted from log - paper 1370
Impact of Technology-enabled Active Learning (TEAL) on Learning,"Using an active learning framework proposed by Shroff et al (2019), our study evaluates the impact of technology-enabled active learning (TEAL) on student learning in an Australian business school. Our study used the individual reflections of accounting students as the data collection strategy and content analysis. Our findings suggest that the three constructs of the framework - interactive engagement enabled by the technology during and after the workshops, development of problem-solving skills throughout the learning process, and individualized learning context facilitated by the accounting technology have all positively contributed to the learning effectiveness. We also found that technology-enhanced scaffolds designed in the learning process have aided in the consolidation of learning and increased learning effectiveness. However, our study observed that while technology was expected to have a positive influence on students’ curiosity and interest - the fourth factor in the framework, lack of student effort, poor timing of feedback, and the absence of a sense of challenge, have limited learning.",CS,AI_ML,0.85,Extracted from log - paper 1371
Exploring the Use of Virtual Reality for Collaborative Learning in Higher Education,"Virtual reality (VR) offers new opportunities for collaborative learning in higher education. This paper explores the use of VR for group projects and teamwork in a university setting. Through interviews and surveys with students and instructors, the study examines the benefits and challenges of using VR for collaborative learning. The findings suggest that VR can enhance communication, engagement, and creativity among students, but also presents technical and logistical challenges. The paper provides recommendations for integrating VR into collaborative learning activities.",CS,AI_ML,0.85,Extracted from log - paper 1372
Is Academic Integrity at Risk? Perceived Ethics and Technology Acceptance of ChatGPT,"The academic community is experiencing a significant disruption thanks to a new technology called ChatGPT. There is news almost daily about how Large Language Models technologies have the potential to allow students to cheat undetected. This leads many universities to discuss ways to regulate and even ban its use. But is this necessary? The previous research focuses mainly on universities' and lecturers' views but fails to consider the students’ views. Do students even want to use the system, and if they do, will they use it for nefarious purposes? Extending the Technology Acceptance Model with Perceived Ethics, this quantitative study researched how students perceive ChatGPT from an ethical perspective and how that impacted their intention to use the system for school work. 277 Students from 40 countries took part in the survey. Results indicated that Perceived Ethics has a direct positive influence on the attitude toward usage but only an indirect effect on the behavioral intention to use the system for school work. This study is one of the first to consider how students perceive ChatGPT from an ethical perspective and how that impacts their perception of and intention to use the Large Language Models technology system.",CS,AI_ML,0.85,Extracted from log - paper 1373
A Review of the Literature on Gender and Sexual Minorities and ICT,"Gender is essential to our identity. As information and communications technology (ICT) permeates every aspect of everyday life, a body of literature has emerged that broadens our understanding of IT and gender. A growing number of studies have shed light on the implications of ICT for gender and sexual minority populations in various areas. Gender and sexual minorities commonly referred to as LGBT consist of people whose gender identity diverges from predominant conceptions of masculinity and femininity or whose sexual orientation differs from the majority. Previous literature reviews on IT and gender have focused on the underrepresentation of women in the IT landscape. This paper presents the preliminary results of a literature review focusing on gender and sexual minorities and ICT. This paper contributes to the IS literature on gender by providing a foundation for ideas on ICT’s role in mitigating the stigma associated with gender and sexual minority status.",CS,AI_ML,0.85,Extracted from log - paper 1374
Augmenting the Framework for Design Science Research Goals and Evaluation Criteria with Aspects of Society 5.0,"In the field of information systems, new technologies are giving rise to a wide range of new research topics that present researchers with new challenges. Innovations, such as artificial intelligence, have an enormous impact on society, in addition to technological progress towards automation, above all or precisely because of this. In this context, Society 5.0 was initiated to establish criteria for research and its artifacts. In this paper, evaluation criteria of a Society 5.0 are identified, which should be considered for Design Science Research artifacts in the future. Through a comprehensive literature review, 14 evaluation criteria and a new DSR goal were identified and integrated into the existing framework of Hevner et al. (2018). The paper provides researchers with an augmented DSR evaluation framework with aspects of Society 5.0.",CS,AI_ML,0.85,Extracted from log - paper 1375
Barriers to the Use of Action Research in Information Systems - A Literature Review,"The current digitalization is a disruptive, challenging process that causes a social-technical transition in organizations. Due to that, there are more and more use cases in companies. Action research as a form of social research combines theoretical knowledge and practical execution, which is why it is suitable for current challenges. In several cycles, a literature analysis is carried out to build the knowledge base and establish hypotheses. Based on this, an action is then performed, which is subsequently analyzed, whereupon the knowledge base is expanded through literature. Since action research has not been used much in information systems so far, it is important to identify the reasons for the low use of action research in the research area of information systems. In this paper a systematic literature review was used to identify the barriers that explain the low use of action research in information systems. The findings are twelve different barriers, which are classified into three categories. The categories are ""Human Factors"", ""Method Factors"" and ""Application Factors"". In addition, the research paper concludes with specific implications for these three categories.",CS,AI_ML,0.85,Extracted from log - paper 1376
Design Principles in Information Systems Research: Trends in Construction and Formulation,"Design knowledge has become increasingly important in information systems research in recent years, with Design Science Research (DSR) as an approach to developing innovative and effective artifacts. Design Principles (DPs) have proven to be a popular tool for contributing to abstract design knowledge. The construction and formulation of DPs has become more professional in recent years, with publications providing guidance. However, the implementation by researchers of these rules needs to be investigated. To address this gap, we conducted a systematic literature review, analyzed the various forms of design knowledge that have emerged, and examined the state of design principle construction and formulation. Our analysis shows that in recent years there has been a significant increase in the number of publications dealing with DP design. Our results shed light on the characteristics and evolution of DPs, as well as their construction and formulation, and provide valuable guidance for future research.",CS,AI_ML,0.85,Extracted from log - paper 1377
Digital Transformation: From Traditional IS Theories to Emerging Assemblage,"Digital transformation (DT) is a complex and dynamic phenomenon that requires a significant shift in organizational culture and mindset, necessitating new theoretical perspectives to understand the interactions between technology and people. This article conducts a critical literature review of the extant literature on DT, contrasting traditional Information Systems (IS) theories with assemblage theory. Assemblage theory provides a dynamic framework to analyze the various components of DT and their interactions, acknowledging the significance of power dynamics and politics in shaping the direction and outcomes of DT initiatives. This paper provides different theoretical perspectives in digital transformation literature by analyzing 70 articles published in premier IS journals. This article is helpful for scholars and practitioners interested in understanding the complexities of DT through assemblage theory perspectives.",CS,AI_ML,0.85,Extracted from log - paper 1378
Emerging ICT for Sustainable Development. Research Concept of Literature Analysis,"The large number of publications in databases such as Web of Science or Scopus points to the growing interest of scientists in the importance of emerging Information and Communications Technologies (EICT) for ensuring sustainable development (SD). This article describes the concept of the research focusing on recognizing the state of research on artificial intelligence, big data, cloud computing, and Internet of Things adoption in various sustainability areas. The study concept was inspired by the authors' experience and the previous study results which aimed at providing a holistic view of the importance and scope of ICT research in the SD context. The article presents the four-stage research procedure and describes the results of the first three stages, which are preparatory. The scope of the study was indicated, the choice of the bibliometric database was justified, examples of prepared queries were given, the need to unify keywords was explained and examples of standardized words were given.",CS,AI_ML,0.85,Extracted from log - paper 1379
Gamification and Marketing Management: A Literature Review and Future Agenda,"Given the motivational effect of game elements and mechanisms on user experience in various non-game contexts, gamification has widely been used as an effective marketing technique to enhance the performance of business practices. In the past decade, a variety of studies have explored and investigated the value that gamification can provide in consumer-facing marketing activities. However, there is still a dearth of granular understanding of how gamification in marketing has been studied in the current literature. This paper follows the PRISMA literature review process and systematically reviews 93 papers consisting of 111 empirical studies on gamification and marketing management. The synthesized findings provide a holistic picture of the adopted research methods, different investigated gamification affordances and advergames, various gamified industries, and marketing performance indicators regarding products, services, and brands. Five agenda points, mainly relating to methodology and themes, are further suggested.",CS,AI_ML,0.85,Extracted from log - paper 1380
"Implications of Axiomatic Theory: I Come to Praise, not Bury, this Approach to Accumulating IS Knowledge Introduced by Lee et. all (2021)","Axiomatic theory is a concept introduced in Lee et al. (2021) who illustrated its application in connection with three theories borrowed from reference disciplines and extensively used in IS research. This paper aims to extend consideration of the concept to more general application beyond these particular theories. The concept suggests that some theory is so close to being a truism that it needn’t be further tested and could reliably be the basis upon which additional knowledge can be built. Thus, such theoretical propositions might be treated like axioms in mathematics rather than tentative theory subject to further testing. In this paper this premise is examined in terms of the general role of theory in accumulation of knowledge, when such axiomatic status might be earned by the assertion of particular relationships, and how such axiomatic theory might be usefully applied.",CS,AI_ML,0.85,Extracted from log - paper 1381
Management of IT Costs in the Digital Age – A Literature Review,"Digitalization requires organizations to strategically invest in information technology (IT). As a result, the costs associated with IT in companies are rising and technological progress changes the setting for IT management. This poses challenges for IT managers to ensure spend-efficiency and manage IT costs transparently. However, no current literature review gives an overview of how IT cost management (ITCM) research dealt with past transformations. This paper aims to investigate ITCM concepts considering their historical context. It then derives implications for the digital age and identifies future research fields. The historical literature review reveals that ITCM research evolved with technological advances and the target to manage all IT-related costs and evaluate the impact of IT spend. However, the presented concepts lack consideration of current changes that hamper spend-efficiency and strategic decisions. Hence, this paper enables future research to address the identified research gaps. Additionally, practitioners gain awareness of how they can benefit from developed ITCM concepts.",CS,AI_ML,0.85,Extracted from log - paper 1382
Research methods in 30 years of information systems – a bibliometric trend study,"The research methods used by information systems scholars have changed in the course of thirty years. For example, the availability of big datasets and processing power have opened new avenues of computational research. Yet few studies have quantified the changes, and we know little about their impact on and implications for the discipline. In this paper, we present a two-stage bibliometric study. We first develop search terms that are associated with research methods in IS. We then use those in 62 searches in eight leading IS journals, to chart the relative popularity of methods over time. We identify key methodological trends, including a steady rise of longitudinal research. We also find evidence of a diversification of methods, with more methods growing than declining in relative occurrence, and identify the possible implications of this changing portfolio that deserve further study.",CS,AI_ML,0.85,Extracted from log - paper 1383
Social Media Enabled Social Movements in Information Systems Research: A Lakatos Approach to Review of Literature,"Research on social media applications in the context of social movements is still in its nascent stages in information systems research. However, there have been a number of empirical studies in addition to conceptual studies in recent years. The objective of this paper is to propose a literature review to assess the current state of knowledge in this research program and, thus, identify opportunities and avenues to build on this body of knowledge. Informed by Lakatos (1970) approach, we analyze the core and the protective belt of the current scholarship in this area. We utilize Boell’s (2014) hermeneutic framework to conduct our literature review. Our research will assist scholars in this field to identify research papers and core theories in the research program relevant to their work, in addition to suggesting avenues for future theoretical development and research, utilizing hitherto unexplored methodologies, technological platforms, and social movement contexts.",CS,AI_ML,0.85,Extracted from log - paper 1384
Using Digital Performance Feedback for Behavior Change: A Review,"Feedback on task performance is a key component for improvements and learning for many behaviors. Although the IS literature has provided relevant individual contributions on the design, outcomes, and mechanisms of performance feedback, there exists no overarching conceptual framework or theory to integrate those results for a holistic explanation of behavior change through IS-enabled feedback. Further, an overview of the existing literature to facilitate theory development is still missing. This paper presents a systematic literature review on performance feedback in the IS discipline by reviewing 23 articles from high-quality journals. The review provides an overview on the research landscape by coding characteristics of feedback systems and outcomes of the research. It highlights four behavioral mechanisms that performance feedback influences to improve task performance (attention, behavioral control, personal norms, and perceived social norms) and concludes by presenting possible directions for future research.",CS,AI_ML,0.85,Extracted from log - paper 1385
A Framework to Understand the Emergence of SQB: Observations Through a New Lens,"Although change initiatives are a frequent and critical need in contemporary organizations, an individual’s propensity to resist change is frequently reported. Resistance is the consequence of the cognitive and behavioral responses of people affected by the change. This phenomenon has the potential to change the current status quo of many individual and group-level theories, particularly those addressing the ""why"" and ""how"" of resistance to change. This study uses an interpretive research approach with the use of the grounded theory method and adopts the SQB theory to observe the resistance to change in response to SQB emergence. This study employs four focus group discussions that yield a framework to understand the emergence of SQB. The study identifies an inter-play among SQB emergence framework constructs: actor, entity and time. This study assists in identifying new frameworks and paradigms for the SQB perspective that can be used in changing conditions.",CS,AI_ML,0.85,Extracted from log - paper 1386
An Investigation to reduce Overreliance on Explainable AI (XAI) in light of Two System Theory,"As technology is evolving, there is a rise in the use of AI systems. The increased use of AI systems has revealed issues of gender and racial biases. To address these issues, explainable AI (XAI) is introduced, but the use of XAI has triggered various kinds of biases leading to issues such as overreliance. In this study, we seek to devise interventions to mitigate the issue of overreliance on AI by better understanding cognitive biases and acknowledging that different users have different cognitive abilities, and we need to be mindful of that when we design XAI systems. We will conduct multiple experiments using the recidivism dataset collected by ProPublica and to develop a better understanding of and solutions to mitigate the issue of overreliance. The findings from this research will allow us to design XAI systems better, improving user trust in AI and further improving AI adoption.",CS,AI_ML,0.85,Extracted from log - paper 1387
Decision-Making Styles in Metaverse: Effects of Immersion and Embodiment,"Decision-making is a vital skill of our daily cognitive arsenal. The rise of virtual reality (VR) worlds like the metaverse, have created a new need to investigate human behavior under a technologically novel and multisensory prism. In this study, we experimentally investigate types of decision-making in artificial realities mediated by different levels of immersion (PC monitor vs. VR HMD) and sense of embodiment (self-motion vs. self-anchored). Participants (N=183) conducted a daily-life decision-making task of financial allocation, based on evaluating either a 3D graph or 2D graph containing price information across different periods. Five decision-making styles are evaluated including Rational, Intuitive, Dependent, Avoidant, and Spontaneous. Our results indicate that decision-making styles do not differ between diverse virtual realities, and instead remain similar to the control 2D condition. However, due to the flexible nature of decision-making it is possible that content and environmental factors are still likely to influence decision-making in VR experiences.",CS,AI_ML,0.85,Extracted from log - paper 1388
Does being emotionally inexpressive in personal profile photos affect hiring assessments in online professional social networks?,"Online personal profile photos (PPPs) can serve as a source of impression formation and judgment about individuals. This study investigates how the emotionality expressed in a job candidate’s online PPP on a professional social network (e.g., LinkedIn) affects a recruiter’s perceptions of them. Drawing on social role theory and classifying job roles along agency (agentic) and communal dimensions, this paper proposes that job candidates whose emotional expression or inexpression in their online PPP is consistent with qualities associated with agentic and communal job roles are more likely to be perceived as competent and fit for the job role.",CS,AI_ML,0.85,Extracted from log - paper 1389
Early Chatbot Assistance Can Enhance Team Decision-Making by Promoting Cognitive Diversity and Information Elaboration,"As AI increasingly assists teams in decision-making, the study examines how technology shapes team processes and performance. We conducted an online experiment of team decision-making assisted by chatbots and analyzed team interaction processes with computational methods. We found that teams assisted by a chatbot offering information in the first half of their decision-making process performed better than those assisted by the chatbot in the second half. The effect was explained by the variation in teams’ information-sharing process between the two chatbot conditions. When assisted by the chatbot in the first half of the decision-making task, teams showed higher levels of cognitive diversity (i.e., the difference in the information they shared) and information elaboration (i.e., exchange and integration of information). The findings demonstrate that if introduced early, AI can support team decision-making by acting as a catalyst to promote team information sharing.",CS,AI_ML,0.85,Extracted from log - paper 1390
Lifting the Magic Curtain - How Transparency on Social Norm Nudging affects its Efficacy,"This study analyses how the effect of a social norm nudge changes when its function and purpose become transparent. For this purpose, a quantitative, experimental online survey was conducted among 474 participants in the context of voluntary CO2 offset. The non-transparent social norm nudge and the one with information solely on its function showed no significant effect on behaviour. However, the social norm nudge disclosing both, its function and purpose, did effect behaviour positively. Based on the results, it is recommended that social norm nudges are made transparent in practice. The recipient of a social norm nudge should therefore be explicitly notified about its function and purpose.",CS,AI_ML,0.85,Extracted from log - paper 1391
In or Out of Sync? A Psychophysiological Approach to Understanding Mood and Team Performance in Online vs. In-Person Dyads,"In this study, we conduct a laboratory experiment with online vs. in-person dyads who are placed in convergent or divergent moods using mood induction.",CS,AI_ML,0.85,Extracted from log - paper 1392
The use of cognitive neuroscience tools for evaluating the cognitive overload caused by social advertising,"Social campaigns influence attitudes in society, but too much information can cause cognitive overload. Traditional methods for measuring cognitive overload are subjective. There is potential for using cognitive neuroscience methods because these tools measure consumers' unconscious responses and provide better understanding of cognitive functions. We have elaborated an experimental framework to enable the assessment of media messages in social campaigns concerning the cognitive overload that they cause. The proposed framework will be tested by performing an experiment on specific social advertising to prove its usefulness and show the research possibilities that it offers. As a result, we expect to expand the framework into a system that could be used by practitioners for evaluating social advertising before launching the campaign.",CS,AI_ML,0.85,Extracted from log - paper 1393
Who is More Likely to Initiate Referrals? Effect of User’s Regulatory Focus on Referral Intention,"The online referral reward program (ORRP) is a social marketing method by which firms reward existing users and encourage them to recommend products or services to their friends. Prior research has primarily focused on the impact of ORRP design on users’ participation, however, the role of individuals’ characteristics is unexplored. Based on regulatory focus theory and self-efficacy theory, this research proposes and investigates the effect of users’ regulatory focus (promotion-focused vs. prevention-focused) on their referral intention and explores its mechanism and boundary condition. The results of three experiments show that compared to prevention-focused users, promotion-focused users have higher self- efficacy to complete the referral task; and thus, have higher referral intention. This effect will be attenuated when the tie strength between inviters and invitees is strong. The findings not only contribute to the research on ORRP and regulatory focus but also provide guidance for firms to optimize ORRPs.",CS,AI_ML,0.85,Extracted from log - paper 1394
A Configurational Approach to Examine Technostress Creators,"The phenomenon of Technostress (TS) - the stress experienced by end-users of Information and Communication Technologies (ICTs) - is often considered as a dark side of technology use. TS creators, factors that create TS, is a second-order construct with dimensions techno-overload, techno-invasion, techno-complexity, techno-insecurity, and techno-uncertainty. This study takes a holistic approach by proposing how the complex interactions among individual characteristics, IT features, and environmental factors influence employees’ perceptions of TS creators. Based on past research, we include three personality traits, i.e., neuroticism, personal innovativeness in IT, and IT mindfulness for individual characteristics. Additionally, two technological features, presenteeism and anonymity, are considered, while literacy facilitation, technical support provision, and social support are the environmental factors. Data will be collected using an online survey from full-time employees and analyzed using fuzzy sets qualitative comparative analysis (fsQCA). We also discuss the expected results and contribution of the study.",CS,AI_ML,0.85,Extracted from log - paper 1395
A Double-edged Sword? Algorithmic Management and Workers' Proactive Service,"Platforms increasingly rely on AI algorithms to execute automated task allocation, monitoring, and performance evaluation of workers in the gig economy. Platforms also expect AI algorithms can prompt gig workers to provide proactive services to customers for a better reputation. However, it remains unclear how algorithmic management impacts the proactive service behavior of gig workers. Based on self-determination theory, this emergent research forum paper proposes a double-edged sword effect of algorithmic management due to conflicting work motives: algorithmic management can increase gig workers' proactive service behavior via autonomy motivation, whereas it can also inhibit proactive service behavior via controlled motivation. A three-wave longitudinal survey will be conducted to test the posited double-edged sword effect. This study has the potential to contribute to the algorithmic management literature in IS field by uncovering the 'black box' between algorithm management and proactive service behavior of gig workers from the perspective of work motivation.",CS,AI_ML,0.85,Extracted from log - paper 1396
A Typology of Social Media Affordances during Social Movements,"This research proposes a typology of social media affordances in the context of social media-enabled social movements. Social media technologies shape and facilitate the spread of such social movements by lowering individuals' participation costs, increasing general accessibility to information, fostering connectivity, and creating a platform where users can generate content. We propose a typology of affordances to theorize the spread of these movements from the embryonic stage to its established stage. To do so, coalesced affordance, a new type of affordance, and the existing shared, collective, and connective affordances are used. We also integrate the role of social networks and network theory in developing the proposed typology. Finally, the study's implications, limitations, and future research direction are discussed.",CS,AI_ML,0.85,Extracted from log - paper 1397
Affordance Theory for Information Systems project implementation: a process and organizational outlook.,"This paper explores the Information Systems project implementations in organizations. It focuses on the actualization of the affordances that result from the intertwining of the Information Technology (IT) artefact and the organization and we answer to the following research question “How do organizations actualize affordances?” With a qualitative multiple case study on the different local entities of an international leading retailer, this research identifies that previous research omitted the top management sponsorship as one of the main influences for the actualization process. Moreover, constrains perception is observed in the collected data and its role is assessed. This paper contributes the development of the affordance theory by providing an updated process-based integrative theoretical framework for affordances at the organizational level, aimed to support further research on Information Systems.",CS,AI_ML,0.85,Extracted from log - paper 1398
Exploring the Impact of Blockchain and Distributed Ledger Technology Adoption on Business Process Innovation,"The goal of the current paper is to examine the impact of blockchain technology adoption on an organization’s business process innovation. Blockchain technology has been widely acknowledged for its usefulness in securing business transactions and benefits from decentralization and transparency. While blockchain implementation has notable advantages in a business process, such outcomes also rely on the nature of the business. Our research employed two theoretical concepts to measure blockchain adoption and its impact on an organization’s business process innovation: technological, organizational, and environmental (TOE) framework and task-technology fit. We specified hypotheses and tested them using survey responses. A total of 419 responses were collected and we analyzed them using partial least squares (PLS). Detailed results and discussions are addressed.",CS,AI_ML,0.85,Extracted from log - paper 1399
Application of Fourth Industrial Revolution Technologies in Healthcare,"The aim of this systematic literature review (SLR) is to explore how Fourth Industrial Revolution (4IR) digital technologies are transforming the healthcare sector. Peer-reviewed, full-text research papers published between January 2016 and April 2021, which focused on data-driven healthcare, health data management systems, and the disruptive role of digital technologies in healthcare were retrieved from three electronic databases, namely PubMed Central, Institute of Electrical and Electronics Engineers (IEEE) and MEDLINE. Articles included in the SLR were identified and selected using the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guideline. Three main themes were identified from 84 research papers included in the SLR, namely, (i) digital technologies driving the transformation of healthcare, (ii) the impact of digital transformation on healthcare, and (iii) areas of transformation in healthcare. The results showed that technological innovations can mitigate the impact of unequal distribution of healthcare services, reduce healthcare costs, improve the quality of care, and the quality of life for patients and healthcare professionals.",CS,AI_ML,0.85,Extracted from log - paper 1400
Artificial Intelligence applications in diagnostic medicine: a decade of expectations,"Considered a new paradigm in healthcare, AI is expected to change diagnostic medicine in the coming decades. However, its future is still uncertain - partly because it is an emerging technology being gradually applied to a mature field of medicine. We conducted a global cross-sectional survey with more than 1,400 authors of recent scientific publications indexed by the Web of Science to foresee the future of AI in diagnostic medicine. Most respondents expect AI to change diagnostic medicine in this decade radically. In this period, the two most likely outcomes are reduced screening costs and increased diagnostic reliability. X-ray diagnosis and heart rhythm interpretation are the two diagnostic tools most likely to be integrated with AI. The two main barriers are the difficulty of incorporating it into clinical practice and the ethical-regulatory issues. Respondents’ expectations align with the literature and suggest that AI may substantially change diagnostic medicine within this decade.",CS,AI_ML,0.85,Extracted from log - paper 1401
Black-box Models’ Explainability: A Theoretical and Practical Perspective,"The lack of explainability remains a critical challenge to the widespread adoption of artificial intelligence (AI) in many fields. “Understanding brings in trust”, while machine-learning models offer superior prediction accuracy, understanding the underlying logic is equally important to foster trust in these models. In this paper, we present eXplainable AI (XAI) as a solution to this challenge. Our research focuses on three key aspects of XAI: mathematics, humanities and social sciences, and practical applications. We demonstrated the feasibility of XAI through the use of artificially-constructed and model-derived ground truth, and verified performances of different XAIs. We also explored three dimensions of explainable consistency and emphasized the significance of human-machine consistency. Finally, we applied our research to a real-world scenario by cooperating with a national bank in China. Our findings highlight that XAI is both mathematically and practically meaningful, but more efforts need to be dedicated to this human-machine communication field.",CS,AI_ML,0.85,Extracted from log - paper 1402
Can AI Chatbots with Anthropomorphic Attributes Enhance User Engagement in Emotional Support Settings? Investigating the Role of Conversational Styles and Avatar Type,"Although AI-based chatbots have become increasingly prevalent in various business practices and show promise in providing emotional support, there is still limited understanding of the mechanism that underlies the relationship between the anthropomorphic attributes of AI-based relational chatbots and user engagement. To address this gap, we draw on the social judgment theory and propose that emotional judgment mediates the relationship between conversational styles of chatbots (agentic-style and communal-style) and user engagement. We also examine the potential boundary effect of two types of AI avatars (humanlike vs. cartoonlike) on this relationship. To test our hypotheses, we plan to conduct two studies, which will include online and lab experiments. We aim to extend existing theories involving social judgment theory and human reactivity to AI-based chatbots in emotional support settings and identify how different types of anthropomorphic attributes of AI-based relational chatbots may jointly impact user behaviors.",CS,AI_ML,0.85,Extracted from log - paper 1403
Can Enterprise Architecture Management Professionals Measure Technostress Levels and Help Implement Coping Strategies?,"Modern ICTs can lead to higher stress levels among employees due to increased demands and always being accessible, which is known as “technostress”. Traditionally, Human Resources (HR) departments take actions against technostress. Yet, due to its complicated nature, we argue that Enterprise Architecture Management (EAM) can also contribute in technostress management by measuring technostress levels and help co-designing coping strategies in organisations. Comparing the knowledge levels of EAM to HRM professionals in a survey, we found no significant superiority of the latter group in measuring technostress. However, EAM professionals had higher scores in five technostress and three inhibitor dimensions. The study contributes to theory and practice by i) studying the technostress knowledge levels in different contexts, ii) offering EAM as a further technostress inhibitor, and iii) inviting EAM professionals into the discourse to combat technostress in their organisations by demonstrating that they have the required knowledge to lead such conversations.",CS,AI_ML,0.85,Extracted from log - paper 1404
Catastrophe Bond Trading Can Boost Security Improving Cyber (Re-)Insurance Markets,"The interdependent and correlated nature of enterprise cyber-risk is a cause of failed low capital cyber (re-)insurance markets to improve cyber-security. In this paper, we propose the use of catastrophic (CAT) bonds as a radical and alternative residual cyber-risk management methodology to alleviate the big supply-demand gap in the current cyber (re-)insurance industry, by boosting capital injection in the latter industry. We lay down conditions under which it is feasible for a cyber-insurer to invest in (a) CAT bonds, and (b) cyber re-insurance services only - both (a) and (b) contributing to dense security-improving cyber insurance markets. Our main result proves that while the use of cyber-CAT bond instruments solves the capital injection problem in cyber (re-)insurance markets to improve enterprise cyber-security, the level of cost to mitigate IT/ICS client information asymmetry (IA) on enterprise cyber-posture is an important factor that can decide the density of cyber-CAT bond trading markets.",CS,AI_ML,0.85,Extracted from log - paper 1405
Cloud-based Accounting Information Systems in SMEs. Insights from Poland,"The literature proves that the implementation of Accounting Information Systems (AIS) and Enterprise Resource Planning (ERP) systems in Small and Medium-sized enterprises (SMEs) positively affects their performance. As to the systems offered in cloud computing, the previous research mainly focused on the benefits of choosing such a solution, eliminating many barriers in SMEs in the case of traditional on-premise solutions. In Poland, studies into Cloud-based AIS or Cloud ERP have not been previously described in scientific research. This article presents the results of an empirical study of 35 Polish SMEs running their accounts. The thematic scope went beyond the issues examined so far and included the dominant data processing models, types of AIS systems, the use of AIS to perform accounting functions, and the integration of functional areas with the accounting system. The main results indicate that in the surveyed SMEs: 1) cloud computing (CC) is the dominant model of data processing, 2) slightly more companies use stand-alone AIS than integrated ERP systems, 3) AIS is not used to perform all important accounting functions, and 4) most often the accounting system is integrated with the purchasing and sales system and the fixed assets system, and less frequently with the payroll and production service systems.",CS,AI_ML,0.85,Extracted from log - paper 1406
Consumer Participation in Virtual Product Design: 2D vs 3D Product Configurators,"The purpose of the proposed study is to investigate the effect of configurator properties, such as the type of visualization and interaction offered, on the value that consumers attach to co-created products and the satisfaction they derive from the design process. To do this, this study will compare 2D product configurators with 3D product configurators. 2D product configurators use a series of images for product visualization whereas 3D configurators use 3D models that provide consumers with the ability to rotate objects in three dimensions. Drawing from the theory of cognitive dissonance, the theory of effectance motivation and the behavioral decision-making literature, this paper suggests that configurator properties influence perceptions of control and effort expenditure which in turn elicit various processes that influence the perceived value of co-created products and consumers’ satisfaction with the design process.",CS,AI_ML,0.85,Extracted from log - paper 1407
Corporate Social Responsibility Consistency: Moral Sensemaking from Top Management Team to Online Social Media,"Drawing on organization character theory and corporate social responsibility (CSR) sensemaking, this study explores the CSR consistency of moral sensemaking in the company from top management to social media communication channels. First, we measure CSR moral sensemaking in top management team (TMT) letters and firm-generated content (FGC) on social media platform based on the dictionary developed from qualitative content analysis, topic modeling, and CSR expert rating. Second, we compare the moral sensemaking between two communication channels. Third, we investigate the relationship between moral sensemaking consistency and CSR engagement of the company. Our results contribute to the literature in business ethics, management, and social media research in IS. The study will also offer an alternative way for CSR evaluation and bring practical implications to CSR investors and rating agencies.",CS,AI_ML,0.85,Extracted from log - paper 1408
Deep Neural Network at Edge: An Exploration of Hyper Parameter Tuning on Plant Seedling Dataset,"This study explored different hyperparameters and their outcomes on multiple DNN architectures by incorporating transfer learning technique on the plant seedling dataset. Optimizers can have different strengths and weaknesses, and their performance may depend on the specific characteristics of the model and the dataset being used. We have observed that the choice of the optimizer is just one of many hyperparameters that can affect the performance of deep neural network (DNN) models. Other hyperparameters, such as the droprate, number of epochs, and batch size, can also have a significant impact. MobileNetV2 demonstrated superior performance while maintaining a smaller model size, making it a highly valuable option for edge devices where size is a crucial factor.",CS,AI_ML,0.85,Extracted from log - paper 1409
Conversational Agents in Service Context: Towards a Classification of Human-like Design Expectations,"The increasing application of Conversational Agents (CAs) changes the way customers and businesses interact during a service encounter. For instance, chatbots are the first point of contact for many customers. In this context, prior research has shown that CAs implemented with a human-like design lead to improved service satisfaction, perceived service quality, and trustworthiness, among others. Developers have become accustomed to adopting a one-size-fits-all approach by designing CAs human-like. However, not every human-to-human service requires the same social and human-like interaction (e.g., contract cancelation vs. doctors’ appointment), which has also been shown in current research. At present, existing research lacks a synthesis of the relationship between CA design and service encounter context. Against this background, we conducted a literature review and derive classifications based on the dimensions of service context (professional/private) and human-like design (low/high), which enables the identification of relevant research gaps and related literature.",CS,AI_ML,0.85,Extracted from log - paper 1410
Detection of Malicious Bots on Twitter through BERT Embeddings-based Technique,"Cutting-edge Conversational Artificial Intelligence (CAI) technologies bring ease to human life and the invention of social Artificial Intelligence (AI) bots is one of the ultimate devices in the social media sphere. However, malicious social AI bots lead to societal challenges such as data breaches, information loss, and the proliferation of misinformation. Thus, in this work, significant research has been conducted to address the problem of detecting malicious social AI bots on a microblogging platform such as Twitter. We perform classification through Bidirectional Encoder Representations from Transformer (BERT) embedding-based approach. Utilizing tagging-text features, our preliminary results show the potential of the proposed model to classify tweets as malicious AI bot-generated or human-generated.",CS,AI_ML,0.85,Extracted from log - paper 1411
Quantum Computing Concepts for Information Systems Researchers: A Tutorial,"Quantum computing has left the realm of science fiction and is becoming a reality. There are many challenges to overcome before quantum computers can be used to solve practical problems. However, the pace of research and development is accelerating and there are many opportunities for information systems (IS) researchers to contribute. This tutorial provides a brief overview of quantum computing concepts and identifies some research opportunities for IS researchers.",CS,AI_ML,0.85,Extracted from log - paper 1412
Wrestling with the Truth: Post-Truth and the Crisis of (Dis)information,"The post-truth condition has significant implications for society. This paper argues that the loss of shared reality represents a crisis. As information systems (IS) scholars, we have much to contribute. After all, IS are at the center of many post-truth debates (e.g., fake news, echo chambers, filter bubbles). We must engage with the post-truth condition. Consequently, we must wrestle with truth. This paper introduces a panel that aims to stimulate discussion about the implications of post-truth for IS research, education, and practice.",CS,AI_ML,0.85,Extracted from log - paper 1413
Gamified Pedagogical Recommendation Agent Design for Adaptive Learning Environments: An Action Design Research (ADR) Approach,"This research investigates the design of a gamified pedagogical recommendation agent (PRA) to enhance student engagement and improve learning outcomes in adaptive learning environments (ALEs). Adopting an action design research (ADR) approach, we developed a gamified PRA prototype, incorporating game mechanics such as points, badges, and leaderboards. Preliminary findings suggest that the gamified PRA can positively influence student motivation, participation, and overall learning experience. This study contributes to the growing body of knowledge on designing effective gamified learning interventions.",CS,AI_ML,0.85,Extracted from log - paper 1414
The Role of Digital Transformation in Higher Education: A Case Study of a Large Public University,"This paper explores the role of digital transformation in higher education through a case study of a large public university. The study identifies key drivers, challenges, and success factors associated with digital transformation initiatives in the higher education context. Findings indicate that a clear vision, strong leadership, stakeholder engagement, and a culture of innovation are crucial for successful digital transformation. The study also highlights the importance of addressing challenges related to infrastructure, faculty development, and student preparedness.",CS,AI_ML,0.85,Extracted from log - paper 1415
A Design Science Approach to Building a Conversational Agent for Mental Health Support,"This study proposes a design science approach to developing a conversational agent to support mental health. The system aims to provide empathetic responses, evidence-based advice, and crisis escalation pathways. Through iterative design and testing, the agent integrates natural language processing with cognitive behavioral techniques. Preliminary evaluations suggest improvements in perceived support and user engagement.",CS,AI_ML,0.85,Extracted from log - paper 1416
Ontological Approach in Smart Service Systems: A Literature Review,"Smart service systems leveraging ontologies have been investigated for decades, but the conception and design of those systems remain difficult tasks due to the lack of an overall framework served as a guideline. This paper aims at conducting a literature review of ontology-based smart service systems to address this challenge. The findings include a classification and an analysis to determine essential elements of those systems, identified research gaps, and suggestions for future research.",CS,AI_ML,0.85,Extracted from log - paper 1417
Digital Transformation Readiness: A Literature Review and A Proposed Framework,"With the recent evolution of Information and Communication Technologies and the rapid dissemination and use of digital technologies in almost all fields and levels of modern society, two highly context-dependent polysemic constructs have arisen, namely e-readiness and digital transformation. However, although the literature on e-readiness and digital transformation has increased in the last years, digital transformation readiness is still an exploratory construct, lacking a unified definition. This literature review intends to contribute to filling the conceptual gap related to a consolidated definition of the theme by performing a thematic analysis to propose a digital transformation readiness framework.",CS,AI_ML,0.85,Extracted from log - paper 1418
How to Navigate the Uncharted Waters of Cyberwar,"This paper discusses the unprecedented combination of traditional military strategy with digital warfare observed during the war in Ukraine. It explores the challenges related to technology and cyberspace before and during the conflict, the lessons learned, and the implications for software engineers, cybersecurity experts, and IT specialists.",CS,AI_ML,0.85,Extracted from log - paper 1419
Metaverse Knowledge Graph Construction: An Unsupervised Relation Extraction Approach based on Semantic Mining,This study develops a novel framework integrating deep learning (BERT-BiLSTM-CRF) with unsupervised relation extraction based on semantic mining to construct knowledge graphs in the Metaverse domain. The approach aims to reduce labeling efforts and facilitate intelligent tasks such as recommendations and information retrieval.,CS,AI_ML,0.85,Extracted from log - paper 1420
Towards Solving Ontological Dissonance Using Network Graphs,"The paper analyzes data models from 13 different domains to address semantic interoperability in emerging Data Spaces. Using network graphs, it identifies central data models and ontology attributes, describing the semantic heterogeneity qualitatively and proposing ways to connect different Data Spaces across domains.",CS,AI_ML,0.85,Extracted from log - paper 1421
Decentralizing Data Governance: A Case Study in TELCO Data Ecosystems,"This study presents a case study in the telecommunications industry, evaluating data governance maturity models and classifying data governance mechanisms across companies. It highlights the significant differences in governing data ecosystems and offers foundational mechanisms for socio-technical networks.",CS,AI_ML,0.85,Extracted from log - paper 1422
From Data Exchanges to Data Markets: An Institutional Perspective,"The paper discusses the conflict between privacy and competition policies regarding platform data. It proposes an institutional approach to make platform data saleable, aiming to balance these conflicting objectives and clarifying the differences between data markets and other concepts like data spaces.",CS,AI_ML,0.85,Extracted from log - paper 1423
Kill Two Birds with One Stone: Using Multihoming Boundary Resources in Mobile App Development,"This research examines the impact of using multihoming SDKs on app quality in mobile app development. It provides insights into the trade-offs between technical design and platform governance, offering practical guidance to developers in managing resources and strategic goals.",CS,AI_ML,0.85,Extracted from log - paper 1424
Effect of Digitization on Management Accountants’ Tasks and Tools,"Digitization and the increased available data volumes are triggering a fundamental change in Management Accountants' (MAs) scope of tasks and activities. In particular, the current technological developments, including analytics or automation tools, are eliminating standard tasks and processes. This article examines the shift in the job description of a MA regarding changes in technologies, methods, and tools in times of digitization. The results of an analysis of job advertisements provide an overview of the development of tasks, trends and technologies for Management Accountants (MA) in Germany. Our results highlight an increasing demand for Machine Learning, Predictive Analytics and Robotic Process Automation backgrounds, suggesting a shift of the MAs role towards a business partner role with minimization of repetitive tasks.",CS,AI_ML,0.85,Extracted from log - paper 1425
What Constitutes a Dataspace? Conceptual Clarity beyond Technical Aspects,"In the data economy, data has become an essential strategic resource for gaining a competitive advantage. Data spaces represent a relatively new phenomenon aimed at encouraging businesses to fully leverage the potential of data. Despite various approaches for definitions, there remains a lack of clarity surrounding the conceptualization of data space, its perceived value, and the factors that drive its adoption. This paper addresses these issues by proposing primary properties of data space...",CS,AI_ML,0.85,Extracted from log - paper 1426
Towards an Architecture for Data Monetization as a Service,"Data monetization as a service (DMaaS) is a new term in both academia and practice. DMaaS seeks to marry data monetization with servitization and has become important with the advent of technologies such as big data, cloud computing, and blockchain. As organizations seek to establish infrastructure for data monetization, an architecture for DMaaS provides them with a starting point to develop data monetization platforms. This paper leverages the service-dominant logic perspective and research...",CS,AI_ML,0.85,Extracted from log - paper 1427
From Full-fledged ERP Systems Towards Process-centric Business Process Platforms,"Enterprise Resource Planning (ERP) systems are critical to the success of enterprises, facilitating business operations through standardized digital processes. However, existing ERP systems are unsuitable for startups and small and medium-sized enterprises that grow quickly and require adaptable solutions with low barriers to entry. Drawing upon 15 explorative interviews with industry experts, this study examines the challenges of current ERP systems using the task-technology fit theory...",CS,AI_ML,0.85,Extracted from log - paper 1428
The Role of Combinational Usage Diversity of ES Functions in Affecting Employee Productivity: Considering the Contingent Value of Job Experience,"This study explores the value of the combinational usage diversity of Enterprise System (ES) functions in employee productivity and the potential nonlinear moderating effect of job experience. By collecting daily ES operation log records and monthly sales records, the study provides new insights into the value of ES usage. The findings identify the positive impact of the combinational usage diversity of ES functions—namely combinational variety, disparity, and separation—on employee performance...",CS,AI_ML,0.85,Extracted from log - paper 1429
Exploring ERP Architecture in a Global Corporation,"In today’s rapidly changing global business environment, organizations rely on their information technology infrastructure to coordinate global operations. An important part of this infrastructure is a company’s ERP systems. While common wisdom suggests that a company’s ERP architecture should be designed on a global basis to align with strategies and organizational structures, little is known about how these systems are implemented in practice. A case study conducted to explore how ERP...",CS,AI_ML,0.85,Extracted from log - paper 1430
"Understanding Barriers, Enablers, and Best Practices for Creating Effective Multi-generational Digital Workspaces","Based on two systematic literature reviews, this study contributes to the understanding of multi-generational digital workspaces by consolidating a list of known barriers and enablers in digital workplaces and identifying best practices to mitigate the former and promote the latter. Further, it analyzes how these factors are perceived by different generations—X, Y, and Z—which increasingly work together. These findings are important as companies struggle to create future digital or hybrid workplaces.",CS,AI_ML,0.85,Extracted from log - paper 1431
The Impact of Metaverse Technology Use on Team Creativity in Virtual Teams,"As a crucial contributor to organizational performance and competitive advantage, the ability to develop original and effective ideas has been identified as a key component. As digital technologies advance, businesses increasingly utilize virtual teams due to their cost-effectiveness and efficiency. Moreover, in recent years, there has been an increase in interest in a new concept of work environments known as metaverses. However, the creative performance of virtual teams in the metaverse setting...",CS,AI_ML,0.85,Extracted from log - paper 1432
Supporting Effective Communication in Remote Cybersecurity Analyst Teams,"The COVID-19 pandemic has led to increased remote work in the cybersecurity industry, posing challenges to effective communication among cybersecurity analysts. A collaborative analysis support system called AOH-Map has been developed to address this problem by capturing and integrating the analysis processes of a group of analysts into a visual map. This study explores the media capability of AOH-Map in supporting communication among cybersecurity analysts in a virtual team setting...",CS,AI_ML,0.85,Extracted from log - paper 1433
To Assert or Defend My Role as an OSS Developer: How IT Infrastructure Access Changes the Effect of Digital Platform Behavior on Firm Mobility,"Increasingly, firms seek and employ open source software (OSS) developers using digital platforms (e.g., LinkedIn and GitHub). OSS developers display their skills in various ways on these platforms to attract firms. However, it remains unclear whether all OSS developer behaviors influence an OSS developer’s firm mobility in the same way. This work takes an impression formation lens to understand how OSS developer behaviors enacted in digital platforms lead to firm mobility...",CS,AI_ML,0.85,Extracted from log - paper 1434
From the Pandemic to War: The Role of Digital Technologies in Ukrainian Businesses Responding to Discontinuities and Building Resilience,"Disasters and crises are a part of human existence. While upsetting the normal operations of societies and revealing the inherent fragility of infrastructure and social order, they also push us to rethink “the normal” way of living and working. At this moment, IS research on the experiences of businesses in times of war as an extreme crisis is very limited. Taking the case of Ukrainian businesses, this study asks: How have organizations adapted to war discontinuities and built resilience?...",CS,AI_ML,0.85,Extracted from log - paper 1435
Digital Transformation Team’s Capabilities for Process Automation,"This study argues that digital transformation teams should build the right competence profiles to deliver changes using digital tools. Based on dynamic capability theory, it proposes that the team’s digital problem-solving capability and benefits management capability are essential for digital transformation success. Using process automation as a context, the paper suggests that digital transformation teams should build data literacy, business process management competence, and...",CS,AI_ML,0.85,Extracted from log - paper 1436
Developing a Data Literacy Assessment Tool for the Business Workforce,"This paper aims to develop a tool to assess the data literacy of individuals in the business workforce. Using a design science research approach, a data literacy assessment tool was developed based on 18 core competencies across three skill levels. The survey tool was tested with 243 respondents in a large North American financial company. The artifact provided an accurate assessment of intra- and inter-individual competencies and the identification of typical profiles required...",CS,AI_ML,0.85,Extracted from log - paper 1437
Artificial Intelligence in the Workplace: Implementation Challenges and Opportunities,"This pilot study investigates the challenges and opportunities of implementing human-machine systems in organizations. Findings indicate that organizations have realized positive benefits from AI projects through high levels of communication, stakeholder consultation, problem management, ethics, and transparency. However, significant work is required in managing staff motivation and empowerment, trust in AI technologies, and managing novel cyberthreat issues...",CS,AI_ML,0.85,Extracted from log - paper 1438
Vantagens e Desvantagens do Desenvolvimento de Sistemas de Informação com Tecnologias Low-Code – Uma Revisão de Literatura,"In a world marked by great unpredictability, organizations need to respond quickly and with quality to the evolving needs of their stakeholders. Organizational development using Information Technologies (IT) is fundamental in this context. However, companies often face difficulties in hiring qualified human resources for efficient and effective IT development. Low-code technologies present themselves as a solution to this problem, allowing rapid development of applications and enabling...",CS,AI_ML,0.85,Extracted from log - paper 1439
Impact of Court’s Case Management Systems: Perspectives on Efficiency,"Justice organizations are central to democracy. However, even democratic countries report high costs, slowness, and unreasonable backlogs in the justice system. Many countries have invested heavily in Information Technology (IT) to address efficiency issues. This research analyzes whether IT fulfills its promise of higher court efficiency. The research model contemplates three hypotheses related to the efficiency of courts of justice, focusing on clearance rate, process duration, and...",CS,AI_ML,0.85,Extracted from log - paper 1440
Um Framework de Concepção de Software com Design Thinking em um Programa de Intraempreendedorismo,"Companies need to develop more assertive and innovative solutions. In this context, Design Thinking (DT) is an approach that allows immersion in the problem, creation of collaborative solutions, and validation with prototypes. However, literature shows difficulties in adopting approaches like DT, especially for inexperienced professionals and due to cultural resistance. This work used action research to develop and apply a framework for implementing software design with DT...",CS,AI_ML,0.85,Extracted from log - paper 1441
"The Future of System Goals and Human Rights: Data Governance, AI Ethics, and Lessons Learned in Covid-19 for Research Data","This symposium explores the current state of data tools, guidelines, and proposals for policies and regulations promoting trust in AI and federated systems, ultimately advancing Open Science and fundamental human rights. The implementation of advanced technologies and big data initiatives in smart cities urges attention to AI ethics, data governance, cybersecurity, confidentiality, and issues of privacy and surveillance...",CS,AI_ML,0.85,Extracted from log - paper 1442
Navigating Uncertainty: Preparing Organizations for Digital Disruption,"This professional development symposium discusses how emerging technologies such as AI, blockchain, AR, and IoT are transforming organizations and value chains. The panel explores mechanisms that technology leaders may use as guardrails to assist in navigating complex technical, ethical, and regulatory environments common to emerging technologies...",CS,AI_ML,0.85,Extracted from log - paper 1443
"How to Make a Respectful Impact in Service to Underserved Communities Before, During and After the Paper Gets Published","Focusing on social inclusion research, this symposium provides guidance on ethically engaging with underserved communities throughout the research process. It highlights tools and techniques for building collaborative projects that contribute to meaningful solutions, emphasizing the importance of avoiding a savior mentality and ensuring that research efforts positively impact the communities involved.",CS,AI_ML,0.85,Extracted from log - paper 1444
The Metaverses’ Edge for Learning: Body and Identity,"This study explores how immersive technologies, such as virtual reality headsets, enable users to interact with virtual environments—referred to as metaverses—using their bodies. Prior research suggests that embodiment features in metaverses facilitate learning. However, limited attention has been given to how users’ perceptions of themselves within these environments impact learning effectiveness...",CS,AI_ML,0.85,Extracted from log - paper 1445
"Assessing and Controlling the Social Desirability Bias in the Context of Victims, Perpetrators, and Bystanders of Cyberbullying","This study examines social desirability (SD) bias in the context of cyberbullying perpetrators, victims, and bystanders. To assess and control SD bias, the study uses the 16-item Balanced Inventory of Desirable Responding and the covariance technique.",CS,AI_ML,0.85,Extracted from log - paper 1446
"Integrating Technology Readiness, Learning Goal Orientation with TAM to explain E-learning adoption","This research proposes an integrated model that combines Technology Readiness dimensions, Learning Goal Orientation, and the Technology Acceptance Model to explain e-learning adoption among Indian MBA students. The model was tested using PLS-SEM, revealing significant relationships among the constructs.",CS,AI_ML,0.85,Extracted from log - paper 1447
Instances of Digital Dark Nudging: Findings of a Systematic Literature Analysis,"This paper provides a literature analysis of digital dark nudging instances, identifying 14 relevant publications and categorizing them into seven instances within five application domains. The study highlights the adverse effects of digital dark nudging and the need for clear differentiators between regular and dark nudging.",CS,AI_ML,0.85,Extracted from log - paper 1448
Investigating Users' Continuous Adoption of Cryptocurrency,"This study explores users’ continuous adoption intention of cryptocurrency by developing a model based on the Transaction Cost Economics Theory and perceived value. Data from 173 cryptocurrency users indicate that perceived value, influenced by benefits and transaction costs, determines continuous adoption.",CS,AI_ML,0.85,Extracted from log - paper 1449
Exploring Parental Influence on Children's Cyber Hygiene,"This study examines how parents influence their children's cyber hygiene, proposing that parental practices, guidance, restrictions, and digital safety measures positively impact children's cyber hygiene. A mixed-methods design, including interviews and surveys, will test the hypotheses.",CS,AI_ML,0.85,Extracted from log - paper 1450
Smart Contracts’ Adoption in the Healthcare Sector: a Privacy Calculus Perspective in China and U.S.A.,"This research explores factors influencing the intention to adopt smart contracts for electronic health records in the U.S.A and China. Integrating the privacy calculus model and Hofstede’s cultural dimensions, the study surveys patients in both countries to understand perceived risks and benefits.",CS,AI_ML,0.85,Extracted from log - paper 1451
Investigating the Determinants of Compliance Intention in Behavior Change Support Systems,"This study investigates determinants of compliance intention in Behavior Change Support Systems by extending the persuasive system design model. Surveying 234 prospective users, the findings suggest that perceptions of support and credibility influence enjoyment, persuasiveness, and compliance intention.",CS,AI_ML,0.85,Extracted from log - paper 1452
All Together Now: Fuzzy-set Qualitative Comparative Analysis of Personality Profiles and their Coping with Technostress,"Using the Big Five personality inventory and configuration theory, this study analyzes how personality profiles affect coping behavior in response to technostress. Survey data from 233 respondents and fuzzy-set qualitative comparative analysis reveal distinct personality configurations for coping strategies.",CS,AI_ML,0.85,Extracted from log - paper 1453
The Big Five Under Pressure in E-Commerce: Exploring the Role of Coping Strategy and Technostress,"This study examines the effects of the Big Five personality dimensions on purchasing intention in e-commerce, considering technostress and coping strategies. An online survey with 479 participants indicates that certain personality traits influence coping strategies, which in turn affect purchase intention.",CS,AI_ML,0.85,Extracted from log - paper 1454
Discovering Telemedicine Usage Motivation Using a Trust-based Valence Framework,"This research investigates patient motivation for adopting telemedicine services using the Extended Valence Framework, including perceived trust, risk, benefit, and intent to use telemedicine services. Findings show that trust and benefit significantly influence intent, while perceived security and familiarity impact trust.",CS,AI_ML,0.85,Extracted from log - paper 1455
Cross-cultural and Institutional Perspectives on Information Technology Governance,"This study examines how different organizational cultures, as defined by the OCAI model, influence employees' perceptions of IT Governance (ITG) institutionalization. Surveying 513 workers worldwide, the research found that Market and Hierarchy cultures are associated with higher perceived ITG.",CS,AI_ML,0.85,Extracted from log - paper 1456
AI Healthcare Adoption: A Privacy Calculus Model Incorporating Emotions and Techno-social Factors,"This study investigates the adoption of AI-based healthcare services in the USA using a privacy calculus model incorporating AI technical characteristics, techno-social factors, and emotional states. It proposes and tests a model examining the effect of these factors on adoption intention.",CS,AI_ML,0.85,Extracted from log - paper 1457
A Review and Meta-analysis of Mobile Government Adoption,This meta-analysis reviews 38 empirical studies to identify critical factors influencing mobile government adoption and analyzes how economic development levels moderate these effects. The results provide theoretical and practical implications for e-government development strategies.,CS,AI_ML,0.85,Extracted from log - paper 1458
Youth Exodus! A Framework of Social Media Migration by Young Adults,"This study explores factors influencing young adults' migration from social media platforms like Facebook. A theoretical framework examines how attitudes, social norms, fake news, and privacy concerns influence abandonment. Survey results support the proposed model.",CS,AI_ML,0.85,Extracted from log - paper 1459
The Impact of Cloud Computing on Firm Performance through Operational Capability,"This study investigates how cloud computing adoption enhances firm operational capabilities, leading to better performance. It uses a two-stage econometric model and analyzes the role of CIO presence in moderating operational capability.",CS,AI_ML,0.85,Extracted from log - paper 1460
Responsible Digital Innovation in Dark: Toward Access-Control-Transparency Theory,"This paper introduces the Access-Control-Transparency (ACT) theory, a framework for designing digital artifacts that promote responsible digital innovation. It emphasizes access, control, and transparency and offers research directions.",CS,AI_ML,0.85,Extracted from log - paper 1461
Social Media Addiction: A Techno-centric View,"This study adopts a techno-centric perspective to explore the role of social media platform features in addiction, utilizing the needs-affordances-features framework. The analysis reveals feature-need alignments contributing to problematic use.",CS,AI_ML,0.85,Extracted from log - paper 1462
How Does AI-Work Interaction Affect Individuals' Behavior? The Role of Appraisals,This study applies the transactional theory of stress to understand how AI integration in workplaces affects behavior. It explores how individuals appraise AI as augmentation or automation and how expertise shapes their reactions.,CS,AI_ML,0.85,Extracted from log - paper 1463
Investigating the Role of Gender in Active and Healthy Ageing Supported by ICT,This paper examines gender differences in the use of ICT for active ageing in Poland and Sweden. Value-Focused Thinking is used to identify key motivations and differences in technology adoption patterns.,CS,AI_ML,0.85,Extracted from log - paper 1464
Unrevealing the Dynamic Impact of Extended Reality Adoption on Offline Channel Performance,"This study analyzes the impact of extended reality (XR) adoption on offline channel performance in multichannel retail. Findings suggest a non-linear relationship: initial improvement, then diminishing returns.",CS,AI_ML,0.85,Extracted from log - paper 1465
The Impact of Use of Big Data-Driven Systems on Decision-Makers’ Power,"This research studies how big data-driven systems affect the power of decision-makers in organizations. Using strategic contingencies theory, it models power redistribution due to increased data use.",CS,AI_ML,0.85,Extracted from log - paper 1466
Empirical Assessment of Big Data Technology Adoption Factors for Organizations with Data Storage Systems,"This study identifies factors influencing big data technology adoption in organizations with existing storage systems. A survey and PLS-SEM reveal industry pressure, system compatibility, and cost as major drivers.",CS,AI_ML,0.85,Extracted from log - paper 1467
Managing Misinformation within the Public Sector: Cases from the Global South,This empirical study investigates how public sector organizations in developing countries manage misinformation. It highlights short- and long-term responses and contextual challenges.,CS,AI_ML,0.85,Extracted from log - paper 1468
Digital Transformation in the Public Sector: A Comparative Study of E-Government Implementation in Developing Countries,"This comparative case study explores e-government implementation in developing countries. It identifies technological, institutional, and human factors that affect success and suggests policy recommendations.",CS,AI_ML,0.85,Extracted from log - paper 1469
Blockchain Adoption in Supply Chain Management: An Empirical Investigation,"This paper analyzes blockchain adoption in supply chains, focusing on transparency and trust. Through empirical investigation, it identifies drivers, barriers, and performance impacts.",CS,AI_ML,0.85,Extracted from log - paper 1470
Understanding Blockchain Adoption from an Institutional Theory Perspective,"The study explores blockchain adoption through the lens of Institutional Theory, proposing a framework that includes technological pressure alongside the traditional institutional pressures. Data was gathered via semi-structured interviews with blockchain experts using the Delphi method.",CS,AI_ML,0.85,Extracted from log - paper 1471
Towards the Dark Side of AI Adoption: How Generative AI Extenuates the Perception of Chatbot Errors,"Investigates how generative AI chatbots, like ChatGPT, affect user perceptions of errors. A 2x2 experiment reveals that errors are less noticeable in generative-based chatbots, leading to higher perceived competence and trust, despite the presence of errors.",CS,AI_ML,0.85,Extracted from log - paper 1472
Fit in a Bit: eSport and eFitness Adoption in Qatar,"This study proposes a model to understand factors influencing the adoption of VR-based eSports and eFitness in Qatar, considering environmental conditions and socio-cultural practices. It draws from IT adoption literature to conceptualize adoption factors in the GCC region.",CS,AI_ML,0.85,Extracted from log - paper 1473
What’s the Harm in Waiting? Examining Disfluency from System Response Time in Decision Tasks with Conversational Agents,"Examines how system response time affects user decision-making with conversational agents. Findings suggest that slower responses can discourage users from using helpful features like filters, leading to suboptimal decisions, especially in disfluent situations.",CS,AI_ML,0.85,Extracted from log - paper 1474
Infodemic and its Cure: A Digital Nudging Approach,"Addresses the spread of health misinformation during the COVID-19 pandemic. The study explores how digital nudging can shift users from psychological ownership to dis-ownership of false information, using concepts from psychological ownership motivation and social exchange theory.",CS,AI_ML,0.85,Extracted from log - paper 1475
"“Jane Sent Me This Article, So It Must Be True!” – How Tie Strength and Emotional Tone Influence Information Behavior","Investigates how emotional tone and the strength of social ties affect the spread of fake news via instant messaging. An experimental study examines their impact on willingness to fact-check and intention to share, mediated by sender credibility and news believability.",CS,AI_ML,0.85,Extracted from log - paper 1476
Battling Disinformation Intermediaries: An Analysis of Information Policies,"This study investigates disinformation policies, focusing on intermediaries—channels and mechanisms that facilitate disinformation dissemination. Through inductive thematic analysis of policies in China and the United States, the research identifies various intermediaries subject to regulation and examines the contexts of disinformation regulation in both countries, providing empirical insights for researchers and policymakers.",CS,AI_ML,0.85,Extracted from log - paper 1477
The Interplay of IT Identity and Digital Mindset in the Workplace,"This paper examines how IT identity and digital mindset influence job satisfaction in the workplace. Surveying 167 employees, the study finds that IT identity's effect on job satisfaction is fully mediated by job identity, with digital mindset moderating the relationship between IT and job identities, offering insights into identity formation and its impact on job satisfaction in digitally-enabled workplaces.",CS,AI_ML,0.85,Extracted from log - paper 1478
Gamification Impacts on Technostress,"This research explores how gamification affects technostress by applying basic psychological needs theory. It proposes a framework where game element characteristics influence technostress through the satisfaction or frustration of psychological needs, suggesting that incorporating gamification elements that fulfill these needs can help manage technostress in individuals.",CS,AI_ML,0.85,Extracted from log - paper 1479
Institutional Legitimation of IT Fashions,"Many emerging information technology (IT) innovations are proclaimed to revolutionize business – if not the world. This recurring phenomenon of hyperbolic expectations followed by their disillusionment has drawn vast attention amongst researchers in the fields of management and information systems (IS), leading to its conceptual embedding within fashion theory. This proposed study aims to extend the sparse empirical body on IT fashion research, thereby deepening the understanding of the associated organizational consequences regarding an essential organizational outcome, namely organizational legitimacy. In this course, I focus on organizational legitimacy in the eyes of sell-side securities analysts which acknowledges recent findings that identify them as a key source of institutional pressures for legitimacy. Further, I plan to analyze whether and how the respective perception of legitimacy might differ contingent upon different firm characteristics such as industry affiliation or ownership structure.",CS,AI_ML,0.85,Extracted from log - paper 1480
Smart Parking Systems and their Impact on the Efficiency of Parking Officers,"Analyzing data from Los Angeles between 2015 and 2019, this study examines how smart parking meters and time limit policies affect parking officers’ efficiency. Utilizing transaction cost economics and a fixed-effect estimation model, findings indicate these technologies positively influence the number of parking citations.",CS,AI_ML,0.85,Extracted from log - paper 1481
How Would Individuals Like to Use AI-Enabled Applications? An Empirical Study Based on HCI Framework,"This research applies the Human-Computer Interaction (HCI) framework to study factors influencing individuals’ acceptance of AI-enabled applications. Survey and PLS-SEM results show user trust, interaction, and tech fit significantly affect usage intention.",CS,AI_ML,0.85,Extracted from log - paper 1482
A Process Model for the Practical Adoption of Federated Machine Learning,"This paper outlines a structured process model for managing Federated Machine Learning (FML) projects. It details activities, resources, and interdependencies to aid transparent and effective FML adoption.",CS,AI_ML,0.85,Extracted from log - paper 1483
The Art of Inspiring Creativity: Exploring the Unique Impact of AI-generated Images,"Investigates how DALL-E-2 impacts user creativity. An online experiment shows AI-generated images significantly enhance creative output, highlighting their role as creativity boosters.",CS,AI_ML,0.85,Extracted from log - paper 1484
Dynamics of Trust: Unpacking Trust in Human-AI Collaboration in Decision-Making,"This review categorizes human, AI, and context-based trust factors in AI collaboration. Proposes a dynamic trust development framework to guide trustworthy system design.",CS,AI_ML,0.85,Extracted from log - paper 1485
Building Trust in AI: A New Understanding of the Role of Reliability,Interviews at a global auto firm reveal that users tolerate less-than-perfect AI reliability when supported by organizational trust-building measures.,CS,AI_ML,0.85,Extracted from log - paper 1486
Unlocking the Potential of ChatGPT: A Grounded Theory Exploration of Its Impact on the Business Landscape,"A grounded theory study across 11 industries shows ChatGPT’s dual roles in automation and augmentation, reshaping business models and customer behavior.",CS,AI_ML,0.85,Extracted from log - paper 1487
Using AI and ChatGPT in Brand Storytelling,"This study explores how AI chatbots, like ChatGPT, can assist companies in creating brand stories and communicating with stakeholders. It examines the effects of varying language formality and the role of informed communicators on individuals’ attitudes toward the brand, considering psychological distance.",CS,AI_ML,0.85,Extracted from log - paper 1488
Revealing the Dark and Bright Sides of ChatGPT: An Exploratory Study on User Perceptions,"An exploratory qualitative content analysis of Reddit posts reveals both positive and negative user perceptions of ChatGPT. While users appreciate its assistance and value creation, concerns include algorithm limitations, job displacement, privacy issues, and misuse for cheating.",CS,AI_ML,0.85,Extracted from log - paper 1489
A Genetic Algorithm Based Consensus Reaching Method on Malware Labels,"This paper introduces a novel scoring system, the Pairwise Consensus Score (PCS), combined with a Genetic Algorithm to achieve consensus on malware labeling. The approach aims to provide more consistent and trustworthy antivirus labels by addressing inconsistencies among vendors.",CS,AI_ML,0.85,Extracted from log - paper 1490
The Work of Students and ChatGPT Compared: Using Machine Learning to Detect and Characterize AI-Generated Text,"An experiment compares student-written assignments with ChatGPT-generated responses. Using text classification, the study identifies stylistic differences, noting that AI-generated text tends to be more formal. The findings aid in detecting AI-generated content in academic settings.",CS,AI_ML,0.85,Extracted from log - paper 1491
AI-based Technologies for Conversational Agent Design – Development Tools and Architectures for Intelligent Interactions,"This research develops a taxonomy to characterize AI-based tools for conversational agents. Through cluster analysis, it derives archetypes and meta-architectures, providing insights into integrating AI services into conversational agent designs for enhanced human-machine interaction.",CS,AI_ML,0.85,Extracted from log - paper 1492
Alexa is the New Influencer: An Empirical Study Based on a Relational View,"Investigating voice assistants (VAs) as influencers, this study finds that factors like parasocial relationships, peer influence, and self-image congruence positively affect consumers’ decisions to follow VA recommendations, highlighting VAs’ potential in marketing strategies.",CS,AI_ML,0.85,Extracted from log - paper 1493
How Does User Engagement Support Content Moderation? A Deep Learning-based Comparative Study,"This study proposes a framework incorporating user engagement metrics into content moderation. By integrating credibility and stance into graph learning, the model outperforms traditional deep learning approaches, enhancing moderation decisions for user-generated posts.",CS,AI_ML,0.85,Extracted from log - paper 1494
From “Handmade” to “AI-Made”: Mitigating Consumers’ Aversion towards AI-Generated Textual Products,"This study explores consumer aversion to AI-generated textual products. Through 41 consumer interviews, it identifies five signals—quality control, hard output paradigm, information transparency, process transparency, and positive reviews—that can mitigate concerns and enhance acceptance of AI-generated content.",CS,AI_ML,0.85,Extracted from log - paper 1495
Toward Identifying the Factors Associated with Conversational Agents’ Performance,"Focusing on text-based conversational agents (chatbots), this research identifies chatbot attributes, user attributes, and task attributes that impact performance from the users’ perspective. Interviews with users of rule-based chatbots inform a theoretical framework for chatbot performance evaluation.",CS,AI_ML,0.85,Extracted from log - paper 1496
Response Failure of Mental Health Chatbots: Examining the Impact on Discontinuance,"This paper investigates how response failures in digital mental health interventions (chatbots) influence user discontinuance. Utilizing expectation disconfirmation theory and cognitive dissonance theory, it examines the effects of satisfaction, ease of use, and usefulness on continued usage.",CS,AI_ML,0.85,Extracted from log - paper 1497
Perceived Warmth and Intelligence of AI: Impact on Employee Performance,The study examines how employees’ perceptions of AI’s warmth and intelligence affect their identification with AI technologies and subsequent job performance. Data from 319 customer service representatives interacting with chatbots reveal significant impacts on employee performance.,CS,AI_ML,0.85,Extracted from log - paper 1498
Exploring perceptions of pro-environmental educational mobile applications based on semantic field analysis,"This paper identifies user perceptions of pro-environmental educational mobile apps through semantic field analysis and emotional temperature measurement. Findings suggest that reward systems reinforce environmentally friendly behavior, while user behavior monitoring is viewed negatively.",CS,AI_ML,0.85,Extracted from log - paper 1499
IT Security and Espoused Cultural Values: A Comparative Analysis of Pakistan and the United States,"Extending Protection Motivation Theory, this study examines how espoused cultural values influence IT security behaviors in Pakistan and the US. It introduces indulgence vs. restraint and survival vs. self-expression as cultural dimensions affecting security measure adoption.",CS,AI_ML,0.85,Extracted from log - paper 1500
Psychological resilience and job satisfaction and the effectiveness of social media marketing employees,"This study investigates the relationship between mental resilience, job satisfaction, and effectiveness among social media marketing employees in Poland. Utilizing the SPP-25 resilience scale and job satisfaction measures, the research analyzes data from 51 respondents across three marketing companies.",CS,AI_ML,0.85,Extracted from log - paper 1501
Navel Gazing or the Long View?: The Influence of Firm Positioning with Respect to OSS Communities on a Developer’s Firm Embeddedness,"This paper explores how a firm’s orientation towards open-source software (OSS) communities affects developers’ embeddedness within the firm. Using a conservation of resources framework, the study finds that developers perceive internal-focused firms as more supportive.",CS,AI_ML,0.85,Extracted from log - paper 1502
The Changing Nature of Telemedicine Processes: Adaptations and Triggers,"Investigating telemedicine in two Indian public hospitals, this study examines how digitalization prompts adaptations in care processes. Applying Adaptive Structuration Theory, the research identifies triggers and mechanisms leading to changes in technology use, tasks, and roles.",CS,AI_ML,0.85,Extracted from log - paper 1503
The Use of Gamification to Enhance Internal Software Quality - Structured Literature Analysis,This structured literature review analyzes the application of gamification to improve internal software quality aspects like code complexity and documentation.,CS,AI_ML,0.85,Extracted from log - paper 1504
The Role of ICT Permeability on Work and Family Outcomes When Working from Home,This research develops a framework to assess how ICT permeability impacts work and family outcomes among dual-career couples working from home.,CS,AI_ML,0.85,Extracted from log - paper 1505
Blockchain Transformation for Banking and Financial Services: Examining the Opportunities and Risks,This paper explores how blockchain technology can transform banking and financial services. It identifies opportunities and risks and offers implementation recommendations.,CS,AI_ML,0.85,Extracted from log - paper 1506
Digital Transformation: How Scaling Agility Affects Value Creation Paths,"Investigating the impact of scaling agility on digital transformation, this study conducts multiple case studies to understand how structural changes influence value creation.",CS,AI_ML,0.85,Extracted from log - paper 1507
Leveraging Digital Platforms Through Business Model Innovation in SMEs: A Capability Perspective,This research examines how SMEs can leverage digital platforms through business model innovation.,CS,AI_ML,0.85,Extracted from log - paper 1508
Take It or Not? Impact of Taking Investments from Tech Giants on IT Startups’ Future Funding,This study examines how taking investments from tech giants influences future funding of IT startups.,CS,AI_ML,0.85,Extracted from log - paper 1509
Organizational Capabilities in Emerging Blockchain-Enabled Platform Ecosystems,This paper identifies organizational capabilities needed for value creation in blockchain ecosystems.,CS,AI_ML,0.85,Extracted from log - paper 1510
Digital Entrepreneurs: What Do We Know about Them,This review analyzes 43 papers to understand digital entrepreneur typologies and capabilities.,CS,AI_ML,0.85,Extracted from log - paper 1511
Cryptocurrency Market: The Interplay Between Microblog Dynamic Opinion Leader Sentiments and Bitcoin Pricing,This study investigates how Bitcoin tweet sentiment and price affect each other using VAR modeling.,CS,AI_ML,0.85,Extracted from log - paper 1512
Navigating SMEs’ Innovation in the Digital Age: From Manufacturing to Servitization,A case study on how a manufacturing SME used digital servitization for innovation.,CS,AI_ML,0.85,Extracted from log - paper 1513
Design Thinking for Effective Collaboration in Multi-Stakeholder Requirements Engineering,An action research study proposes a design-thinking-enhanced requirements engineering process.,CS,AI_ML,0.85,Extracted from log - paper 1514
A Collective Opinion Mechanism for Crowdfunding Recommendation,This paper proposes a system to recommend crowdfunding projects based on backer opinion and engagement.,CS,AI_ML,0.85,Extracted from log - paper 1515
Corporate Trade War Uncertainty and Patent Bubble,This paper investigates how perceived trade uncertainty influences strategic patenting in China.,CS,AI_ML,0.85,Extracted from log - paper 1516
Understanding the Theoretical Foundations of Digital Transformation Literature: A Systematic Review,A review of 183 journal articles identifying theoretical foundations of digital transformation literature.,CS,AI_ML,0.85,Extracted from log - paper 1517
On Technology Ecosystems: De-Jure Standards Setting by Firms,This study analyzes collaboration factors that affect standard-setting in tech ecosystems.,CS,AI_ML,0.85,Extracted from log - paper 1518
Digital Platform Entry Barriers for Family-run SMEs,"Family-run SMEs usually have a long tradition. They have specified their organizational structures and processes to their product or service over the years. Now these companies are facing various difficulties, such as the increasing shortage of skilled workers or new competition from digital technologies. We argue that digital platforms can be a challenge as well as an opportunity. In our perception, however, German family-run SMEs show a restrained adoption of those technologies. This article therefore examines the barriers to the use of digital platforms in the value creation process of family businesses. For this purpose, we followed the approach of design-oriented information systems. We evaluated transcripts of eleven interviews with family-run SMEs and identified five categories as barriers to enter: Customer, Socio-technical, Personal, Resources, and non-specific Others. Finally, in the discussion we mapped these barriers to actors we assume to be capable of tearing them down in the future.",CS,AI_ML,0.85,Extracted from log - paper 1519
Causal Matching for Startups: Methods for Controlling Confounding,"Causal inference is a critical methodology that allows researchers to empirically test hypotheses and build theories. Ideally, the design method for establishing this causality would be through randomized controlled trials. However, studying the impact of different practices (hereinafter treatments) on initial outcomes when treatments are not applied exogenously is a challenge. Matching methods are an appropriate way to provide these study groups. The objective of this work is to provide an algorithm-based solution to match startups in a way that can mimic a human match. To do this, we use the human matching effort by Yu (2020) as a ground truth to evaluate various natural language processes and compare the performance of each with human raters. By comparing automated approaches with matching, we provide guidance for researchers interested in the causal analysis of startups using textual data and other covariates.",CS,AI_ML,0.85,Extracted from log - paper 1520
Evolution of Digital Innovation Labs – How Organizational Learning Contributes to Digital Transformation,"Digital Innovation Labs (DILs) have become a tool for companies to handle disruption and digital innovation. However, success was mixed. Some DILs failed to reach their targets whereas others provided the pathway toward digital transformation. This study examines the evolution of DILs and shows how Organizational Learning advances innovation. Using a case study approach, it looks at two developmental stages of one particular DIL. Over time, the DIL has changed fundamentally and established a funnel approach where the lab structure supports the entire innovation process. Eight recommendations are derived that may contribute to the success of DILs in companies.",CS,AI_ML,0.85,Extracted from log - paper 1521
How is my IT department leading to Digital Innovation Success?,"Digital innovation, driven by IT departments, is a key enabler for success in all types of organizations around the world. Although there are a lot of touted benefits, there is not much scholarly attention as to how digital innovation success can be achieved in organizations. Drawing upon social capital theory, this study plans to examine how functional IT employees’ relationship with the Chief Information Officer (CIO) can help organizations in achieving digital innovation success. The study proposes that the social capital of the relationship between CIO and functional IT employees will enhance both external and internal digital innovation success through more closely engaging functional IT employees in the decision-making processes of CIO. Furthermore, the study aims to identify the role of environmental turbulence and organizational culture as moderators in the proposed model.",CS,AI_ML,0.85,Extracted from log - paper 1522
Decoding Digital Risk from Corporate Disclosure: A Neural Network Approach,"Digital risk-or the likelihood of losses from organizational context, digital infrastructure, IS sourcing, data management and IS applications, is a key consideration in the valuation of firms. We apply a neural network approach to construct and measure digital risks by extracting linguistic relations from the 10-K disclosure (Section “Item 1A”). We develop novel firm-level digital risk measures based on these linguistic relations from three perspectives: (i) presence (whether the digital risk is mentioned or not), (ii) intensity (text coverage of digital risk relative to other issues), and (iii) diversity (the different types of digital risk that are mentioned). We validate our measures for digital risk by demonstrating that they correlate significantly with firm risk, as proxied by the volatility of the stock market. Overall, our findings suggest that the utility of leveraging this type of text-based measure for digital risk is practically feasible, scalable, and economically meaningful.",CS,AI_ML,0.85,Extracted from log - paper 1523
Investigating the Impact of Digital Transformation on Organizational Identity in an SME: Insights from an In-Depth Case Study,"The relationship between digital transformation and organizational identity is an immanent topic in IS, which is especially relevant for small- and medium-sized enterprises (SMEs) due to their reliance on implementing digital technologies to stay competitive. However, little is known about this relationship in this specific context. Based on theories and concepts about digital transformation, organizational identity, and SMEs, we conducted an in-depth case study with a German Mittelstand company in the engineering sector. Building on a conceptual archetype framework, we reveal that different manifestations of digital transformation have different impact on organizational identity. Thereby, we contribute to IS research by highlighting the mutual dependency of these concepts and by underlining the necessity for being aligned to the specifics of SMEs. Moreover, we derive practical implications for managers that help to anticipate the effects of digital transformation within their strategic decision processes.",CS,AI_ML,0.85,Extracted from log - paper 1524
Exploring the Synergies in Human-AI Hybrids: A Longitudinal Analysis in Sales Forecasting,"Despite the promised potential of artificial intelligence (AI), insights into real-life human-AI hybrids and their dynamics remain obscure. Based on digital trace data of over 1.4 million forecasting decisions over a 69-month period, we study the implications of an AI sales forecasting system’s introduction in a bakery enterprise on decision-makers’ overriding of the AI system and resulting hybrid performance. Decision-makers quickly started to rely on AI forecasts, leading to lower forecast errors. Overall, human intervention deteriorated forecasting performance as overriding resulted in greater forecast error. The results confirm the notion that AI systems outperform humans in forecasting tasks. However, the results also indicate previously neglected, domain-specific implications: As the AI system aimed to reduce forecast error and thus overproduction, forecasting numbers decreased over time, and thereby also sales. We conclude that minimal forecast errors do not inevitably yield optimal business outcomes when detrimental human factors in decision-making are ignored.",CS,AI_ML,0.85,Extracted from log - paper 1525
Outside the Bean Belt: A Study on the Suitability and Sustainability of Coffee Plants in Southern California due to Climate Change using GIS,"California has a unique climate that is suitable for growing a variety of agricultural crops. As stated by the California Department of Food and Agriculture, California’s agricultural abundance includes more than 400 commodities, and of these, over a third of the vegetables and three-quarters of the fruits and nuts consumed in the United States, are grown in California. Mark Gaskell, Ph.D., a California Cooperative Extension Farm Advisor, recently began an experiment to grow coffee plants; he believed that due to changing climatic conditions, California farmers could begin to grow coffee plants in their fields. Several farmers agreed to experiment as well and found that the plants did indeed grow. The issue, however, is that farmers would like to know if they can reliably plant coffee on their farms and sustain coffee production for years to come. As a result, this study will utilize the maximum entropy approach, a well-established algorithm for modeling habitat suitability over geographical domains, to measure the impact of future climate change on the sustainability of growing coffee plants in California in the years ahead. Layers of worldwide climate data, that represent 19 bioclimatic variables, will be utilized in this study. WorldClim is an online repository that contains historic, monthly, meteorological data averaged across 30 years (1970–2000) as well as climate data representing possible future conditions based on various climate prediction models. While this study is focusing on the impact of climate change on coffee plants, it is expected the methodological processes developed, and the resultant findings will be applicable to a wide variety of agricultural crops.",CS,AI_ML,0.85,Extracted from log - paper 1526
Identifying Intimacy of Self-Disclosure: A Design Based on Social Penetration Theory and Deep Learning,"Research on the peer-to-peer (P2P) platforms, privacy, and digitalized business environment has overwhelmingly treated the intimacy of self-disclosure as a survey-based, subjective, and cognitive construct. A few studies have conducted topic analysis using objective data, but are still limited by the difficulty of capturing the degree of intimacy, which hinders the development of the transaction antecedents of P2P platforms. Building upon social penetration theory, we propose an innovative approach to identifying the intimacy of self-disclosure using a deep learning algorithm and an expert-compiled intimacy corpus in the context of P2P platforms. Adopting a sample dataset of 10,000 hosts’ self-descriptions in Airbnb, we introduce the computational and verification process of operationalizing the intimacy of self-disclosure. Through an empirical study, we demonstrate the theoretical feasibility of our quantification method of intimacy and show the potential of using deep learning to measure self-disclosure, expanding the theoretical development of social penetration theory and self-disclosure.",CS,AI_ML,0.85,Extracted from log - paper 1527
Skills of IT Graduates and Cross-border Mobility: Experiences from Norway and Poland,"The dynamic nature of the ICT field makes it extremely difficult and challenging for educators to determine what skills and technologies are continuously pertinent and in-demand. Therefore, a frequent evaluation of critical skills supply and demand is necessary for education programs to stay relevant and effectively teach state-of-the-art skills. The main aim of our ongoing research project is to analyze how the current educational curricular guidelines address the requirements for competences of modern ICT workplaces in Poland and Norway, and thus, prepare the academia to adequately respond to the challenges imposed by the industry. The goal is to produce a general framework and models for future curriculum guidelines, and course planning and design, as a step towards workforce development that possess skills relevant for the future challenges of the modern ICT workplace environments.",CS,AI_ML,0.85,Extracted from log - paper 1528
Enhancing Citizen Engagement: Experiences from a Virtual Reality Workshop,"In e-Governance, information and communication technologies such as virtual reality are used to aid communication between government and citizens, and engage citizens in government policies and strategies. The aim of this study is to explore virtual reality as a tool to engage citizens in social issues such as mobility, climate change and sustainable urban development. A study was conducted in a workshop setting, where virtual reality displays were utilized to engage users. The users rated their experiences on a 7-point Likert scale, and the results indicated that perceived usability, intention to use, enjoyment, and novelty positively influenced citizen engagement. Further, it was revealed that citizen engagement had a positive impact on self-efficacy and citizens’ interest in the topics. Our contributions include theoretical and practical implications, along with a preliminary relationship model. Overall, the findings suggest that virtual reality displays have the potential to be an effective tool for engaging citizens.",CS,AI_ML,0.85,Extracted from log - paper 1529
Digital Divide in the Public Sector in Panama,"Digital transformation has taken over the world post pandemic, and the digital divide has increased just as much.  In this paper, this subject will be looked at from the perspective of the gap that continues to grow between people that are able to adapt and have access to new technologies and those that do not.   The lack of scholarly research on the subject in Panama and the need to gain a deeper understanding of the issue so eventually new solutions start to emerge, is the research gap this paper aims at.  Panama is a small country and one of the most technologically advanced in the Latin American region whose public sector has undergone massive digital transformation during the pandemic, thus the focus of the research. The method used, is an explorative and qualitative case study, that looks at the perceptions of the participants, to get their insight and gain more understanding.",CS,AI_ML,0.85,Extracted from log - paper 1530
Citizen Participation Level in Smart Governance: A Literature Review,"This literature review used and expanded Cardullo & Kitchin's (2018) “scaffold of smart citizen participation” to classify the participation level found in the smart governance literature. This review highlights the importance of the use of a common metric for measuring citizens level of participation in smart governance. The departure point of this work is the literature review developed by Tomor et al. (2019). Their review was updated and compared. We confirmed some challenges positioned by the literature on the indiscriminate use of citizen participation as both an intrinsic characteristic of governance and as an outcome of the collaboration (Meijer & Bolívar, 2015; Tomor et al., 2019). This lack of a common understanding of the level of citizen autonomy creates a confusion in the literature regarding the actual level of participation, which highlights a need for further research about citizen participation and collaboration in smart governance.",CS,AI_ML,0.85,Extracted from log - paper 1531
The NYC311 App & Community Engagement in Coproducing Municipal Services,"In the public sector, governments and the people they serve increasingly collaborate to coproduce public services. To support the coproduction of municipal services, specifically, local governments have incorporated various digital technologies into their information systems. How do digital technologies affect community residents' engagement in coproducing municipal services? Our analysis of the service requests submitted to New York City's municipal 311 system shows that both the introduction and use of the NYC311 app were associated with increased request volume in every community, but the app's positive effect on community engagement was weaker for minority communities, especially those with high percentage of black populations at median-low to median-high income levels. These findings help bridge public administration and technology perspectives on community engagement by showing how ethnoracial and socio-economic factors moderate the effectiveness of digital technology. Accordingly, governments are advised to heed these factors when adopting digital technology to support public service coproduction.",CS,AI_ML,0.85,Extracted from log - paper 1532
Household Digital Twin for Disaster Response,"Using simulation of digital twins can help save lives and reduce the impact of disasters on households. By accessing a virtual process and making recommendations to families before a disaster hits, the risk of harm can be minimized. Developing a household digital twin for disaster response can lay the foundation for effective household disaster management. This study proposes a design of household digital twin for disaster response and demonstrates how the platform can be used to simulate hazardous processes before they occur for damage control.",CS,AI_ML,0.85,Extracted from log - paper 1533
Attributes of Continuity in Support Information Systems: A Study of Social Media Listening Experiences,"The increasing recurrence of crises threatens the continuity of services provided by public service organizations (PSOs) to their communities, especially those in need. Beyond contingency plans, PSOs must rely on robust support information systems able to withstand transitions between day-to-day activities and crisis response with as minimal interruptions as possible. Through the analysis of experiences of use of social media analytics as a support system in PSOs, this study proposes a set of attributes that take place while transitioning between non-crisis and crisis states. The sociotechnical nature of transitions calls for a zoomed-out analysis based on shared patterns that could lead to generalization in the future. The results of our study contribute towards understanding how continuity and equilibrium are exercised in volatile and dynamic environments. In addition, our results could help organizations self-assess their social media analytics teams or other support systems within the proposed parameters.",CS,AI_ML,0.85,Extracted from log - paper 1534
E-participation from the Perspective of ICT Adoption in Local Government Units: Drivers and Association with  Sustainability,"This research examines the ICT adoption drivers of e-participation and the association between e participation and sustainability. A theoretical model of e-participation driven by ICT adoption in government units and associated with sustainability was proposed and verified among randomly selected 304 government units in Poland. It was revealed that e-participation is driven by ICT adoption in government units, particularly by thoughtful spending on ICT in government units, employees' ICT skills, managers' mastery, implementation of legal regulations on ICT usage, and front-office system security. Furthermore, a positive association between e-participation and sustainability was confirmed.",CS,AI_ML,0.85,Extracted from log - paper 1535
Fostering the Adoption of Smart E-Government Services in Germany,"Governments worldwide recognize a need to provide services online to their citizens. Increasingly, these services have become sophisticated through the application of advanced technologies. Nevertheless, the adoption of such services often lags behind expectations. Thus, this study proposes and tests an extension of the unified model of electronic government adoption (UMEGA) to the context of smart e-government services in Germany. It was empirically tested using the data of 330 respondents. The adapted model greatly exceeds other studies that applied an adapted UMEGA regarding the explanatory power on attitude, while the study proves that resistance to change in the investigated context unfolds a highly significant impact on behavioral intention compared to other findings. Concluding, the proposed model supports governments in planning smart e-government services and corresponding strategies more holistically by understanding the factors that influence citizens' adoption.",CS,AI_ML,0.85,Extracted from log - paper 1536
Trust in E-Government: An Investigation of the Socio-Technical in Election Systems,"The fallout of the 2020 US presidential election has left a legacy of distrust in election systems in some subsets of the population. It is critical to the continued success of the US democratic system to analyze the mechanics of how the electorate forms a basis of trust that leads to their intention to participate in democracy. If enough people reject the process that supports democracy, this can result in instability and chaos in social systems. This study analyzes these constructs through the lens of the Socio-Technical Model (STM) (Bostrom and Heinen, 1977). This lens facilitated the creation of a new Election Security Model (ESM) and survey instrument. This survey instrument was validated in a pilot study. A subsequent large-scale study of the entire US population was then carried out. The findings showed the STM precursors were strongly correlated to trust. Surprisingly, trust was only weakly correlated to intention to use the system.",CS,AI_ML,0.85,Extracted from log - paper 1537
Personality and E-petition Success: Perspectives of Online Leadership,"Online petitions have been a popular and effective means for the public to voice their opinions and potentially influence society. E-petitioners can be regarded as influencers or leaders who leverage their social impacts beyond the digital world. In the current study, we collected 41,952 e-petitions from change.org and used an advanced text-mining model (Roberta-base model) to detect petitioners' Big Five personality traits. Then, by applying the trait theory and leadership literature, we developed a research model and provided an in-deep understanding of how petitioners' personalities can influence the petition's success. Our findings indicate that while neuroticism turns out to have a positive effect, agreeableness and conscientiousness are significant negative factors that can hinder the success of an online petition.",CS,AI_ML,0.85,Extracted from log - paper 1538
Examining The Moderating Effects of The Digital Divide on The Nexus of E-Government Use And Public Value: Evidence From a Developing Economy,"Continuous investments into the digitalization of public services call for assessments of the success of such initiatives. However, studies on e-government evaluation from citizens’ perspectives are scanty, particularly in developing countries. Despite the negative effects the digital divide has on emerging economies, the impact of multi-dimensional digital divide on the success of e-government systems in such countries has not been adequately examined. Contemporary Internet-based systems characterized by personalization and customization generate varying user experiences. Hence, calls for information systems (IS) success development measures to capture intangible and subjective benefits derived not only from traditional and utilitarian values but also social values to reflect the contemporary complexities of online user interactions. This study investigates e-government success as public value from citizens’ perspective amidst the effects of the digital divide. Data will be collected through a survey of e-government users in Ghana and analyzed with partial least square structural equation modeling (PLS-SEM) approach to evaluate the proposed model.",CS,AI_ML,0.85,Extracted from log - paper 1539
Towards a multi-stakeholder framework for evaluating digital government success,"In this paper, we develop an inclusive framework for identifying the direct and indirect stakeholders in digital government projects. Using a public value lens, we develop a systems view of digital government initiatives. We use an illustrative case study to show how stakeholders, especially indirect stakeholders, can exert influence on digital government projects, including what projects are perceived as legitimate, what factors are considered as important for success, and what stakeholders are involved in post-implementation evaluation. We also review several lenses commonly used for evaluation of digital government projects and illustrate how they fit into our framework.",CS,AI_ML,0.85,Extracted from log - paper 1540
Is COVID-19 a Driver for e-Participation? Insights from Participatory Budgeting in Poland,"This paper explores how COVID-19 has impacted e-participation, i.e., participation by electronic means, in public decision-making, specifically in certain forms of local community budgeting. We expound the concept of e-participation and its sub-concepts and investigate these as applicable to participatory budgeting. 34 managers in five City Halls in Poland were interviewed on their views and experiences with moving public interactions on-line during the COVID-19 pandemic, particularly as these interactions relate to participatory budgeting. The findings indicate that COVID-19 has indeed accelerated the digitalization of the participatory budgeting procedures, and to some extent may have increased community participation in general.",CS,AI_ML,0.85,Extracted from log - paper 1541
Pro-innovation Behavioral Profile of the organization in e-government managers’ view in Poland,"This research analyzes the Pro-innovation Behavior Profile of government organizations in Poland from the perspective of e-government managers during the COVID-19 Pandemic in Poland, a post-transformation economy. We adopted the Pro-innovation Behavioral Profile of Organization Questionnaire from Barbosa et al. (2018) for data collection via a random online survey of 61 e-government managers. They worked at certification organizations representing the Certification Authority - providing e-services, verifying entities' identities (websites, email addresses, companies, or individuals), and binding them to cryptographic keys by issuing electronic digital certificates. E-government managers perceive their organization as supporting creativity, structure, and environmental factors that lead to innovation. Although leaders motivate pro-innovation behavior in employees, the organization's culture could be more innovative. The main contribution of our study is the first adaptation and use of the PIB questionnaire to collect data from e-government managers in Poland, a post-transformation economy. The results can support e-government managers and political leaders in shaping the Pro-innovation Behavioral Profile of government organizations.",CS,AI_ML,0.85,Extracted from log - paper 1542
Longitudinal analysis assessing swift guanxi on live streaming shopping,"Given the cultural uniqueness of China, swift guanxi has been demonstrated as a substantial factor in directing purchase intention in social commerce. Although many studies examined swift guanxi in live-streaming shopping, the majority of the studies were conducted using a cross-sectional design, suffering from shortages in which no understanding of the long-term causal effect of swift guanxi and its antecedents. The purpose of this study is to examine how swift guanxi has a continuous long-term effect on purchase intention. In addition, how streamers' characteristics (i.e. attractiveness, expertise) and social interaction (i.e., WOM) affect swift guanxi over a period of time. A conceptual model is developed and empirically tested based on four waves of a longitudinal survey of viewers who have repeated watching experiences for the same live-streaming shopping program. This study contributes to increasing the validity of the research model concerning swift guanxi in the context of live-streaming shopping using th",CS,AI_ML,0.85,Extracted from log - paper 1543
Improving Communication at a University: an Action Research,"In the paper, the proposal for a system improving communication at universities is presented. First, problems concerning the use of multiple communication channels are identified. Second, examples of systems for improving communication are discussed, and their properties are analysed. Next, factors of effective communication, especially at universities, are examined. Then, a system for improving content management by intelligent messages targeting to interested parties only is proposed. Finally, a SWOT analysis of the system is carried out, and further actions leading to the implementation of the proposed solution and its evaluation are indicated.",CS,AI_ML,0.85,Extracted from log - paper 1544
e-Justice for Socioeconomic Development,"e-Justice, also called cyberjustice, refers to the incorporation of information and communication technologies (ICT) into the justice system. This includes offering court services via electronic means as well as other digital services for dispute resolution purposes. The objective of this study is to explore the potential contribution of e-justice as one of the components of e-democracy to socioeconomic development. Based on the analysis of the examples of e-justice initiatives, an initial, conceptual framework that links socioeconomic development with e-justice is constructed.",CS,AI_ML,0.85,Extracted from log - paper 1545
Information and Communication Technology and Human Emancipation Among Vulnerable Groups,"Skilled human capital is crucial for information and communication technology (ICT) progress, yet many countries lag far behind in this demand for digital skills. In low- and middle-income countries (LMIC), inclusive technological education faces enormous challenges, such as a lack of digital infrastructure, socioeconomic instability, and low quality of formal education. Drawing upon Freire's tenets on emancipatory pedagogy and the components of human emancipation in IS literature, we investigate how local actors can promote human emancipation through ICT programs focusing on vulnerable groups. We adopted an interpretive case study associated with a Brazilian ICT program focused on human values. Our findings show that emancipatory pedagogy facilitates the understanding of human need, and the expansion of autonomy in life. However, contradictions and tensions in social impact initiatives hinder sustainable and equitable solutions. In that sense, technology is a tool that changes the world, but it must be reoriented in the right direction: solving social needs.",CS,AI_ML,0.85,Extracted from log - paper 1546
Digital Transformation of Public Sector Manual Procurement and Socioeconomic Development: A Developing Country Case Study,"Digital transformation in the public sector offers opportunities for developing countries to address socioeconomic development challenges related to citizen wellbeing. While countries in the developed world digitally transform their public sector procurement for socioeconomic development, many in the developing world are yet to do so. Existing information systems research on digital procurement in developing countries has focused more on adoption, strategy, and design. Little is known about how manual procurement practices are digitally transformed in the public sector of developing countries for socioeconomic development. This study employs interpretive case study as a methodology and activity theory as analytical lens to investigate how manual procurement activities in the public sector get digitally transformed and its effect on socioeconomic development in a developing country context. The findings have implications for research, practice, and policy.",CS,AI_ML,0.85,Extracted from log - paper 1547
Bridging Healthcare Barriers with mHealth: A Systematic Review of Tuberculosis Applications and their Limitations,"Tuberculosis remains one of the most significant burdens for resource-constrained environments with limited healthcare supply, such as the Global South. However, factors such as the adoption of mobile phones create a favorable environment for mobile health technology, yielding a promising avenue for the development of the Global South through digital innovation. Previous studies, however, have not provided a comprehensive assessment of available tuberculosis applications and their features. To bridge this gap, we conducted a systematic review of existing tuberculosis applications to gain a deeper understanding of the current landscape. Our findings indicate that current tuberculosis apps have significant limitations, such as requiring an internet connection or being available in only one language. Our study contributes to extant research by presenting the first systematic analysis of tuberculosis applications and provides practical insights by highlighting areas of improvement that should be addressed in future app development.",CS,AI_ML,0.85,Extracted from log - paper 1548
Linking United Nations Sustainable Development Goals with the Impact of ICT4D Research – A Theory of Change Approach,"Information and communication technology for development (ICT4D) research are increasingly impactful in the field of IS, as they aspire to deliver developmental benefits across the different domains of application. Unfortunately, many ICT4D projects often fail to demonstrate in a quantifiable way how they have contributed to development. The United Nations mechanisms for global development, currently the Sustainable Development Goals (SDGs), offers a quantifiable basis for assessing developmental goals attainment in general, and has been adopted for some ICT4D projects.. However, there is a knowledge gap regarding how SDGs can be achieved through ICT4D initiatives at the local levels, especially when indicators for SDGs are all at a global level. In this research, we propose a Theory of Change (ToC) approach to link ICT4D project outcomes to specific SDG targets through a set of clearly articulated change pathways. We demonstrate how project-level indicators can then be developed to measure the ICT4D project’s impact on SDGs. An example of the approach is provided in the context of the Techies Without Borders project called 'Continues Medical Education Solutions (CMES). The case study shows how ToC is used to explicitly create project-level indicators that are linked to a specific SDG target. This research makes an important contribution by providing guidance to IS researchers seeking to align ICT4D projects with UN SDGs.",CS,AI_ML,0.85,Extracted from log - paper 1549
Blockchain technology perception regarding supporting the digital transformation of Supply Chain Management,"The globalization of the world economy increases the role of value chains, where disruptions in the realization of the logistics processes occur repeatedly. Climate changes, pandemics, and military conflicts further increase the difficulties in matching supply and demand in a vast majority of networks. As a result, the digital transformation of Supply Chain Management (SCM) often referred to as Digital Supply Chain (DSC) is considered an important solution for several SCM concerns. Extensive subject matter literature research pointed out that several technologies have been studied and implemented as DSC solutions contrary to blockchain technology. Therefore, the research aims of this article is to examine the challenges of DSC with particular emphasis on blockchain technology perception in this context. Research has been conducted via structured in-depth interviews with SCM experts. Study results highlighted that blockchain is perceived by SCM experts as a technology with significant capability to support the creation of DSC, but unfortunately used at all. They also pointed out that blockchain should not be the technology of the first choice when initiating the digitization of SCM processes, but potentially beneficial for mature DSC.",CS,AI_ML,0.85,Extracted from log - paper 1550
Smart Mobility Meets Industry: Enhancing Energy Flexibility Potentials by Combining Industrial Production & Electric Vehicle Charging,"The increasing share of renewable energy sources poses the challenge of volatile energy generation, requiring demand side management (DSM) to manage such volatility. In view of the high industrial electricity demand and the increasing charging demand of electric vehicles (EVs), both industry and mobility represent relevant areas of DSM. However, the combination of EV charging and energy-intensive industrial processes still contains untapped synergy potentials. Thus, we present a mixed-integer linear program and quantify the economic and ecologic potential of combining energy flexibilities of industry and electric mobility within a case study. For our evaluation, we compare the results of our model to a benchmark case that separately manages industry and EV flexibilities. Our findings suggest that implementing our approach would yield both economic and ecological benefits, resulting in a reduction in anticipated costs and emissions, as well as a decrease in associated uncertainty.",CS,AI_ML,0.85,Extracted from log - paper 1551
Scalable Sustainability Monitoring of Financial Reports: A Design Science Artifact,"The United Nations Climate Change (UNCC) report in 2022 warns that the likelihood of global warming exceeding 1.5°C by 2026 has surpassed 50%. Climate change is a global problem that requires collective action, and companies have a significant role to play in mitigating climate change. Companies have already started to incorporate climate change risk and liabilities in their annual reports. Monitoring a company's progress in transitioning to sustainability can help stakeholders make informed decisions. However, existing approaches for sustainability monitoring of annual reports are limited in scalability, mainly because of the manual steps involved. Therefore, we designed and evaluated a more scalable artifact using state-of-the-art natural language processing (NLP) techniques to monitor companies' sustainability targets based on their annual reports. This work presents a prototype that improves upon the current state of practice and contributes to the body of knowledge by outlining key design decisions.",CS,AI_ML,0.85,Extracted from log - paper 1552
The Trash is Always Greener on the Other Side: A Life Cycle Assessment of IOT Implementation,"The 2030 Agenda has pushed practitioners as well as academia to renew their efforts on promoting sustainability, e.g. in how digital technologies can support cities to improve their environmental performance. However, as scholars focus their attention on the positive outcomes of implementation, they often neglect the environmental impact of the artefact itself. We present a study of a Green IS implementation – a municipal Internet of Things (IoT) solution which was expected to decrease the carbon emissions produced by urban waste collection in Sweden. Using a mixed methods approach, we present qualitative findings from interviews & project meetings as well as quantitative findings from a Life Cycle Assessment (LCA). We find that (1) the environmental impact of the connected litter-bins – however small – is not necessarily offset by any significant benefits, and (2) the most significant way for the stakeholders to reduce environmental impact is to utilize more ecologically friendly trash bags",CS,AI_ML,0.85,Extracted from log - paper 1553
The Effects of Digital Nudging on User Satisfaction in Online Customization,"As an emerging technological means to influence user, digital nudging is being used by more and more custom manufacturers. This research explores the influence of digital nudging design in online customization system on user satisfaction. I propose that for novices, the adoption of digital nudging can reduce the design effort and increase their satisfaction; for experts, the adoption of digital nudging will reduce users’ feeling of accomplishment and reduce satisfaction. In addition, the influence of overt digital nudging (ODN) is stronger than that of covert digital nudging (CDN). We designed 3 experiments to investigate them. This research can extend the understanding of the study on user customization satisfaction and the effect of digital nudging and provide guidance for manufacturers to optimize the design of customization systems.",CS,AI_ML,0.85,Extracted from log - paper 1554
Barriers Driving Nonuse of Online Medical Records: Latent Class Analysis of Chronic Patients,"Online Medical Records (OMR) platforms can provide benefits to chronic disease patients. Yet, OMR use among them is suboptimal. The study identifies clusters among nonusers of OMR among chronic patients. The Health Information National Trends Survey (HINTS) iteration 5, Cycle 3 data were used to analyze 1071 respondents. Latent Class Analysis was run on the six reasons for nonuse (no record, speaking directly, privacy or security of the website, no Internet, login issues, and no need to access) and resulted in 3 clusters. About 19% subjects expressed multiple reasons and 69% just one strong reason. Demographic and clinical attributes were partially associated. For electronic wearable/tracking device use or electronic communication, differences among clusters were noted; persistent resisters showed lower propensities to use. Interventions to improve patient use of Internet-based health technologies should be customized and help produce patient-generated data facilitating healthcare decision-making.",CS,AI_ML,0.85,Extracted from log - paper 1555
Instademic: Deriving spatiotemporal contact networks from tagged social media images,"Identification of persons who have come in contact with COVID-19 and future viruses is necessary for early identification of potential carriers to improve public health. A Spatiotemporal Colocation Network (SCN) represents entities in terms of being present at the same location within a specified temporal window. A key factor of image sharing systems is user annotation, or tagging. By generating the network of collocated entities in image files across a temporal window we can map expected viral diffusion and identify potential carriers. Actual absolute location of entities is not important here. In colocation analysis for virus carrier detection “relative colocation” is important. i.e. we want to know that X was collocated with Y at time t, and Y was collocated with Z at time t’ which will enable us to build a “contact network”. Our proposed method uses tagging meta-data and does not require sharing of actual images.",CS,AI_ML,0.85,Extracted from log - paper 1556
From Seeker to Provider: User Ability and Role Switching in Online Health Communities,"Online health communities offer a platform for patients to seek support from other users with similar conditions. As those support seekers’ situation improves, they may ultimately want to give back to the community and provide support to others. This study explores how seekers’ ability to provide support, operationalized using their cognitive state and social network structure, can influence the seekers’ propensity to switch roles and become support providers. We leverage the Dual Perspective Model of Agency and Communion and Social Networks literature to theorize a seeker’s ability to provide support. We apply survival analysis to study seekers’ first role switch to a provider and the duration from seekers’ initial post to role switch. We find that while seekers who have higher positive agency and closeness centrality are more likely to switch roles to a provider, those having higher positive communality, degree centrality, and eigenvector centrality are less likely to switch.",CS,AI_ML,0.85,Extracted from log - paper 1557
Content Quality in Online Q&A Communities: An Approach for Measuring Content Quality,"Research into online support communities is becoming more important as researchers strive to understand the dynamics behind these communities and the interactions of their participants. Since these communities are largely Q&A platforms where seekers request information or support, the quality of responses is increasingly relevant in most studies, yet this is a difficult variable to measure. We propose a method for measuring answer quality by using text analytics to determine the topics that are addressed in the questions and then determining the quality of responses based on whether the topics in the answer match the topics in the question. This research discusses how to generate this type of metric and demonstrates its validity using a unique data set from multiple mental health communities in China.",CS,AI_ML,0.85,Extracted from log - paper 1558
"Policy Change, Social and Cultural Capital and Medical Crowdfunding Use: A Quasi-Natural Experiment","Medical crowdfunding is an emerging means for patients to seek financial support. This study investigates the impact of health policy, specifically the Individual Mandate Repeal (IMR), on the use of medical crowdfunding. We employ a difference-in-differences research design and find that IMR has led to an increase in medical crowdfunding usage, with significant heterogeneity among users. Specifically, individuals with high cultural capital and bridging social capital, but low bonding social capital, are more likely to increase their use of medical crowdfunding. These results suggest that the socio-technology for healthcare financing purpose (i.e., online medical crowdfunding) responds to the gap of the Affordable Care Act (i.e., IMR), and its usage is shaped by individual differences in cultural and social capital. The policy implication is that medical crowdfunding platform provides opportunities to take on the shortcomings of health policy, but the opportunity exploration or its usages are disparate across populations with varying levels of capital. Public health policymakers may develop policies fostering bridging social capital to expand the healthcare affordable opportunities for underserved populations without imposing taxes and mandate regulations.",CS,AI_ML,0.85,Extracted from log - paper 1559
Social Media Analysis of Modifiable Lifestyle Factors in Multiple Sclerosis,"Around 1 million people in the USA and 2.8 million people around the globe are living with Multiple Sclerosis (MS), which currently has no cure. Incomplete recovery, long-term disability, worsening of symptoms leads to disruption in the Quality of Life among MS patients. However, modifiable lifestyle factors (MLFs) have demonstrated the potential to improve health and wellbeing in MS patients. Therefore, by leveraging social media analytics, this study aims to understand the public perspective on MLFs as reflected in the online generated information. The study identified topics pertaining to MLFs and evaluated the relative prevalence of these topics in social media discourse. The results and analysis showed that Diet and Temperature are the two main factors for MS patients to constantly maintain and monitor. Further, this study can help the medical community in recommending the best lifestyle measures that is necessary for an MS patient.",CS,AI_ML,0.85,Extracted from log - paper 1560
Towards A Unified Utilitarian Ethics Framework for Healthcare Artificial Intelligence,"Artificial Intelligence (AI) aims to elevate healthcare to the pinnacle by aiding clinical decision support. Overcoming the challenges related to designing ethical AI will enable clinicians, physicians, healthcare professionals, and other stakeholders to use and trust AI in healthcare settings. This study attempts to identify the major ethical principles influencing the utility performance of AI at different technological levels such as data access, algorithms, and systems through a thematic analysis. We observed justice, privacy, bias, lack of regulations, risks, and interpretability are the most important principles for ethical AI. This data-driven study has analyzed secondary survey data from the Pew Research Center (2020) of 36 AI experts to categorize the top ethical principles of AI design. In order to incorporate the resolution to the ethical issues identified by the meta-analysis and domain experts, we propose a new utilitarian ethics-based theoretical framework for designing ethical AI in the healthcare domain.",CS,AI_ML,0.85,Extracted from log - paper 1561
Cross-cultural Analysis of Online Patient Reviews from the Caregiver Perspective,"Online reviews have had an undeniable impact in the field of healthcare. Despite the growing trend of online patient reviews, physician attitudes towards them vary greatly and their effect on physician behavior is uncertain. We propose a study exploring how physicians perceive online reviews through the lens of social-psychological theory and propose a framework for examining the phenomenon of online patient review valuation from the caregiver perspective.",CS,AI_ML,0.85,Extracted from log - paper 1562
How the Uniqueness of Initial Posts Affect User Participation in Online Cancer Communities,"Online cancer communities (OCCs) are crucial resources for cancer patients, providing support for the complex and challenging aspects of the disease. Patients often face unique problems, and OCCs offer a place to seek support when friends and family may not understand. This study explores how unique topics discussed in OCCs impact user participation in a thread, measured by the number of replies, repliers, length of replies, and response speed. A deep-learning-based natural language processing topic model is used to calculate the uniqueness scores of posts in OCCs, based on the prevalence of each topic. The study has implications for OCC administrators, researchers, and physicians in terms of understanding and addressing unique health topics expressed in online communities.",CS,AI_ML,0.85,Extracted from log - paper 1563
The Influence of Health Data on the Use of the Electronic Health Record (EHR) – a Mixed Methods Approach,"The use of electronic health records (EHRs) offers many benefits, but users are also critical of them because of concerns about privacy and the sensitivity of health data. Given the ambiguity of data sensitivity, we examine which characteristics of health data influence users' upload behavior, and which are perceived as sensitive. A semi-structured interview (N=30) was conducted to locate characteristics of health data, which influence the user's upload behavior. These characteristics were then empirically evaluated in a user study (N=50). The complexity of diseases and their stigmatization potential are characteristics which influence the uploading behavior. We verify empirically that the uploading of health data is rejected when there is a high potential for stigmatization, while it is accepted when the complexity of the disease increases. Health data are considered by users to have different sensitivities, relative to stigmatization potential. Privacy Calculus regarding EHRs should consider these characteristics of health data.",CS,AI_ML,0.85,Extracted from log - paper 1564
"Technological, Human, and Procedural Factors that Influence Nursing Documentation Errors","Over the years, there has been a drastic increase in awareness of medical mistakes by researchers, hospitals, insurance companies, etc. The rise in mistakes has occurred for several reasons. One reason is that hospitals and other healthcare facilities spend millions incorporating information systems and technology into healthcare, which is thought to help reduce the number of mistakes. Even with the significant improvements technology has brought, mistakes are made every day, costing the United States healthcare system billions and patients’ lives. This study explores technological, human, and procedural factors that influence nursing documentation errors, resulting in negligent medical mistakes. There is no easy solution to reducing the number of medical mistakes, but improving documentation in Electronic Medical Records (EMR) systems is a start.",CS,AI_ML,0.85,Extracted from log - paper 1565
Towards Technology of Global Governance – A Novel Approach to Health System Performance Assessment by MCDA,"Healthcare systems must constantly evolve to meet the growing demands arising from the health needs of populations conditioned by an aging society, multimorbidity, and infectious diseases. In recent years, the Covid-19 pandemic has forced healthcare systems worldwide to increase efforts and implement new solutions. Assessment of coping with the Covid-19 pandemic requires appropriate indicators and methods that consider both multiple criteria with different objectives and the dynamics of performance in successive years. Therefore, our paper presents the developed multi-criteria model for evaluating the healthcare system in terms of coping with the Covid-19 pandemic, which includes nine indicators belonging to five main dimensions. Next, we introduce a novel multi-criteria temporal method called DARIA-TOPSIS. The research results showed that among the countries that coped best with the pandemic concerning the selected criteria are Scandinavian countries such as Denmark, Norway, and Finland and Western European countries such as Luxembourg and Ireland.",CS,AI_ML,0.85,Extracted from log - paper 1566
Mechanical ventilation settings advisory system Odyn,"COVID-19 pandemic impaired function of healthcare systems globally by increased workload at intensive care units in hospitals that underwent a massive influx of patients in critical condition in need of tailored and specialized care. Prevalent respiratory failure increased the demand for ventilators and respiratory therapists. Due to personnel shortages, doctors and nurses, not trained in the field of intensive care, were expected to provide adequate care for patients, supporting themselves with help of consulting specialists and navigating equipment shortages. To deal with this problem we present the research result, conducted in cooperation between computer scientists and anesthesiologists, namely the advisory system that supports physicians who do not have expertise in operating ventilators for typical COVID-19 patients. The system has been implemented at two hospitals. We present the system architecture and some internals of its recommendation modules based on decision graphs and recurrent neural networks. Further research will embrace analysis and optimization of the modules to provide yet more accurate support, as early as possible and in a broader spectrum of patient’s conditions.",CS,AI_ML,0.85,Extracted from log - paper 1567
A Suggested Blockchain Architecture for Healthcare Data Sharing,"In today’s knowledge economy, healthcare generates massive amounts of highly sensitive data ranging from genomic data to medical records that are central for clinical knowledge discovery and decision making. However, storage and sharing of these health data has led to several challenges most notably around ownership, privacy, data breaches and data theft. Intelligent digital transformation of healthcare is required to overcome these challenges by storing and sharing such health data responsibly and securely. In recent years, blockchain, particularly private blockchain, has been explored as a technology to address some of these issues in various industries. Blockchain developments in the healthcare sector are still very nascent. To address this key void, we proffer a generic private blockchain platform for healthcare to address the growing data challenges. We adopt the design science research methodology in our work. In this paper, we present our proffered model and the proposed blockchain architecture.",CS,AI_ML,0.85,Extracted from log - paper 1568
Enhancing Information Systems Success Outcomes with Electronic Health Records Systems Post-Adoption Implementation,"Despite the monetary incentives and the meaningful use mandate to implement electronic health record (EHR) systems in the workplace, users still feel threatened due to the mixed outcomes of using EHR systems. This current study examines success outcomes that enhance users’ EHR adoption in an organizational setting through the lens of information systems success model. A theoretical framework is proposed to understand what EHR implementation success means to healthcare professionals, and what systems and user characteristics contribute to EHR implementation success. The proposed model suggests that user capability, EHR usability, and EHR usefulness will affect EHR success outcomes (perceived satisfaction with usage and perceived helpfulness over usage process). The study considers the interaction between technology and user capabilities to impact IS success outcomes and discusses some implications to theory and management.",CS,AI_ML,0.85,Extracted from log - paper 1569
Enabling Innovation in Digital Hospital Ecosystems - A Literature Review,"Healthcare information technology holds great potential for improving services in hospital environments. This transformation is fueled by the fact that current extraordinary situations, such as the COVID-19 pandemic, indicate that the realization of digital health innovation is essential to delivering most healthcare services for patients. To facilitate an environment that allows digital innovation and integration of internal and external stakeholders a movement toward a digital hospital ecosystem is needed. We conducted a structured literature review to identify the current state of knowledge about enabling mechanisms that facilitate the development of a digital hospital ecosystem. Therefore, we draw on the current literature on digital infrastructures and platform ecosystems. We identified the challenges hospitals face during their transformation and address them by deriving key mechanisms of the digital hospital ecosystem. Thus, this study contributes to research by presenting insights into the transformation toward a digital hospital ecosystem.",CS,AI_ML,0.85,Extracted from log - paper 1570
What Factors Affect the Elderly’s Adoption of Digital Technologies for Daily Life and Health Management: A UTAUT Study,"In 2021, 30% (97.6 million) of the U.S. population were 55 years or older, and this number will keep growing. This segment of the U.S. population has been hit the hardest by the COVID-19 pandemic which has led to an increase in their social and physical isolation. One way that people have coped with such isolation is to rely on digital technologies to connect with others and maintain daily life activities, and even manage their chronic health conditions. However, little research has focused on how the elderly use digital technologies in their daily life and managing their chronic health issues. The present study utilizes the Unified Theory of Acceptance and Use of Technology (UTAUT) framework to examine which factors affect the acceptance of digital technology and chronic health management application by seniors in the U.S.",CS,AI_ML,0.85,Extracted from log - paper 1571
Adoption of Blockchain to Address Healthcare Financing System Challenges: A Systematic Review,"The healthcare industry is currently going through a digital transformation, and the implementation of blockchain technology has emerged as a promising solution to overcome many of the difficulties faced by healthcare financing systems. This research paper presents a systematic review of the existing literature to recognize the issues that healthcare financing systems are currently facing and how they can be addressed through blockchain solutions. The review also identifies potential solutions and frameworks proposed for the implementation of blockchain solutions. The study has revealed that the combination of blockchain technology and machine learning has high potential in developing a smooth healthcare financing system that is secure, accessible, and has data integrity. The use of blockchain can provide a secure and transparent platform for storing and sharing healthcare data, leading to improved accuracy and efficiency of healthcare financing systems. By integrating blockchain 3.0 with healthcare 4.0 processes and technologies, a real-time and reliable blockchain-based healthcare system for financing can be developed that is highly secure, transparent, and can execute transactions on any IoT-enabled device. The study has revealed that the Ethereum blockchain network is the most suitable platform to implement a distributed ledger to address the challenges faced by healthcare financing systems. This is due to the various applications of the platform already being used within remote medical healthcare operations and other healthcare processes as found in the literature. The study's findings are expected to contribute to the ongoing discussion on the digital transformation of the healthcare sector through emerging technologies.",CS,AI_ML,0.85,Extracted from log - paper 1572
The Longitudinal Financial Impact of Health Information Technology Investments in Hospitals: A Panel Data Analysis,"Investments in health information technology (HIT) are known to improve operational and financial outcomes in hospitals. However, it is less understood whether this effect is short-term, medium-term, or long-term. This paper investigates the effect of HIT investments on hospitals’ cost-to-charge ratio, a financial metric that accounts for hospital costs and revenues, at different time lags following the initial investment. Using panel data on U.S. hospitals from 2010 to 2021, we report that the impact of HIT on hospital cost-to-charge ratio is realized with a lag of 0 to 4 years, when controlled for hospital differences such as rural vs urban location, public vs private ownership, proportion of uncompensated care, and year-over-year variations. This effect becomes non-significant in subsequent years as the effect of HIT wears out. We also quantify the returns from HIT investment. Every 1% increase in HIT investment results in a reduction of 3.3 to 6.0% in cost-to-charge ratio between 0 and 4 years after the HIT investment. Implications of these findings for research and practice are described.",CS,AI_ML,0.85,Extracted from log - paper 1573
A Qualitative Study on Aging Adults' Opinions and Perceptions about Using Companion Robots in Daily Activities,"In the 21st century, Agetech (age technology) and tech devices can be integral to healthcare systems in increasing patient engagement, maintaining independence, and helping seniors live happier and healthier lives. Studying how seniors use and interact with technology can help researchers, technology designers, and vendors understand the possible benefits and risks of using technology products designed to meet older adults’ needs. This study attempts to use a qualitative approach to examine the elderly opinions about using intuition robotics (i.e., companion robots). The findings indicate a positive attitude toward using this Agetech due to health and wellness, companionship and support, and technology design benefits. However, participants also raise concerns about digital dependency and social disconnection concerns, information integrity and online resilience risks, and implementation costs. Through a model, we suggest a need for more education and awareness about robotics's potential gains and concerns.",CS,AI_ML,0.85,Extracted from log - paper 1574
Would You Accept Doctor ChatGPT: An Empirical Study Based on the UTAUT Model,"Since its introduction, ChatGPT has generated significant interest in many areas in just a few months, including healthcare. Recently, researchers have claimed that it has passed the U.S. medical licensure examination (Kung et. al, 2022). However, few empirical studies have investigated whether ChatGPT would be valued and accepted by public in the healthcare context. To understand the public's willingness to accept Artificial Intelligence-Generated Content (AIGC) applications like ChatGPT in the healthcare sector, this study proposes a model of factors affecting the user acceptance of ChatGPT for healthcare purposes integrating the Unified Theory of Acceptance and Use of Technology (UTAUT) with the Trust Theory, the Perceived Risk Theory, and the Perceived Illness Theory. We will analyze the data collected from questionnaires using Structural Equation Modeling (SEM). The findings of this study will provide insights into the factors affecting the user acceptance of ChatGPT for healthcare services.",CS,AI_ML,0.85,Extracted from log - paper 1575
The Heterogeneous Effects of Remote Patient Monitoring on Patients with Chronic Heart Conditions,"While prior research has provided evidence that, on average, Remote Patient Monitoring (RPM) use has a positive impact on patient outcomes, very few have examined the heterogeneous effects of RPM, and none have looked at how different hospital and county configurations fit different types of patients with chronic heart conditions. Using Resource Orchestration as our theoretical framework, we will leverage causal machine learning to find patterns of heterogeneity on the impact of RPM and describe how different combinations of hospital and county configurations facilitate RPM use for each type of patient with chronic heart conditions. We expect to find differences in treatment effects across patients with different ages, socioeconomic statuses, and payment sources. Hence, our research will contribute to the literature by providing insights that can help health care providers craft strategies to facilitate RPM use for different types of patients with chronic heart conditions.",CS,AI_ML,0.85,Extracted from log - paper 1576
"Studying the Relationship Between Timely and Effective Care, Preventive Care, and Patient Hospital Recommendation","The problem of managing timely and effective care within emergency departments (ED) and preventive care in hospital facilities are often associated with patients’ overall satisfaction. Hence, the objective of this study is to determine the relationship between independent variables such as average ED wait time, percentage of left without being seen (LWBS), ED stay time, hospital overall rating, percentage of sepsis care, and percentage of patients’ response to hospital recommendation as a dependent variable. Given the objective, the study performed a linear regression analysis. The results of the study determined that patients’ willingness to recommend a hospital was significantly related to the average time a patient spent in ED (p < 0.01), sepsis care (p < 0.026), left without being seen percentage in ED (p < 0.001), and hospital overall rating (p < 0.001). Our findings suggest that variables related to ED throughput, and preventive care have a significant relationship with patient’s willingness to recommend / not recommend the hospital.",CS,AI_ML,0.85,Extracted from log - paper 1577
Mapping the Emotional Journey of STD patients in Online Forums,"People inflicted with diseases such as STDs experience a tremendous number of varying emotions from the onset of symptoms to being cured/living with it. Leveraging the power of bidirectional encoder representation transformers our study builds a custom-named entity recognition model which identifies uniquely associated emotions at various stages of the disease. Our study uses over 1 million posts derived from online communities regarding STDs. We further provide analytical insights into varying emotions at different stages based on contextual reasonings. Finally, we utilize the resulting insights to produce prescriptive measures for future online recommendation systems on online communities, and for stakeholders hoping to build devices that address problems about stigmatized disorders such as STDs.",CS,AI_ML,0.85,Extracted from log - paper 1578
IT Project Selection and Organisational Agility – An Integrated Conceptual Framework,"Sustained competitive advantage is elusive to organisations, in a volatile contemporary context. Organisational agility (OA), enabled by information technology, offers hope. However, organisations that pursue agility encounter incompatibility with IT project selection (ITPS) – as they select and fund projects, before implementation. Existing ITPS literature provides rich insights into methods, processes and people but is fragmented. Without integration, the incompatibility cannot be resolved. To address this problem, this study conducted a literature review of 159 articles to develop an integrated framework for understanding OA and ITPS. Findings show that methods, processes and people in ITPS can both enable and disable OA. Following conceptual-framework-analysis, ITPS is redefined as an enabler of agility. The resultant three-layered conceptual framework shows that firms can deploy ITPS methods, processes and people that enable sensing, deciding and responding, leading to OA. This study contributes by integrating literature to facilitate further research in this area.",CS,AI_ML,0.85,Extracted from log - paper 1579
A Survey of Technology Selection Approaches in Data Science Projects,"In a world where the amount of generated and stored data is steadily increasing, Data Science (DS) has emerged as an integral discipline for organizations to extract knowledge and value from these resources. Due to the high failure rate in these undertakings, new approaches to support the project management are urgently needed. The use of appropriate technologies constitutes a success factor in DS and its selection process is regarded as a strategic project management activity. As technology selection is not described sufficiently in current DS process models, a structured literature review is performed in this article that concentrates on the addressed technology layers, the applied decision procedures, and the specific technology instances of the identified approaches. The study shows that holistic technology selection approaches that consider all activities of the DS lifecycle and the entire required tool stack are severely lacking and should, therefore, be further researched in the future.",CS,AI_ML,0.85,Extracted from log - paper 1580
Exploring Hybrid Project Management Methodologies and Selection Processes,"Project management methodology is important to the success of a project. However, changes in technology and practitioner approach have led to a mix of methodologies known as hybrid. This research sought to identify the types of project management methodologies practitioners currently use and determine factors that impact how a project manager selects a methodology. A survey questionnaire was used to identify methodologies and types of project management tools, techniques, and processes used. Correlation and chi-square analysis analyzed if three specific factors (industry, project manager experience level, and technology project type) influenced the selection of a predictive, adaptive, or hybrid methodology. The results showed that industry and technology project type are factors in the methodology selection, but project manager years of experience were not. Overall, the analysis supported that there is no “one size fits all” when it comes to methodology selection, but hybrid is emerging as the predominant approach.",CS,AI_ML,0.85,Extracted from log - paper 1581
Agile CRISP-DM for Analytics Projects: Understanding Diabetes Treatment Using fsQCA,"Despite the increasing popularity and demand for analytics projects, the uncertainty in goals and dynamic processes involved in these projects behoove IS researchers to further understand the project management techniques applicable to these projects. In this paper, I illustrate the process of carrying out an analytics project using an agile project management approach. In this project, following the project management concept of ""tailoring"", fsQCA is used to analyze the effective treatment configurations of traditional Chinese medicine and Western medicine for diabetes. This illustrated process could be potentially used in various small-sized analytics projects carried out by IS researchers, and R&D efforts. The practical implications of the analysis for medical providers are also discussed.",CS,AI_ML,0.85,Extracted from log - paper 1582
Balancing Agility and Hierarchical Culture through Leadership Sensemaking,"Many organizations are embracing agile values and principles as a strategic priority. However, improving agility is never an easy task. In their agile journeys, organizations still face significant barriers and challenges, such as culture clashes and lack of leadership. Agile principles and values are especially hampered by hierarchical cultures, which are still common in many companies and organizations. Using a sense-making theoretical lens, this study conducted a case study in two organizations to understand agile leadership roles in achieving a balance between agility and hierarchical cultures. The results show how agile leaders play mediator and navigator roles in vertical and horizontal sense-making processes, respectively, to build psychological safety and an agile mindset at an organizational level.",CS,AI_ML,0.85,Extracted from log - paper 1583
Towards a domain-driven distributed reference architecture for big data systems,"The proliferation of digital devices, rapid development of software and the infrastructure of today, have augmented user’s capability to produce data at an unprecedented rate. The accelerated growth of data could be called the era of big data and forced a paradigm shift in data engineering because the variety, velocity and volume of data overwhelmed existing systems. While companies attempt to extract benefit from big data, success rates are still low. Challenges such as rapid changes in technology, organizational culture, complexity in data engineering, impediments to system development, and a lack of effective big data architectures mean that only an estimated 20% of companies achieved their goals. To this end, this study explores a domain-driven distributed big data reference architecture that addresses issues in data architecture, data engineering, and system development. This reference architecture is empirically grounded and evaluated through deployment in a real-world scenario as an instantiated prototype, solving a problem in practice. The results of the evaluation demonstrate utility and applicability but with architectural trade-offs and challenges.",CS,AI_ML,0.85,Extracted from log - paper 1584
"Exploring Privacy Concerns, Privacy Risks and Benefits on Users’ Trust and Engagement in Continual Use of Service Robots","Service robot applications continue to increase globally which create competitive advantages for organizations and individuals. This study investigates the privacy concerns, privacy risks and perceived benefits on users’ trust and engagement in continual use of service robots, leading to the development of a conceptual framework within the theoretical background of the theory of planned behavior (TPB) and the privacy calculus theory for better exploring users’ trust and engagement in the continual use of service robots that can be tested and validated. From the theoretical side, this study will contribute to the IS body of knowledge on privacy and risk determinants, benefits and technology adoption by identifying factors affecting users’ trust and engagement in the continual use of service robots. On the practical side, this study will provide information into the influence of privacy concerns, privacy risks and perceived benefits on users’ trust and engagement in continual use of service robots, and provides insight for service robot providers to address those issues, and make a sensible decision in the adoption of service robots.",CS,AI_ML,0.85,Extracted from log - paper 1585
Digital Nudging and Transparency: An Experimental Study of Two Types of Recommendation Badges,"This paper investigates the impacts of digital nudging on customer purchase decisions. Digital nudge is an online choice architecture that alters individual’s behavior in a predictable way while preserving all the available options and keeping the same economic incentives. Most recently, academic research started to address the relationship between nudges and Artificial Intelligence/Machine Learning (AI/ML) and found that personalized targeting algorithms influences individuals and collective behaviors in various ways that include undesired consequences for both end-users and firms. Drawing on literature of nudge and anchoring effect, this study proposes two types of nudges based on the transparency level: ambiguous badge (ex., Amazon’s Choice) and specific badge (ex., Best Seller). We further hypothesize that specific badge will manipulate user’s preferences to a less extent than ambiguous badge. This study will contribute to the ethical use of digital nudging in different contexts.",CS,AI_ML,0.85,Extracted from log - paper 1586
Effect of AI Decision Speed on User Adoption in Human-AI Collaboration: The Moderating Role of Historical Decision Quality,"Artificial intelligence (AI) has been widely used in many products and services and has become an important means to assist users in decision-making. In the context of human-AI collaboration, both the quality and speed of AI’s decision-making are the two system features that users can readily identify. However, the existing research focuses on decision-making quality and pays little attention to the effect of AI’s decision-making speed. Drawing from the theory of cue utilization, this research explored the effect of AI decision speed on user’s adoption intention. The results of two experiments show that users have higher AI decision adoption intention at high AI decision speed than at low; the perceived intelligence and perceived risk in decision-making play a mediating role in the above effects. Additionally, historical decision quality moderates the impact of AI decision speed on users’ adoption by weakening the above impact in the high-quality condition. The findings enrich the research on AI adoption and have some practical implications for AI service providers and developers.",CS,AI_ML,0.85,Extracted from log - paper 1587
The Role of AI Assistants in Online Shopping Platforms: Evidence from Livestream Selling,"Livestream technology is transforming consumers’ online shopping experience. This research addresses how livestream selling platforms mitigates the tension between streamers’ constrained service capacity and individual service demands with AI assistants. We conduct a large-scale field experiment wherein the consumers in the treatment group have access to an AI assistant that predicts consumers’ potential needs and provides individualized services, while the consumers in the control group do not have access to such an AI assistant. We find that the introduction of the AI assistant increases sales by 2.61% and reduces product returns by 62.86%. Our models on multiple-stage purchase decision-making reveal that an AI assistant increases the duration of the awareness and consideration stages, improves the probability of placing an order in the evaluation stage and reduces the likelihood of product returns. Further, our analyses reveal that interacting with the AI assistant also reduces consumers’ expressions of affection and positive emotions.",CS,AI_ML,0.85,Extracted from log - paper 1588
The Invisible Hand: Uncovering the Impact of AI Incidents on Human Dignity,"In the last decade, Artificial Intelligence (AI) has undergone significant advancements and breakthroughs. At the same time, AI incidents, defined as events of unexpected, undesirable, or unintended action or outcome resulting from AI-powered systems, may take various forms, such as malfunction, security breaches, and even ethical violations. Hence, it is important to understand how such AI incidents may influence human dignity (i.e., how people perceive their values). Drawing on the dignity literature, this paper seeks to answer the research question: How do AI incidents violate three dimensions of human dignity, namely inherent dignity, meritocratic dignity, and behavioral dignity? We proposed three new constructs of dignity violations, i.e., perceived violation of inherent dignity (PVID), perceived violation of meritocratic dignity (PVMD), perceived violation of behavioral dignity (PVBD). An experiment is to be designed to test how AI incidents may lead to dignity violations. The paper highlights the need for ethical considerations in the development and deployment of AI systems to enhance, rather than inhibit, human dignity. The findings of this paper contribute to the ongoing discourse on the responsible use of AI and its implications for society.",CS,AI_ML,0.85,Extracted from log - paper 1589
It's Time to Smarten Up! - A Framework for Building Smart Service Systems,"Smart service systems are a network of people, technologies, as well as processes and rely on learning, adaptivity, and decision-making to provide smart services to their users. The complexity of designing smart service systems results in a plethora of potential implementation obstacles, often leading to an overly technical focus and resulting in isolated solutions. There is, therefore, a need for a guideline for companies that helps them facilitate the development of complex smart service systems. To this end, we apply action design research used in a multi-year design study for digital twins in the manufacturing industry. The resulting four-quadrant framework (domain, system design, organization, and ecosystem) can help researchers and companies guide the development of smart service systems or evaluate past progress. The evaluation of the framework shows that there might be dominant quadrants and certain paths in the framework, which we would like to investigate in further research.",CS,AI_ML,0.85,Extracted from log - paper 1590
Multi-tier Supply Chain Complexity and Buyer Performance in ICT Industry,"The characteristics of suppliers in multi-tier supply chains have drawn more attention recently in the post-pandemic age. Due to the increasing reliance on suppliers, information-communication technology (ICT) buying firms expect to map out their upstream suppliers to identify the characteristics of those suppliers, which might affect their performance. In this paper, we utilize horizontal, vertical, and spatial complexities to examine how complex characteristics of the first- and second-tier suppliers affect the ICT buying firms’ financial performance. Our empirical test shows that ICT buying firms’ sales growth is affected by multi-tier suppliers from different industries. Our results bring insights into the role of multi-tier suppliers and their effects on ICT buying firms’ financial performance. The research findings contribute to the literature on the complexity of multi-tier supply chains. We also provide practical implications for managers from ICT firms to improve their decision-making and collaboration with multi-tier suppliers.",CS,AI_ML,0.85,Extracted from log - paper 1591
What is Digital about Digital Innovation? An Ontological Discussion of Digitality,"“Digital” is a popular adjective for concepts related to electronic, computerized or networked processes in current research and practice. From an ontological-theoretical perspective, the prevalence of digital things triggers the question, how Digitality itself could be conceptualized. This article studies the ontology of Digitality and consequently discusses Digital Innovation (DI) from a new perspective. First, literature on Digitality is analyzed. Based on the findings, an ontological conceptual framework of Digitality (OCFD) is proposed. It describes four layers of Digitality: physical foundations, virtuality, digital culture and world, and digital metaphysics. From a human perspective, requirements to grasp and embody Digitality are highlighted, as well as the expansion of reality. Finally, Digitality is discussed as a paradigm of DI and exemplary digital phenomena are projected onto the OCFD. This paper contributes to DI research and practice through theory-building towards a better understanding of ontological characteristics and practical implications of Digitality in DI.",CS,AI_ML,0.85,Extracted from log - paper 1592
Strategies for the Aesthetic Experience of Business Processes,"Business processes require knowledge and generate knowledge when data and information are combined with personal experience. Information systems can support business processes in highlighting the importance of formal, but also hidden knowledge structures. In particular, the implicit knowledge incorporated in people and organizational routines contributes to the uniqueness of business processes. Aesthetic business processes offer a holistic knowledge management concept that aims to mobilize implicit knowledge. Based on C. Alexander's pattern theory and visualizations, it is about perceiving the Gestalt of business processes. The metaphor Gestalt refers to the atmosphere of business processes, which is to be sensually experienced. This article aims to (a) show different strategies, (b) model the Gestalt of aesthetic business processes, and (c) visualize information culture to make aesthetic experiences successful in everyday organizational life. Aesthetic experiences might remain fragmentary or become fulfilling when we ""have an experience"", but ultimately open up a variety of perspectives.",CS,AI_ML,0.85,Extracted from log - paper 1593
What can Information System Consultants do to Assist Client Leveraging Digital Transformation? A Case Study in China,"What can information system consultants do to assist client leveraging digital transformation to serve better the well-being of employees? This paper describes the case of a high-tech company in China which initially wanted to rely on an external consulting group for conducting digital transformation in order to solve organizational problems. However, the first evaluation of these organizational problems conducted internally led the company to decide to postpone the digital transformation. The focus shifted toward an internal consulting project in charge of first investigating further these organizational problems and then conducting an internal transformation independent from technology. These organizational problems appeared to be related to the well-being of employees. From “How to assist client leveraging digital transformation to serve better well-being of employees”, the project became “How information system consultants and managers can assist with organizational and systems design for work procedures, and processes that improve performance”. The theoretical contribution of this paper is to suggest a framework for analyzing the well-being of employees. This framework of ambidexterity reveals the way how employees are torn apart between exploration and exploitation. On the practical standpoint, the contribution of this paper is to identify a collaborative communication network approach for solving ambidextrous organizational problems related to the well-being of employees that rely less on technology than on human and organizational transformation.",CS,AI_ML,0.85,Extracted from log - paper 1594
Operationalizing Ethics for Information Systems Design – A Tool for Ethical Software Assessment,"Propagation of information technology in the increasingly diverse domains raises questions concerning moral implications. While the number of ethical principles in IT design is growing, they do not necessarily apply to specific products, which means developers and, e.g., purchasing agents need to contextualize them for each specific IT product. This paper describes an actionable artifact that allows assessment of ethical non-functional requirements of an information system through operationalization of ethical values, Ethical Software Assessment Tool (ESAT). It is based on a survey of values that are currently considered in research and practice as well as in legal initiatives in IT context. The ethical approach and principles operationalized in ESAT are presented and discussed.",CS,AI_ML,0.85,Extracted from log - paper 1595
Digital Transformation and Human-Digital Competitiveness Model,"Despite the potential benefits that digital transformation can bring, the success rate of its implementation remains low at only 30% (BCG, 2020). The situation is even more critical and challenging in a highly competitive manufacturing industry. The dynamic environment with increasing uncertainties leads to decreased communication between departments, thus diminishing the advantages of joint venture cooperation. This paper analyzes the challenges of implementing information technology (IT) and organizational reform in an international joint venture through the perspective of the Human-Digital Competitiveness Model (Monod, et al., 2021). Since three dimensions of the Model can analyze those problems in a comprehensive way. It adopts a qualitative methodology by conducting a case study (Yin, 2017) through interviews with 17 staff and related party-in-interests from both sides, as well as corporate observation. Theoretically, this paper contributes to a thorough analysis of the challenges of digital transformation and organizational changes within companies from three different dimensions. Practically, it contributes to an understanding of the impact that IT and digital technologies have on the joint venture and offers insights into how business processes can be innovated to enhance competitive advantages.",CS,AI_ML,0.85,Extracted from log - paper 1596
Digital Transformation and Entrepreneurship,"The implementation of digital transformation often fails. This paper aims to identify what kinds of organizational issues exist in a leading technology company regarding the implementation of information technologies. The theories are Socio-Economic Approach to Management (SEAM) and Socio-Technical Approach. The research method is a case study of OMRON Corporation. The research findings include dysfunctions and the impact of IT and digital technologies: The teammate's work in the department needs to be more cohesive, and there needs to be more communication and coordination between teammates. This may cause a delay in new product development and quality problems later; in introducing new materials, problems often occur among the R&D department, materials procurement department, and quality department. The social factors of these dysfunctions are the organization is too vertically divided and sectionalism is strong. The managers should lead the transformation with entrepreneurship. The technical aspects are the tremendous success of the company's past technological development. It could have been more objective toward product development that emphasizes too much technology and function on the product. The contribution to theory is applying SEAM and STA theory to clarify the organizational problem in this research. The contribution to practice is analyzing the factor of dysfunction and the impact of implementing IT and data technologies.",CS,AI_ML,0.85,Extracted from log - paper 1597
Digital Transformation from Sun Tzu’s System Thinking Perspective,"About Sun Tzu’s Art of War, lots of research has been done. However, most of them were fragment studies, that is, done by sampling one or two management doctrines. About its core, the system thinking, few have been conducted using a General Systems Theory. What’s more, these studies were in most cases a theoretical one. The objective of digital transformation (DT) was re-establishing organization’s business model through innovation by using digital technologies to sharpen organization sustainable competitiveness, it is just like a war campaign. This study is to use case study method to show how Sun Tzu’s System Thinking could be used in DT to defeat potential rivals to maintain competitive advantage. The combination of theoretical study and practitioner application will be of high value both to those practitioners in digital transformation process and contribute academically to research methods in the future study of Sun Tzu’s Art of War.",CS,AI_ML,0.85,Extracted from log - paper 1598
Overuse of Social Media Influence Process Performance,"Overuse and even addiction of social media has gradually evolved to problematic issue, especially most of the employees in working hours. However, a little research has been done regarding overuse and addiction of social media could distract energy and occupy too much time that should be otherwise used to study work related knowledge, such as digital key technologies and domain knowledge. Excessive use also leads to coordination and cooperation issue, which decrease team/process performance. The study shall adopt case study method to explore in-depth causes of overuse of social media and its potential hazardous effects to regular operations of organizations, especially in a medium sized typical manufactory scenario. A potential solution for top management in organization would be extremely important, especially in the manufacturing sector of the No. 2 Economy of the world.",CS,AI_ML,0.85,Extracted from log - paper 1599
Fostering Scalable Citizen Development in Organizations: Towards a Guiding Framework,"Low-code development platforms (LCDPs) offer organizations the potential to reassign time-intensive software development tasks from professional to citizen developers, freeing up resources for professionals while empowering domain experts. However, while software can be rapidly built through citizen development (CDD), organizations must look beyond implementation and consider whether software built by individuals with limited technical backgrounds is scalable and durable. In this study, we investigate the scalability of CDD in organizations by conducting a multivocal literature review, integrating both the current phenomenon (CDD) as well as related, established research streams like end-user development. We identify four dimensions affecting CDD scalability and four temporal stages of CDD maturity, and present these as a CDD scaling framework. Moreover, we identify three overarching themes in scaling CDD. Finally, we present an integrated research agenda based on the CDD scaling framework and overarching themes.",CS,AI_ML,0.85,Extracted from log - paper 1600
Survival of the Fittest: A Business Model Perspective to explain Innovation Ecosystem Membership,"The manufacturing industry is one of the biggest beneficiaries of artificial intelligence (AI). However, to reap the value and fulfill all the necessary activities, they realize it involves a multitude of different capabilities and resources. Therefore, firms increasingly establishing innovation ecosystems that animate both, (inter)-organizational cooperation, and intra-ecosystem competition among participants. But beyond resources and capabilities, what are the reasons that independent organizations become members in the first place, and how do they secure their position in a field of dynamic competition and collaboration? This study applies an exploratory multi-case study with ten cases from the manufacturing industry. Based on a business model and role of value network perspective, our findings reveal 16 BM characteristics that add to a competitive advantage and therefore ecosystem membership. Our study contributes to the research on competitive advantage in innovation ecosystems, strategic management, and the growing streams of research on artificial intelligence.",CS,AI_ML,0.85,Extracted from log - paper 1601
"Information Management Capability, IT Ambidexterity and Agility: A Site Management Dashboard Study","The research topic in this paper is how can the relationship of Information Management Capability and IT Ambidexterity impact on Agility? The question was answered through an action research, proposing a research model and developing a dashboard in two phases: firstly using existing resources, which served for the initial analysis and results, and secondly, the final dashboard with existing and new resources (IT Ambidexterity), resulting in better levels of information management and organizational agility, through IMC and the ability to predict and respond to problems quickly, in Agility. The research contributed positively to the capability’s relationship, corroborating the research model, adding the information dashboard as a crucial link between capabilities. At the organizational level, information management through visual elements on the dashboard made it possible to quickly identify gaps in production, reducing the analysis of problems from more than 2 hours to 5 minutes, positively affecting the organization's decision-making.",CS,AI_ML,0.85,Extracted from log - paper 1602
CIO Turnover and its Consequences for Competitors,"As information technology is considered a key driver of firm’s innovation, the role of a chief information officer (CIO) has been emphasized. However, firms face frequent CIO turnover. Thus, in this study, we argue that it is important to investigate the impact of CIO turnover on a firm’s innovativeness, and the spillover effect of CIO turnover on competitors’ innovativeness. We then propose a novel approach to testing these causal inferences of CIO turnover. We found that CIO turnover did not have a significant direct effect on its focal firm. However, the spillover effect is significant and positive, implying that firms produce more patents when their competitors experience CIO turnover than if CIOs at competing firms had not been dismissed. Since losing a CIO has ripple effects throughout the whole industry, firms should consider the unintended consequences of their decisions regarding CIO turnover.",CS,AI_ML,0.85,Extracted from log - paper 1603
Doing More with Less: A Resource-Considerate Framework for SMEs on the Digital Transformation Journey,"Digital transformation (DT) is widely acknowledged as an effective strategy to enhance revenue and business operations efficiency. However, Small and Medium-sized Enterprises (SMEs) often face significant challenges in implementing DT initiatives due to their inherent resource constraints, such as limited financial resources or a small workforce. Unfortunately, existing literature on how SMEs can effectively embrace digital transformation is limited; available frameworks are primarily tailored to large organizations with more resources. Hence, this study aims to develop a pragmatic DT framework specifically designed to aid SMEs in overcoming resource constraints that impede their successful DT transformation, while also leveraging their agile capability. The digital transformation framework was developed by combining prior literature with insights gathered through focus group discussions with industry experts from various industries. Our research contributes to the existing digital transformation literature by formulating a novel resource-considerate framework that caters to SMEs' unique characteristics, allowing them to implement DT initiatives without overburdening their limited resources.",CS,AI_ML,0.85,Extracted from log - paper 1604
A Configurational Set-Theoretic Approach to an Innovation Capability Maturity Model,"Constantly evolving economic demands caused by phenomena such as digital transformation necessitate businesses and their underlying models to adapt. One possible solution for enhancing innovation potential and increasing revenue is implementing an innovation capability maturity model. This research replicates an existing maturity model development approach, which accounts for the unique characteristics of a class of companies based on their size and sector. Thus, by using a data-driven method, we provide an innovation capability maturity model for small industrial companies. We identified 21 combinations of factors that can affect innovation capability maturity, grouped into a knowledge and organizational dimension. Using a set-theoretic approach, we identified configurations of these factors that lead to low, medium, and high levels of innovation capability maturity. Based on these findings, we support an empirically driven, set-theoretic approach for developing a maturity model that can be used for higher applicability and representativity in individual classes of companies.",CS,AI_ML,0.85,Extracted from log - paper 1605
Creating Data Policies for Digital Business Ecosystems,"Data policies are high-level guidelines that define how the organization handles data. In the age of cross-organizational digital business ecosystems, organizations are facing the need to define data policies that allow them to operate in decentralized scenarios. We conducted a Design Science Research (DSR) project to develop an approach for data policy management for digital business ecosystems. Our artifact was developed and demonstrated in a leading European IT provider. Our results include (1) an approach for data policy creation and (2) a data policy cycle. For theory, our work extends the literature with an approach for data policy management in digital business ecosystems in a highly regulated sector. For practice, our approach can support practitioners in developing the necessary data policies based on their context, considering their location, data-related regulations, available data assets, and organizational environment.",CS,AI_ML,0.85,Extracted from log - paper 1606
Inter-Organizational IT-Governance for a System of Systems,"Critical Infrastructures (CI) are systems that are essential for maintaining vital societal functions, and their disruptions can have a significant negative impact on a nation's well-being. CI has become the focus of many digital advancements, resulting in them becoming increasingly intertwined. Therefore, a more holistic approach to infrastructure management must be adopted, in which CI typically organize themselves as a System of Systems (SoS). The main issue that CI organizations face is the unclarity of how to make their IT function as one system. IT-Governance (ITG) is critical to guide the implementation of SoS within individual organizations. However, current literature acknowledges the lack of research on Inter-organizational IT-Governance (IoITG), which is necessary for creating a digital SoS. This research-in-progress paper first proposes a definition for digital SoS for CI and then highlights the need for further research on IoITG capability in the context of SoS for CI.",CS,AI_ML,0.85,Extracted from log - paper 1607
Knowledge Management Systems and Artificial Intelligence Adoption for Increasing Business Sustainability,"Sustainable business success is essential for achieving performance in a market in the contemporary technology age. Worldwide, businesses should overcome a number of challenges by using innovative practices and technologies to achieve long-term business sustainability. This study intended to inspect the relationship between knowledge management systems (KMS) and sustainable business, and to examine the moderating role of artificial intelligence (AI) adoption in the relationship. To achieve this objective, we collect data from Iraqi firms. Using structural equation modelling, the finding of this study show that knowledge management systems significantly increased sustainable business. In addition, AI adoption moderates the relationship between KMS and sustainable business. Hence, this research makes significant implications to literature and policymakers regarding applying and understanding knowledge management systems and AI adoption to enhance sustainable business.",CS,AI_ML,0.85,Extracted from log - paper 1608
Cryptocurrency Payment Method for Retailers: A Systematic Review,"The retail industry is highly competitive and requires businesses to continuously explore and implement technological innovations that will set them apart. Adopting cryptocurrency as a payment method is one innovation that gives businesses a competitive edge. We conducted a systematic literature review based on an in-depth analysis of 28 published articles. The literature analysis shows that cryptocurrency payments adoption is a growing research area, although only a few studies have been conducted in the last decade. Our analysis of the results showed that retailers consider a few factors regarding cryptocurrency payment adoption. The considerations include the company's size, innovativeness, regulatory requirements, perceived ease of use, perceived usefulness, attracting new customers, competitive advantage, meeting demand from existing customers, and using the technology as a marketing tool. The study's findings will be helpful to those retailers who wish to leverage cryptocurrency payments as a business tool to gain a competitive advantage.",CS,AI_ML,0.85,Extracted from log - paper 1609
"Right Place, Right Time: Configurations of Technology Use for Business Model Innovation","In the rapidly evolving digital era, firms need to innovate their business models (BMs) to thrive and benefit from new digital technologies. While digital BM innovation (DBMI) can create competitive advantage through differentiation, firms often struggle to create and extract value from digital technologies. In this paper, we analyze 41 cases of BM change and identify three configurations that lead to DBMI and two configurations that do not. These configurations illustrate the BM elements in which digital technologies are used for DBMI and explain how firms use digital technologies for strategic innovation. Combined with existing research on DBMI, this research provides a missing piece of the puzzle to connect specific technologies in the BM to implement a digital business strategy and undergo a digital transformation. For practical purposes, the findings suggest three alternative strategies for DBMI that identify which elements need to be changed together to achieve DBMI.",CS,AI_ML,0.85,Extracted from log - paper 1610
Unravelling the Business Value of Artificial Intelligence,"This study aims to investigate how firms can prepare for artificial intelligence (AI) adoption and how AI creates business values for adopting firms. We propose that, first, leveraging AI for value creation depends on the presence of an enabling resource foundation, consisting of a stack of technological and managerial resources. According to the real options theory, the stronger such a resource foundation is, the more powerful the option it creates for a firm to adopt AI, and consequently the firm is more likely to exercise the option to adopt. Second, we propose that, after adoption, the depth of AI use can have negative impacts on employee productivity, while the breadth of AI use can have positive impacts. We empirically validate these hypotheses based on a sample of 6,845 manufacturing firms in a coastal province in China.",CS,AI_ML,0.85,Extracted from log - paper 1611
Embracing the Chaos: A Framework for Chaos Engineering,"Complexity has always been a topic for IS scholars. While research has addressed this problem in the past, the exponential rise of complexity due to the pervasive use of IS in our society has made complexity an even bigger justification of concern for both scholars and organizations. One of the outcomes of this risen complexity is the number and impact of IS incidents that occur due to organizations failing to prepare for a plethora of possible scenarios that can have negative consequences. We expand on Mehrizi and colleagues’ (2022) theoretical framework for learning from IS incidents. Our research presents a Chaos Engineering (CE) Framework that helps organizations in generating knowledge through experimentation, in a capacity to tame complexity. CE is the discipline of experimenting on a system to build confidence and resilience, by simulating different complex behaviors. This allows organizations to learn more effectively without the real-life costs of other learning processes.",CS,AI_ML,0.85,Extracted from log - paper 1612
"Reconciling Strategy, Maturity, and Performance Measurement in Industry 4.0","Maturity models are essential tools to evaluate Industry 4.0 readiness. However, they have limitations in supporting organizational strategies over time and are insufficient to create company-specific Industry 4.0 roadmaps. This paper presents a research project to steer Industry 4.0 transformation in a leading paper pulp company listed on the Euronext Lisbon stock exchange. The contribution to the body of knowledge is a novel approach for Industry 4.0 adoption that extends the Balanced Scorecard with a fifth perspective and exposes the value of maturity model fragments: fractions of maturity models pertinent to a custom strategy. The proposed solution can support continuous improvement in long-term digital transformation strategies. Moreover, the lifecycle of maturity model fragments, namely, (1) dimensions and criteria elicitation, (2) personalization, and (3) strategic alignment, opens innovative prospects for maturity model design, tailorability, and practical relevance.",CS,AI_ML,0.85,Extracted from log - paper 1613
"Supply Chain Analytics Capability: Conceptualization, Instrument Development","Grounded in the capability-building mechanism, business analytics literature, and the Supply Chain Operations Reference model, this study proposes a framework for developing a context-specific and comprehensive instrument for supply chain analytics capability (SCAC). The proposed framework lays the foundation for and is the first stage of a large-scale in-progress research project that aims to develop and validate a context-specific and comprehensive measurement for SCAC.",CS,AI_ML,0.85,Extracted from log - paper 1614
Achieving Strategic IT Alignment in Dynamic Environments: On the Value of Loose and Tight Coupling,"Executives continue to grapple with a variety of issues involving strategic IT alignment (SITA) and adaptation to unexpected market events. For many firms, the rise of global competition and increasing volatility have frustrated efforts to pursue SITA and with it, efforts to achieve organizational agility. In the face of a volatile environment, aligning IT to a changing business strategy can be difficult if IT is slow to respond-any ensuing rigidity traps could hinder agility. This paper introduces a model of SITA based on the concept of loose coupling. Focusing on the forms of SITA based on degrees of coupling between strategy and business strategy, we look beyond performance impacts of SITA to investigate how SITA is shaped by environmental change and the resulting implications to organizational agility and performance. Propositions are introduced to help guide future research on SITA in dynamic environments.",CS,AI_ML,0.85,Extracted from log - paper 1615
A deep learning-based Cyber-risk Management framework for smart cities,"A malicious hacker can compromise the electronic speed limit signs in a smart city, causing an autonomous vehicle to misread the speed limit signs and result in collision or congestion on the pathways. Based on protection-motivation theory (PMT), we propose a Deep Learning-based Cyber-risk Assessment and Mitigation (DL-CRAM) model comprising three modules for the smart city administrator. In line with the threat appraisal component of PMT, our cyber-risk assessment (CRA) module uses a Convolutional Neural Network (CNN) algorithm, takes electronic two-digit speed limit sign images as an input, learns its features, and outputs the probability of misreading it. Then, using the Bayesian inference model, we compute the conditional probability of an autonomous vehicle misreading both the digits or one of the two digits. Subsequently, based on the concepts of risk theory, the cyber-risk quantification (CRQ) module calculates the expected loss for a smart city due to collision or congestion on the pathways. In line with the coping appraisal component of PMT, our cyber-risk mitigation (CRM) module proposes strategies for the smart-city administrator to reduce cyber-risk using technological means and pass the residual risk to third-party cyber-insurer.",CS,AI_ML,0.85,Extracted from log - paper 1616
DEFIYIELD: Exploitation of Open Blockchain Platforms,"We analyze the most common security vulnerabilities in distributed ledger systems. We performed predictive analytics on the REKT database to predict the attack pattern using predictive algorithms, including logistic regression and random forests. The results show that the month of the attack and the cryptocurrency chain affected were significant predictors of the type of scam that occurred. The most important predictors were the Log of funds lost, the chain or platform of the cryptocurrency attacked, and the Log of funds returned after the attack. The study highlights the need for greater scrutiny and improved security measures in DeFi projects to mitigate the risks associated with the DeFi ecosystem.",CS,AI_ML,0.85,Extracted from log - paper 1617
Benchmarking the Robustness of Phishing Email Detection Systems,"Social engineering attacks are currently the most cited cybersecurity threat to organizations. Phishing emails are the most salient form of social engineering attacks. Organizations are increasingly implementing AI-enabled systems to detect phishing emails. However, AI-enabled systems are often susceptible to textual perturbations, where an adversary makes a small change to cause a misclassification. In this study, we sought to identify the performance of prevailing phishing email detection systems (PEDS) against character, word, sentence, and multi-level adversarial text perturbations. Through a principled benchmarking framework, we quantitatively demonstrated the lack of robustness prevailing PEDS have to specific types of text-based adversarial perturbations (e.g., character, word, sentence, multi-level). The results of this study provide new insights into the robustness of AI-based PEDS and highlight the need for organizations to adopt a multi-layered approach to phishing protection. Additionally, organizations can implement our benchmark framework to test their PEDS against adversarial perturbations.",CS,AI_ML,0.85,Extracted from log - paper 1618
Navigating the Wave of Cybersecurity Regulations: A Systematic Analysis of Emerging Regulatory Developments,"As the volume and publicity about cyber-attacks continue to increase, new cybersecurity regulations are being enacted or proposed by governments around the world. But will they be beneficial or harmful in improving cybersecurity? This paper provides an overview of the recent regulatory developments and trends in cybersecurity, with a focus on those in the US and Europe. Specifically, this paper presents a categorization of the main regulatory areas that are currently being discussed. We then identified patterns, gaps, and areas of improvement in the current regulatory environment. Our findings provide valuable direction to organizations navigating the flood of new cybersecurity regulations and the governments enacting these regulations.",CS,AI_ML,0.85,Extracted from log - paper 1619
Are Information Security Practices Driven by the Likelihood and potential Impact of Security Events?,"Organizations have opened up various digital interfaces through which customers gain access to online services. Digital interaction between the organization and its customers increases security risks. This study examines how the risk profile of customers defined by their perception of the likelihood and potential impact of a security incident is related to their information security behaviors. We collected data in the context of online banking services to empirically evaluate this research question. The results showed that individuals with low likelihood and high impact perceptions of security incidents had authentication and device security practices significantly higher than that of other groups. Surprisingly, there was no difference in security practices across groups with high likelihood/high impact and low likelihood/low impact perceptions of security incidents. Implications of these results are discussed along with ways we plan to extend this research study.",CS,AI_ML,0.85,Extracted from log - paper 1620
Root Causes of Healthcare Data Breaches – A Text Analytic Approach,"What might be the root cause of the breaches in healthcare industry? Is it human negligence or technical? If it is human or insider, then what types of insider threats are there? How can these threats be detected? These are the questions this emergent research is going to address. A literature search returned very few empirical research in this area. This research analyzes the textual data as reported by the entities and examines the root causes of the data breaches. Specifically, we found nearly 40% of breaches were due to insiders. We are interested in characteristics of these insider threats since the breaches are based inside the technical perimeter of the hospital. While most research is based on numerical data, this research analyzes the textual data of breach reports. Preliminary results indicate there are different types of insider threats and awareness training has poor effectiveness.",CS,AI_ML,0.85,Extracted from log - paper 1621
Value-Driven Policy Agenda to Manage Online Privacy of Young Adults,"This paper discusses the challenges associated with managing online privacy of young adults and how a policy can be developed. The paper argues that online privacy for GenZ is important to protect. It also argues that protection can be ensured if we understand and know what privacy-related values behold GenZ and hence define their objectives accordingly. Information privacy is defined as a process by which one can have freedom from unauthorized intrusion hence resulting in seclusion. In a final synthesis, 22 main objectives are identified – 6 fundamental objectives and 16 means objectives. Collectively the objectives help in developing online privacy policies for young adults.",CS,AI_ML,0.85,Extracted from log - paper 1622
A Qualitative Study on Acceptance Factors of Economic Approaches on IT Security Investment Decisions,"Cyber-attacks can cause severe economic damages to businesses today. Therefore, every company needs to protect its IT systems from such attacks by implementing effective IT security measures. When it comes to deciding on an IT security measure, there is a variety of decision approaches that can offer support on choosing an economically reasonable one. In practice, however, such approaches are rarely applied and a company’s security investment decisions are often based on intuitive factors. We therefore want to explore which factors drive the acceptance and application of economic approaches on IT security investment decisions. For this, we used semi-structured interviews and a moderated focus group to generate insights from both science and practice. Not only did our research yield new scientific knowledge about IT security investment decisions, but the results can also be useful in increasing the application of economic decision approaches.",CS,AI_ML,0.85,Extracted from log - paper 1623
Vulnerability-based Cyber-Risk Management: A Text-mining Approach,"Vulnerable information systems inside an organization make it prone to cyber-attacks, leading to loss of reputation, financial loss, customer churn, and loss of future prospects. In our study, we assess, quantify, and mitigate the cyber-attack risk generated due to the vulnerable information technology assets using our proposed Vulnerability-based Cyber-Risk Management Model (VCRMM). We leverage Protection Motivation Theory and cyber-kill chain to assess the cyber risk based on specific characteristics of vulnerabilities. We perform text mining using the topic modelling technique, Latent Dirichlet Allocation, find a correlation between the topics, and then classify the severity rating of vulnerabilities. The higher the severity rating of any vulnerability, the greater the probability of cyber-attack (p) any organization faces. Next, we quantify the cyber-attack risk in terms of expected losses. Finally, based on Rational Choice Theory and NIST-guided Vulnerability Management Process, we propose mitigation strategies to reduce, accept, or transfer the cyber-attack risk.",CS,AI_ML,0.85,Extracted from log - paper 1624
"A Social Engineering Research Partnership in Higher Education to Improve Information Security Education, Training, and Awareness (SETA) Programs","The education sector continues to be challenged by phishing attacks. A simulation study of phishing response behavior across four industries found that the education sector had the highest number of employees opening and clicking a phishing email. Understanding the factors contributing to the high phishing susceptibility within the education sector would help develop more comprehensive information security education, training, and awareness (SETA) programs. This would allow universities to become better protected from attempts at using social engineering to gain access to sensitive personal and university data. However, despite the increased levels of phishing attacks within educational institutions, limited research has systematically examined how conditions unique to the sector can influence user engagement and phishing susceptibility. Against this backdrop, our research collaborates with the IT department of a large public university to examine how certain attributes within educational institutions influence phishing susceptibility.",CS,AI_ML,0.85,Extracted from log - paper 1625
The Effect of Neutralization: Before and After Employee Violating ISP,"Researchers have adopted neutralization techniques to explain why individuals conduct norm-violating behaviors, such as violating information security policy (ISP). Even though those techniques may be used before and after taking action, more past studies focused on the pre-action stage and attempted to explore the impact of the technique utilization on behavioral intention. In this study, we followed Kaptein and van Helvoort (2019) work and classified those techniques into two types: denying personal responsibilities and denying deviant behavior. In addition, we attempted to extend the application of neutralization techniques from pre-action to post-action. We argue that while denying deviant behavior is more important in the pre-action stage, denying personal responsibility is more critical in the post-action stage. By doing so, we contribute to neutralization studies by including the post-action stage and highlighting the relative importance of different techniques in different stages.",CS,AI_ML,0.85,Extracted from log - paper 1626
Identifying Practical Challenges in the Implementation of Technical Measures for Data Privacy Compliance,"Modern privacy regulations provide a strict mandate for data processing entities to implement appropriate technical measures to demonstrate compliance. In practice, determining what measures are indeed “appropriate” is not trivial, particularly in light of vague guidelines provided by privacy regulations. To exacerbate the issue, challenges arise not only in the implementation of the technical measures themselves, but also in a variety of factors involving the roles, processes, decisions, and culture surrounding the pursuit of privacy compliance. In this paper, we present 33 challenges faced in the implementation of technical measures for privacy compliance, derived from a qualitative analysis of 16 interviews with privacy professionals. In addition, we evaluate the interview findings in a survey study, which gives way to a discussion of the identified challenges and their implications.",CS,AI_ML,0.85,Extracted from log - paper 1627
Exploring Post-Quantum Cryptographic Schemes for TLS in 5G Nb-IoT: Feasibility and Recommendations,"Narrowband Internet of Things (NB-IoT) is a wireless communication technology that enables a wide range of applications, from smart cities to industrial automation. As a part of the 5G extension, NB-IoT promises to connect billions of devices with low-power and low-cost requirements. However, with the advent of quantum computers, the incoming NB-IoT era is already under threat by these devices, which might break the conventional cryptographic algorithms that can be adapted to secure NB-IoT devices on large scale. In this context, we investigate the feasibility of using post-quantum key exchange and signature algorithms for securing NB-IoT applications. We develop a realistic ns-3 environment to represent the characteristics of NB-IoT networks and analyze the usage of post-quantum algorithms to secure communication. Our findings suggest that using NIST-selected post-quantum key-exchange protocol Kyber does not introduce significant overhead, but post-quantum signature schemes can result in impractical latency times and lower throughputs.",CS,AI_ML,0.85,Extracted from log - paper 1628
Security Networks as an Effective Means to Reduce Spear Phishing Susceptibility,"Over the past few years, cybersecurity has gained importance for organizations as social engineering attacks in general, and in particular phishing attacks evolved. Previous phishing research focused on improving the susceptibility of individuals towards phishing attacks, while collective security behavior has been neglected. In this emergent research forum paper, we aim to contribute to the existing phishing literature by extending the scope from the individual towards the collective and the effect of security networks on phishing susceptibility. To observe the collective behavior, we performed a field experiment with three teams and targeted them with spear phishing messages on an online social network. The work plans to evaluate the effect of trust, interdependence, connectivity, relational strength, position within a network and the impact within a network on spear phishing susceptibility. Thus, our work intends to show whether security networks can improve the collective behavior when being targeted on online social networks.",CS,AI_ML,0.85,Extracted from log - paper 1629
Phishing Susceptibility – a Cognitive Dissonance Persuasion View,"Phishing remains the most commonly employed technique for executing cybercrime activity. At its core, phishing relies on persuasive techniques that exploit human vulnerabilities. Yet, the current knowledge and understanding of how people respond to persuasiveness in phishing are scarce. Looking through the lens of cognitive dissonance theory, this research proposes a five-step theoretical framework and derives an initial psychometric model to examine and compare the six persuasion techniques on phishing susceptibility. We argue that the cognitive dissonance generated by persuasive techniques influences phishing susceptibility. We also argue for the mediating mechanism of preference for cognitive consistency and mindful attention awareness. This research contributes to understanding human vulnerabilities to phishing by introducing a general sequential model. The model permits the manipulation and testing of different contextual and individual attributes’ constructs, provides flexibility to the whole and part assessment, and allows building and expanding knowledge about the persuasive effect of phishing.",CS,AI_ML,0.85,Extracted from log - paper 1630
Cybersecurity Awareness and Adaptive Behavior: Does Prior Exposure Lead to Adaptive Behavior?,"We adopt and use technology almost as fast as it comes into existence, however, our mental models and intuitions advance at a much slower pace to the vulnerabilities and threat that come with new technologies. This creates an opportunity for perpetrators to take advantage of the cyber citizens. It is common knowledge that human beings will remain as the ultimate defense (or the final firewall) of cyber assets, and yet they are most vulnerable to a variety of cyber-attacks. Considering these assertions, this research aims to explicate whether knowledge and experience build the necessary awareness that leads to action. The proposed model expands on the Knowledge-Attitude-Behavior model to explore the role of prior cybersecurity training, incidents, awareness, and attitude towards cybersecurity education in individuals' adaptive responses/behaviors given the cybersecurity risks. The findings could inform the academic community and policymakers on creating effective cybersecurity awareness programs to promote cyber hygiene in today's digital world.",CS,AI_ML,0.85,Extracted from log - paper 1631
The Interplay of InfoSec Mindfulness and Sanctions on Extra Role Security Behaviors: A Trait Activation Perspective,"Organizational information security performance is inextricably tied to the security behavior of employees. While employee behavior has long been considered a challenge to information security, recent shifts to a more remote work force have amplified organizational focus on employees to protect information assets. Under these conditions, InfoSec Extra Role Behaviors (ERBs) have become increasingly important to mitigating security threats. The literature suggests that mindfulness is an important human factor that may lead to InfoSec ERBs. We examine mindfulness through the lens of trait activation theory. Trait activation theory (TAT) explicitly ties individual traits to situational features of the environment to predict trait related behavior. In TAT, a relevant feature of the organizational environment is situation strength which constrains behavioral variance through trait expression. Thus, we operationalize sanctions as a measure for situation strength in the InfoSec context and propose a model that examines the interplay between a relevant human factor, mindfulness, and a common organizational intervention, sanctions, and the impact that interplay has on InfoSec ERB enactment.",CS,AI_ML,0.85,Extracted from log - paper 1632
Using a Nudge to Close the Intention–Behavior Gap for Information Privacy,"The acceptance and use of information technology (IT) and information systems (IS) are two of the most critical topics in IT and IS research. In these fields, intention has usually functioned as a direct and immediate predictor of actual behavior according to most theories and models of behavioral change. However, there exists a considerable gap between the two, particularly in the area of privacy protection. Despite the intent to protect their privacy, many users continue to disclose highly personal information and accept intrusive privacy policies without using privacy-protection tools. This research aims to bridge the gap by investigating the efficacy of nudging interventions in lowering cognitive load and helping users in carrying out their intentions to protect their online privacy. By utilizing persuasive technology and the PSD framework, we designed and implemented nudges to overcome cognitive limitations for privacy protection. Our study is expected to make a significant contribution to the field of privacy and information technology/information systems by providing a potential solution to the privacy paradox issue, extending existing intention-oriented behavioral models, and emphasizing the importance of actual behavior. Furthermore, our findings should aid in users’ safeguarding of their privacy, designers’ development of privacy-protection tools, and policymakers’ improvement of general privacy-protection systems.",CS,AI_ML,0.85,Extracted from log - paper 1633
Using stakes consciousness to better understand information security awareness,"Several studies have identified employees within organizations as the weakest link in information security. Although many measures are in place to stimulate employee interest in cyber security, the challenge seems not always at the organizational level but sometimes at the employee behavioral level. In this study, we develop a construct named stakes consciousness to provide a better understanding of employee information security awareness within organizations. We focus on employees' origins and environments to explain their risk averseness and understand their information security awareness. We use the characteristics of organizations, such as the firm sector, sector density, and network centrality, to explain the importance of employees' information security awareness. Finally, we provide an overview of the application of this construct in technological, organizational, and individual contexts.",CS,AI_ML,0.85,Extracted from log - paper 1634
How Does Technological Entitlement Affect Cyberdeviance? The Role of Organizational Justice.,"The widespread use of technology in the workplace has led to an increase in cyberdeviance, where employees deliberately violate organizational norms when using IT resources. Drawing on literature on technological entitlement and organizational justice, this study explores the relationship between cyberdeviance, technological entitlement, and organizational justice. We propose that technological entitlement is a personality trait of employees who are more likely to engage in cyberdeviance and use organizational justice as a justification for their behavior. This paper contributes to recent studies on technological entitlement and has implications for organizations seeking to mitigate cyberdeviance.",CS,AI_ML,0.85,Extracted from log - paper 1635
Workarounds from an Information Security Perspective: Literature Review,"The security of every organization is critical and should be proactively planned. The research field of security, workplace behavior, security safeguards, workarounds, and cybersecurity explores how organizations can effectively strategize and use adaptive security plans to minimize the security vulnerability of organizations. These solutions rely heavily on the technology adopted by organizations that seek to align with policies that safeguard their information assets consciously. However, despite several works in the literature on workarounds, the concept of workarounds that poses a security risk in an organization is still riddled with ambiguities. This review covers current research involving 28 articles across the Association for Information Systems (AIS) basket of eight conferences and other IS security-related journals. Our results identify four major categories that reveal the intersection of workarounds and security in the workplace from recent literature. We further discuss their implications and propose avenues for future research.",CS,AI_ML,0.85,Extracted from log - paper 1636
Digital Forensics Use-Case of Blockchain Technology: A Review,"Digital forensics is an ever-growing field of science that is heavily involved in various fields. However, because of the expanding complexity of computer science, many new problems have arisen for digital forensics such as data scalability and cloud computing forensics data privacy, data traceability, integrity, and chain of custody. Blockchain can be useful for digital forensics to ensure security, like Proof-of-Work and Proof-of-Stake. Proof-of-Work verifies that one node has done adequate computational work to other nodes on the network. Proof-of-Stake is a similar consensus mechanism but relates more to cryptocurrency. In literature, there are several attempts to synthesize the blockchain’s digital forensics capabilities from a technical standpoint. After analyzing these review papers, we identify there is still a lack of classifying the solutions following an artifact classification taxonomy, understanding the broader impact of technical solutions following theories such as socio-technical theory, and summarizing the research contributions towards developing a design theory. The objective of this paper, thus, is to develop research propositions regarding the digital forensics use-case of blockchain following artifact taxonomy, socio-technical theory, and design theory.",CS,AI_ML,0.85,Extracted from log - paper 1637
Risk Factors for Malicious Insider Threats – An Analysis of Attack Scenarios,"Regarding malicious (intentional) insider threats, the damage potential is particularly high. There are many suggestions for prevention and defense, but there is a lack of reliable knowledge about which measures actually are effective in regard to this kind of attacks. Using the serious game ""Operation Digital Butterfly"", 40 roles and attack scenarios as well as possible countermeasures were collected in 10 game sessions with participants from research institutions, companies and public authorities. These are the starting point for an analysis of effective risk factors at the levels of technology, organization, people and infrastructure, which is presented in this paper.",CS,AI_ML,0.85,Extracted from log - paper 1638
"A Process-Based Approach to Information Security Investment Evaluation: Design, Implementation, and Evaluation","In recent years, the importance of information security has grown significantly due to the rise of cyber threats and attacks. However, evaluating investments in information security can be challenging, as traditional methods often rely solely on monetary factors and fail to capture the dynamic nature of business processes. This paper introduces a novel process-based evaluation method for assessing the effect of investments in information security on business processes. The paper outlines practical design requirements for the method and its instantiation as a prototype, which is then evaluated using a three-step approach with two companies from the healthcare and energy sectors. The evaluation results demonstrate the proposed method's usefulness in information security investment decisions. This paper contributes to the field of information security investment evaluation by providing a proof-of-concept that potentially paves the way for future research to increase the quality and economics of investments in information security.",CS,AI_ML,0.85,Extracted from log - paper 1639
Predicting Analysts' Needs for Explainable Artificial Intelligence (XAI) in Cybersecurity Analysis,"As cyberattacks become more sophisticated and prevalent, analysts face several new challenges, such as a data influx and a high percentage of false alerts. Many AI-driven tools have been utilized in detection systems to help analysts find anomalies. However, the black-box nature of many AI models makes it difficult for analysts to utilize them effectively. This leads to a growing need for Explainable AI (XAI) to make AI models more transparent. However, there is a lack of personalized XAI that would enable analysts to receive explanations tailored to their needs. To address this problem, we develop a system that can predict an analyst’s need for explainability based on a Bayesian Network (BN). We first identify the factors that impact analysts' needs for explainability and then use a data-driven method to build the network structure. The performance of the system will be evaluated in an experiment involving twenty participants.",CS,AI_ML,0.85,Extracted from log - paper 1640
Re-Assessing Privacy in the Blockchain-based Metaverse,"The metaverse refers to merging the physical and virtual environments, providing new opportunities for transferring real-world identities into the digital space. Mapping identities into the metaverse is done using blockchain tokens. Therefore, we define the set of metaverse services associated with a blockchain as “blockchain-based metaverse” (BM). While blockchain applications have long been considered more privacy-preserving than centralized applications, we argue that the underlying premises of this assumption have changed for the BM context. Since blockchain transactions are pseudonym-based, it is generally possible to link them to real-world identities. This probability has increased in the BM, making inferences about identities more likely, which completely changes the privacy situation. In our paper, we conceptually re-assess the privacy assumption of blockchains and suggest four propositions to demonstrate how the privacy level changes in the BM. We also provide technical and organizational measures to address the privacy shortcomings and present a research agenda.",CS,AI_ML,0.85,Extracted from log - paper 1641
Detecting Privacy Threats with Machine Learning: A Design Framework for Identifying Side-Channel Risks of Illegitimate User Profiling,"Privacy leakage has become prevalent and severe with the increasing adoption of the internet of things (IoT), artificial intelligence (AI), and blockchain technologies. Such data-intensive systems are vulnerable to side-channel attacks in which hackers can extract sensitive information from a digital device without actively manipulating the target system. Nevertheless, there is a scarcity of IS research on how businesses can effectively detect and safeguard against side-channel attacks. This study adopts the design science paradigm and lays the groundwork for systematic inquiry into the assessment of privacy risks related to side-channels. In this paper, we a) highlight the privacy threats posed by side-channel attacks, b) propose a machine learning-driven design framework to identify side-channel privacy risks, and c) contribute to the literature on privacy analytics using machine learning techniques. We demonstrate a use case of the proposed framework with a text classification model that uses keystroke timings as side-channel.",CS,AI_ML,0.85,Extracted from log - paper 1642
Roles of Peer Support and Perceived Organizational Support in Reducing Nonmalicious Information Security Violations: The Interacting Effect of Personal Goal Setting,"Nonmalicious information security violations (NISV) that employees engage in can pose a problem to organizations. The Social Cognitive Theory was used to investigate the roles of three constructs, (a) peer support, (b) perceived organizational support, and (c) personal goal setting in reducing NISV. The study also examined the interactions among the three variables. Relevant hypotheses were formulated, and survey data were obtained from 204 German workers. The partial least squares technique was used for data analysis. The results showed that the three factors could reduce employee engagement in NISV. Another key finding of the study indicates that the interaction between personal goal setting in reducing engagement in NISV and peer support significantly reduced employee participation in NISV. In contrast, the interaction between perceived organizational support and personal goal setting related to NISV avoidance did not. The implications of the study’s findings for practice and contributions to research were discussed.",CS,AI_ML,0.85,Extracted from log - paper 1643
Revisiting the Privacy Paradox: A Novel Framework for Privacy Calculus,"This paper aims to address the gaps in the existing literature on the privacy paradox and privacy calculus by exploring the role of contextual and situational factors, such as the specific circumstances in which privacy decisions are made, and the influence of individual moral value differences across diverse populations. To achieve this, we present three research questions that aim to shed light on how users' privacy values influence paradoxical behavior and decision-making in online contexts. Our research is expected to contribute to a more comprehensive understanding of the privacy calculus framework and provide insight into how individuals' privacy values and situational factors shape their privacy-related decision-making. Ultimately, our research aims to improve individual's ability to protect their personal information in online contexts, by developing effective privacy policies and interventions that are sensitive to cultural differences and values.",CS,AI_ML,0.85,Extracted from log - paper 1644
Conception and Requirements Identification of Gaia-X-Based Service Offerings,"Gaia-X is an initiative to develop the next generation of a secure and federated European data infrastructure to promote digital sovereignty for data exchange to fuel innovations. This paper introduces the basics of Gaia-X, in particular the mobility domain, followed by the federated system and its standards. In addition, a research methodology is presented to help conceptualize and derive requirements for service offerings on a Gaia-X-based data space. This is elaborated with a use case from the project “GAIA-X4 AMS” (Gaia-X4-Advanced Mobility Services), which reflects implementation in Gaia-X ecosystems and its added value.",CS,AI_ML,0.85,Extracted from log - paper 1645
Design Knowledge for GAIA-X-compliant Ecosystems: A Literature Review,"Integrating digital technologies offer companies a range of potentials for optimizing business processes, exploring new business models and collaborating along value chains to co-create value. However, this integration increases complexity, especially within SMEs that lack extensive resources to drive data transformation. GAIA-X offers potential solutions in form of federated services as a low-threshold way to participate in federated data ecosystems. Thus, in this paper we survey the current state of research on GAIA-X by means of a literature review extracting and collecting design knowledge as well as to prepare further implementation within the scope of our research project. Therefore, we build a concept matrix in which we differentiate the identified body of knowledge by three concepts with 18 characteristic expressions in total. Our analysis of the identified papers highlights architectural approaches for designing a GAIA-X compatible data ecosystem, augmented by additional factors to consider when designing these ecosystems.",CS,AI_ML,0.85,Extracted from log - paper 1646
Designing Emerging Social Determinants of Health Apps for Novice Digital Health Users,"This paper investigates the design considerations for health information technologies (HITs) that are aimed at supporting lower socioeconomic status (SES) individuals in addressing social determinants of health (SDOH) needs. While emerging technologies offer new opportunities for improving quality of life outcomes, many lower SES individuals may be novice users of digital health technologies, including SDOH technologies. To address this gap, we conducted qualitative research using user-centered design tools during the development of an app designed to help individuals address SDOH needs. Our findings provide insight into the unique characteristics, needs, and preferences of lower SES digital novices, and highlight multiple aspects of their eHealth literacy. We discuss the implications of these findings for designing and implementing HITs for novice users.",CS,AI_ML,0.85,Extracted from log - paper 1647
Gratitude with Expectations: Exploring the Role of Restaurant Delivery and Food Availability on Online Customer Reviews During the COVID-19 Pandemic,"Online reviews have become a popular tool for understanding consumers' perceptions of service and product quality. The COVID-19 pandemic has significantly impacted the restaurant industry, including negative impacts on profitability. This study utilizes gratitude theory and expectation confirmation theory to investigate how food delivery and food desert status moderated the impact of the pandemic on consumers' perceptions of restaurant quality. We find that consumers still held expectations for restaurants despite the COVID-19 pandemic crisis and their lack of food availability. Our findings also highlight the importance of food accessibility and availability, particularly in underprivileged communities during a crisis. This study has broader implications for businesses operating during crisis events providing insights into how they can improve their service and product quality to meet the needs of their consumers.",CS,AI_ML,0.85,Extracted from log - paper 1648
Controversies' Roots in Digital Business Models-The Case of Academic Publishing,"Digital business models (DBMs) are built on digital technologies with complexity-inducing characteristics. Thus, they form ever-more complex (eco)systems as they connect heterogeneous actors for value co-creation and blur the boundaries of organizations, markets, and industries. The co-existence of actors’ multiple logics in pluralistic settings has been associated with tensions and controversies. However, information systems (IS) research has ignored controversies’ role in DBMs in settings of logic multiplicity. Drawing on the ‘orders of worth’ framework, we follow the controversies over DBMs in the academic publishing ecosystem. Building on our in-depth case study data, we develop a model highlighting three roots of controversies in sociotechnical DBMs. We add to the literature by furthering our understanding of controversies’ role in DBMs and enriching the community’s methodological toolbox in applying a theory from French pragmatic sociology to study pluralistic contexts.",CS,AI_ML,0.85,Extracted from log - paper 1649
Women Mentoring Programs to Reduce the Gender Gap in IT Professions: A Literature Review and Critical Reflection,"Technical professions and industries remain heavily male-dominated. To counteract this imbalance, measures are being taken, including mentoring programs for women. In this study, a literature review is conducted to answer the research question of how women's mentoring programs need to be designed in order to contribute to the reduction of the gender gap. The results of 13 empirical studies from 2013 to 2022 are analyzed and 21 factors influencing the design of women's mentoring programs are identified and grouped under three headings (relationship aspects, content-related aspects, and organizational aspects). On this basis, an exemplary women's mentoring program is discussed, the core of which consists of a hybrid e-mentoring program. While mentoring programs are considered helpful at the individual level, closing the gender gap at the structural level definitely needs more focus from both research and practice.",CS,AI_ML,0.85,Extracted from log - paper 1650
Designing Culturally Appropriate Hackathons to Increase Data Literacy in Indigenous Communities,"Data literacy skills are extremely important, yet it is still lacking in many Indigenous communities across Canada due to systemic barriers faced by Indigenous peoples. Hackathons have been used as a common approach to promote data literacy. However, hackathons are socio-technical innovation processes in which local cultures and values play a role in their conduct and outcomes. Hackathons that are designed with Indigenous cultures and values in mind can be more effective at promoting data literacy for Indigenous peoples, yet there is limited guidance on how to do so. In this paper, we adopt the Action Design Research approach and Indigenous research methods to collaboratively develop and deliver culturally appropriate hackathons to increase data literacy within Indigenous communities. During this process we will also co-develop a suite of design principles to guide the design of these hackathons.",CS,AI_ML,0.85,Extracted from log - paper 1651
Impact of social influence on continuous use of mobile game applications,"Mobile games dominate the mobile application market in terms of the share of applications and total revenue. Organizations and developers of mobile game applications are challenged to identify factors that influence users’ intentions and continuous use of the games. From this perspective, we draw on social identity theory (SIT) and plan to identify the moderating effect of social influence on the relationship between user satisfaction and continuous use of mobile game applications. This study also aims to investigate the direct impact of social influence on continuous use of mobile game applications. We propose that social influence (compliance, internalization, and identification) will positively moderate the transition from initial adoption to continuous use. We will conduct an online survey in two phases to test the research model and hypotheses.",CS,AI_ML,0.85,Extracted from log - paper 1652
A Longitudinal Examination of AI Fairness on Online Labor Markets,"While online labor markets (OLMs) provide many benefits including flexibility and data-driven AI matching systems, gender and other social biases have been shown in OLMs, and research demonstrates AI can also perpetuate bias. However, previous OLM research assumes bias is static over time and independent of the AI algorithm. To help design OLMs that minimize the detrimental impact of biases on marginalized social groups, we investigate the interaction among individual characteristics and AI sources of biases over the long-term and evaluate auditing strategies using an agent-based simulation model. We then plan to develop and empirically test a framework to evaluate AI fairness and the interaction of different biases on OLMs and test an audit strategy to mitigate biases. We plan to extend the literature on OLMs by integrating fairness and intersectionality research to evaluate the impact of biases.",CS,AI_ML,0.85,Extracted from log - paper 1653
Examining Underrepresented Communities’ Intention Towards Digital Entrepreneurship: A Dual-Theory Framework,"Digital entrepreneurship has been touted as accessible with promises of opportunities for all and has the potential to create a more inclusive entrepreneurial environment because, compared to traditional entrepreneurship, it can lower the barriers to start a new venture. Yet, there are concerns that the underrepresented communities face the challenge in engaging in digital entrepreneurship due to lack of technology access, usage, and skills. This paper proposes a conceptual framework based on a dual-theory perspective from Theory of Planned Behavior (TPB) and Technology Acceptance Model (TAM), to examine factors affecting the underrepresented communities’ intentions toward digital entrepreneurship. We plan to conduct a survey study at Historical Black Colleges and Universities (HBCUs), where the students’ population are representative of the underrepresented communities, to identify what are the key factors that influence the intentions of this sector towards selecting digital entrepreneurship as a career option.",CS,AI_ML,0.85,Extracted from log - paper 1654
Native American Rural Community Digital Divide: Student Insights,"The digital divide continues to be an issue for many Native American individuals in rural tribal areas. This research used a qualitative grounded theory method from the data collection of semi-structured interviews with Native American university students. The open coding of the transcribed responses was used to analyze the text data from individual Native American experiences. The data analysis codes included cost, location, access, digital literacy, and technology knowledge as continuing issues. The coding also shows limited technical support or training availability in the communities. The absence of technology use increases the need to understand factors that remain digital divide barriers for Native American communities. The digital divide - individual experiences model (DD-IEM) is based on three main categories: community, education, and home environments. Six propositions produced the DD-IEM that encompasses digital environments within the three settings that are unique to each individual.",CS,AI_ML,0.85,Extracted from log - paper 1655
ICT for Good: Digital Accessibility in Local Government,"This paper describes a case study of a local government in the United States that is in the process of developing an inclusive and accessible ICT infrastructure for the delivery of e-government resources and services to its citizens with specific attention paid to people with disabilities. Developing such an inclusive infrastructure has proven to be difficult for many municipalities. Attempts to comply with the legal requirements to improve the possibilities for civic engagement for people with disabilities have met with mixed results. The research reported here focuses on a municipality engaged in a complex, user-driven initiative intended to transition its existing ICT infrastructure to one that is more inclusive and accessible to all.",CS,AI_ML,0.85,Extracted from log - paper 1656
Exploring the Interactions between Social Movement Activists and Target Organizations on Social Media,"Social movements are recognized as significant agents of change in modern societies. Their influence is increasingly channeled through social media platforms to mobilize support and direct collective action toward targeted organizations. However, targeted organizations also resort to social media channels to counter the actions of social movements that are directed at them. There is little research on organizations’ responses to movement's actions on social media. This study endeavors to explore the dynamics of interactions between social movement activists and their target organizations by utilizing narrative network analysis. Drawing on framing theory, this research endeavors to elucidate how social movement actors employ various frames. Specifically, we examine the Twitter accounts of various Canadian banks and activists targeting them for their investments in fossil fuels. We analyze the narrative networks and counter-networks to develop a theoretical understanding of how the social media interactions between activists and target organizations evolve over time.",CS,AI_ML,0.85,Extracted from log - paper 1657
Psychological factors affecting social media usage: A U&G theory perspective,"The COVID-19 pandemic has led to a significant increase in social media usage, raising concerns about its potential impact on mental health. The pandemic has created unique stressors and challenges that have worsened the mental health conditions of individuals due to prolonged social media usage. Hence, this study explores the psychological factors affecting social media usage post-pandemic. A mixed-method approach was utilized grounded on U&G theory to identify fear of missing out, peer pressure, self-esteem, loneliness, social comparison, and habit as factors affecting social media usage. The study found that those with higher levels of peer pressure, social comparison, habit and fear of missing out (FOMO) tend to use social media more frequently, suggesting its use as a coping mechanism. The study emphasizes the need for continued investigation to understand the complex relationship between social media usage and mental health post-pandemic.",CS,AI_ML,0.85,Extracted from log - paper 1658
Exploring Overall Service Quality and Customer Contribution in Online-to-Offline Commerce,"Customer contribution is the most important in the company’s profit. However, due to difficulties in data collection, little research has investigated the antecedents and measurement of customer contribution. This study mainly explores the effect of overall service quality and cloud trust on consumer contribution to enhancing online customers’ actual purchasing behaviors. Through literature review and expert interview, we identify five factors of overall service quality: website design, fulfillment, customer service, security/privacy, and third-party payment. The research model treats cloud trust as a second-order reflective construct driven by perceived warranty, perceived competence, consumer confidence, source credibility, and diagnosticity. The hypotheses proposed by the study help develop the best customer value management strategy in the electronic commerce industry.",CS,AI_ML,0.85,Extracted from log - paper 1659
Disasters Information-Seeking Behavior via Social Networking Sites,"Disasters bring uncertainties and substantial health, economic, social, and psychological challenges. Effective management of disasters requires a thorough understanding of their causes, nature, and consequences. Affected individuals may, therefore, seek instant, trustworthy information from different sources, including social networking sites (SNSs). This study investigates what leads individuals to seek disaster information on SNSs. The investigated potential drivers are source credibility (SC), argument quality (AQ), self-efficacy (SEF), perceived usefulness of information (OUI), and behavioral intention (INT). The results confirm the newly emerged source & argument quality (SAQ) construct, SEF, and PUI are significant antecedents of INT, which in turn influences disaster information-seeking behavior (ISB) via SNSs.",CS,AI_ML,0.85,Extracted from log - paper 1660
The Influence of Professional Embeddedness and Public Reputation on Critic Review Behavior,"As one popular form of online word-of-mouth, critic reviews are provided by experts as a quality signal. In this research-in-progress, we consider two forms of social features of critics’ reviewing environment and their impact on critics’ reviews: critics’ professional reputations based on how embedded they are in their professional networks, and critics’ public reputations enabled by reviewing platforms. We collected a dataset of 68,450 critic reviews posted by 716 critics for 3,654 movies from Rotten Tomatoes and constructed these critics’ professional networks based on the publications they work for. We used social network analysis techniques to obtain each critic’s embeddedness in their networks and used their “top critic” status to capture their public reputation. We discussed future analysis techniques and anticipated contributions.",CS,AI_ML,0.85,Extracted from log - paper 1661
Insights on Attention Capture Dark Patterns in Social Networking Sites,"Over the last decade, organizations have almost doubled the users' average daily time spent on social networking sites. A potential driver is the incorporation of attention capture dark patterns in social networking sites, which are designed to maximize users' time spent, daily visits, and interactions with social networking sites. However, up to this date, the actual impact of attention capture dark patterns on users is largely unexplored. Our research aims to bridge this gap by examining the impact of attention capture dark patterns on users through a full factorial between-subjects vignette study. We expect our final research to contribute to dark patterns research by generating insights into how users respond to attention capture dark patterns. This enables us to derive practical implications for social networking sites' interface and interaction design.",CS,AI_ML,0.85,Extracted from log - paper 1662
Online Sex Trafficking Policy Research: A Systematic Review,"Online sex trafficking cases have been increasing every day. The field of Information Systems (IS) has not significantly explored research on online sex trafficking specifically to evaluate existing literature and provide future research directions. In this paper, we use Latent Semantic Indexing (LSI) to review the literature on online sex trafficking. Our focus is on policies related to online sex trafficking. We then summarize our findings and suggest a future research agenda.",CS,AI_ML,0.85,Extracted from log - paper 1663
Immediate and Long-term Effects of Live Streaming Selling on Product Sales,"Live streaming commerce has gained considerable traction in recent years, which transforms the traditional online shopping experience by enabling real-time interactions between streamers and customers with the leverage of live streaming technology. While existing studies have suggested that live streaming selling could enhance sales by increasing customers’ purchase intentions, scant research has confirmed this effect with large-scale empirical sales data. Additionally, extant literature failed to distinguish between the immediate and long-term impact of live steaming on product sales. Based on the data collected from Amazon Live and Amazon.com, this study examines the immediate and long-term influence of live streaming on product sales. By using discontinuous growth modeling, the preliminary results show that product sales will increase immediately after live streaming selling, but will have a declining trend in a long run, strongly supporting our hypotheses. These findings can provide significant theoretical and practical implications for live streaming commerce operation.",CS,AI_ML,0.85,Extracted from log - paper 1664
How negative emotions spread on social media: the case of celebrity suicides,"Social Media can spread messages, emotions, and behaviors among large audiences. Particularly, the consequences of emotion propagation become alarming when negative and shocking events like celebrity suicides happen and influence many vulnerable users. Analyzing social media discussions enables us to understand the mechanisms by which negative emotions spread, and also design effective health interventions. Here we investigate the suicide events of four celebrities and the subsequent Twitter discussions that appeared in the form of cascades – chains of retweets. By using a state-of-the-art BERT-based language model to identify emotion scores, we find that sadness and fear are the leading emotions expressed in each event and that the speed, size, and lifetime of dialogues vary depending on their emotional composition. Further analysis aims to provide new theoretical explanations.",CS,AI_ML,0.85,Extracted from log - paper 1665
To Skimp or Step in: Does Timely Service Recovery via Social Media Win Customers’ Hearts?,"Due to the public nature of the interactions and the ability to foster quicker responses than traditional media, social media has become a powerful channel for both travelers to post their trip-related questions and issues, and airlines to build their brand images. Based on the Signaling theory and Expectations-Confirmation theory, we propose that airline service recovery via Twitter will boost profitability, and such influence is more vital for network airlines than focused airlines and airlines with higher corporate social responsibility (CSR).",CS,AI_ML,0.85,Extracted from log - paper 1666
Fake Review Detection - The Value of Domain-Specificity,"The issue of fake reviews, a perennial challenge faced by e-commerce companies, has been worsened in recent years. In this paper, we investigate the trade-off between the application of general-purpose and domain-specific model by selecting training data using transformer-based models to identify fake reviews. Therefore, we compare two scenarios using different set ups of data selection. First, a general-purpose model was identified and applied on specific domains. Afterwards, domain-specific models were trained and tested. Then, the results from these scenarios were compared, yielding the conclusion that models trained on data from a similar product category outperform general-purpose models in classification performance up to 21% (on average by 4%). Our findings send an important message to e-commerce companies to rethink their strategy on training general-purpose models to identify fake reviews to create several, more domain-specific models on that task according to present data domains.",CS,AI_ML,0.85,Extracted from log - paper 1667
Values of Social Commerce: Influence of Affordance,"Social commerce is growing. This paper aims to understand how users interact on social commerce platforms. The design of the social commerce platform is unique, giving both buyers and sellers autonomy. The paper seeks to understand the consumer values of both buyers and sellers, how those have led to the social commerce platform design, and what feature enabled affordances help to satisfy the consumers' values. The study will help system designers better design social commerce platforms and understand some of the consumer pain points. Additionally, in literature, the seller behavior is understudied, so the nature of the platform and the autonomy it provides to the seller allows us to study seller behavior. We aim to understand consumer behavior better, how sellers and buyers interact on a social commerce platform, and how these platforms enable those interactions to facilitate transactions.",CS,AI_ML,0.85,Extracted from log - paper 1668
A Chat-based personal recommendation mechanism with opinion intelligence,"With the rapid development of the Internet and mobile devices, users around the world have accumulated a large amount of comment data on the Internet. Nowadays, users tend to search for information on the Internet before making purchase decisions. How to provide personalized services while avoiding information overload has gradually become an important issue. This study proposes a dialogue-based personalized recommendation mechanism with opinion intelligence. In addition to understanding user preferences through dialogue and interaction with users, we further analyze the content of reviews and the social influence of review authors to better help users. The results show that the dialogue mechanism proposed in this study has a good recommendation effect and can effectively improve user experience.",CS,AI_ML,0.85,Extracted from log - paper 1669
SNS Guilt Feelings: Causes and Influence on Discontinuance,"Although experiencing guilt feelings after social networking service (SNS) usage has become a prevalent phenomenon among SNS users, these SNS guilt feelings-and especially their causes-have received little scholarly attention. This paper therefore focuses on why SNS users may develop SNS guilt feelings and their impact on discontinuance intention. We utilize the lens of normative dissociation to develop a model to identify causes of SNS guilt feelings and apply a quantitative research approach to test the model. We contribute by demonstrating that two identified causes-perceived time spent and perceived procrastination-significantly influence SNS guilt feelings, while perceived meaningfulness does not. Furthermore, SNS guilt feelings influence discontinuance intention and mediate the relationship between several constructs and discontinuance intention. By employing the perspective of normative dissociation, we reinforce the relevance of SNS guilt feelings.",CS,AI_ML,0.85,Extracted from log - paper 1670
Influencing the Influencers? The Case of @CelebJets and the Role of Social Media in Empowering Citizens to Conduct Climate Justice Activism,"Online climate change activism is growing. However, its effectiveness is not yet firmly established. One of the challenges is for citizens to make sense of the vast amount of data to understand the impact of specific behaviors. The aggregation of publicly available information in a comprehensible and playful manner could potentially trigger effective evidence-based activism. However, it is not yet clear if and how such an intervention works. In this paper, we make a preliminary analysis of a use case of such an approach by investigating the @CelebJets phenomena, which automatically tracks and shares the private jet flights of celebrities on Twitter. Our analysis demonstrates that this approach can elicit user engagement and provide pertinent material for the press, potentially leading to a shift in behavior, particularly among celebrities who are promoting sustainability.",CS,AI_ML,0.85,Extracted from log - paper 1671
Relationships between Involuntary Visual Cueing and Sensemaking of YouTube Videos,"Sensemaking plays a critical role in the everyday use of social media, which are increasingly visual as video, images, and other visual content permeate. Yet, research on the sensemaking of visual information on social media is minimal. This study addresses this research gap by investigating the relationships between involuntary visual cues (color contrast and visual complexity) and making sense of videos on YouTube and Reddit. It finds that color contrast has a significant association with sensemaking on both platforms and that visual complexity is a significant predictor of sensemaking only on Reddit. Our study adds to the research on sensemaking on social media. It expands social media analysis from textual to visual information and contributes to the knowledge of visual sensemaking on social media. It also provides practitioners with guidelines for designing videos that facilitate visual sensemaking.",CS,AI_ML,0.85,Extracted from log - paper 1672
Social Norms and Misinformation Sharing of Politicized Products,"Misinformation has been on the rise, and in many cases, misinformation is intentionally shared for political reasons. This phenomenon first started with political news but now has spread in consumer products. To understand how misinformation sharing occurs differently between politicized versus non-politicized products, we conducted an experiment and surveyed 800 respondents in an online crowdsourcing platform. The results show that for a politicized product, individual bias disposition and social norms indeed sway an individual’s evaluation of information credibility, her trust, and intention to share. However, this effect is not observed for non-politicized products. This calls for more research to understand the role of social norms in stopping the spread of misinformation.",CS,AI_ML,0.85,Extracted from log - paper 1673
Online Video Sharing Platform: Exploring Factors Affecting User Tipping Behavior,"In this study, we examine user tipping behavior on China's leading online video platform, BiliBili.com. We analyze a dataset of 57,767 videos and categorize them into two types: knowledge content and entertainment content. We investigate the differences in user tipping behavior between these content types and also explore the impact of contributor experience and content length on tipping behavior. Additionally, we investigate the moderating effect of content type on these influencing factors. Our findings reveal that compared to entertainment content, knowledge content receives fewer tipping rewards from users. We also find that contributor experience has a negative impact on user tipping behavior, while content length has a positive impact.",CS,AI_ML,0.85,Extracted from log - paper 1674
Can Influencers Overcome the Impact of Weak Ties to Achieve Viral Propagation? Applying Social Identification & Social Capital to Predict Virality,"Marketers have struggled with the question of how to achieve virality. In this paper we seek to understand and provide a model that predicts virality of their content. We draw from the social identity theory and social capital theory to argue that in-group/out-group classification plays a greater role in predicting virality than social capital cues like tie strength and number of connections. Furthermore, we use a decision-making approach to understand users' processes as they decide on whether or not to propagate content. Using the Lens Model and Fast and Frugal Decision-Making approach, we propose a 3-step model that integrates both social identification and social capital cues to predict user content propagation behavior. The model expands our understanding of content propagation behavior and virality. The model can be tested through experimental work to further understand the decision-processes of users that lead to virality.",CS,AI_ML,0.85,Extracted from log - paper 1675
Privacy is Important! Or not? – Commenting and Liking Under Confirmation Bias on Social Media,"Social media provides great opportunities to engage with content based on commenting or liking. Especially confirmation bias was found to be highly influential in engaging with content. However, social media also imposes privacy risks on its users. Past research found that trust in the platform facilitates, while privacy risk beliefs reduce engagement on social media. However, research did not address how confirmation bias interacts with privacy concepts on social media. To address this research gap, we conducted a scenario-based study and manipulated confirmation bias in the context of COVID-19 vaccination. The results show that privacy risk beliefs are heavily diminished under confirmation bias to the point where trusting beliefs dominate. We provide a novel view of the influence on commenting and liking by entangling another piece of the privacy paradox.",CS,AI_ML,0.85,Extracted from log - paper 1676
Referral Intention vs. Continuous Referral Intention: Incentive Mechanism in Multi-tasking Social Referral Programs,"Social referral reward programs (SRRPs) aim to incentivize existing customers to recommend a product or service to others. Based on the reward threshold, we classify social referrals into two categories: single-tasking social referral (SSR) and multi-tasking social referral (MSR). Considering that MSR involves multiple responders, we explore how to design an effective reward mechanism in this new context. Our primary interests in outcomes include users’ willingness to recommend and their continuous referral intentions. Drawing from fairness theory and loss aversion theory, we propose three reward models based on the keeping percentage of rewards obtained: keep-it-all (KIA), discounted keep-it-all (DKIA), and all-or-nothing (AON). We designed an experiment to test the hypotheses regarding the effects of reward types on referral intention and continuous intention. This study will provide important implications for research and practice in designing an effective reward mechanism in MSR.",CS,AI_ML,0.85,Extracted from log - paper 1677
5504 Sharing Patterns in the Digital Sharing Economy,"Large-scale sharing networks rely on digitally enabled platforms that connect users beyond the limits of social and geographical proximity. The phenomenon of the Digital Sharing Economy (DSE) embraces a broad diversity of sharing patterns. This paper aims to present an exhaustive classification of these patterns. It illustrates 5504 different sharing models that vary in the attributes of their fundamental business model components, i.e. the type of shareable resources, resource providers, resource receivers and sharing practices. This classification can be useful to characterize the DSE and visualize sharing and exchange patterns within its networks. In particular, the classification of digital sharing models can help platform providers position themselves in the broad spectrum of market and non-market-based sharing and tap into practicable business model innovations in the DSE domain.",CS,AI_ML,0.85,Extracted from log - paper 1678
Why Do I Share My Predictions of Stock Returns in Online Communities? An Empirical Study on StockTwits,"Online investment communities have been widely adopted by investors to disclose investment-related information, such as predictions of stock returns. Although it benefits both platforms by attracting more users and other investors by providing additional finely-processed information, it may hurt publishers due to the potential loss of their unique valuable private information. Therefore, understanding why users share their own predictions in online communities becomes an important issue. Drawing on the ability-motivation-opportunity framework, we seek to identify three important factors influencing users’ willingness to share predictions. Utilizing data obtained from StockTwits, our preliminary results show that the number of followers, prediction accuracy, and historical stock performance negatively affect users’ sharing of their predictions of stock returns. Our findings can contribute to the literature on information sharing and provide managerial implications for online investment communities.",CS,AI_ML,0.85,Extracted from log - paper 1679
Designing Intelligent Characters Representing Organisations in Digital Worlds,"Advances in technology have led to the creation of virtual worlds where people can interact with intelligent characters representing organizations. As these interactions increase, it's crucial to understand digital character design and its societal and business implications. This research aims to investigate the design elements of digital characters and provide prescriptive knowledge for organizations. Using the design science research paradigm, we'll answer the question of how to design digital characters that best represent a company. We draw on theories from human-computer interaction, marketing and brand design, corporate communications, and character design from industries such as game design and screenwriting.",CS,AI_ML,0.85,Extracted from log - paper 1680
Exploring Socio-Technical Factors of Group Engagement in Metaverse Gaming: A Multi-Method Approach,"As an emerging virtual platform, the Metaverse is increasingly applied to various collaboration contexts such as group gaming, team collaboration in work, and online group learning. To investigate users’ group engagement in the Metaverse, this study applies a socio-technical perspective to the context of Metaverse gaming. We propose a theoretical model and test it with a multi-method approach, consisting of structural equation modeling (SEM) and fuzzy-set qualitative comparative analysis (fsQCA). The SEM results show that all the proposed social (social presence and group harmony), individual (demand toward social activities and acceptance toward virtual world), and technological factors (self-representation) positively determine users’ group engagement in Metaverse gaming. Our fsQCA results further reveal the interdependent relationships among those factors (i.e., multiple configurations), which are nuanced under different contexts of their gender and level of prior experience. While the current findings are insightful and useful, they should be further studied through future research.",CS,AI_ML,0.85,Extracted from log - paper 1681
"Hello, Avatar: The impact of avatars on individuals’ emotions and behaviors from three congruity perspectives","The rapid growth of social media platforms in the digital age has led to an explosion of the metaverse, injecting new vitality into avatars. Previous research has mainly focused on the application of avatars in virtual environments, with limited research on the impact of avatar interactions on individuals, except for avatar similarity, customization, and the uncanny valley effect. Considering the positive and negative perceptions brought about by avatars to individuals, we adopted a mixed-methods approach to propose a model that reveals the influencing mechanism of individuals’ perceptions of an avatar on their emotions and behaviors from a holistic perspective. This study contributes to existing knowledge and expands dual-congruity theory. Additionally, the findings have practical implications for managers of metaverse platforms and individuals alike.",CS,AI_ML,0.85,Extracted from log - paper 1682
Text-image Relationships in Social Media Communication: A Literature Review,"This study synthesizes the literature on content factors in social media communication within the Information Systems (IS) field and identifies five dimensions of text-image congruence: quality congruence, quantity congruence, emotion congruence, connotation congruence, and information appeal. Utilizing the S-O-R framework, the paper provides a comprehensive understanding of text-image congruence and suggests future research directions in IS. The findings contribute to the knowledge of effective social media communication strategies and lay the groundwork for future research on text-image congruence and relationships.",CS,AI_ML,0.85,Extracted from log - paper 1683
How Does (In)consistent Performance Feedback Affect Business Model Digitalization？,"Despite the merits of business model (BM) digitalization to sustain organizational competitive advantages, many firms are cautious to digitalize their BMs due to the efforts needed and the high risks accompanying them. To date, the existing understanding of the antecedents of BM digitalization is still underdeveloped. We draw on the behavioral theory of the firm to develop a comprehensive understanding of what motivates firms to promote BM digitalization. In so doing, this study explores how (in)consistency between historical and social performance feedback affects BM digitalization. Moreover, we theorize that board experience with BM digitalization from interlock firms is an important contingency that influences these relationships. Based on panel data from 2,373 Chinese-listed firms from 2008 to 2019, the results provide general support for our arguments. This study contributes to the BM digitalization literature by disentangling the critical roles of (in)consistent performance feedback and board experience.",CS,AI_ML,0.85,Extracted from log - paper 1684
The Roles of Opinion Leaders in Elevating Online Health Discussion: Evidence from Reddit,"Social networking sites and online communities are used by billions of people globally. It is important to understand the participation in these collaborative platforms and the roles different participants play. In this study, we examine the roles both local and global opinion leaders play and their influence on the community using content related to Covid mental health from Reddit. We employed social network analysis to identify opinion leaders based on their network position and connections, and we used sentiment analysis and econometric models to examine their impact on online health community participation. Our study reveals that global opinion leaders have more influence on thread participation than local opinion leaders. In addition, negative emotions expressed encourage more participants’ involvement in online health community discussion.",CS,AI_ML,0.85,Extracted from log - paper 1685
The e-government adoption ecosystem from the perspective of stakeholder theory: A case study on the village information systems in Indonesia,"The village information system (VIS) is a form of e-government that villages in Indonesia have adopted. However, many village administrations still have difficulty implementing it. Gunungkidul Regency, Indonesia, is a regency where all villages have successfully adopted VIS in a sustainable manner. This study aims to portray the ecosystem that the adoption or implementation of VIS in the regency has formed. The researchers carried out data collection using observation techniques, document studies, and interviews with entities involved in the management and utilization of VIS in the regency. We used a historical approach and a stakeholder-theory lens to capture the roles and interests of each entity that comprises the ecosystem. Our reveal the role and interest in VIS of regional heads, several local government (or supra-village) organizations, vendors or non-governmental organizations (NGOs), village-owned enterprises (BUMDes), civil society organizations (CSO), and villagers in an ecosystem portrait. They are entities that contribute to the management and utilization of VIS. These results can provide an overview of how VIS must be managed collaboratively by involving various stakeholders. Collaborative governance is still rarely found in e-government applications in general, such as e-service, e-procurement, or e-participation, which only tend to provide general e-government roles such as automation and increasing information flow. The results of this study also offer lessons learned for other village administrations in sustainably implementing the VIS.",CS,AI_ML,0.85,Extracted from log - paper 1686
ACCEPTANCE ANALYSIS OF TECHNOLOGY-BASED PERSONNEL MANAGEMENT INFORMATION SYSTEMS (SIMPATIK) IN SUPPORTING THE IMPLEMENTATION OF E-GOVERNMENT IN THE SEMARANG CITY MANPOWER SERVICE,"In this digital era, information technology has an important role for institutions, companies, communities, and governments. In carrying out the main tasks of the Semarang City Manpower Service, one of them is evaluating employee performance. In assessing the performance of the Semarang City Manpower Service employees using SIMPATIK which was developed by the Semarang City BKPP which was launched in 2018. Where the implementation of the new system needs to be evaluated further. This is due to the failure to implement e-government services, not because of the quality and capacity of the system, but because of the low user acceptance of these services. Thus, it is necessary to analyse the level of acceptance in using SIMPATIK services at the Semarang City Manpower Service whether it can be evaluated further. So that SIMPATIK can still exist to be used for various kinds of services contained in the application by ASN at the Semarang City Manpower Service. The method applied to data collection is the interview and questionnaire method. While the method applied to evaluate SIMPATIK is the Technology Acceptance Model (TAM) by applying four research variables, namely perceived usefulness (PU), perceived ease of use (PEOU), attitude toward using (ATU), and behavioural intentions (BI). To examine the data adopted the method of Structural Equation Model-partial Least Square (SEMPLS). The conclusion reached in this study is that all hypotheses developed in this study were accepted, except for PU on BI rejected because the of T-Statistics (0.846) < T-Table (1.96) and P-s were more than 0.05 with a of 0.398 .",CS,AI_ML,0.85,Extracted from log - paper 1687
Understanding citizen's continuance intention to use e-government services: the case of the Indian railway e-ticket booking site,"The emergence and expansion of e-government initiatives has brought interesting facets of e-services on the internet. To sustain such initiatives, it is important to understand the factors that improve the users' intention to reuse such e-services. This research tries to identify the factors that influence the citizens to avail the e-service provided by the Ministry of Railways, Government of India. The result of this study shows that the two major determinants of technology acceptance model (TAM) namely perceived usefulness and perceived ease of use along with trust and computer self-efficacy explain over 70% of the variance in the user's intention to reuse the e-ticket booking site of the Indian railways. The show that the e-ticketing website should contain user-friendly, high-quality information content and excellent website quality in order to enhance citizen's intent to reuse their services.",CS,AI_ML,0.85,Extracted from log - paper 1688
E-Government Information Systems (IS) Project Failure in Developing Countries: Lessons from the Literature,"E-government information systems (IS) projects experience numerous challenges that can lead to total or partial failure. The project failure factors have been identified and studied by numerous researchers, but the root causes of such failures are not well-articulated. In this study, literature on e-government IS project failures in developing-world contexts is reviewed through the application of qualitative meta-synthesis, design–reality gap analysis, and root cause analysis. In the process, 18 causal factors and 181 root causes are identified as responsible for e-government IS project failures. The most prevalent of the 18 causal factors are found to be inadequate system requirements engineering (with 22 root causes), inadequate project management (19 root causes), and missing or incomplete features (16 root causes). These can be of use to future researchers, policymakers, and practitioners seeking to identify methods of avoiding e-government IS failures, particularly in developing-world contexts.",CS,AI_ML,0.85,Extracted from log - paper 1689
An examination of citizen satisfaction with mandatory e-government services: comparison of two information systems success models,"The of this study is to empirically examine the utility of information systems (IS) success models in mandatory e-government services, as opposed to the volitional ones that have been the focus of previous studies. The models include the technology acceptance model (TAM) (1989) and Seddon’s model (1997), which involve three (ease of use, usefulness and citizens satisfaction) and four variables (system quality, information quality, usefulness and citizen satisfaction). /approach The models were compared based on a survey conducted on 780 foundation year students of government universities in Saudi Arabia. The Saudi Government has launched a mandatory e-government service geared to assist high school graduates in the university academic admission process. The goodness-of-fit and parsimony of fit indices and the explanatory power were used to compare the two models. The structural equation modeling techniques revealed that overall, the two models both exhibited reasonable fit with the collected data, whereas TAM showed the best fit to the sample data and yielded superior goodness-of-fit indices over Seddon’s model. In terms of explanatory power, Seddon’s model predicted 28% (R2 = 0.28) of the variance explained for citizen satisfaction, whereas TAM predicted 21% (R2 = 0.21). All the parsimony of fit indices favored TAM over Seddon’s model. Research limitations/implications This study examined the validity of TAM and Seddon’s model, using citizen satisfaction as the dependent variable to compare them. TAM and Seddon’s model were modified to better fit the current research context of mandatory e-government services; thus, the may not hold for their original or other voluntary settings. In addition, the focus on a single survey for a certain time in a certain territory of mandatory e-government service may have limited the generalizability of the results to other mandatory contexts. Future research should make use of large, cross-sectional samples in different mandatory contexts to enhance result generalization. This study’s can provide e-government practitioners with deeper perceptions of how to address citizen satisfaction with mandatory e-government services. The results exposed usefulness as the common and major construct, having the strongest influence on citizen satisfaction in both TAM and Seddon’s model; thus, maximizing the benefits of e-government services for citizens is crucial to their success. The causal relationship between information quality and citizen satisfaction was not supported. This supports the perspective that e-government services are currently evolving quickly, becoming more integrated and easier-to-use, generally requiring only a few clicks and less information. /This study has extended the assessment of the validity of IS success models to a mandatory IS usage setting. The comparison study of different IS success models is crucial as it acts as a guide for researchers to determine the trade-off between the models used to conduct research on a particular context. The study concludes that TAM is the most parsimonious and universal model for the study of user satisfaction in mandatory contexts. The will provide e-government practitioners with insights into IS success measures suited to enhance the effectiveness of newly and future mandated e-government services.",CS,AI_ML,0.85,Extracted from log - paper 1690
"Mobile infrastructure quality, regulatory quality, government effectiveness: Does e-government development matter?","There is a great deal of interest in how mobile infrastructure quality, regulatory quality, and a nation's capacity to develop efficient e‐government systems affect government effectiveness. The literature ignores the part electronic government plays in either the regulatory quality‐government effectiveness or mobile infrastructure quality‐government effectiveness nexuses, despite the fact that existing research indicates that the effects of these factors on government effectiveness vary across nations. Secondary data for 52 African nations was extracted from World Governance Indicator (WGI), Mobile Connectivity Index (MCI), and United Nations E‐government Database to empirically investigate the mediating relationship of e‐governments using hypothesized model. The positivist paradigm is considered appropriate for this study because it focuses on validating and testing a hypothesized model. The results show a significant relationship between mobile infrastructure quality and e‐government development, regulatory quality and e‐government development, e‐government and government effectiveness. However, there was no significant relationship between mobile infrastructure quality and government effectiveness. Additionally, e‐government was found to mediate the relationship between regulatory quality and government effectiveness and the relationship between mobile infrastructure quality and government effectiveness. This suggests that government effectiveness is enhanced when a country is able to develop and implement a good electronic system. The successful implementation of e‐governance practices offers better delivery of services to citizens, improved interactions with business, citizen empowerment through access to information, greater convenience, and cost reductions. In addition, establishing protections and legal reforms will be needed to ensure, among other things, the privacy, security and legal recognition of electronic interactions and electronic signatures.",CS,AI_ML,0.85,Extracted from log - paper 1691
Watch who you trust! A structured literature review to build a typology of e-government risks,"The information systems, e-business, and e-government literature has unanimously shown that trust and risk are antecedents of the use of information technology and technology-based services. However, a deeper understanding of the relationship between trust and risk, especially when taking into account the extensive knowledge created in fields such as organisational science and psychology, is often missing. With this article, we aim at conceptualizing risk in e-government use. Based on a structured review of the trust-related e-government literature, we derive a typology of relevant e-government risks. We analyse this typology in light of extant trust and risk literature. The typology can be used both to understand the behaviour of system or service users and to design systems and services that can be and are trusted. As such, this research can serve as a basis for future research on the role of trust and risk in designing and using e-government services. The generalizability to e-business services and information systems in general is discussed.",CS,AI_ML,0.85,Extracted from log - paper 1692
INVESTIGATING THE ROLE OF SELF-EFFICACY ON ACCEPTANCE OF E-GOVERNMENT IN TANZANIA,"E-government is the process of delivering government services through electronic media or platforms. It provides alternative options for offering services with minimal need for physical contact. Through E-government, there has been improvement in all sectors on the way citizens access services and share important information. The acceptance of E-government practices and tools is influenced by factors as explained by prominent models such as TAM, UTAUT and TRA. This research involves empirical evidence from the Tanzanian context to e-government self-efficacy has a significant impact on the acceptance of E-government systems. The research employs a survey of 159 respondents followed by analysis using Smart PLS 4. The conceptual framework was developed by extending the Technology Acceptance Model with E-government Self-Efficacy before testing it in quantitative research. Results of the model show that all the relationships were found to be significant. Among others, this research provides theoretical underpinnings to the area of acceptance of technologies as well as providing areas for future research and policy implications.",CS,AI_ML,0.85,Extracted from log - paper 1693
The Effectiveness And Challenges Of E-Government Implementation Through The Media Center In The City Of Palangka Raya,"Public administration is crucial for government operations and public welfare as mandated by the 1945 Constitution of Indonesia. Technology, especially E-Government, has made public services more efficient. Since Presidential Instruction No. 3 of 2003, various government agencies, including local governments, have adopted E-Government to improve services. However, challenges such as isolated information systems, security issues, data inconsistency, and inadequate infrastructure require standardization. The Palangka Raya City Government utilizes information technology through the Media Center, supported by the Ministry of Communication and Informatics, to enhance information dissemination and networking between local, provincial, and central governments. This study analyzes the effectiveness of the Media Center in Palangka Raya, identifying challenges such as a lack of competent human resources, outdated facilities, limited internet access in remote areas, and insufficient administrative budgets. Data were collected through documentation, observation, and interviews, and analyzed qualitatively. The highlight major challenges and suggest improvements for electronic public services, ultimately contributing to increased community engagement and welfare.",CS,AI_ML,0.85,Extracted from log - paper 1694
Analysis and Design of Village Management Information Systems (VMIS) based on MVC and E-Government in Indonesia,"E-Government is the government's program and commitment in the effort to develop electronic-based governance and transforms to facilitate the activities of society and business to the knowledge-based society. E-Governmet can be said to be a system that contains collections of modules that can be integrated with others. Considering the many module components in the e-government authors in this study limits only to population modules such as service and management of population data, KK data, data on population mutations such as population (moving, coming, born, dead) built using the Model View method Web-based and online controller. As well as how to simplify the management of the letter, in addition to facilitate the search population data and information about the development of villagers in each village in real time with terintegerasinya data to each village in addition to facilitate and accelerate the service request and manufacture reporting. So with the existence of this system the kecamatan easier to see the development of data of the population of each village and with this system the kecamatan and village easier to manage the data letter and can facilitate in sending mail to each village. With this research, it is expected Sub-district offices can provide improvement of information service and also data processing of its population.",CS,AI_ML,0.85,Extracted from log - paper 1695
Information Systems User Satisfaction: Application of a model for e-Government,"The growing increase in the digitization of government services points to the need to measure the success of the information systems used. One solution to evaluate these systems is to measure user satisfaction. The aim of this article is to present a model for measuring user satisfaction in information systems for government use. The research was of the descriptive type, carried out through a questionnaire to a sample of 1260 users of the system. The research model was calculated using structural equations via variance. The results revealed that the DeLone & McLean model is an adequate model for measuring government information systems. In this study, the model managed to explain 76.9% of user satisfaction in information systems, with perceived utility, quality of information, quality of the system and quality of services being the most important variables, respectively.",CS,AI_ML,0.85,Extracted from log - paper 1696
Assessment of E-Government Portals,"This research revealed the importance of public service web portals for an e-government information system. An e-government portal is interacting with its administrators, citizens, businesses and other governments helping them increase their operations performance. The authors have developed, modeled, formulated and compared an efficient assessment framework for e-government portals. In order to accomplish such task many quantitative factors and indicators were taken under consideration; also, other frameworks have been studied and compared. The authors focused on the web portals services quantity that the interested parties should use, in order to create an well designed public services’ web portal. This research provides a framework model to evaluate the basic common digital public services that a government offers to its interactive stakeholders, so that all other countries across the world can predefine weaknesses and strengths, improve existing or formulating new e-services. The importance of the assessment framework model is thoroughly explained through the results.",CS,AI_ML,0.85,Extracted from log - paper 1697
"Violated factors in building citizen-centric e-government websites: insights from the performance of the federal, state and local governments websites in Malaysia","Common evaluation tools on e-government websites are available globally and locally to standardise and improve the quality of information and services. However, a commonly ignored aspect is the way to obtain detailed measurements of factors influencing citizen centricity; in other words, how official websites cater to the needs and contributions of citizens at different levels of government. Thus, this paper aims to apply a citizen-centric framework in evaluating the e-government websites of three different levels of authority in Malaysia: federal, state and local. /approach The adapted citizen-centric checklist for e-government websites (aCCEW) with 40 characteristics across four components – openness (21), transparency (5), participation (10) and responsiveness (4) – was adopted to evaluate case studies of 36 government agency websites in Malaysia. Any conformity between the characteristics was marked using a binary measure, and the citizen-centric was calculated for each component/characteristic. Through website observations, ratings and descriptive comparisons, this study found that the aCCEW is a useful tool, especially for identifying certain critically violated factors. These were deficiencies in e-decision-making, revealing successful initiatives created through open data, revealing fund transfers and expenditure records and the level of social media responsiveness. Research limitations/implications The research contributes theoretically by improvising characteristics in the CCEW to become aCCEW and testing it in multiple levels of government in Malaysia to see its applicability to be adopted in other similar research of e-governments. This could become a new benchmark through the additional research insights it offers into similar perspectives of public s realisation in e-government website design that focuses on more than merely functionality. Attempt to relate the violated factors and strengths of aCCEW website design components to the level of centralisation (power) of federal, state and local governments was also genuine in the e-government research. Regardless of the many different government systems, federal, state and local governments can benchmark the examples assessed in this study, rethink their power relationships, and further improve their e-platforms to suit the contexts of their users/residents’ needs and contributions. /To the best of the authors’ knowledge, this study contributed to the first Malaysia-based research that identifies and compares factors that contribute to citizen-centric e-government website building at the federal, state and local government levels. The discussion adds by comparing different systems and levels of e-government websites to their power possession.",CS,AI_ML,0.85,Extracted from log - paper 1698
E-government System Based on Blockchain,"With the advancement of digitalization and intelligence, the government affairs system is experiencing significant transformations. The E-government system, which is founded on blockchain technology, is a novel government service mode that offers numerous advantages, such as enhanced data security, facilitated information sharing, reinforced system stability, and improved system performance. Consequently, this system presents a fresh solution for fostering innovation and development within government services. By examining and evaluating the fundamental framework and algorithm optimization of the blockchain-based E-government affairs system, this paper concludes that the system can successfully achieve the objectives of data sharing and protection, business collaboration and automation, intelligent and compliant governance. Consequently, it can effectively enhance the efficiency and credibility of government services.",CS,AI_ML,0.85,Extracted from log - paper 1699
Success evaluation model of E-government systems for tax administrations: Recognizing the public value from the taxpayer's perspective,"Through the E-government systems, the tax administrations of the different levels of government interrelate with their target user, in this case the taxpayer, ensuring that the latter achieves compliance with its tax obligations in an agile, timely and efficient manner, promoting in turn, the construction of public s in this interested party. However, it is important to assess whether these information systems (IS) having the capabilities to meet the objectives pre-established by this type of organization, which have missions, interest groups, perspectives, contexts, factors and particular benefits compared to other government entities, and in which its success or failure, influences government finances and the development of a territory. Therefore, this document proposes the construction of a model for evaluating the success of E-government systems for tax administrations, which proposes a particular and not a general character, where the traditional factors of success theories must be complemented with relevant constructs to this context, such as: demographic considerations, the capacity for timely parameterization due to regulatory changes (flexibility), the digital gap, allowing to classify its benefits from social, economic, individual, organizational, environmental and public aspects, developing in this way a holistic model for this type of government organization, seeking that the taxpayer and their intermediaries are the protagonists of this assessment.",CS,AI_ML,0.85,Extracted from log - paper 1700
Clarifying the Role of E-Government Trust in E-Government Success Models: A Meta-analytic Structural Equation Modeling Approach,"E-government implementation success is of critical importance for nations. Prior information systems (IS) success models emphasize the effects of information quality, service quality, system quality, and user satisfaction but do not consider e-government trust. This study incorporates e-government trust into the IS success model and empirically tests the model on empirical reported in 67 prior studies using meta-analysis methods and structural equation modeling. Our analysis shows that: a) information quality, service quality, system quality, and user satisfaction influence e-government trust, and b) system use mediates the effect of e-government trust on intention to use e-government systems in the future.",CS,AI_ML,0.85,Extracted from log - paper 1701
"The impact of e‐government services on customer satisfaction in the private sector: A case study of the Kingdom of Bahrain (SIJILAT), an online commercial registration","Countless e‐government program initiatives have been introduced to profit each section of society in the Kingdom of Bahrain: citizens, residents, government entities, and workers' business ventures. It is getting more important to have e‐Government systems to supply services to businesses that use them (e‐G2B). To improve the Kingdom's national economy, the Information and Government Authority (IGA) has launched a virtual one‐stop solution available through multiple channels offering a unified service where citizens and Commercial Registration holders can use a single streamlined form to apply, renew or terminate multiple licenses concurrently; this is the Business License Information System (SIJILAT). This study aims to assess Bahraini customers' satisfaction with the SIJILAT service. A quantitative method is followed with a questionnaire to gather data; six factors: accessibility, information, security, reliability, trust, and perceived ease of use found to effectively affect service quality, and, in which it affects positively and enhances customer satisfaction. The theoretical contribution of this research will enhance the theoretical literature and knowledge related to its topic. The current research will contribute to the few studies conducted in Bahrain and the Middle East on e‐government and customer satisfaction with these services. A key outcome of the research will be the ability of e‐government officials who make decisions to determine the significant factors that play a role in G2B success, particularly ones to which they should pay attention to obtain the maximum return on their technology investment.",CS,AI_ML,0.85,Extracted from log - paper 1702
A Framework for An Integrated E- Government System for Public Service Sectors in Developing Countries Using Design Science Research Methodology,"This study aimed to develop an integrated framework to enhance government services and improve service delivery for the Namibian government. The Delone and McLean Information Systems success model and the Organizational Information Processing Theory served as the theoretical foundations for this study. The target population consisted of members of the public and a Government Ministry in Namibia. A sample size of 25 participants was selected for the study, utilizing the purposive sampling technique for government employees and the snowball sampling technique for members of the public. Thematic analysis was employed to analyze the data gathered. The study's revealed several significant insights. Despite the existence of multiple e-government systems, poor integration among them hindered the ability of the Namibian public service to provide effective and efficient services to citizens. Additionally, the study identified a strong demand from the public for services to be more accessible, convenient, and responsive, including a preference for online applications and faster service delivery. Moreover, the study developed an integrated e-government framework specifically tailored for the Namibian public service, aiming to enhance accessibility, convenience, responsiveness, and cost-effectiveness of services provided to citizens. The implementation of this framework is expected to promote an open and accountable government. It is crucial to emphasize that the successful achievement of e-Government goals relies on factors such as political leadership, support across all government levels, key infrastructure, skilled human resources, a suitable legal and regulatory framework, information resources, and citizen-centric services.",CS,AI_ML,0.85,Extracted from log - paper 1703
Online security in e-government as an antecedent of cost-effectiveness and quality in business operations,"The Colombian Government launched an e-government initiative in 2008 to facilitate communication among the government, citizens and organizations. Considering the high level of mistrust of citizens and businesses toward governmental institutions, the government must ensure the security of the information handled and provided by online users. Results to date have not been adequate in the usage of e-government systems. The of this study is to evaluate whether the level of online security affects usage and impacts the cost-effectiveness and quality of the operations and, consequently, the operational effectiveness of organizations using e-government systems. /approach Structural equation modeling was used to analyze the antecedents and outcomes of operational effectiveness. To this end, 440 usable questionnaires were collected from managers and personnel from Colombian organizations using e-government systems. According to the , there is a positive predictive relationship between online security and the dimensions of electronic government effectiveness. Furthermore, neither online security nor any of the dimensions of electronic government effectiveness affect the operational costs of organizations. Nonetheless, the quality of information has a positive effect on the quality of operations. As a result, through the quality of the information, online security has an indirect impact on the quality of operations. Research limitations/implications The authors used a convenience sample, carefully selecting respondents based on their operations and practice knowledge and implementation of online security processes. Besides, compared to previous research conducted in developed nations, the sample size is relatively small. Because the survey is based on responses from official companies, it must also be taken into account that over 50% of Colombian labor is informal. Furthermore, Colombia is a nation with a high level of mistrust. When considering these factors, generalizability to all industrial sectors is questionable. Nevertheless, the of this study offer relevant information that indicates the need for more extended and comprehensive quantitative research. Improvements in organizations that use e-government systems, based on the benefits that high-quality information brings to operational performance – cost and quality – will help them survive and become more sustainable and competitive. Furthermore, this study supports the assertion that aspects like online security are critical in promoting information and communication technology uptake and user acceptance in transition and rising economies like Colombia. /There is still a scarcity of information on assessing the effectiveness of electronic government systems and their impact on the quality and cost of operations in organizations that use them. Additionally, Colombia, as a country with low levels of trust between citizens, organizations and government, still lacks information about the impact of online security on the effectiveness of its operations.",CS,AI_ML,0.85,Extracted from log - paper 1704
Design Science in Information Systems Research,"Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.",CS,AI_ML,0.85,Extracted from log - paper 1705
An updated and expanded assessment of PLS-SEM in information systems research,"Following the call for awareness of accepted reporting practices by Ringle, Sarstedt, and Straub in 2012, the purpose of this paper is to review and analyze the use of partial least squares structural equation modeling (PLS-SEM) in Industrial Management & Data Systems (IMDS) and extend MIS Quarterly (MISQ) applications to include the period 2012-2014.,Review of PLS-SEM applications in information systems (IS) studies published in IMDS and MISQ for the period 2010-2014 identifying a total of 57 articles reporting the use of or commenting on PLS-SEM.,The results indicate an increased maturity of the IS field in using PLS-SEM for model complexity and formative measures and not just small sample sizes and non-normal data.,Findings demonstrate the continued use and acceptance of PLS-SEM as an accepted research method within IS. PLS-SEM is discussed as the preferred SEM method when the research objective is prediction.,This update on PLS-SEM use and recent developments will help authors to better understand and apply the method. Researchers are encouraged to engage in complete reporting procedures.,Applications of PLS-SEM for exploratory research and theory development are increasing. IS scholars should continue to exercise sound practice by reporting reasons for using PLS-SEM and recognizing its wider applicability for research. Recommended reporting guidelines following Ringle et al. (2012) and Gefen et al. (2011) are included. Several important methodological updates are included as well.",CS,AI_ML,0.85,Extracted from log - paper 1706
Understanding Information Systems Continuance: An Expectation-Confirmation Model,"This paper examines cognitive beliefs and affect influencing one's intention to continue using (continuance) information systems (IS). Expectation-confirmation theory is adapted from the consumer behavior literature and integrated with theoretical and empirical findings from prior IS usage research to theorize a model of IS continuance. Five research hypotheses derived from this model are empirically validated using a field survey of online banking users. The results suggest that users' continuance intention is determined by their satisfaction with IS use and perceived usefulness of continued IS use. User satisfaction, in turn, is influenced by their confirmation of expectation from prior IS use and perceived usefulness. Post-acceptance perceived usefulness is influenced by users' confirmation level. This study draws attention to the substantive differences between acceptance and continuance behaviors, theorizes and validates one of the earliest theoretical models of IS continuance, integrates confirmation and user satisfaction constructs within our current understanding of IS use, conceptualizes and creates an initial scale for measuring IS continuance, and offers an initial explanation for the acceptance-discontinuance anomaly.",CS,AI_ML,0.85,Extracted from log - paper 1707
Information Systems Success: The Quest for the Dependent Variable,"A large number of studies have been conducted during the last decade and a half attempting to identify those factors that contribute to information systems success. However, the dependent variable in these studies-I/S success-has been an elusive one to define. Different researchers have addressed different aspects of success, making comparisons difficult and the prospect of building a cumulative tradition for I/S research similarly elusive. To organize this diverse research, as well as to present a more integrated view of the concept of I/S success, a comprehensive taxonomy is introduced. This taxonomy posits six major dimensions or categories of I/S success-SYSTEM QUALITY, INFORMATION QUALITY, USE, USER SATISFACTION, INDIVIDUAL IMPACT, and ORGANIZATIONAL IMPACT. Using these dimensions, both conceptual and empirical studies are then reviewed a total of 180 articles are cited and organized according to the dimensions of the taxonomy. Finally, the many aspects of I/S success are drawn together into a descriptive model and its implications for future I/S research are discussed.",CS,AI_ML,0.85,Extracted from log - paper 1708
A Set of Principles for Conducting and Evaluating Interpretive Field Studies in Information Systems,"This article discusses the conduct and evaluatoin of interpretive research in information systems. While the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three published interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflect and debate on the important subject of grounding interpretive research methodology.",CS,AI_ML,0.85,Extracted from log - paper 1709
Information systems success: the quest for the dependent variable,"Article history: Received July 18, 2014 Accepted December 1 2014 Available online December 11 2014 This paper presents an empirical investigation to identify and rank the factors of information market development systems influencing on the market share development. The population of this survey includes all managers who work for SMEs in city of Tehran, Iran. The study selects a sample of 230 people randomly and a questionnaire is distributed among them in Likert scale. Cronbach alpha has been calculated as 0.814, which is well above the minimum desirable level. Using structural equation modeling the study has determined seven factors including valid data, information, strategic information, organizational information, supportive information, customer information, development information and data analysis, which influence market share development. Growing Science Ltd. All rights reserved. 5 © 201",CS,AI_ML,0.85,Extracted from log - paper 1710
The Nature of Theory in Information Systems,"The aim of this research essay is to examine the structural nature of theory in Information Systems. Despite the importance of theory, questions relating to its form and structure are neglected in comparison with questions relating to epistemology. The essay addresses issues of causality, explanation, prediction, and generalization that underlie an understanding of theory. A taxonomy is proposed that classifies information systems theories with respect to the manner in which four central goals are addressed: analysis, explanation, prediction, and prescription. Five interrelated types of theory are distinguished: (1) theory for analyzing, (2) theory for explaining, (3) theory for predicting, (4) theory for explaining and predicting, and (5) theory for design and action. Examples illustrate the nature of each theory type. The applicability of the taxonomy is demonstrated by classifying a sample of journal articles. The paper contributes by showing that multiple views of theory exist and by exposing the assumptions underlying different viewpoints. In addition, it is suggested that the type of theory under development can influence the choice of an epistemological approach. Support is given for the legitimacy and value of each theory type. The building of integrated bodies of theory that encompass all theory types is advocated.",CS,AI_ML,0.85,Extracted from log - paper 1711
The Case Research Strategy in Studies of Information Systems,This article defines and discusses one of these qualitative methods - the case research strategy. Suggestions are provided for researchers who wish to undertake research employing this approach. Criteria for the evaluation of case research are established and several characteristics useful for categorizing the studies are identified. A sample of papers drawn from information systems journals is reviewed. The paper concludes with examples of research areas that are particularly well-suited to investigation using the case research approach.,CS,AI_ML,0.85,Extracted from log - paper 1712
Specifying Formative Constructs in Information Systems Research,"While researchers go to great lengths to justify and prove theoretical links between constructs, the relationship between measurement items and constructs is often ignored. By default, the relationship between construct and item is assumed to be reflective, meaning that the measurement items are a reflection of the construct. Many times, though, the nature of the construct is not reflective, but rather formative. Formative constructs occur when the items describe and define the construct rather than vice versa. In this research, we examine whether formative constructs are indeed being mistaken for reflective constructs by information systems researchers. By examining complete volumes of MIS Quarterly and Information Systems Research over the last 3 years, we discovered that a significant number of articles have indeed misspecified formative constructs. For scientific results to be valid, we argue that researchers must properly specify formative constructs. This paper discusses the implications of different patterns of common misspecifications of formative constructs on both Type I and Type II errors. To avoid these errors, the paper provides a roadmap to researchers to properly specify formative constructs. We also discuss how to address formative constructs within a research model after they are specified.",CS,AI_ML,0.85,Extracted from log - paper 1713
Construction with digital twin information systems,"The concept of a “digital twin” as a model for data-driven management and control of physical systems has emerged over the past decade in the domains of manufacturing, production, and operations. In the context of buildings and civil infrastructure, the notion of a digital twin remains ill-defined, with little or no consensus among researchers and practitioners of the ways in which digital twin processes and data-centric technologies can support design and construction. This paper builds on existing concepts of Building Information Modeling (BIM), lean project production systems, automated data acquisition from construction sites and supply chains, and artificial intelligence to formulate a mode of construction that applies digital twin information systems to achieve closed loop control systems. It contributes a set of four core information and control concepts for digital twin construction (DTC), which define the dimensions of the conceptual space for the information used in DTC workflows. Working from the core concepts, we propose a DTC information system workflow—including information stores, information processing functions, and monitoring technologies—according to three concentric control workflow cycles. DTC should be viewed as a comprehensive mode of construction that prioritizes closing the control loops rather than an extension of BIM tools integrated with sensing and monitoring technologies.",CS,AI_ML,0.85,Extracted from log - paper 1714
"Review: the resource-based view and information systems research: review, extension, and suggestions for future research","Information systems researchers have a long tradition of drawing on theories from disciplines such as economics, computer science, psychology, and general management and using them in their own research. Because of this, the information systems field has become a rich tapestry of theoretical and conceptual foundations. As new theories are brought into the field, particularly theories that have become dominant in other areas, there may be a benefit in pausing to assess their use and contribution in an IS context. The purpose of this paper is to explore and critically evaluate use of the resource-based view of the firm (RBV) by IS researchers. The paper provides a brief review of resource-based theory and then suggests extensions to make the RBV more useful for empirical IS research. First, a typology of key IS resources is presented, and these are then described using six traditional resource attributes. Second, we emphasize the particular importance of looking at both resource complementarity and moderating factors when studying IS resource effects on firm performance. Finally, we discuss three considerations that IS researchers need to address when using the RBV empirically. Eight sets of propositions are advanced to help guide future research.",CS,AI_ML,0.85,Extracted from log - paper 1715
Information systems in the age of pandemics: COVID-19 and beyond,"This issue of the European Journal of Information Systems (EJIS) is a special issue on Business Process Management and Digital Innovation. Guest editors Jan Mendling, Brian Pentland, and Jan Recker...",CS,AI_ML,0.85,Extracted from log - paper 1716
Accounting information systems,"ACNT 2332 Accounting Information Systems. This course will The course reflects how IT is altering the nature of accounting. Specifically . Accounting Information Systems 2nd. Edition by Robert L. Hurt. Publisher: Tutors and solution manuals are availa Aug 21, 2012 Accounting Information Systems: Basic Concepts and Current Issues, Robert L. Hurt,. McGraw-Hill, 3rd Edition, ISBN-13: 978-0-07-802533-4. Systems Understanding Aid, Arens & Ward, Armond . your solutions be the result of your own effor Mason: OH 2005. 14. Hurt, Robert L. Accounting Information Systems: Basic Concepts and Current Issues. 2nd Edition. McGraw-Hill, New York: NY 2010. 15. book. I know that can present a challenge to folks who have used the earlier Dr. Robert L. Hurt California State Polytechnic University, Pomona . mously helpful in preparing the third edition of Accounting Information Systems: Basic Instructor's Res.",CS,AI_ML,0.85,Extracted from log - paper 1717
Research Commentary - The New Organizing Logic of Digital Innovation: An Agenda for Information Systems Research,"In this essay, we argue that pervasive digitization gives birth to a new type of product architecture: the layered modular architecture. The layered modular architecture extends the modular architecture of physical products by incorporating four loosely coupled layers of devices, networks, services, and contents created by digital technology. We posit that this new architecture instigates profound changes in the ways that firms organize for innovation in the future. We develop (1) a conceptual framework to describe the emerging organizing logic of digital innovation and (2) an information systems research agenda for digital strategy and the creation and management of corporate information technology infrastructures.",CS,AI_ML,0.85,Extracted from log - paper 1718
A Framework for Information Systems Architecture (Abstract of Tutorial),"With increasing size and complexity of the implementations of information systems, it is necessary to use some logical construct (or architecture) for defining and controlling the interfaces and the integration of all of the components of the system. This paper defines information systems architecture by creating a descriptive framework from disciplines quite independent of information systems, then by analogy specifies information systems architecture based upon the neutral, objective framework. Also, some preliminary conclusions about the implications of the resultant descriptive framework are drawn. The discussion is limited to architecture and does not include a strategic planning methodology.",CS,AI_ML,0.85,Extracted from log - paper 1719
Linking information systems and entrepreneurship: A review and agenda for IT‐associated and digital entrepreneurship research,"More than 50 years ago, information technology (IT) began to change society, the economy, and industries worldwide. This change has included waves of technological disruption that have been exploited by entrepreneurial actors who seize the associated new opportunities. Research on related phenomena is spread across different disciplines. Recently, there have been calls for further research on the marriage of information systems (IS) and entrepreneurship. We review 292 articles in the IS, entrepreneurship, and general and strategic management literature to create an overview of the IT‐associated entrepreneurship research landscape. On the basis of that review, we elaborate on the different roles that IT can assume to support entrepreneurial operations and value creation in these settings. Our findings suggest that IT plays four major roles in entrepreneurial operations: as a facilitator, making the operations of start‐ups easier; as a mediator for new ventures' operations; as an outcome of entrepreneurial operations; and as a ubiquity, becoming the business model itself. Leveraging these roles of IT, we develop a set of definitions to clear up definition uncertainties surrounding IT‐associated new ventures such as digital start‐ups and digital business models. We also outline a research agenda for IT‐associated entrepreneurship research based on identified roles, types, and gaps.",CS,AI_ML,0.85,Extracted from log - paper 1720
How Habit Limits the Predictive Power of Intention: The Case of Information Systems Continuance,"Past research in the area of information systems acceptance has primarily focused on initial adoption under the implicit assumption that IS usage is mainly determined by intention. While plausible in the case of initial IS adoption, this assumption may not be as readily applicable to continued IS usage behavior since it ignores that frequently performed behaviors tend to become habitual and thus automatic over time. This paper is a step forward in defining and incorporating the ""habit"" construct into IS research. Specifically, the purpose of this study is to explore the role of habit and its antecedents in the context of continued IS usage. Building on previous work in other disciplines, we define habit in the context of IS usage as the extent to which people tend to perform behaviors (use IS) automatically because of learning. Using recent work on the continued usage of IS (IS continuance), we have developed a model suggesting that continued IS usage is not only a consequence of intention, but also of habit. In particular, in our research model, we propose IS habit to moderate the influence of intention such that its importance in determining behavior decreases as the behavior in question takes on a more habitual nature. Integrating past research on habit and IS continuance further, we suggest how antecedents of behavior/behavioral intention as identified by IS continuance research relate to drivers of habitualization. We empirically tested the model in the context of voluntary continued WWW usage. Our results support the argument that habit acts as a moderating variable of the relationship between intentions and IS continuance behavior, which may put a boundary condition on the explanatory power of intentions in the context of continued IS usage. The data also support that satisfaction, frequency of past behavior, and comprehensiveness of usage are key to habit formation and thus relevant in the context of IS continuance behavior. Implications of these findings are discussed and managerial guidelines presented.",CS,AI_ML,0.85,Extracted from log - paper 1721
Information Systems Success Measurement,"Researchers and practitioners alike face a daunting challenge when evaluating the ""success"" of information systems. The purpose of this monograph is to deepen, researchers and practitioners, understanding of the complex nature of IS success measurement driven by the constantly changing role and use of information technology. This monograph covers the history of IS success measurement as well as recent trends and future expectations for IS success measurement. The monograph also identifies the critical success factors that drive information system success and provides measurement and evaluation guidance for practitioners. This comprehensive study of IS success measurement is designed to improve measurement practice among researchers and managers.",CS,AI_ML,0.85,Extracted from log - paper 1722
Knowledge and information systems,"Knowledge and Information Systems (KAIS) provides an international forum for researchers and professionals to share their knowledge and report new advances on all topics related to knowledge systems and advanced information systems. This monthly peer-reviewed archival journal publishes state-of-the-art research reports on emerging topics in KAIS, reviews of important techniques in related areas, and application papers of interest to a general readership. The journal focuses on knowledge systems and advanced information systems, including their theoretical foundations, infrastructure and enabling technologies. We solicit submissions of original research, and experience and vision papers that address this theme. We publish critical review papers to discuss the state of the art in particular areas, as well as state-of-the-art research reports. Accepted papers are grouped for publication so that individual issues focus on a small number of theme areas. In addition to archival papers, the journal also publishes significant on-going research in the form of Short Papers (limited to 3000 words), and very short papers on ""visions and directions"" (no more than 1000 words, excluding bibliography). We conduct reviews in a timely fashion and inform authors of decisions with a target turnaround time of 3 months. Selected papers from relevant conferences are welcome. Good papers with high quality reviews can be accepted after the expansion and revision is verified by an Associate Editor of the Editorial Board. Conference organizers are invited to contact the Editor-in-Chief kais@cs.uvm.edu for further information.",CS,AI_ML,0.85,Extracted from log - paper 1723
Toward Meaningful Engagement: A Framework for Design and Research of Gamified Information Systems,"Gamification, an emerging idea for using game design elements and principles to make everyday tasks more engaging, is permeating many different types of information systems. Excitement surrounding gamification results from its many potential organizational benefits. However, few research and design guidelines exist regarding gamified information systems. We therefore write this commentary to call upon information systems scholars to investigate the design and use of gamified information systems from a variety of disciplinary perspectives and theories, including behavioral economics, psychology, social psychology, information systems, etc. We first explicate the idea of gamified information systems, provide real-world examples of successful and unsuccessful systems, and, based on a synthesis of the available literature, present a taxonomy of gamification design elements. We then develop a framework for research and design: its main theme is to create meaningful engagement for users; that is, gamified information systems should be designed to address the dual goals of instrumental and experiential outcomes. Using this framework, we develop a set of design principles and research questions, using a running case to illustrate some of our ideas. We conclude with a summary of opportunities for IS researchers to extend our knowledge of gamified information systems, and, at the same time, advance existing theories.",CS,AI_ML,0.85,Extracted from log - paper 1724
Paradox Lost? Firm-Level Evidence on the Returns to Information Systems Spending,"The ""productivity paradox"" of information systems IS is that, despite enormous improvements in the underlying technology, the benefits of IS spending have not been found in aggregate output statistics. One explanation is that IS spending may lead to increases in product quality or variety which tend to be overlooked in the aggregate statistics, even if they increase output at the firm-level. Furthermore, the restructuring and cost-cutting that are often necessary to realize the potential benefits of IS have only recently been undertaken in many firms. Our study uses new firm-level data on several components of IS spending for 1987-1991. The dataset includes 367 large firms which generated approximately 1.8 trillion dollars in output in 1991. We supplemented the IS data with data on other inputs, output, and price deflators from other sources. As a result, we could assess several econometric models of the contribution of IS to firm-level productivity. Our results indicate that IS spending has made a substantial and statistically significant contribution to firm output. We find that the gross marginal product MP for computer capital averaged 81% for the firms in our sample. We find that the MP for computer capital is at least as large as the marginal product of other types of capital investment and that, dollar for dollar, IS labor spending generates at least as much output as spending on non-IS labor and expenses. Because the models we applied were similar to those that have been previously used to assess the contribution of IS and other factors of production, we attribute the different results to the fact that our data set is more current and larger than others explored. We conclude that the productivity paradox disappeared by 1991, at least in our sample of firms.",CS,AI_ML,0.85,Extracted from log - paper 1725
Information Systems,"An information system is a primitive structure that defines which agents can initially get information and how such information is then distributed to others. From political and organizational economics to privacy, information systems arise in various contexts and, unlike information itself, can be easily observed empirically. We introduce a methodology to characterize how information systems affect strategic behavior. This involves proving a revelation principle result for a novel class of constrained information design problems. We identify when such systems better distribute information and, as a result, impose more constraints on behavior. This leads to a novel notion of an agent’s influence in the system. Finally, we apply our theory to examine how current patterns of news consumption from mass media may affect elections.",CS,AI_ML,0.85,Extracted from log - paper 1726
"Message Equivocality, Media Selection, and Manager Performance: Implications for Information Systems",A field study of middle- and upper-level managers was undertaken to explain managers' selection of communication media. The findings indicate that media vary in their capacity to convey information cues. Managers prefer rich media for ambiguous communications and less rich media for unequivocal communications. The data suggest that high performing managers are more sensitive to the relationship between message ambiguity and media richness than low performing managers. Implications for managers' use of information systems and electronic media are discussed.,CS,AI_ML,0.85,Extracted from log - paper 1727
[Geographic information systems].,"The aim of this study was to geospatially explore the occurrence rates of car accidents involving pedestrians in Cercado de Lima (Lima District), Peru. Car accidents involving pedestrians recorded in the 2015 National Police Station Census of the National Statistics and Information Institute were described and georeferenced. Subsequently, a Kernel Density analysis was carried out to locate areas with high, medium, and low density events. Records of 171 car accidents involving pedestrians were studied: the types of vehicles involved were automobiles (56.7%) and smaller vehicles (22.8%). The highest percentage of car accidents involving pedestrians (38.6%) took place between 12:00 p.m. and 5:00 p.m. There were two densely populated areas and two areas with intermediate density for car accidents involving pedestrians, locations that were previously reported as critical due to their deficiencies and high probability of traffic accidents. The use of geographic information systems offers a quick overview of the occurrence rates of car accidents involving pedestrians to make comparisons and enable the local implementation of strategies.",CS,AI_ML,0.85,Extracted from log - paper 1728
Standing on the Shoulders of Giants: Challenges and Recommendations of Literature Search in Information Systems Research,"The “standing on the shoulders of giants” metaphor is often used to acknowledge the work of others when undertaking research and, in particular, stresses the importance of literature reviews in scientific inquiry. Though the significance of literature reviews has never been in doubt, researchers, especially novice researchers, still struggle with developing effective strategies for reviewing literature. An important reason for this difficulty is the rapidly increasing number of potentially relevant publications—not all of which necessarily add value to a literature review. As such, avoiding standing on the shoulders of dwarfs literature search emerges as a major issue in crafting an effective literature review. In this paper, we discuss challenges of literature searches in the increasingly dynamic context of information systems (IS) research and make recommendations for how to deal with them. We present practical guidelines and a checklist to help researchers with planning and organizing their literature searches.",CS,AI_ML,0.85,Extracted from log - paper 1729
The Sustainability Imperative in Information Systems Research,"This paper reports on a panel discussion at the pre-ICIS 2015 Workshop on Green Information Systems on the current state and future perspectives of SIGGreen—the Association of Information Systems’ special interest group on green information systems—and of green information systems (green IS) research in general. Over the past years, IS scholars have made important contributions advancing our knowledge about how information systems can contribute to solving problems associated with the degradation of the natural environment. However, it would appear that many view green IS as just another research topic in the IS field and not a very important one at that. This is questionable because sustainability is too important to be relegated as a footnote in the greater scheme of things. We suggest that the IS community should embrace sustainability as a core research imperative and integrate sustainability-related dimensions to research in theory and method, in rigor and relevance, and in the areas one chooses to research. We provide some actionable recommendations on how we as IS researchers and, indeed, how the IS field could help society and business interests make the transition to a sustainable world.",CS,AI_ML,0.85,Extracted from log - paper 1730
An Integrated Model of Information Systems Adoption in Small Businesses,"Based on theories from the technological innovation literature, this study develops an integrated model of information systems (IS) adoption in small businesses. The model specifies contextual variables such as decision-maker characteristics, IS characteristics, organizational characteristics, and environmental characteristics as primary determinants of IS adoption in small businesses. A questionnaire survey was conducted in 166 small businesses. Data analysis shows that small businesses with certain CEO characteristics (innovativeness and level of IS knowledge), innovation characteristics (relative advantage, compatibility, and complexity of IS), and organizational characteristics (business size and level of employees' IS knowledge) are more likely to adopt IS. While CEO and innovation characteristics are important determinants of the decision to adopt, they do not affect the extent of IS adoption. The extent of IS adoption is mainly determined by organizational characteristics. Finally, the environmental characteristic of competition has no direct effect on small business adoption of IS.",CS,AI_ML,0.85,Extracted from log - paper 1731
Towards a Framework of Literature Review Process in Support of Information Systems Research,"This paper introduces an initial effort towards developing a framework for writing an effective literature review. The target audience for the framework are novice IS researchers or other researchers who are constantly struggling with the development of an effective literature-based foundation for the proposed research. The proposed framework follows the systematic data processing approach comprised of three major stages: 1) inputs (literature gathering and screening), 2) processing (Blooms Taxonomy), and 3) outputs (writing the review). This paper provides the rationale for developing a solid literature review and addresses the central stage, processing the literature. The paper concludes by providing arguments for the value of an effective literature review as well as implications for future work in this proposed framework.",CS,AI_ML,0.85,Extracted from log - paper 1732
A method for taxonomy development and its application in information systems,"A fundamental problem in many disciplines is the classification of objects in a domain of interest into a taxonomy. Developing a taxonomy, however, is a complex process that has not been adequately addressed in the information systems (IS) literature. The purpose of this paper is to present a method for taxonomy development that can be used in IS. First, this paper demonstrates through a comprehensive literature survey that taxonomy development in IS has largely been ad hoc. Then the paper defines the problem of taxonomy development. Next, the paper presents a method for taxonomy development that is based on taxonomy development literature in other disciplines and shows that the method has certain desirable qualities. Finally, the paper demonstrates the efficacy of the method by developing a taxonomy in a domain in IS.",CS,AI_ML,0.85,Extracted from log - paper 1733
A Guide to Conducting a Systematic Literature Review of Information Systems Research,"This working paper has been thoroughly revised and superseded by two distinct articles. The first is a revised and peer-reviewed version of the original article: Okoli, Chitu (2015), A Guide to Conducting a Standalone Systematic Literature Review. Communications of the Association for Information Systems (37:43), November 2015, pp. 879-910. This article presents a methodology for conducting a systematic literature review with many examples from IS research and references to guides with further helpful details. The article is available from Google Scholar or from the author's website. The second extension article focuses on developing theory with literature reviews: Okoli, Chitu (2015), The View from Giants’ Shoulders: Developing Theory with Theory-Mining Systematic Literature Reviews. SSRN Working Paper Series, December 8, 2015. This article identifies theory-mining reviews, which are literature reviews that extract and synthesize theoretical concepts from the source primary studies. The article demonstrates by citation analysis that, in information systems research, this kind of literature review is more highly cited than other kinds of literature review. The article provides detailed guidelines to writing a high-quality theory-mining review.",CS,AI_ML,0.85,Extracted from log - paper 1734
Review: A Review of Culture in Information Systems Research: Toward a Theory of Information Technology Culture Conflict,"An understanding of culture is important to the study of information technologies in that culture at various levels, including national, organizational, and group, can influence the successful implementation and use of information technology. Culture also plays a role in managerial processes that may directly, or indirectly, influence IT. Culture is a challenging variable to research, in part because of the multiple divergent definitions and measures of culture. Notwithstanding, a wide body of literature has emerged that sheds light on the relationship of IT and culture. This paper sets out to provide a review of this literature in order to lend insights into our understanding of the linkages between IT and culture. We begin by conceptualizing culture and laying the groundwork for a values-based approach to the examination of IT and culture. Using this approach, we then provide a comprehensive review of the organizational and cross-cultural IT literature that conceptually links these two traditionally separate streams of research. From our analysis, we develop six themes of IT-culture research emphasizing culture's impact on IT, IT's impact on culture, and IT culture. Building upon these themes, we then develop a theory of IT, values, and conflict. Based upon the theory, we develop propositions concerning three types of cultural conflict and the results of these conflicts. Ultimately, the theory suggests that the reconciliation of these conflicts results in a reorientation of values. We conclude with the particular research challenges posed in this line of inquiry.",CS,AI_ML,0.85,Extracted from log - paper 1735
PEGASIS : Power-Efficient Gathering in Sensor Information Systems,"Sensor network consisting of nodes with limited battery power and wireless communications are deployed to collect useful information from the field. The main idea in PEGASIS is for each node to receive from and transmit to close neighbors and take turns being the leader for transmission to the BS. This approach distributes the energy load evenly among the sensor nodes in the network. Sensor nodes are randomly deployed in the sensor field, and therefore, the i th node is at a random location. The nodes will be organized to form a chain, which can either be accomplished by the sensor nodes themselves using a greedy algorithm. The algorithm to resolve the unbalanced energy consumption problem caused by long distance data transmission of some nodes in a chain formed by the greedy algorithm.",CS,AI_ML,0.85,Extracted from log - paper 1736
Digital Innovation as a Fundamental and Powerful Concept in the Information Systems Curriculum,"The 50-year march of Moore's Law has led to the creation of a relatively cheap and increasingly easy-touse world-wide digital infrastructure of computers, mobile devices, broadband network connections, and advanced application platforms. This digital infrastructure has, in turn, accelerated the emergence of new technologies that enable transformations in how we live and work, how companies organize, and the structure of entire industries. As a result, it has become important for all business students to have a strong grounding in IT and digital innovation in order to manage, lead, and transform organizations that are increasingly dependent on digital innovation. Yet, at many schools, students do not get such grounding because the required information systems core class is stuck in the past. We present a vision for a redesigned IS core class that adopts digital innovation as a fundamental and powerful concept (FPC). A good FPC serves as both a foundational concept and an organizing principle for a course. We espouse a particularly broad conceptualization of digital innovation that allows for a variety of teaching styles and topical emphases for the IS core class. This conceptualization includes three types of innovation (i.e., process, product, and business model innovation), and four stages for the overall innovation process (i.e., discovery, development, diffusion, and impact). Based on this conceptualization, we examine the implications of adopting digital innovation as an FPC. We also briefly discuss broader implications relating to (1) the IS curriculum beyond the core class, (2) the research agenda for the IS field, and (3) the identity and legitimacy of IS in business schools.",CS,AI_ML,0.85,Extracted from log - paper 1737
Principles of Geographical Information Systems for Land Resources Assessment,"Geographical information systems Data structures for thematic maps Digital elevation models Data input, verification, storage, and output Methods of data analysis and spatial modelling Data quality, errors, and natural variation: sources of error Errors arising through processing The nature of boundaries Classification methods Methods of spatial interpolation Choosing a geographical information system Appendices Index.",CS,AI_ML,0.85,Extracted from log - paper 1738
A Systems Approach to Conduct an Effective Literature Review in Support of Information Systems Research,"This paper introduces a framework for conducting and writing an effective literature review. The target audience for the framework includes information systems (IS) doctoral students, novice IS researchers, and other IS researchers who are constantly struggling with the development of an effective literature-based foundation for a proposed research. The proposed framework follows the systematic data processing approach comprised of three major stages: 1) inputs (literature gathering and screening), 2) processing (following Bloom’s Taxonomy), and 3) outputs (writing the literature review). This paper provides the rationale for developing a solid literature review including detailed instructions on how to conduct each stage of the process proposed. The paper concludes by providing arguments for the value of an effective literature review to IS research.",CS,AI_ML,0.85,Extracted from log - paper 1739
Privacy in the Digital Age: A Review of Information Privacy Research in Information Systems,"Information privacy refers to the desire of individuals to control or have some influence over data about themselves. Advances in information technology have raised concerns about information privacy and its impacts, and have motivated Information Systems researchers to explore information privacy issues, including technical solutions to address these concerns. In this paper, we inform researchers about the current state of information privacy research in IS through a critical analysis of the IS literature that considers information privacy as a key construct. The review of the literature reveals that information privacy is a multilevel concept, but rarely studied as such. We also find that information privacy research has been heavily reliant on studentbased and USA-centric samples, which results in findings of limited generalizability. Information privacy research focuses on explaining and predicting theoretical contributions, with few studies in journal articles focusing on design and action contributions. We recommend that future research should consider different levels of analysis as well as multilevel effects of information privacy. We illustrate this with a multilevel framework for information privacy concerns. We call for research on information privacy to use a broader diversity of sampling populations, and for more design and action information privacy research to be published in journal articles that can result in IT artifacts for protection or control of information privacy.",CS,AI_ML,0.85,Extracted from log - paper 1740
Geographic Information Systems for Geoscientists: Modelling with GIS,Chapter headings. Introduction to GIS. Spatial data models. Spatial data structures. Spatial data input. Visualization and query of spatial data. Spatial data transformations. Tools for map analysis: single maps. Tools for map analysis: map pairs. Tools for map analysis: multiple maps.,CS,AI_ML,0.85,Extracted from log - paper 1741
Generalizing Generalizability in Information Systems Research,"Generalizability is a major concern to those who do, and use, research. Statistical, sampling-based generalizability is well known, but methodologists have long been aware of conceptions of generalizability beyond the statistical. The purpose of this essay is to clarify the concept of generalizability by critically examining its nature, illustrating its use and misuse, and presenting a framework for classifying its different forms. The framework organizes the different forms into four types, which are defined by the distinction between empirical and theoretical kinds of statements. On the one hand, the framework affirms the bounds within which statistical, sampling-based generalizability is legitimate. On the other hand, the framework indicates ways in which researchers in information systems and other fields may properly lay claim to generalizability, and thereby broader relevance, even when their inquiry falls outside the bounds of sampling-based research.",CS,AI_ML,0.85,Extracted from log - paper 1742
Kriging: a method of interpolation for geographical information systems,"Geographical information systems could be improved by adding procedures for geostatistical spatial analysis to existing facilities. Most traditional methods of interpolation are based on mathematical as distinct from stochastic models of spatial variation. Spatially distributed data behave more like random variables, however, and regionalized variable theory provides a set of stochastic methods for analysing them. Kriging is the method of interpolation deriving from regionalized variable theory. It depends on expressing spatial variation of the property in terms of the variogram, and it minimizes the prediction errors which are themselves estimated. We describe the procedures and the way we link them using standard operating systems. We illustrate them using examples from case studies, one involving the mapping and control of soil salinity in the Jordan Valley of Israel, the other in semi-arid Botswana where the herbaceous cover was estimated and mapped from aerial photographic survey.",CS,AI_ML,0.85,Extracted from log - paper 1743
Information Systems Innovation for Environmental Sustainability,"Human life is dependent upon the natural environment, which, most would agree, is rapidly degrading. Business enterprises are a dominant form of social organization and contribute to the worsening, and enhancement, of the natural environment. Scholars in the administrative sciences examine questions spanning organizations and the natural environment but have largely omitted the information systems perspective. We develop a research agenda on information systems innovation for environmental sustainability that demonstrates the critical role that IS can play in shaping beliefs about the environment, in enabling and transforming sustainable processes and practices in organizations, and in improving environmental and economic performance. The belief-action-outcome (BAO) framework and associated research agenda provide the basis for a new discourse on IS for environmental sustainability.",CS,AI_ML,0.85,Extracted from log - paper 1744
A Framework and Guidelines for Context-Specific Theorizing in Information Systems Research,"This paper discusses the value of context in theory development in information systems IS research. We examine how prior research has incorporated context in theorizing and develop a framework to classify existing approaches to contextualization. In addition, we expound on a decomposition approach to contextualization and put forth a set of guidelines for developing context-specific models. We illustrate the application of the guidelines by constructing and comparing various context-specific variations of the technology acceptance model TAM---i.e., the decomposed TAM that incorporates interaction effects between context-specific factors, the extended TAM with context-specific antecedents, and the integrated TAM that incorporates mediated moderation and moderated mediation effects of context-specific factors. We tested the models on 972 individuals in two technology usage contexts: a digital library and an agile Web portal. The results show that the decomposed TAM provides a better understanding of the contexts by revealing the direct and interaction effects of context-specific factors on behavioral intention that are not mediated by the TAM constructs of perceived usefulness and perceived ease of use. This work contributes to the ongoing discussion about the importance of context in theory development and provides guidance for context-specific theorizing in IS research.",CS,AI_ML,0.85,Extracted from log - paper 1745
Rigor in Information Systems Positivist Case Research: Current Practices,"Case research has commanded respect in the information systems (IS) discipline for at least a decade. Notwithstanding the relevance and potential value of case studies, this methodological approach was once considered to be one of the least systematic. Toward the end of the 1980s, the issue of whether IS case research was rigorously conducted was first raised. Researchers from our field (e.g., Benbasat et al. 1987; Lee 1989) and from other disciplines (e.g., Eisenhardt 1989; Yin 1994) called for more rigor in case research and, through their recommendations, contributed to the advancement of the case study methodology. Considering these contributions, the present study seeks to determine the extent to which the field of IS has advanced in its operational use of case study method. Precisely, it investigates the level of methodological rigor in positivist IS case research conducted over the past decade. To fulfill this objective, we identified and coded 183 case articles from seven major IS journals. Evaluation attributes or criteria considered in the present review focus on three main areas, namely, design issues, data collection, and data analysis. While the level of methodological rigor has experienced modest progress with respect to some specific attributes, the overall assessed rigor is somewhat equivocal and there are still significant areas for improvement. One of the keys is to include better documentation particularly regarding issues related to the data collection and analysis processes.",CS,AI_ML,0.85,Extracted from log - paper 1746
Information Systems: Towards a System of Information Systems,"Information Systems are viewed as a set of services creating a workflow of information directed to specific groups and members. This allows individuals to share ideas and their talents with other members. In such manner, tasks can be carried out both efficiently and effectively. Due to the nature of Information Systems that revolves around creating information useful to users, and in some higher forms of Information Systems creating knowledge, management of information and/or knowledge is part of their functionalities. In this paper we aim to study the placement of Information Systems as part of a System of Systems (SoS), as these large systems poses significant technical improvement in terms of information interoperability that overcomes conceptual and technical barriers. Therefore, we move towards defining and modeling System of Information Systems (SoIS). This paper discovers what is currently known about Information Systems and Systems of Systems, and proceeds towards suggesting an architecture of a System of Information Systems that integrates several Information Systems and allows information to be transferred at ease between those different components.",CS,AI_ML,0.85,Extracted from log - paper 1747
User Awareness of Security Countermeasures and Its Impact on Information Systems Misuse: A Deterrence Approach,"Intentional insider misuse of information systems resources (i.e., IS misuse) represents a significant threat to organizations. For example, industry statistics suggest that between 50%--75% of security incidents originate from within an organization. Because of the large number of misuse incidents, it has become important to understand how to reduce such behavior. General deterrence theory suggests that certain controls can serve as deterrent mechanisms by increasing the perceived threat of punishment for IS misuse. This paper presents an extended deterrence theory model that combines work from criminology, social psychology, and information systems. The model posits that user awareness of security countermeasures directly influences the perceived certainty and severity of organizational sanctions associated with IS misuse, which leads to reduced IS misuse intention. The model is then tested on 269 computer users from eight different companies. The results suggest that three practices deter IS misuse: user awareness of security policies; security education, training, and awareness (SETA) programs; and computer monitoring. The results also suggest that perceived severity of sanctions is more effective in reducing IS misuse than certainty of sanctions. Further, there is evidence that the impact of sanction perceptions vary based on one's level of morality. Implications for the research and practice of IS security are discussed.",CS,AI_ML,0.85,Extracted from log - paper 1748
Research on information systems failures and successes: Status update and future directions,"Information systems success and failure are among the most prominent streams in IS research. Explanations of why some IS fulfill their expectations, whereas others fail, are complex and multi-factorial. Despite the efforts to understand the underlying factors, the IS failure rate remains stubbornly high. A Panel session was held at the IFIP Working Group 8.6 conference in Bangalore in 2013 which forms the subject of this Special Issue. Its aim was to reflect on the need for new perspectives and research directions, to provide insights and further guidance for managers on factors enabling IS success and avoiding IS failure. Several key issues emerged, such as the need to study problems from multiple perspectives, to move beyond narrow considerations of the IT artifact, and to venture into underexplored organizational contexts, such as the public sector.",CS,AI_ML,0.85,Extracted from log - paper 1749
Geographic Information Systems: An Introduction,Foreword. Preface. 1 Geographical Information Systems and Graphical Information. 2 Historical Development: Geographical Data and GIS. 3 From the Real World to GIS. 4 Basic Data Models. 5 Advanced Data Models. 6 Georeferencing Systems. 7 Hardware and Communication Technology for GIS Applications. 8 Basic Software and Databases for GIS. 9 Data Collection I. 10 Data Collection II. 11 Data Quality. 12 Database Implementation and Spatial Indexing. 13 Housekeeping Tools. 14 Basic Spatial Analysis. 15 Advanced Analysis. 16 Visualization. 17 Choosing a GIS: Organizational Issues. 18 Choosing a GIS: Technical Issues. 19 Standards and Geospatial Infrastructure. 20 Formal Problems in Establishing GIS. 21 A Vision for the Future. References. Index.,CS,AI_ML,0.85,Extracted from log - paper 1750
Predictive Analytics in Information Systems Research,"textabstractThis research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analytics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.",CS,AI_ML,0.85,Extracted from log - paper 1751
Interpretation of Formative Measurement in Information Systems Research,"Within the Information Systems literature, there has been an emerging interest in the use of formative measurement in structural equation modeling (SEM). This interest is exemplified by descriptions of the nature of formative measurement (e.g., Chin 1998a), and more recently the proper specification of formatively measured constructs (Petter et al. 2007) as well as application of such constructs (e.g., Barki et al. 2007). Formative measurement is a useful alternative to reflective measurement. However, there has been little guidance on interpreting the results when formative measures are employed. Our goal is to provide guidance relevant to the interpretation of formative measurement results through the examination of the following six issues: multicollinearity; the number of indicators specified for a formatively measured construct; the possible co-occurrence of negative and positive indicator weights; the absolute versus relative contributions made by a formative indicator; nomological network effects; and the possible effects of using partial least squares (PLS) versus covariance-based SEM techniques. We provide prescriptions for researchers to consider when interpreting the results of formative measures as well as an example to illustrate these prescriptions.",CS,AI_ML,0.85,Extracted from log - paper 1752
"The Sociomaterialty of Information Systems: Current Status, Future Directions","Our motivation for putting together this special issue on “Sociomateriality of Information Systems and Organizing” was the mounting interest in the relationship between the social and the material, in the context of our increasingly digital society. The attention to this relationship is manifested in the emergence of studies of technology intended to augment and complement, but also and importantly, to question the received views on technology in social life (see Carlile et al. 2013a; Leonardi et al. 2012; Suchman, 2007).",CS,AI_ML,0.85,Extracted from log - paper 1753
Information Systems and Environmentally Sustainable Development: Energy Informatics and New Directions for the IS Community,"While many corporations and Information Systems units recognize that environmental sustainability is an urgent problem to address, the IS academic community has been slow to acknowledge the problem and take action. We propose ways for the IS community to engage in the development of environmentally sustainable business practices. Specifically, as IS researchers, educators, journal editors, and association leaders, we need to demonstrate how the transformative power of IS can be leveraged to create an ecologically sustainable society. In this Issues and Opinions piece, we advocate a research agenda to establish a new subfield of energy informatics, which applies information systems thinking and skills to increase energy efficiency. We also articulate how IS scholars can incorporate environmental sustainability as an underlying foundation in their teaching, and how IS leaders can embrace environmental sustainability in their core principles and foster changes that reduce the environmental impact of our community.",CS,AI_ML,0.85,Extracted from log - paper 1754
Interpreting Information Systems in Organizations,"From the Publisher: A coherent integrated source for an interpretive approach to understanding information systems in organizations to aid readers in their own processes of defining computer systems. Examines four major IS issues--strategy, evaluation, design and development, implementation. Features in-depth case studies to illustrate key points.",CS,AI_ML,0.85,Extracted from log - paper 1755
Benefits and challenges of cloud ERP systems – A systematic literature review,"Enterprise Resource Planning (ERP) systems provide extensive benefits and facilities to the whole enterprise. ERP systems help the enterprise to share and transfer data and information across all functions units inside and outside the enterprise. Sharing data and information between enterprise departments helps in many aspects and aims to achieve different objectives. Cloud computing is a computing model which takes place over the internet and provides scalability, reliability, availability and low cost of computer reassures. Implementing and running ERP systems over the cloud offers great advantages and benefits, in spite of its many difficulties and challenges. In this paper, we follow the Systematic Literature Review (SLR) research method to explore the benefits and challenges of implementing ERP systems over a cloud environment.",CS,AI_ML,0.85,Extracted from log - paper 1756
A Study on the Acceptance of Mobile-Banking Applications in India—Unified Theory of Acceptance and Sustainable Use of Technology Model (UTAUT),"This research makes an attempt to understand various factors that influence the adoption of mobile applications. Within the context of the “Unified theory of acceptance and use of technology” (UTAUT) modified model, considering the upcoming demand and increase in demand for mobile-banking applications, the researcher tried to explore the theoretical concept between random people of various states in India. The primary data was collected by preparing a questionnaire and circulating it using Google Forms. The collected data was further coded into Smart PLS 4 to understand the model and structural equation with reference to mobile-banking technological adoption and factors that had a significant impact. The conclusions derived from the study is that social influence, “effort expectancy”, and “trust” factors had a very strong influence on the “purchase intention”, whereas “effort” and “risk” factors had a negligible impact on purchase intent. It was also found that the UTAUT model is appropriate for evaluating the technological adoption of mobile-banking applications. With the advent of many players in the market and their unique banking management applications on mobile platforms, consumers are moving towards different third-party app than their origin bank in which they hold account. This has forced banking institutions to up the pace in the competition, introducing a lot of new features. It is also important to understand that, as a customer, there are a lot of attributes that he would be looking into for adoption. This paper is an attempt to understand the advancements in various variables that consumers would look at in the area of mobile-banking applications.",CS,AI_ML,0.85,Extracted from log - paper 1757
Measuring e-learning systems success: Implementing D & M is success model,"Electronic learning (e-learning) has been widely used as a complement subject to traditional learning methods. Implementing e-learning is becoming one of many solutions in effecting the study process offered by higher educational institutions, such as Universitas Pembangunan Nasional “Veteran” Jakarta = UPNVJ and Sekolah Tinggi Teknologi Telematika Telkom (ST3 Telkom) Purwokerto, Central Java. The objective of this research is to measure the e-learning system success implemented by UPNVJ and ST3 Telkom Purwokerto and then compare the similarities and differences between of two. The modified DeLone & McLean Information Systems Success Model and part of Technology Acceptance Model are adopted in this study. The respondents are students who have been experience in using e-learning and are selected randomly from the Faculty of Computer Science of UPNVJ and from ST3 Telkom Purwokerto. The total respondents are 387, which consist of 215 students from UPNVJ and 172 from ST3 Telkom Purwokerto. For the results at UPNVJ, there are two hypotheses that were proven insignificant and at ST3 Telkom there are three hypotheses that were proven insignificant. Findings at UPNVJ show that the e-learning benefit can be explained by its independent variables by 53.8%%, and at ST3 Telkom by 60.6%. These percentages show that the predictors explained 53.8% for UPNVJ's e-learning benefit and 60.6% e-learning benefit for ST3 Telkom Purwokerto. These findings could be considered as novelties, because they are different from the original model.",CS,AI_ML,0.85,Extracted from log - paper 1758
Project management: a case study of a successful ERP implementation,"The success rate of enterprise resource planning (ERP) implementations is not high in view of the sums invested by organisations in these applications. It has often been indicated that a combination of inadequate preparedness and inappropriate project management have been responsible for the low‐success rate of ERP implementations. The purpose of this paper is to present a case study of a successful ERP implementation. In this paper, the authors use a case study of a very successful roll out of an ERP application in the Irish subsidiary of a UK multinational to investigate the validity of one of the most commonly cited project management frameworks, the project management body of knowledge (PMBOK), to ERP projects. Discussing each category of the framework in turn, the case data to illustrate where the PMBOK framework is a good fit or needs refining for ERP projects is used. It is found that, by and large, PMBOK, because it is a very broad framework, can shed light on most of the key aspects of an ERP project. However, the specificities of this type of project require a different emphasis on some of the factors, as discussed in the authors conclusions. The case analysis also raised some interesting insights into how companies evaluate the success of such highly complex change management initiatives. This research work will need to be extended to cover other case studies of ERP implementation across other industries and organisational contexts; for example in less tightly regulated industries and smaller organisations. This discussion will be of great value to ERP project managers who are in the early stages of a project and need to understand and anticipate the areas which will require specific attention on their part, based on their knowledge of the specific circumstances within their organisational context. This paper presents an investigation into the project management strategy adopted in the Pharma Inc. case and illustrates the mechanics of a successful ERP project implementation, categorised using the PMBOK framework.",CS,AI_ML,0.85,Extracted from log - paper 1759
Mobile-based advisory services for sustainable agriculture: Assessing farmers’ information behavior,"Mobile-based advisory services have significant benefits, including access to agricultural information, knowledge sharing, meteorological information, marketing-related information, and financial services for smallholder farmers. This study aimed to assess farmers’ information behavior regarding mobile-based advisory services and how farmers with different characteristics and attitudes access and adopt information. Data were collected from 382 farmers in Dakhalia governorate, Egypt. The most frequently received information was related to best agricultural practices, weather forecasts, seed varieties and treatment, and water management. Cluster analysis revealed that 47% of the farmers had low information behavior. Seventy-one of the respondents had a favorable attitude toward information retrieval from mobile agricultural services. The information behavior groups of the farmers significantly differed in education, farm size, diversity of agricultural production, and attitude regarding trust and quality of the information provided. Information behavior among farmers has useful implications for policymakers in supporting the long-term benefits of mobile-based advisory services.",CS,AI_ML,0.85,Extracted from log - paper 1760
Factors influencing decision support system acceptance,"While clinical DSS have many proven benefits, their uptake by GPs (general practitioners) is limited. The purpose of this research was to develop and explore a UTAUT (Unified Theory of Acceptance and Use of Technology) based model of how and why GPs accept DSS. Insight into the reasons why GPs do not use clinical DSS combined with knowledge of why GPs use DSS will allow the development of strategies to facilitate more widespread adoption with consequent improvements across many areas. Depth interviews were conducted with 37 GPs comprising a mix of education backgrounds, experience and gender. The developed model indicated that four main factors influence DSS acceptance and use including usefulness (incorporating consultation issue, professional development and patient presence), facilitating conditions (incorporating workflow, training and integration), ease of use and trust in the knowledge base.",CS,AI_ML,0.85,Extracted from log - paper 1761
Critical success factors for ERP implementation in SMEs,"ERP implementation is regarded as complex, cumbersome and costly, and, very often, it exceeds the initial estimated resources. The process involves a thorough examination of the business processes in the organisation; selection of the best available software solution that matches the requirements of the enterprise; configuration of the selected systems;, training of staff; and customisation of the selected software solutions including development of required interfaces. Finally, the existing MIS of the organisation is replaced totally or partially by the new system. All the implementation processes should be carried out without affecting the daily operations across the whole enterprise. This can only be achieved by having an understanding of the key elements forming the infrastructure of the organisation, an effective plan for the implementation and an effective procedure to measure and evaluate the project throughout the implementation process. This paper presents the results of a study to identify and analyse the interrelationships of the critical issues involved in the implementation of ERP in small and medium sized enterprises (SMEs). Three basic research questions were addressed. First, what are the main critical success factors? Second, how do these factors interact throughout the implementation process? Third, which factors have their highest impact and in what stages? In order to answer these questions, over 50 relevant papers were critically reviewed to identify the main critical success factors (CSFs) for ERP implementation in large organisations. Then, the applicability of the identified CSFs to SMEs was investigated. Next, an industrial survey was also undertaken to identify which CSF has highest impact in what stages. The findings on relationships of the critical success factors have been utilised to develop a tool to monitor, and eventually improve, ERP implementations for SMEs. In the development of the tool, eight people from industry and academia with experience of ERP implementations were interviewed with the aim of validating the model being developed. The overall results provide useful pointers to the interplay of organisational and operational factors for the successful implementation of ERP.",CS,AI_ML,0.85,Extracted from log - paper 1762
The effects of remote work on collaboration among information workers,"The coronavirus disease 2019 (COVID-19) pandemic caused a rapid shift to full-time remote work for many information workers. Viewing this shift as a natural experiment in which some workers were already working remotely before the pandemic enables us to separate the effects of firm-wide remote work from other pandemic-related confounding factors. Here, we use rich data on the emails, calendars, instant messages, video/audio calls and workweek hours of 61,182 US Microsoft employees over the first six months of 2020 to estimate the causal effects of firm-wide remote work on collaboration and communication. Our results show that firm-wide remote work caused the collaboration network of workers to become more static and siloed, with fewer bridges between disparate parts. Furthermore, there was a decrease in synchronous communication and an increase in asynchronous communication. Together, these effects may make it harder for employees to acquire and share new information across the network.",CS,AI_ML,0.85,Extracted from log - paper 1763
Blockchain technology in supply chain management: an empirical study of the factors affecting user adoption/acceptance,"Blockchain overcomes numerous complicated problems related to confidentiality, integrity, availability of fast and secure distributed systems. Using data from a cross-sectoral survey of 449 industries, we investigate factors that hinder or facilitate blockchain adoption in supply chains. To capture the most vital aspects of blockchain adoption in supply chains, our conceptual model integrates the unified theory of acceptance and use of technology (UTAUT) model with the task-technology fit (TTF) and information system success (ISS) models, with trust-based information technology innovation adoption constructs. Using structural equation modelling, we find that the ISS, TTF, and UTAUT models positively influence the key factors affecting supply chain employees’ willingness to adopt blockchain. Our results show that the UTAUT’s social influence factor has no significant effect on the intention to adopt blockchain, while inter-organisational trust has a significant effect on the relationship between the UTAUT dimension and intention to adopt blockchain.",CS,AI_ML,0.85,Extracted from log - paper 1764
Towards business intelligence systems success: Effects of maturity and culture on analytical decision making,"The information systems (IS) literature has long emphasized the positive impact of information provided by business intelligence systems (BIS) on decision-making, particularly when organizations operate in highly competitive environments. Evaluating the effectiveness of BIS is vital to our understanding of the value and efficacy of management actions and investments. Yet, while IS success has been well-researched, our understanding of how BIS dimensions are interrelated and how they affect BIS use is limited. In response, we conduct a quantitative survey-based study to examine the relationships between maturity, information quality, analytical decision-making culture, and the use of information for decision-making as significant elements of the success of BIS. Statistical analysis of data collected from 181 medium and large organizations is combined with the use of descriptive statistics and structural equation modeling. Empirical results link BIS maturity to two segments of information quality, namely content and access quality. We therefore propose a model that contributes to understanding of the interrelationships between BIS success dimensions. Specifically, we find that BIS maturity has a stronger impact on information access quality. In addition, only information content quality is relevant for the use of information while the impact of the information access quality is non-significant. We find that an analytical decision-making culture necessarily improves the use of information but it may suppress the direct impact of the quality of the information content.",CS,AI_ML,0.85,Extracted from log - paper 1765
Challenges of Digital Transformation: The Case of the Non-profit Sector,"Nonprofit organizations (NPOs) are critical to the quality of life in many communities not only due to the valuable services and social impact they create, but also because of the positive economic impact within local communities. However, NPOs, just as for-profits, need to innovate in response to changing customer demands and lifestyles and to capitalize on opportunities offered by technology and changing marketplaces, structures and dynamics. Digitalization is essential to fuel NPO's innovation in order to be a differentiator in the highly competitive environment. In this paper, we first develop a review to identify the challenges of digital transformation and then we examine some of the challenges that the nonprofit sector faces in undertaking digital transformation initiative",CS,AI_ML,0.85,Extracted from log - paper 1766
Motivation or Manipulation? Dark UX and Persuasive Elements in Mobile Game Advertisements,"As we live in the modern time and use social media, the amount of advertisements we get exposed to is increasing more and more daily (Andrews, Van Leeuwen, & Van Baaren, 2013, p. 8), the amount of mobile game ads are parallelly increasing in social media and digital landscape. These persuasive lines can cause UX (User Experience) to become a manipulative practice and turn into Dark UX, known as ”Dark Patterns” (Brignull, 2013). This thesis addresses the Dark UX patterns employed in mobile game advertising, including persuasive techniques and deception elements, and it finds out different types of Dark UX strategies in mobile game advertising. These include explicit psychological tricks and Dark UI methods incorporated within the in-game or in-app ad frameworks. These techniques are discussed concerning empiricism, with detailed descriptions of the complex maneuvers employed. The work analyzes the underlying patterns within which the dark UX elements are hidden for the content/narrative and the mobile game UI designs. The inquiry does not just identify problems but also provides critique and design ethics. By analyzing user experiences, interviews, surveys n = 125, and empirical data, the study sheds light on the immediate effectiveness of these tactics compared to their long-term effects: reduced consumer confidence, and more remarkable instances of advertising aversion. It shows a need to switch to honest, user-focused advertising approaches and friendlier UX and UI. This thesis also contributes significantly to the current discourse on digital ethics and consumer protection in an era dominated by online advertising. It underscores the necessity of perpetually updated surveillance mechanisms since digital environments and user behavior constantly change. Besides, this study calls out to the advertisers and developers of mobile games to reassess their ethics and reorganize their advertising strategies to enhance users’ rights and improve the user experience in the short and long term.",CS,AI_ML,0.85,Extracted from log - paper 1767
Causes influencing the effectiveness of the post‐implementation ERP system,"This article aims to find a chain of causal relations affecting the operating effectiveness of the implemented enterprise resource planning (ERP) system instead of focusing on either the evaluation of software/vendors/consultants or critical successful factors (CSF) identification for ERP implementation, a course followed by the dominant ERP literature. This article is a process‐oriented approach and aims to give a moving picture of how one step affects another step from pre‐implementation stage, to during‐implementation stage, and to post‐implementation stage. A significant insight learned from this study is that end‐users across the organization must be educated from the onset of ERP implementation. Although education is a corner‐stone of ERP implementation, the user training is usually only emphasized and the courses are centered on computer/system operation rather than on understanding the ERP concept and spirit. This article may be interesting to some academic researchers and practical managers, and hopefully can provide a link/step for advanced researches in exploring post‐implementation ERP.",CS,AI_ML,0.85,Extracted from log - paper 1768
Investigating the use of data-driven artificial intelligence in computerised decision support systems for health and social care: A systematic review,"There is growing interest in the potential of artificial intelligence to support decision-making in health and social care settings. There is, however, currently limited evidence of the effectiveness of these systems. The aim of this study was to investigate the effectiveness of artificial intelligence-based computerised decision support systems in health and social care settings. We conducted a systematic literature review to identify relevant randomised controlled trials conducted between 2013 and 2018. We searched the following databases: MEDLINE, EMBASE, CINAHL, PsycINFO, Web of Science, Cochrane Library, ASSIA, Emerald, Health Business Fulltext Elite, ProQuest Public Health, Social Care Online, and grey literature sources. Search terms were conceptualised into three groups: artificial intelligence-related terms, computerised decision support -related terms, and terms relating to health and social care. Terms within groups were combined using the Boolean operator OR, and groups were combined using the Boolean operator AND. Two reviewers independently screened studies against the eligibility criteria and two independent reviewers extracted data on eligible studies onto a customised sheet. We assessed the quality of studies through the Critical Appraisal Skills Programme checklist for randomised controlled trials. We then conducted a narrative synthesis. We identified 68 hits of which five studies satisfied the inclusion criteria. These studies varied substantially in relation to quality, settings, outcomes, and technologies. None of the studies was conducted in social care settings, and three randomised controlled trials showed no difference in patient outcomes. Of these, one investigated the use of Bayesian triage algorithms on forced expiratory volume in 1 second (FEV1) and health-related quality of life in lung transplant patients. Another investigated the effect of image pattern recognition on neonatal development outcomes in pregnant women, and another investigated the effect of the Kalman filter technique for warfarin dosing suggestions on time in therapeutic range. The remaining two randomised controlled trials, investigating computer vision and neural networks on medication adherence and the impact of learning algorithms on assessment time of patients with gestational diabetes, showed statistically significant and clinically important differences to the control groups receiving standard care. However, these studies tended to be of low quality lacking detailed descriptions of methods and only one study used a double-blind design. Although the evidence of effectiveness of data-driven artificial intelligence to support decision-making in health and social care settings is limited, this work provides important insights on how a meaningful evidence base in this emerging field needs to be developed going forward. It is unlikely that any single overall message surrounding effectiveness will emerge - rather effectiveness of interventions is likely to be context-specific and calls for inclusion of a range of study designs to investigate mechanisms of action.",CS,AI_ML,0.85,Extracted from log - paper 1769
Application of social media analytics in tourism crisis communication,"Given the newly established communication environment of social media and highly unpredictable crisis situations, this study questioned how tourists facing an unexpected crisis situation use social media to communicate and search for information. To this end, this study developed a multi-phased social media analytic framework (data crawling, data processing and text mining, social network analysis, semantic network analysis, and network visualization) to assess the structure of information exchanges between the members of a tourism organization’s social network community and identified influential actors and information content within the social network. This study’s findings suggest genuine ways of relating with and utilizing opinion leaders and influencers in social media marketing communication as well as crisis communication. The authors expect this proposed methodological framework of social media analytics to help other scholars scientifically identify and implement the proper methodologies for utilizing social media data.",CS,AI_ML,0.85,Extracted from log - paper 1770
Understanding the acceptance of teleconferencing systems among employees: An extension of the technology acceptance model,"Employing the framework of the technology acceptance model (TAM), the present study investigates the factors that affect employees’ acceptance and use of teleconferencing systems for work-related meetings in business settings. Based on survey data of 155 working professionals, a path analysis confirmed the key propositions of TAM. Importantly, the results also showed that both individual factors such as anxiety and self-efficacy, and institutional factors such as institutional support and voluntariness were significantly related to perceived ease of use (PEOU), perceived usefulness (PU), and actual use of the systems. By examining teleconferencing that typically involves group communication within organizations, this study contributes to theoretical refinement of group-based technology use and adoption. Theoretical and practical implications are discussed.",CS,AI_ML,0.85,Extracted from log - paper 1771
Implementing enterprise resource planning systems with total quality control and business process reengineering: Survey results,"The primary objective of an enterprise resource planning (ERP) system is to help integrate an organization's business operations and processes effectively and efficiently. Not all firms have been successful in their ERP implementations and to that end research has helped to identify many factors that might be critical to a successful implementation. Such factors as the use of business process reengineering (BPR), and establishing a total quality management (TQM) culture have all shown to play important roles in ERP implementation. The focus of this survey research on US electronic manufacturing firms is to identify successful integration sequences of TQM and BPR with ERP. The findings reveal that both the sequence of implementation and the strategies selected to initiate ERP systems can significantly impact business performance successfulness.",CS,AI_ML,0.85,Extracted from log - paper 1772
Building better global data governance,"In this article, we explore the challenges of global governance and the particular challenge presented by global data governance. We discuss a range of challenges to developing meaningful global governance institutions for regulating how companies and governments around the world manage and utilize consumer data. These challenges are compounded by their global nature and the complexities of Internet-based technologies. We argue that the following gaps exist for effective global data governance: (a) there is no overarching global framework for protecting consumer data, and it is partial and incomplete; (b) there is a lack of data protection for international data transfers, as much of the regulation that is being developed is not global in scale; and (c) new areas of data collection and use compound concerns to effective data governance in a globalized digital world. Moreover, we highlight important needs in terms of both global governance and impending challenges related to current and new uses of data. Any global governance framework should recognize the need for an iterative process where communication is ongoing between the necessary stakeholders. Agreements should incorporate common goals to maximize the potential development of global data governance norms. However, goals must remain flexible to the different data environments across nation-states while maintaining a global scope to ensure data protection. In addition, any agreement should consider the emerging challenges in this area. These challenges include new methods of data collection and use, as well as protecting individuals from manipulation and undue influence based on how their data are being used, processed, and collected.",CS,AI_ML,0.85,Extracted from log - paper 1773
A decision support system to improve the efficiency of resource allocation in healthcare management,"Limitations in healthcare funding require hospitals to find more effective ways to utilize resources. An effective patient management system is critically dependent on the accurate analysis of individual patient outcomes and resource utilization. In the current paper, a management-oriented decision support model is thus proposed to assist health system managers in improving the efficiency of their systems. In the first stage of the model, the key variables affecting system efficiency, as well as their causal relationships, are identified through causal maps. Efficiency is measured by the total time spent in the system. In the second stage, a Bayesian Belief Network (BBN) is employed to represent both the conditional dependencies and uncertainties of the key variables. In the third stage, a sensitivity analysis is performed using a BBN to determine the most critical variable(s) in terms of impact on the system. Finally, strategies to improve system efficiency are proposed. The suggested decision support system is applied to the tomography section in the radiology department of a private hospital in Turkey.",CS,AI_ML,0.85,Extracted from log - paper 1774
Factors Affecting Organizations’ Resistance to the Adoption of Blockchain Technology in Supply Networks,"From a supply chain perspective, new technologies such as blockchain can improve the efficiency and competitiveness of logistics and increase customer satisfaction. Although blockchain technology has been lauded as a way for firms to build sustainable supply chain networks, the rate of acceptance of this technology remains low. Therefore, this study seeks to identify the factors that discourage firms from merging blockchain with the supply chain. Instead of providing further reasons for adopting blockchain technology, we try to understand what deters firms from adding blockchain to their operations. Following the deductive approach, a confirmatory factor analysis is conducted on pre-test questionnaires to test, improve, and verify the constructs (questions) to measure the hypothesized factors. A theoretical model is proposed based on the hypotheses, and structural equation modeling is applied. The results are estimated using the partial least squares approach and a sample of 83 respondents. Our findings based on our empirical data support most of our hypotheses. We find that various factors impede the adoption of blockchain technologies, including technological barriers, constraints rooted in organizations and the environment, and system-related governmental barriers. In addition, various factors are critical determinants of resistance to blockchain in the technological, organizational, and environmental dimensions.",CS,AI_ML,0.85,Extracted from log - paper 1775
Digital financial inclusion: A gateway to sustainable development,"The covid-19 pandemic revolutionises digital financial services, and hence digital financial inclusion is essential to ensure everyone can access digital financial services and thus promote sustainable economic growth. The development and activities promoting digital financial inclusion must align and help attain 2030 Sustainable Development Goals (SDGs). While the pandemic is anticipated to increase the usage of digital financial services, it has also created challenges for certain countries. Hence, a systematic literature review explores digital financial inclusion across countries. This research finds that developing countries, mainly Asian countries, embrace and improve digital financial inclusion to help reduce poverty. However, the results indicate that in developing countries, a persistent divide exists between gender, the wealthy and the poor, and urban and rural areas regarding access to and usage of digital financial services. At the end of the study, we propose a few recommendations, focusing on improving digital infrastructure, simplifying the complicated banking procedures, and stressing the importance of financial education, enabling the smooth implication of digital financial inclusion across countries.",CS,AI_ML,0.85,Extracted from log - paper 1776
Disaster and crisis management in Turkey: a need for a unified crisis management system,"Crisis management has gained importance in the policy agendas of many countries around the world due to the increases in the number of natural disasters and terrorist attacks. This paper illustrates how the Turkish Government’s Disaster and Crisis Management System has been developed and makes a qualitative evaluation of the current disaster and crisis management systems. A literature review shows that the disaster and crisis management system in Turkey has been developed after tragic events. The paper examines what kinds of initiatives were introduced and what trends in shift have occurred. After analyzing recent cases and exploring some government initiatives, alternative approaches and suggestions are included. Turkey has developed its disaster and crisis management system since 1930, which mostly depended on experiences. The current system is governed by a centralized structure that is the responsibility of different ministries. Nonetheless, the system is very weak at the local level. Furthermore, participation of non-profit organizations is very limited at both national and local levels. Thus, coordination and management of first-response operations during crises are problematic and ineffective. Particularly, the system is not designed for different types of crises such as terrorist attacks. Crisis management in Turkey needs a more unified and flexible structure to deal with current problems effectively. Further suggestions for better implementation are also provided. The effectiveness of the disaster and crisis management system is analyzed in natural and man-made disasters. Findings show that centralized and decentralized systems have different functions in different situations.",CS,AI_ML,0.85,Extracted from log - paper 1777
Knowledge management system for fourth generation R&D: KNOWVATION,"Since the advent of the embryonic model almost a century ago, R&D systems have gone through an evolutionary process of development that can be classified into three generations. Today, the fourth generation of R&D is emerging that emphasizes both strategic and operational importance of knowledge management (KM). Despite the importance of KM, the network between conceptual scheme of the fourth generation R&D and practical system of KM remains a missing link. In response, the main objective of this paper is to present a framework for designing and implementing knowledge management system (KMS) for the fourth generation R&D. The proposed system is named KNOWVATION, which combines the notions of knowledge and innovation. First, the evolutionary classification of the R&D generations and the corresponding characteristics of the respective generations are defined. Second, the organizational structure and knowledge functions of the fourth generation R&D are derived. Finally, the overall design framework and detailed sub-modules are presented.",CS,AI_ML,0.85,Extracted from log - paper 1778
From conventional governance to e-democracy: Tracing the evolution of e-governance research trends using network analysis tools,"The adoption of e-governing practices has revolutionised the administrative machinery of governments worldwide by improving efficiency, transparency, and accountability. Researchers and administrators often aim to identify emerging research fronts and the timeline of the evolution to forecast and implement technology. In this work, we systematically investigate the trajectory of the global evolution and emerging research fronts as well as the prospects for e-governance using citation network analysis. The growth curve fitted to the number of articles published per year shows that the research activities are still in the ascendant phase. We visualise the global main path of the citation network and investigate the patterns to trace the knowledge diffusion path, major milestones, and emerging research fronts. The cluster analysis identifies the major topics of research as administration and information system management, e-governance framework design, efficiency or quality evaluation, and the application of social networks and open data leading to e-democracy. The adoption of open data and social networking for user interactions with government that leads to participatory governance are the emerging research trends. We also identify research that can have a future impact based on network parameters. The results contribute to the literature by setting the focus of future research, and assisting administrators in selecting suitable models and methodologies, and manufacturers with the development of required technical devices suitable for the upcoming phase of symbiosis.",CS,AI_ML,0.85,Extracted from log - paper 1779
Artificial Intelligence-Augmented Decision-Making: Examining the Interplay Between Machine Learning Algorithms and Human Judgment in Organizational Leadership,"The paper discusses the bright and dark sides of the relationship between human judgment and AI-driven machine learning (ML) algorithms. While discussing important issues, such as algorithm aversion, automation bias, and trust, it probes into how AI improves decision-making efficiency through predictive accuracy, resource optimisation, and data-driven insights. Even as AI can revolutionise decision-making, its effective integration must balance algorithmic output and human judgment. The most critical challenges include automation bias resulting from over-reliance on advice given by AI and algorithm aversion driven by concerns related to AI failures. Open systems, explainable AI (XAI) frameworks, and user-centered design can help to engender confidence in AI systems and alleviate these issues. Accountability, equity, and prejudice concerns raise further ethical considerations with AI. The study proposed several tactics that might mitigate such challenges: audits of ethics, adherence to legal policy, and integration of the AI systems with the company’s values. It underlines the human-AI collaboration that will be increasingly necessary, as well as hybrid models for decision-making that bring algorithmic accuracy to human intuition. It follows the case study review and empirical findings with practical lessons for organisational leaders on ethics, best deployment practices for AI, and tactical ways to engender better collaboration and trust. The conclusion outlines the need to enhance the explainability features of AI, study cognitive dynamics in decision processes, and work out ethical schemata guiding leading positions for AI. Beyond providing a roadmap for organisations to leverage the interaction of human judgment and machine intelligence to drive and achieve more ethical and effective leadership outcomes, this paper tries to contribute to the ongoing debate on AI-augmented decision-making.",CS,AI_ML,0.85,Extracted from log - paper 1780
A Blockchain Ecosystem for Digital Identity: Improving Service Delivery in Canada’s Public and Private Sectors,"Blockchain-based solutions have the potential to make government operations more efficient and improve the delivery of services in the public and private sectors. Identity verification and authentication technologies, as one of the applications of blockchainbased solutions – and the focus of our own efforts at SecureKey Technologies – have been critical components in service delivery in both sectors due to their power to increase trust between citizens and the services they access. To convert trust into solid value added, identities must be validated through highly-reliable technologies, such as blockchain, that have the capacity to reduce cost and fraud and to simplify the experience for customers while also keeping out the bad actors. With identities migrating to digital platforms, organizations and citizens need to be able to transact with reduced friction even as more counter-bound services move to online delivery. In this article, drawing on our own experiences with an ecosystem approach to digital identity, we describe the potential value of using blockchain technology to address the present and future challenges of identity verification and authentication within a Canadian context.",CS,AI_ML,0.85,Extracted from log - paper 1781
Information systems and sustainable supply chain management towards a more sustainable society: Where we are and where we are going,"The objectives of this study are to identify and systematize scholarly articles on the use of information system to support sustainable supply chain management and to suggest future research opportunities. Therefore, a structured literature review was conducted. The most relevant studies identified were classified and categorized into seven dimensions: research context, research focus, research method, sector analyzed, information system (IS) beneficiaries, relationship between IS and green supply chain practices, and performance benefits. The main authors and articles on this particular topic were identified. In addition, it was concluded that IS is an important support tool for sustainable supply chain management practices since it brings benefits to the organization, suppliers, and customers. Furthermore, IS positively influences the operational, financial, and environmental performance of the organization. However, further advances in the literature are still needed. The major contribution of this research is related to the recommendations that provide opportunities for future research.",CS,AI_ML,0.85,Extracted from log - paper 1782
Ensuring information security in the field of remote work,"Remote work is a forced measure introduced by employers in order to prevent a viral infection. For employees, there are pluses in remote work - saving time and money on the road and high labour productivity because nothing distracts. There is a separate issue of information security for the employer when organising such a work regime for their employees. Any use of materials is allowed only with a hyperlink. Nowadays, in the realities of distance work, information security is coming first. Employees send all the information online; they use their data and send confidential information. Protection of personal data becomes a crucial point. The article deals with problems of ensuring information security in the field of remote work. The problems of information security during restrictive actions in connection with the coronavirus pandemic and the transfer of personnel to remote work are discussed. The threat of information leaks through remote workers is relatively high since the specialists responsible for the organisation's information security do not have the opportunity to apply the entire arsenal of technical means and policies, with the help of which security is ensured at workstations in the office. Information leakage will lead to severe problems, so it is essential to consider what means you can use to ensure the company's information security.",CS,AI_ML,0.85,Extracted from log - paper 1783
Information systems integration in mergers and acquisitions: A normative model,"The role of Information Systems in Mergers and Acquisitions (M&A) becomes increasingly important as the need for speed of reaction and information is growing. A model for IS integration in M&A is presented; this includes the categories and strategic objectives of external growth as well as consideration of the possible choices for the hardware and software configuration after completion of the M&A. The basic variables of the model are defined and the possible IS integration strategies are classified in a matrix having, as dimensions, the software configuration and the computer architecture. The IS integration model resulting from these basic elements is divided into a descriptive model and a decision support model: the first one shows the relations among the involved variables, while the second provides a practical tool for decision-making. Both these models have been tested with a survey guided by a questionnaire; Factor Analysis was used to analyze the results.",CS,AI_ML,0.85,Extracted from log - paper 1784
DEVELOPING ADAPTIVE LEARNING MANAGEMENT APPLICATION FOR PROJECT TEAM IN IT-INDUSTRY,"Keeping employees aligned with modern trends and developments in their professional areas is the main focus of a lifelong lea rning approach. That becomes even more important for such dynamic industries like Information Technology. Therefore, it’s crucial to extend existing e-learning management system with an adaptive training component that enables the effective study of on-demand skills, leading to a broader range of candidates available for project management to select from and consequently improving the overall performance of an IT company. To improve the existing learning process according to company's and employee's needs the overview of a typical learning management system functionality is given in this paper. The main benefits from the adoption of a learning management system in small and medium-sized IT-companies were discussed, analysis of their features and problems was given. The adjustment plan for the typical learning management system to be suitable for the information technology domain including module of the adaptive learning content selection using the basic principles of graph theory was proposed to reduce the time of the learning process. LMS OpenOLAT was reworked according to the adjustment plan which is reflected by the number of diagrams such as sequence diagram, IDEF0 business process description, activity diagram that shows search algorithm steps and application component diagram. Also, GUI was adjusted to provide user with a good look and feel. The benefits of the proposed approach in business processes of IT-company are shown using “Academy – Smart”. To prove the efficiency of the proposed algorithm, real courses were used. Based on the learning material, provided by “Coursera”, a number of test cases was formed and analyzed. After applying adaptive content selection algorithm according to the models of “Academy – Smart” employees, learning time was reduced and optimized. This investigation has shown significant improvement in the resource management process and acceleration of the learning process for employees.",CS,AI_ML,0.85,Extracted from log - paper 1785
Rethinking Artificial Intelligence: Algorithmic Bias and Ethical Issues| How Process Experts Enable and Constrain Fairness in AI-Driven Hiring,"Organizations risk losing their competitive edge as they struggle to find and hire qualified talent. Hiring personnel turn to artificial intelligence (AI) tools to help acquire talent, increase efficiency, and reduce costs. Yet despite the best intentions for integrating fair and evidence-based systems, exacerbated levels of bias may occur from using these tools. Drawing from scholarship on process expertise and emerging practices of AI use at work, I provide a case study of 42 high-volume recruiters and uncover how hiring personnel enact and justify unsystematic sourcing practices within the confines of their held expertise, organizational demands, and technology choices. I explain how AI-based hiring decisions in organizations are context dependent and blend the capabilities of algorithmic-powered tools with choices and judgments made by process experts. I conclude by offering theoretical and practical considerations for expertise, hiring, and the integration of algorithms at work.",CS,AI_ML,0.85,Extracted from log - paper 1786
MES-integrated digital twin frameworks,"Industry 4.0-based manufacturing systems are equipped with Cyber-Physical Systems that are characterized by a strong interlinkage between the real world and the digital one: actions in one world have an impact on the other. In this paradigm, Digital Twins (DT) are defined as simulation models that are both getting data from the field and triggering actions on the physical equipment. However, most of the claimed DT in literature are only replicating the real system in a synchronized fashion, without feeding back actions on the control system of the equipment. In literature, these are referred to as Digital Shadows (DS). The paper proposes a way to integrate a DS simulation model with the Manufacturing Execution System (MES) in this way creating a DT. The MES-integrated DT is used for decision making thanks to the presence of an intelligence layer that hosts the rules and the knowledge to choose among alternatives. The paper proposes two frameworks based on the MES-integrated DT: one for managing error states and one for triggering disassembly processes as a consequence of low assembly quality. The DT simulation is developed and integrated with the MES of the Industry 4.0 Laboratory at the School of Management of Politecnico di Milano, where the proposed frameworks have been tested and validated.",CS,AI_ML,0.85,Extracted from log - paper 1787
An Organizational Culture-Based Theory of Clinical Information Systems Implementation in Hospitals,"We propose an organizational culture-based explanation of the level of difficulty of clinical information system (CIS) implementation and of the practices that can contribute to reduce the level of difficulty of this process. Adopting an analytic induction approach, we developed initial theoretical propositions based on a three-perspective conceptualization of organizational culture: integration, differentiation, and fragmentation. Using data from three cases of CIS implementation, we first performed a deductive analysis to test our propositions on the relationships between culture, CIS characteristics, implementation practices, and the level of implementation difficulty. Then, applying an inductive analysis strategy, we re-analyzed the data and developed new propositions. Our analysis shows that four values play a central role in CIS implementation. Two values, quality of care and efficiency of clinical practices, are key from an integration perspective; two others, professional status/autonomy and medical dominance, are paramount from a differentiation perspective. A fragmentation perspective analysis reveals that hospital users sometimes have ambiguous interpretations of some CIS characteristics and/or implementation practices in terms of their consistency with these four values. Overall, the proposed theory provides a rich explanation of the relationships between CIS characteristics, implementation practices, user values, and the level of difficulty of the implementation process.",CS,AI_ML,0.85,Extracted from log - paper 1788
Social media analytics: a tool for the success of online retail industry,"Social media has become a part of life for a larger segment of the people globally. Businesses are also taking advantage of it, to instantly and easily create and share information with the customers and get benefited from this seamless communication. As a result of the increase in both social media platforms and users, the need to monitor, extract, analyse and report the data being created on these platforms has also increased for the Businesses. Present work identifies the social media analytics (SMA) process and online retail application; it identifies the different phases of the SMA process with their integration to the online retail strategy, challenges, and opportunities. SMA metrics for customer life-cycle (acquisition to enhancement) are discussed. A comparative analysis has been presented for different SMA tools to identify their utility to the online retail.",CS,AI_ML,0.85,Extracted from log - paper 1789
Blockchain-based academic credential verification system,"In Ghana, academic record-keeping and verification methods have become increasingly burdensome, error-prone, and susceptible to data security, privacy, and fraud prevention issues. Traditional systems often rely on paper-based records or fragmented digital databases, which can lead to inefficiencies, human errors, and difficulties in authenticating and validating academic credentials. These challenges can hinder students' and graduates' efforts to pursue further education or employment opportunities and create administrative burdens for educational institutions and employers. Blockchain technology, an emerging innovation, offers a promising solution to address these challenges. By digitizing and securely storing academic records on a blockchain, the system will facilitate efficient, accurate, and tamper-proof verification of credentials. This will reduce errors and fraud and improve academic qualifications' overall reliability and credibility. Furthermore, the system will enhance data privacy by allowing individuals to control who can access their records and under what conditions. This applied project creates a user-friendly platform that simplifies the verification process for students, graduates, educational institutions, and employers. By harnessing the power of blockchain technology, we can revolutionize academic record-keeping and verification in Ghana, ensuring a more secure, efficient, and trustworthy system for all stakeholders involved.",CS,AI_ML,0.85,Extracted from log - paper 1790
Adoption of cloud computing as innovation in the organization,"Over the years, there has been a heavy reliance on cloud computing as IT has innovated through time. In recent times cloud computing has grown monumentally. Many organizations rely on this technology to perform their business as usual and use it as a backbone of their companies' IT infrastructure. This paper investigates the organizational adaptation for cloud computing technology - reviewing case studies from various institutions and companies worldwide to provide a detailed analysis of innovative techniques with cloud computing. We investigate the features and delivery approaches cloud computing offers and the potential challenges and constraints we face when adopting cloud computing into the business setting. We also explore the cybersecurity elements associated with cloud computing, focusing on intrusion detection and prevention and understanding how that can be applied in the cloud. Finally, we investigate the future research directions for cloud computing and expand this paper into further articles with experiments and results.",CS,AI_ML,0.85,Extracted from log - paper 1791
Deployment of Infrastructure and Services in the Open Grid Service Architecture,"The ability to deploy Grid infrastructure and services across organizational boundaries (rapidly, reliably, and scalably) is critical for the success of large-scale service based grids such as OGSA. We report the results of the UK-OGSA Evaluation Project infrastructure and services deployment experiments, and analytically compare application versus service deployment. The use of a 3rd party component deployment technology to remotely automate installation and service deployment is discussed, and outstanding problems and potential solutions and benefits are presented. We conclude that grid deployment must be treated as a first-order activity by integrating secure deployment capabilities into the middleware, to enable deployment of secured infrastructure and services across organizations.",CS,AI_ML,0.85,Extracted from log - paper 1792
Exploring the deployment of software libraries in a digital innovation context,"This article reports on the deployment of such software libraries in the web and mobile (Android) contexts by 107 start-up companies in London. Our findings show that libraries owned by big-tech companies, product vendors, and communities coexist; that the deployment of big-tech libraries is unaffected by the scale of the deploying start-up; and that context evolution paths are consequential for library deployment. These findings portray a balanced picture of digital infrastructure as neither the community-based utopia of early open-source research nor the dystopia of the recent digital dominance literature. The study aims at addressing the following research question: How does the ownership of software libraries affect their deployment? A software library is a standard way of packaging code that becomes a boundary resource when it is offered outside of an organization. Software libraries are consistent with the bidirectional causality of boundary resources, as they are designed to advance the goals of both owners and users. However, the trade-offs underlying these resources are different for communities (innovation vs coherence), software vendors (complexity vs flexibility), and large platform owners (generativity vs control). These distinct trade-offs suggest that the deployment of software libraries is affected by who owns them: communities, proprietary software vendors, or big-tech providers. To answer this research question, we observe the deployment of web and mobile (Android) software libraries by 107 start-ups operating in London, one of the largest European digital innovation clusters. Our empirical analysis confirms that ownership is consequential for software library deployment, which is more significantly affected by the software development context (web or mobile) than by the scale of the deploying company.",CS,AI_ML,0.85,Extracted from log - paper 1793
Infrastructure as Software in Micro Clouds at the Edge,"Edge computing offers cloud services closer to data sources and end-users, making the foundation for novel applications. The infrastructure deployment is taking off, bringing new challenges: how to use geo-distribution properly, or harness the advantages of having resources at a specific location? New real-time applications require multi-tier infrastructure, preferably doing data preprocessing locally, but using the cloud for heavy workloads. We present a model, able to organize geo-distributed nodes into micro clouds dynamically, allowing resource reorganization to best serve population needs. Such elasticity is achieved by relying on cloud organization principles, adapted for a different environment. The desired state is specified descriptively, and the system handles the rest. As such, infrastructure is abstracted to the software level, thus enabling ""infrastructure as software"" at the edge. We argue about blending the proposed model into existing tools, allowing cloud providers to offer future micro clouds as a service.",CS,AI_ML,0.85,Extracted from log - paper 1794
Implementing Cloud-Based Enterprise Resource Planning Solutions in Small and Medium Enterprises,"Lacking strategies to implement a cloud-based enterprise resource planning (ERP) solution in small and medium-sized enterprises (SMEs) can lead to a failed implementation. SME owners can improve company performance by integrating company processes by successfully implementing a cloud-based ERP solution. Grounded in the diffusion of innovation theory augmented with business process management design for Six Sigma, the purpose of this qualitative multiple case study was to explore strategies SME owners use to implement cloud-based ERP solutions. The participants consisted of 4 SME owners in Lebanon who successfully implemented a cloud-based ERP solution and improved company performance and growth. Data were collected using semistructured interviews and a review of ERP implementation project documents and analyzed using thematic analysis. Seven themes emerged: top management support for IT implementation, requirements identification, software selection, user involvement, project management, change management, and post-implementation performance monitoring. A key recommendation for SME owners is to support IT implementation and remain involved throughout the implementation process. The implications for positive social change include the opportunity to increase employment opportunities, economic growth, and reducing the adverse environmental consequences of computing by using cloud-based technologies.",CS,AI_ML,0.85,Extracted from log - paper 1795
Effects of IT Infrastructure Services on Business Process Implementation-Focus on Small and Medium Enterprises in Emerging Markets,"An organization’s information technology (IT) infrastructure capability is increasingly realized as a critical part to business effectiveness and efficiency. IT infrastructure services are particularly important for organizations looking to deploy business processes in developing markets. There has also been an interest from many small and medium sized organizations whose core business is not in IT to outsource and manage these services through third party service providers. However there is a need to create an understanding for these organizations to deploy the right infrastructure services in order to enable easier implementation or reengineering of the business process. There has been little research focusing on the patterns of the IT infrastructure capabilities in the small and medium sized organizations in the developing markets. The research aims for a comprehensive coverage by analyzing the requirements in the developing markets and proposing a selection model for the organizations to choose IT service provider in case they decide to outsource the infrastructure services. The effect of the IT infrastructure services on the business process implementation is presented with an emphasis on the boundary crossing services. Using empirical case study, the research analyses a firm in developing markets and compares it against four strategically similar organizations from different industries. Data collection was primarily qualitative and ably supported by secondary data. The requirements in developing markets reflect the same as in mature markets. The pricing is seen to play a major role in the selection of the service providers with service security not very much organization’s priority. The number of boundary crossing services effectively enables information sharing and control. These services are the drivers in simplifying the business process implementation. The findings have implications for both business and technical managers in regard to planning the IT strategy in the long term and developing appropriate infrastructure according to the process needs.",CS,AI_ML,0.85,Extracted from log - paper 1796
B4: Experience with a Globally-Deployed Software Defined WAN (2013),"We present the design, implementation, and evaluation of B4, a private WAN connecting Google’s data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to fully utilize any available capacity, and iii) strict bandwidth and latency requirements that ensure reliable application performance. B4 leverages software-defined networking principles and centralized traffic engineering to achieve high utilization, while carefully managing network updates to avoid transient congestion. Our experiments show that B4 consistently operates at high utilization, carrying 60% more traffic than traditional approaches, and quickly adapts to traffic changes and failures with minimal impact on application performance.",CS,AI_ML,0.85,Extracted from log - paper 1797
Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network (2015),"We present our approach for overcoming the cost, operational complexity, and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First, multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second, much of the general, but complex, decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator, pre-planned datacenter networks; we built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third, modular hardware design coupled with simple, robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites worldwide, scaling capacity by 100× over ten years to more than 1 Pbps of bisection bandwidth.",CS,AI_ML,0.85,Extracted from log - paper 1798
SWAN: Achieving High Utilization with Software-Driven WAN (2013),"We present SWAN, a system that boosts the utilization of inter-datacenter networks by centrally controlling when and how much traffic each service sends and frequently reconfiguring the network’s data plane to match current demand. Done simplistically, such reconfigurations can cause transient congestion because different switches update at different times. We develop a novel technique leveraging a small amount of extra capacity on links to apply updates in a provably congestion-free manner, without assumptions on update ordering. Further, to scale to large networks with limited forwarding table space, SWAN greedily selects a small set of routes that best satisfy current demand. It updates this set without disrupting traffic by using small amounts of spare forwarding table capacity. Experiments on a testbed prototype and simulations on production network traces show that SWAN carries 60% more traffic than current practices.",CS,AI_ML,0.85,Extracted from log - paper 1799
"ONOS: Towards an Open, Distributed SDN OS (2014)","We present our experiences building ONOS (Open Network Operating System), an experimental distributed SDN control platform motivated by the performance, scalability, and availability requirements of large operator networks. We describe and evaluate two ONOS prototypes. The first version implemented core features: a distributed, but logically centralized, global network view; scale-out; and fault tolerance. The second version focused on improving performance. Based on our experience with these prototypes, we identify additional steps required for ONOS to support use cases such as core network traffic engineering and scheduling, and to become a usable open-source, distributed network OS platform that the SDN community can build upon.",CS,AI_ML,0.85,Extracted from log - paper 1800
ClickOS and the Art of Network Function Virtualization (2014),"Over the years middleboxes have become a fundamental part of today’s networks. Despite their usefulness, they come with a number of problems due to being hardware-based: they are costly, difficult to manage, and hard to change. To address these issues, there is a recent trend towards Network Function Virtualization (NFV), turning these middleboxes into software-based, virtualized entities. Towards this goal we introduce ClickOS, a high-performance, virtualized software middlebox platform. ClickOS virtual machines are small (~5 MB), boot quickly (~30 ms), add little delay (~45 µs), and over a hundred can run concurrently while saturating a 10 Gb pipe on commodity hardware. We implement a range of middleboxes (firewall, NAT, load balancer) and show that ClickOS can handle millions of packets per second, demonstrating its viability as an NFV platform.",CS,AI_ML,0.85,Extracted from log - paper 1801
E2: A Framework for NFV Applications (2015),"We present E2, a framework for deploying and managing network function virtualization (NFV) applications at scale. E2 provides a unified system for operators to manage a wide range of network functions (NFs) using a logically centralized controller. It introduces a simple programming model for composing and scaling NFs, and an efficient execution substrate that places and chains NFs across servers. E2’s design balances centralized control with distributed data plane execution to achieve scalability and performance. In evaluations on a cluster testbed, E2 shows low overhead and can manage and scale complex NFV service chains across tens of servers while maintaining high packet processing throughput.",CS,AI_ML,0.85,Extracted from log - paper 1802
"Andromeda: Performance, Isolation, and Velocity at Scale in Cloud Network Virtualization (2018)","This paper presents our design and experience with Andromeda, Google Cloud Platform’s network virtualization stack. Our production deployment poses challenges in performance, isolation, and feature velocity at scale. We describe Andromeda’s architecture, which uses a novel datapath leveraging hardware offloads and custom NIC features to achieve near-native performance for VMs while ensuring strong tenant isolation. We also share how Andromeda’s design and continuous evolution allowed rapid deployment of new network features and protocols without sacrificing performance or reliability. In our evaluation, Andromeda delivers high throughput and low latency close to hardware limits, with minimal interference between tenants, demonstrating an effective balance of performance and multi-tenancy in a large-scale cloud.",CS,AI_ML,0.85,Extracted from log - paper 1803
P4: Programming Protocol-Independent Packet Processors (2014),"We present P4, a high-level programming language for packet processing that allows programming protocol-independent packet processors. P4’s key goals are protocol independence (the ability to reconfigure devices to support new protocols without changing hardware), field reconfigurability (the ability to modify header formats and processing), and target independence (code portability across different device types). We describe the P4 language, including its programming model and example programs, and how a P4 program is compiled down to different targets. P4 enables network operators to specify how packets are processed on switches and NICs in software-defined networks with unprecedented flexibility. Initial evaluations and use cases demonstrate that P4 can express a wide range of protocols and processing behaviors, and that P4 programs can be compiled to run efficiently on existing hardware targets.",CS,AI_ML,0.85,Extracted from log - paper 1804
Hedera: Dynamic Flow Scheduling for Data Center Networks (2010),"Hedera is a scalable, dynamic flow scheduling system for data center networks. It detects long-lived large flows (elephants) at the edge switches and dynamically schedules them across multiple paths to avoid congestion and improve throughput. Hedera employs a central scheduler that periodically collects flow information and computes non-conflicting paths using a global view of network topology and utilization. We implemented Hedera on a testbed with a multi-rooted tree topology. Experiments show that Hedera achieves up to 113% higher bisection bandwidth than static flow scheduling (ECMP) under realistic workloads, effectively balancing load by moving elephant flows without impacting short flows. Hedera demonstrates that dynamic flow scheduling can significantly improve network utilization and application performance in data centers.",CS,AI_ML,0.85,Extracted from log - paper 1805
CONGA: Distributed Congestion-Aware Load Balancing in Datacenter Networks (2014),"We introduce CONGA, a distributed congestion-aware load balancing mechanism for datacenter fabrics. CONGA operates in leaf-spine Clos networks and splits TCP flows (flowlets) over multiple paths based on real-time congestion feedback. Each leaf switch performs distributed load balancing by adjusting traffic splitting in response to congestion metrics piggybacked from remote switches. This avoids a centralized controller and reacts quickly to traffic changes. We implemented CONGA in hardware on a 40 GbE data center switching ASIC. In testbed experiments, CONGA closely tracks the optimal load distribution under varying workloads, achieving near-optimal throughput and low latency. Compared to ECMP, CONGA improves throughput by up to 2.5× in incast scenarios and significantly reduces flow completion times, all with minimal overhead and fast convergence.",CS,AI_ML,0.85,Extracted from log - paper 1806
"Omega: Flexible, Scalable Schedulers for Large Compute Clusters (2013)","Increasing scale and rapidly changing requirements are hard to meet with current monolithic cluster scheduler architectures, which limit feature deployment and efficiency. We present a novel scheduling approach using parallelism, shared state, and optimistic concurrency control. We compare this approach to existing designs, evaluate scheduler interference and its practical impact, propose techniques to mitigate conflicts, and discuss a use-case highlighting advantages of our approach using real Google workload traces. Our system, Omega, uses a shared-state scheduling architecture that allows multiple parallel schedulers to independently make allocation decisions on a common resource pool, resolving conflicts optimistically. Results from simulations and Google’s production workload traces show that Omega’s parallel scheduling can achieve competitive performance relative to traditional architectures while greatly improving scheduler extensibility and agility.",CS,AI_ML,0.85,Extracted from log - paper 1807
Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center (2011),"We present Mesos, a platform for sharing commodity clusters between multiple diverse cluster computing frameworks (such as Hadoop and MPI). Sharing improves cluster utilization and avoids per-framework data replication. Mesos shares resources in a fine-grained manner, allowing frameworks to achieve data locality by taking turns accessing each node’s data. To support today’s sophisticated frameworks, Mesos introduces a distributed two-level scheduling mechanism called resource offers. Mesos decides how many resources to offer each framework, while frameworks decide which resources to accept and what tasks to run on them. Our results show Mesos achieves near-optimal data locality when sharing a cluster among diverse frameworks, can scale to 50,000 nodes, and is resilient to failures. By enabling fine-grained sharing across frameworks, Mesos lets organizations run multiple specialized processing frameworks on a shared cluster efficiently.",CS,AI_ML,0.85,Extracted from log - paper 1808
Edge Computing Architecture for Applying AI to IoT (2017),"The proliferation of connected IoT devices creates a big data challenge for AI-based approaches. The response time requirements of such devices necessitate processing data at the edge, but the edge often lacks resources to train AI models. We present an edge computing architecture that preserves the advantages of edge processing (low latency, reduced bandwidth use) and cloud-centric computing (compute power for AI model training)...",CS,AI_ML,0.85,Extracted from log - paper 1809
"A Survey on Security Challenges in Cloud Computing: Issues, Threats, and Solutions (2020)","Cloud computing has gained huge attention over the past decades because of continuously increasing demands. There are several advantages to organizations moving toward cloud-based data storage solutions. These include simplified IT infrastructure and management, remote access from effectively anywhere in the world with a stable Internet connection and the cost efficiencies that cloud computing can bring. The associated security and privacy challenges in cloud require further exploration. Researchers from academia, industry, and standards organizations have provided potential solutions to these challenges in the previously published studies. The narrative review presented in this survey provides cloud security issues and requirements, identified threats, and known vulnerabilities. In fact, this work aims to analyze the different components of cloud computing as well as present security and privacy problems that these systems face. Moreover, this work presents new classification of recent security solutions that exist in this area. Additionally, this survey introduced various types of security threats which are threatening cloud computing services and also discussed open issues and propose future directions. This paper will focus and explore a detailed knowledge about the security challenges that are faced by cloud entities such as cloud service provider, the data owner, and cloud user.",CS,AI_ML,0.85,Extracted from log - paper 1810
Security Smells in Ansible and Chef Scripts: A Replication Study (2020),"Context: Security smells are recurring coding patterns that are indicative of security weakness, and require further inspection. As infrastructure as code (IaC) scripts, such as Ansible and Chef scripts, are used to provision cloud-based servers and systems at scale, security smells in IaC scripts could be used to enable malicious users to exploit vulnerabilities in the provisioned systems. Goal: The goal of this paper is to help practitioners avoid insecure coding practices while developing infrastructure as code scripts through an empirical study of security smells in Ansible and Chef scripts. Methodology: We conduct a replication study where we apply qualitative analysis with 1,956 IaC scripts to identify security smells for IaC scripts written in two languages: Ansible and Chef. We construct a static analysis tool called Security Linter for Ansible and Chef scripts (SLAC) to automatically identify security smells in 50,323 scripts collected from 813 open source software repositories. We also submit bug reports for 1,000 randomly-selected smell occurrences. Results: We identify two security smells not reported in prior work: missing default in case statement and no integrity check. By applying SLAC we identify 46,600 occurrences of security smells that include 7,849 hard-coded passwords. We observe agreement for 65 of the responded 94 bug reports, which suggests the relevance of security smells for Ansible and Chef scripts amongst practitioners. Conclusion: We observe security smells to be prevalent in Ansible and Chef scripts, similar to that of the Puppet scripts. We recommend practitioners to rigorously inspect the presence of the identified security smells in Ansible and Chef scripts using (i) code review, and (ii) static analysis tools.",CS,AI_ML,0.85,Extracted from log - paper 1811
CherryPick: Adaptively Unearthing the Best Cloud Configurations (2017),"Picking the right cloud configuration for recurring big data analytics jobs running in clouds is hard, because there can be tens of possible VM instance types and even more cluster sizes to pick from. Choosing poorly can significantly degrade performance and increase the cost to run a job by 2-3x on average, and as much as 12x in the worst-case. However, it is challenging to automatically identify the best configuration for a broad spectrum of applications and cloud configurations with low search cost. CherryPick is a system that leverages Bayesian Optimization to build performance models for various applications, and the models are just accurate enough to distinguish the best or close-to-the-best configuration from the rest with only a few test runs. Our experiments on five analytic applications in AWS EC2 show that CherryPick has a 45-90% chance to find optimal configurations, otherwise near-optimal, saving up to 75% search cost compared to existing solutions.",CS,AI_ML,0.85,Extracted from log - paper 1812
SpotOn: A Batch Computing Service for the Spot Market (2018),"Cloud spot markets enable users to bid for compute resources, such that the cloud platform may revoke them if the market price rises too high. Due to their increased risk, revocable resources in the spot market are often significantly cheaper (by as much as 10⇥) than the equivalent nonrevocable on-demand resources. One way to mitigate spot market risk is to use various fault-tolerance mechanisms, such as checkpointing or replication, to limit the work lost on revocation. However, the additional performance overhead and cost for a particular fault-tolerance mechanism is a complex function of both an application’s resource usage and the magnitude and volatility of spot market prices. We present the design of a batch computing service for the spot market, called SpotOn, that automatically selects a spot market and fault-tolerance mechanism to mitigate the impact of spot revocations without requiring application modification. SpotOn’s goal is to execute jobs with the performance of on-demand resources, but at a cost near that of the spot market. We implement and evaluate SpotOn in simulation and using a prototype on Amazon’s EC2 that packages jobs in Linux Containers. Our simulation results using a job trace from a Google cluster indicate that SpotOn lowers costs by 91.9% compared to using on-demand resources with little impact on performance.",CS,AI_ML,0.85,Extracted from log - paper 1813
Legacy Systems to Cloud Migration: A Review from the Architectural Perspective (2023),"Legacy systems are business-critical systems that hold the organization’s core business functions developed in a traditional way using monolith architecture and usually deployed on-premises. Through time, this system is exposed to improvement changes, increasing its size and number of functionalities, thus increasing its complexity, and maintaining it becomes a disadvantage to the organization. Migration to the cloud environment becomes the primary option to improve legacy application agility, maintainability, and flexibility. However, to take advantage of the cloud environment, monolith legacy application needs to be rearchitected as microservice architecture to fully benefit from cloud advantages. This paper aims to understand the motivation for cloud migration, investigate existing cloud migration frameworks, identify the target architecture for the cloud, and establish any empirical quality issues in cloud migration from the implementation point of view. To achieve those objectives, we conducted a systematic literature review (SLR) of 47 selected studies from the most relevant scientific digital libraries covering pre-migration, migration, and post-migration stages. The SLR outcome provided us with the primary motivation for the cloud migration, existing cloud migration frameworks, targeted migration architecture patterns, and migration challenges. The results also highlight areas where more research is needed and suggest future research in this field. Furthermore, our analysis shows that current migration approaches lack quality consideration, thus contributing to post-migration quality concerns.",CS,AI_ML,0.85,Extracted from log - paper 1814
Challenges in the Adoption of Hybrid Cloud: An Exploratory Study (2018),"Cloud computing is a growing computing paradigm that provides Internet-based computer services on-demand basis. Adoption of cloud infrastructure promises enterprises numerous benefits. In particular, hybrid cloud, a combination of both public and private clouds, offers benefits of both the public and private clouds. The objective of this study is to identify the critical challenges, faced by client organisations in the adoption of hybrid cloud computing. The authors have reviewed the literature through systematic literature review (SLR) process. We have followed all the SLR steps by developing SLR protocol first which was then validated and implemented. We have identified a list of ten challenges, by extracting data from a sample of 120 papers, in the adoption of hybrid cloud. The identified challenges include three critical challenges such as: ‘public cloud security concern’, ‘efficient management issue’, and ‘integration complexity’. We have further analysed the identified challenges with respect to time and study strategy. Clients should address all the identified challenges in general and the critical challenges in particular. Our next phase of the study is validation of the identified challenges through industry practitioners and to find solutions/practices for addressing these challenges, which will be published in future.",CS,AI_ML,0.85,Extracted from log - paper 1815
Mobile Edge Computing: A Survey (2017),"Mobile edge computing (MEC) is an emergent architecture where cloud computing services are extended to the edge of networks leveraging mobile base stations. As a promising edge technology, it can be applied to mobile, wireless, and wireline scenarios, using software and hardware platforms, located at the network edge in the vicinity of end-users. MEC provides seamless integration of multiple application service providers and vendors toward mobile subscribers, enterprises, and other vertical segments. It is an important component in the 5G architecture which supports variety of innovative applications and services where ultralow latency is required. This paper is aimed to present a comprehensive survey of relevant research and technological developments in the area of MEC. It provides the definition of MEC, its advantages, architectures, and application areas; where we in particular highlight related research and future directions. Finally, security and privacy issues and related existing solutions are also discussed.",CS,AI_ML,0.85,Extracted from log - paper 1816
Fog Computing: A Comprehensive Architectural Survey,"Fog computing is an emerging technology to address computing and networking bottlenecks in large scale deployment of IoT applications. It is a promising complementary computing paradigm to cloud computing where computational, networking, storage and acceleration elements are deployed at the edge and network layers in a multi-tier, distributed and possibly cooperative manner. These elements may be virtualized computing functions placed at edge devices or network elements on demand, realizing the “computing everywhere” concept. To put the current research in perspective, this paper provides an inclusive taxonomy for architectural, algorithmic and technologic aspects of fog computing. The computing paradigms and their architectural distinctions, including cloud, edge, mobile edge and fog computing are subsequently reviewed. Practical deployment of fog computing includes a number of different aspects such as system design, application design, software implementation, security, computing resource management and networking. A comprehensive survey of all these aspects from the architectural point of view is covered. Current reference architectures and major application-specific architectures describing their salient features and distinctions in the context of fog computing are explored. Base architectures for application, software, security, computing resource management and networking are presented and are evaluated using a proposed maturity model.",CS,AI_ML,0.85,Extracted from log - paper 1817
"Data Center Network Architecture in Cloud Computing: Review, Taxonomy, and Open Research Issues (2014)","The data center network (DCN), which is an important component of data centers, consists of a large number of hosted servers and switches connected with high speed communication links. A DCN enables the deployment of resources centralization and on-demand access of the information and services of data centers to users. In recent years, the scale of the DCN has constantly increased with the widespread use of cloud-based services and the unprecedented amount of data delivery in/between data centers, whereas the traditional DCN architecture lacks aggregate bandwidth, scalability, and cost effectiveness for coping with the increasing demands of tenants in accessing the services of cloud data centers. Therefore, the design of a novel DCN architecture with the features of scalability, low cost, robustness, and energy conservation is required. This paper reviews the recent research findings and technologies of DCN architectures to identify the issues in the existing DCN architectures for cloud computing. We develop a taxonomy for the classification of the current DCN architectures, and also qualitatively analyze the traditional and contemporary DCN architectures. Moreover, the DCN architectures are compared on the basis of the significant characteristics, such as bandwidth, fault tolerance, scalability, overhead, and deployment cost. Finally, we put forward open research issues in the deployment of scalable, low-cost, robust, and energy-efficient DCN architecture, for data centers in computational clouds.",CS,AI_ML,0.85,Extracted from log - paper 1818
A Survey on Software-Defined Networking (2015),"Emerging mega-trends (e.g., mobile, social, cloud, and big data) in information and communication technologies (ICT) are commanding new challenges to future Internet, for which ubiquitous accessibility, high bandwidth, and dynamic management are crucial. However, traditional approaches based on manual configuration of proprietary devices are cumbersome and error-prone, and they cannot fully utilize the capability of physical network infrastructure. Recently, software-defined networking (SDN) has been touted as one of the most promising solutions for future Internet. SDN is characterized by its two distinguished features, including decoupling the control plane from the data plane and providing programmability for network application development. As a result, SDN is positioned to provide more efficient configuration, better performance, and higher flexibility to accommodate innovative network designs. This paper surveys latest developments in this active research area of SDN. We first present a generally accepted definition for SDN with the aforementioned two characteristic features and potential benefits of SDN. We then dwell on its three-layer architecture, including an infrastructure layer, a control layer, and an application layer, and substantiate each layer with existing research efforts and its related research areas. We follow that with an overview of the de facto SDN implementation (i.e., OpenFlow). Finally, we conclude this survey paper with some suggested open research challenges.",CS,AI_ML,0.85,Extracted from log - paper 1819
Energy-Efficient Cloud Computing,"Energy efficiency is increasingly important for future information and communication technologies (ICT), because the increased usage of ICT, together with increasing energy costs and the need to reduce green house gas emissions call for energy-efficient technologies that decrease the overall energy consumption of computation, storage and communications. Cloud computing has recently received considerable attention, as a promising approach for delivering ICT services by improving the utilization of data centre resources. In principle, cloud computing can be an inherently energy-efficient technology for ICT provided that its potential for significant energy savings that have so far focused on hardware aspects, can be fully explored with respect to system operation and networking aspects. Thus this paper, in the context of cloud computing, reviews the usage of methods and technologies currently used for energy-efficient operation of computer hardware and network infrastructure. After surveying some of the current best practice and relevant literature in this area, this paper identifies some of the remaining key research challenges that arise when such energy-saving techniques are extended for use in cloud computing environments.",CS,AI_ML,0.85,Extracted from log - paper 1820
Dapper: A Large-Scale Distributed Systems Tracing Infrastructure (2010),"Modern Internet services are often implemented as complex, large-scale distributed systems. These applications are constructed from collections of software modules that may be developed by different teams, perhaps in different programming languages, and could span many thousands of machines across multiple physical facilities. Tools that aid in understanding system behavior and reasoning about performance issues are invaluable in such an environment. Here we introduce the design of Dapper, Google’s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries. The main goal of this paper is to report on our experience building, deploying and using the system for over two years, since Dapper’s foremost measure of success has been its usefulness to developer and operations teams. Dapper began as a self-contained tracing tool but evolved into a monitoring platform which has enabled the creation of many different tools, some of which were not anticipated by its designers. We describe a few of the analysis tools that have been built using Dapper, share statistics about its usage within Google, present some example use cases, and discuss lessons learned so far.",CS,AI_ML,0.85,Extracted from log - paper 1821
Inter-Datacenter Job Routing and Scheduling with Variable Costs and Deadlines,"To reduce their operational costs, datacenter operators can schedule large jobs at datacenters in different geographical locations with time- and location-varying electricity and bandwidth prices. We introduce a framework and algorithms to do so that minimize electricity and bandwidth cost subject to job indivisibility, deadlines, priorities, and datacenter resource constraints. In doing so, we provide a way for datacenter operators to predict their operational costs for different datacenter placements and capacities, and thus make informed decisions about how to expand their datacenter network. Our distributed algorithm uses estimated job arrivals and day-ahead electricity prices to optimize over sliding time windows. We demonstrate its effectiveness on a Google datacenter trace and investigate the effects of different cost and performance criteria. The algorithm leverages heterogeneous job resource requirements and routing and scheduling flexibility: even deadline and indivisibility constraints yield little cost increase, though they significantly improve job completion times and localization at only one datacenter respectively. We show that our algorithm reduces the cost much more than optimizing only electricity, only bandwidth, or a combination of resource costs and job completion times.",CS,AI_ML,0.85,Extracted from log - paper 1822
Container Orchestration for Dispersed Computing,"In the era of Internet of Things, there is an increasing demand for networked computing to support the requirements of time-constrained, compute-intensive distributed applications. We present a container orchestration architecture for dispersed computing, and its implementation in an open source software called Jupiter. The system automates the distribution of computational tasks for complex computational applications described as an Directed Acyclic Graph (DAG) to efficiently distribute the tasks among a set of networked compute nodes and orchestrates the execution of the DAG thereafter. This Kubernetes based container-orchestration system supports both centralized and decentralized scheduling algorithms for optimally mapping the tasks based on information from a range of profilers: network profilers, resource profilers, and execution time profilers.",CS,AI_ML,0.85,Extracted from log - paper 1823
B4: Experience with a Globally-Deployed Software Defined WAN,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google’s data centres across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximise average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. These characteristics led to a Software-Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4’s centralised traffic engineering service drives links to near-optimised utilisation, while splitting application flows among multiple paths to balance capacity against application priorities and demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",CS,AI_ML,0.85,Extracted from log - paper 1824
Hedera: Dynamic Flow Scheduling for Data Center Networks,"Today's data centers offer tremendous aggregate bandwidth to clusters of tens of thousands of machines. However, because of limited port densities in even the highest-end switches, data center topologies typically consist of multi-rooted trees with many equal-cost paths between any given pair of hosts. Existing IP multipathing protocols usually rely on per-flow static hashing and can cause substantial bandwidth losses due to long-term collisions. In this paper, we present Hedera, a scalable, dynamic flow scheduling system that adaptively schedules a multi-stage switching fabric to efficiently utilize aggregate network resources. We describe our implementation using commodity switches and unmodified hosts, and show that for a simulated 8,192 host data center, Hedera delivers bisection bandwidth that is 96% of optimal and up to 113% better than static load-balancing methods.",CS,AI_ML,0.85,Extracted from log - paper 1825
Distributed Congestion-Aware Load Balancing for Datacenters,"We present the design, implementation, and evaluation of CONGA, a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including the use of regular Clos topologies and overlays for network virtualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed experiments, CONGA has 5 better flow completion times than ECMP even with a single link failure and achieves 2–8 better throughput than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry.",CS,AI_ML,0.85,Extracted from log - paper 1826
VL2: A Scalable and Flexible Data Center Network,"To be agile and cost effective, data centers should allow dynamic resource allocation across large server pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VL2, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. VL2 uses (1) flat addressing to allow service instances to be placed anywhere in the network, (2) Valiant Load Balancing to spread traffic uniformly across network paths, and (3) end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VL2's design is driven by detailed measurements of traffic and fault data from a large operational cloud service provider. VL2's implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scalable and reliable network architecture. As a result, VL2 networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VL2 design using measurement, analysis, and experiments. Our VL2 prototype shuffles 2.7 TB of data among 75 servers in 395 seconds – sustaining a rate that is 94% of the maximum possible.",CS,AI_ML,0.85,Extracted from log - paper 1827
MicroTE: Fine Grained Traffic Engineering for Data Centers,"The effects of data center traffic characteristics on data center traffic engineering is not well understood. In particular, it is unclear how existing traffic engineering techniques perform under various traffic patterns, namely how do the computed routes differ from the optimal routes. Our study reveals that existing traffic engineering techniques perform 15% to 20% worse than the optimal solution. We find that these techniques suffer mainly due to their inability to utilize global knowledge about flow characteristics and make coordinated decision for scheduling flows. To this end, we have developed MicroTE, a system that adapts to traffic variations by leveraging the short term and partial predictability of the traffic matrix. We implement MicroTE within the OpenFlow framework and with minor modification to the end hosts. In our evaluations, we show that our system performs close to the optimal solution and imposes minimal overhead on the network making it appropriate for current and future data centers.",CS,AI_ML,0.85,Extracted from log - paper 1828
Fault Tolerant Traffic Engineering in Software-defined WAN (2018),"Software-defined networking in a wide area network (SD-WAN) allows intelligent control and management of networking, and efficient utilization of network resources through traffic engineering in real time for higher performance WANs. This paper proposes a fault-tolerant reactive routing system, called a smart routing system, for SD-WAN by investigating a variety of network features needed for monitoring in WAN in real time. The system keeps track of various network status data in real time to provide less packet loss and low network latency along with high availability and reliability in SD-WAN. We evaluate our system in a real network provided by OpenLab at Juniper. Experimental results show that our approach successfully demonstrates resilience and efficiency by applying the programmability of SDN for WAN.",CS,AI_ML,0.85,Extracted from log - paper 1829
Resilient Datacenter Load Balancing in the Wild (2017),"Production datacenters operate under various uncertainties such as traffic dynamics, topology asymmetry, and failures. Therefore, datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and timely react to mitigate the fallout. Despite significant efforts, prior solutions have important drawbacks. On one hand, solutions like Presto and DRB are oblivious to path conditions and blindly reroute at fixed granularity. On the other hand, solutions like CONGA and CLOVE can sense congestion, but only reroute when flowlets emerge; thus they cannot always react in time to uncertainties. Moreover, these solutions fail to detect or handle failures such as blackholes and random packet drops, which greatly degrades performance. In this paper, we introduce Hermes, a datacenter load balancer resilient to such uncertainties. Hermes leverages comprehensive sensing to detect path conditions (including failures previously unattended) and reacts with timely yet cautious rerouting. It is an edge-based solution requiring no switch modification. We implemented Hermes with commodity switches and evaluated it via testbed experiments and large-scale simulations. Our results show that Hermes achieves comparable performance to CONGA and Presto in normal cases, and handles uncertainties well: under asymmetric topologies, Hermes achieves up to 10%–20% better flow completion times than CONGA and CLOVE; under switch failures, it outperforms all other schemes by over 32%.",CS,AI_ML,0.85,Extracted from log - paper 1830
Large-Scale Cluster Management at Google with Borg (2015),"Google’s Borg system is a cluster manager that runs hundreds of thousands of jobs—many thousands of different applications—across a number of clusters, each with up to tens of thousands of machines. It achieves high utilization by combining admission control, efficient task packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior. We present a summary of the Borg architecture and features, important design decisions, a quantitative analysis of some policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it.",CS,AI_ML,0.85,Extracted from log - paper 1831
Evaluation of Container Orchestration Systems for NoSQL Databases (2018),"Container orchestration systems such as Docker Swarm, Kubernetes, and Mesos provide automated support for deployment and management of distributed applications as sets of containers. While initially designed for stateless services, they are also being used to run database clusters due to resilience features like fast auto-recovery of failed nodes and location-transparent connections among database instances. In this paper we evaluate the performance overhead of Docker Swarm and Kubernetes for deploying and managing NoSQL database clusters (MongoDB case study). As a baseline, we use an OpenStack IaaS cloud that provides similar resilience attributes in a less automated manner. Our experiments quantify the overhead introduced by container orchestration in terms of throughput and latency, compared to a traditional IaaS-based deployment.",CS,AI_ML,0.85,Extracted from log - paper 1832
Unikernels: Library Operating Systems for the Cloud (2013),"We present unikernels, a new approach to deploying cloud services via applications written in high-level source code. Unikernels are single-purpose appliances that are compiled into standalone kernels and sealed against modification when deployed to a cloud platform. In return they offer significantly reduced image sizes, improved efficiency and security, and should reduce operational costs. Our Mirage prototype compiles OCaml code into unikernels that run on commodity clouds and offers an order-of-magnitude reduction in code size without significant performance penalty. The architecture combines static type-safety with a single-address-space layout that can be made immutable via a hypervisor extension. Mirage contributes a suite of type-safe protocol libraries, and our results demonstrate that the hypervisor can serve as a platform to overcome the hardware compatibility issues that made past library OSes impractical to deploy in the real world.",CS,AI_ML,0.85,Extracted from log - paper 1833
Edge Computing: Vision and Challenges (2016),"The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm: edge computing, which calls for processing data at the edge of the network. Edge computing has the potential to address requirements for low response time, battery life conservation, bandwidth cost savings, and data safety/privacy. In this paper, we introduce the definition of edge computing, followed by several case studies (from cloud offloading to smart homes/cities and collaborative edge) to materialize the concept. Finally, we present several challenges and opportunities in edge computing and hope this paper will attract community attention and inspire further research in this direction.",CS,AI_ML,0.85,Extracted from log - paper 1834
DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning (2017),"Anomaly detection is a critical step toward building secure and trustworthy systems. System logs record states and significant events at various points to help debug failures and perform root cause analysis, and thus are an excellent resource for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing LSTM (Long Short-Term Memory) to model a system log as a natural language sequence. This allows DeepLog to automatically learn log patterns from normal execution and detect anomalies when log sequences deviate from the model trained on normal data. We also show how to incrementally update the DeepLog model online so it can adapt to new log patterns over time. Furthermore, DeepLog constructs workflow models from the log streams so that once an anomaly is detected, users can diagnose and perform root cause analysis effectively. Extensive experiments on large log datasets show that DeepLog outperforms existing log-based anomaly detection methods based on traditional data mining.",CS,AI_ML,0.85,Extracted from log - paper 1835
COLO: COarse-Grained LOck-Stepping Virtual Machines for Non-Stop Service (2013),"Virtual machine (VM) replication provides a software solution for business continuity and disaster recovery by replicating the state of a primary VM (PVM) to a secondary VM (SVM) on a different node. Unfortunately, current VM replication approaches suffer excessive overhead, severely limiting their applicability. In this paper, we leverage the observation that in client–server systems, the PVM and SVM can be considered in the “same” state as long as they generate the same outputs to clients. Using this insight, we propose COLO, a generic and highly efficient non-stop service solution using on-demand VM replication. COLO monitors the output responses of the PVM and SVM, and deems the SVM a valid replica of PVM if their outputs match. If responses diverge, COLO delays the external reply until the PVM’s state is synchronized to the SVM. In this way, we ensure the system is always capable of failover to the SVM. Although nondeterminism may cause internal state differences between PVM and SVM, they remain externally consistent. Unlike prior instruction-level lock-stepping approaches, COLO supports multi-processor workloads with satisfying performance. Results show that COLO significantly outperforms existing approaches, especially on typical server–client workloads like online databases and web servers.",CS,AI_ML,0.85,Extracted from log - paper 1836
A General Approach to Network Configuration Verification (Minesweeper) (2017),"We develop an approach to verify network configurations that is (1) general (not limited by specific protocols or features), (2) accurate (no reliance on simplifying approximations), (3) complete (analyzes all possible sets of routing announcements from external sources, not just one or a few), (4) powerful (can verify a wide range of properties such as reachability, path length, fault tolerance, load balancing), and yet (5) scalable. Our approach translates unmodified network configurations into a logical formula capturing both control-plane and data-plane behaviors. An SMT solver then checks if all converged control-plane behaviors satisfy desired properties. We implement our approach in a tool called Minesweeper and apply it to configurations of 152 real networks from a large cloud provider. Despite these networks being in production for years, Minesweeper found 96 bugs, some posing serious security vulnerabilities. We also tested Minesweeper on synthetic benchmarks and found it can verify rich properties in networks of hundreds of routers in under 5 minutes, thanks to model-slicing and constraint-hoisting optimizations that speed up verification by over 460×.",CS,AI_ML,0.85,Extracted from log - paper 1837
Energy Saving Evaluation of an Energy-Efficient Data Center using a Model-Free Reinforcement Learning Approach (2022),"To reduce cooling energy consumption, data centers are often advised to raise the server inlet air temperature. However, in tropical climates operators still tend to run at lower temperatures. In this paper, we demonstrate that using a floating setpoint (dynamically lowering the temperature when needed) yields greater overall energy savings for tropical data centers than simply raising the temperature statically. We achieve this via a deep reinforcement learning algorithm applied to a hybrid data center model built from traces of a highly efficient facility. The learned control policy minimizes energy costs while respecting operational constraints. We evaluate the policy to identify where the energy savings come from. The agent is trained by interacting with the data center model without any prior knowledge. Tests show that additional energy savings of up to 3% (full load) and 5.5% (part load) are achievable with targeted cooling provision in an already-efficient data center. We find that, contrary to common assumptions, most savings come from reducing server fan usage rather than chiller power reduction in our RL-driven data center scenario.",CS,AI_ML,0.85,Extracted from log - paper 1838
Automated Infrastructure-as-Code Program Testing (ACT/ProTI) (2024),"Infrastructure as Code (IaC) automates software operations and is key to high-velocity software delivery. Modern IaC frameworks like Pulumi and AWS CDK let developers write IaC programs in general-purpose languages (TypeScript, Python, etc.), but bugs in IaC can disrupt entire systems. Surprisingly, testing practices are rarely applied to IaC code: as of August 2022, under 1% of Pulumi-based IaC projects on GitHub had any tests. Existing IaC testing methods either slow development or demand heavy manual effort. We propose Automated Configuration Testing (ACT), a methodology to rapidly test IaC programs under many configurations with minimal effort. ACT automatically mocks all cloud resources in the IaC program, and uses pluggable generator and oracle modules for test input generation and output validation. We implement ACT in ProTI, a testing tool for Pulumi TypeScript that provides type-driven input generation, oracles, and supports application-level specifications. Our evaluation on 6,081 Pulumi scripts from GitHub plus benchmarks shows that ProTI can be directly applied to real IaC programs without changes, finds bugs within seconds in cases where prior techniques are infeasible, and leverages existing generators/oracles via its plugin architecture.",CS,AI_ML,0.85,Extracted from log - paper 1839
ABACUS: A FinOps Service for Cloud Cost Optimization (2023),"As enterprises migrate infrastructure to the cloud, they face challenges in achieving holistic visibility into cloud spending and optimizing costs. FinOps practices offer a way to bring financial accountability and cost optimization to cloud usage. This paper presents ABACUS – Automated Budget Analysis and Cloud Usage Surveillance – a FinOps solution for cloud cost optimization. ABACUS allows organizations to set cloud budgets, enforce those budgets by blocking new deployments when necessary, and alert teams if spending exceeds thresholds. It leverages Infrastructure-as-Code integration to warn engineering teams about the expected cost of a deployment before cloud resources are provisioned. We discuss ABACUS’s architecture and demonstrate how it helps reduce waste, improve cost forecasting, and scale cloud usage sustainably. Finally, we outline future research directions to advance FinOps practices for cloud cost management.",CS,AI_ML,0.85,Extracted from log - paper 1840
Firecracker: Lightweight Virtualization for Serverless Applications (2020),"Serverless containers and functions are widely used for cloud deployment due to their lower operational costs, improved hardware utilization, and faster scaling compared to traditional methods. The economics and scale of serverless computing require that multi-tenant workloads run on shared hardware with minimal overhead while providing strong security and performance isolation. Traditionally, one had to choose between VM-based virtualization (strong isolation, high overhead) and container-based systems (low overhead, weaker isolation) – a tradeoff unacceptable to cloud providers who need both. We developed Firecracker, an open-source Virtual Machine Monitor (VMM) specialized for serverless workloads but broadly useful for containers, functions, and other computations under certain constraints. Firecracker provides the security of hardware-grade VMs with the low overhead of containers. It has been deployed in AWS Lambda and AWS Fargate, supporting millions of workloads and trillions of requests per month. We describe how focusing on serverless use-cases shaped Firecracker’s design and share lessons from migrating AWS’s serverless customers transparently to Firecracker.",CS,AI_ML,0.85,Extracted from log - paper 1841
A Survey of Data Center Network Architectures,"Large-scale data centers form the core infrastructure support for the ever expanding cloud based services. Thus the performance and dependability characteristics of data centers will have significant impact on the scalability of these services. In particular, the data center network needs to be agile and reconfigurable in order to respond quickly to ever changing application demands and service requirements. Significant research work has been done on designing the data center network topologies in order to improve the performance of data centers. In this paper, we present a survey of data center network designs and topologies that have published recently. We start with a discussion on various representative data center network topologies, and compare them with respect to several properties in order to highlight their advantages and disadvantages. Thereafter, we discuss several routing protocols designed for these topologies, and compare them based on various criteria: the basic algorithms to establish connections, the techniques used to gain better performance and the mechanisms for fault-tolerance. A good understanding of the state-of-the-art in data center networks is essential for designing future architectures.",CS,AI_ML,0.85,Extracted from log - paper 1842
"Secure Cloud Infrastructure: A Survey on Issues, Current Solutions","Cloud computing is currently becoming a well-known buzzword in which business titans, such as Microsoft, Amazon, and Google, among others, are at the forefront in developing and providing sophisticated cloud computing systems to their users in a cost-effective manner. Security is the biggest concern for cloud computing and is a major obstacle to users adopting cloud computing systems. Maintaining the security of cloud computing is important, especially for the infrastructure. Several research works have been conducted in the cloud infrastructure security area; however, some gaps have not been completely addressed, while new challenges continue to arise. This paper presents a comprehensive survey of the security issues at different cloud infrastructure levels (e.g., application, network, host, and data). It investigates the most prominent issues that may affect the cloud computing business model with regard to infrastructure. It further discusses the current solutions proposed in the literature to mitigate the different security issues at each level. To assist in solving the issues, the challenges that are still unsolved are summarized. Based on the exploration of the current challenges, some cloud features such as flexibility, elasticity and the multi-tenancy are found to pose new challenges at each infrastructure level.",CS,AI_ML,0.85,Extracted from log - paper 1843
Design and Implementation of Scalable Network Architecture for High-Density Data Centers,"This paper presents the design and implementation of a scalable network architecture tailored for high-density data centers, which are essential for modern cloud computing, big data, and distributed applications. As data centers grow in size and complexity, traditional network designs face challenges in delivering the required bandwidth, fault tolerance, and low-latency communication between a large number of servers. Our design emphasizes the role of 5G and Edge Computing and also Machine Learning in network operations. This scalable solution meets the growing demands of high-density data centers, offering a practical and efficient network infrastructure for enterprises and cloud service providers.",CS,AI_ML,0.85,Extracted from log - paper 1844
Cloud Computing: Survey on Energy Efficiency,"In this article, we perform a comprehensive analysis of an infrastructure supporting the cloud computing paradigm with regards to energy efficiency. We start with an overview of energy efficiency in cloud computing and then discuss metrics and frameworks for measuring and optimizing energy consumption. We analyze existing solutions for improving energy efficiency at different levels of the cloud infrastructure, including hardware, virtualization, and workload management. The survey identifies key challenges and future research directions for sustainable cloud computing systems.",CS,AI_ML,0.85,Extracted from log - paper 1845
Rethinking Fat-Tree Topology Design for Cloud Data Centers,"Data center network (DCN) topologies have recently been the focus of many researchers due to their vital role in achieving high DCN performances in terms of scalability, power consumption, throughput, and traffic load balancing. This paper presents a comprehensive comparison between two most commonly used DCN topologies, Fat-Tree and BCube, with a focus on structure, addressing and routing, and proposes a new DCN topology that is better suited for nowadays data center networks. We show that our proposed topology, termed Circulant Fat-Tree, alleviates traffic congestion at the core switches, improves network latency, and increases robustness against switch and server failures when compared to traditional Fat-Tree DCN topologies.",CS,AI_ML,0.85,Extracted from log - paper 1846
Energy efficiency and low carbon enabler green IT framework for data centers considering green metrics,"The increasing demand for storage, networking and computation has driven intensification of large complex data centers that run many of today’s Internet, financial, commercial and business applications. A data center comprises of many thousands of servers and can use as much energy as small city. Massive amount of computation power is required to drive and run these server farms resulting in many challenging like huge energy consumptions, emission of green house gases, backups and recovery; This paper proposes energy efficiency and low carbon enabler green IT framework for these large and complex server farms to save consumption of electricity and reduce the emission of green house gases to lower the effects of global warming. The framework uses latest energy saving techniques like virtualization, cloud computing and green metrics to achieve greener data centers. It comprises of five phase to properly implement green IT techniques to achieve green data centers. The proposed framework seamlessly divides data center components into different resource pools and then applies green metrics like Power Usage Effectiveness, Data Center Effectiveness and Carbon Emission Calculator to measure performance of individual components so that benchmarking values can be achieved and set as standard to be followed by data centers.",CS,AI_ML,0.85,Extracted from log - paper 1847
An optimal infrastructure design method of cloud computing services from the BDIM perspective,"For IT service providers, infrastructure construction plays a significant role in the maximization of business profits. Infrastructure design is becoming more and more crucial as IT service environments evolve from deployment on site to software as a service (SaaS), and, most recently, to cloud computing. In this paper, a cloud computing architecture is proposed from the viewpoint of business-driven IT management (BDIM); then an optimal cloud infrastructure design methodology is devised, whereby numbers of servers, routers and communication bandwidth can be calculated through considering both infrastructure costs and business losses incurred by service level agreement (SLA) violations. Finally, a complete numerical example is discussed to testify the proposed method.",CS,AI_ML,0.85,Extracted from log - paper 1848
Fast autoscaling algorithm for cost optimization of container clusters,"Container clusters are widely used to execute containerized applications in cloud environments. An essential characteristic implemented by these clusters is autoscaling, which is the ability to automatically adapt the computing resources of a cluster to support variable workloads. Precise adjustment of cluster resources to its workload in each autoscaling operation is essential to control cluster deployment costs. Several resource allocation models have been developed with the objective of cost minimization. However, as the number of containers and virtual machines of the cluster increases, resource allocation problems become too complex, and cannot be solved in reasonable time by existing resource allocation models. In this paper we present FCMA (Fast Container to Machine Allocator), a resource allocation algorithm designed to calculate a suitable allocation of the resources of a cluster in autoscaling operations, to minimize cluster deployment costs. The main motivation for the development of FCMA has been to significantly reduce the solving time of the resource allocation problem compared to a previous state-of-the-art optimal Integer Linear Programming (ILP) model. In addition, FCMA addresses secondary objectives to improve fault tolerance and reduce container and virtual machine recycling costs, load-balancing overloads and container interference. We have conducted an experimental evaluation to assess the effectiveness of FCMA, using the ILP model and two heuristics as a baseline. The experiments show that FCMA is much faster than the ILP model, with an average solving time reduction of two orders of magnitude. This gain in speed does not compromise the quality of the solutions, which have a cost on par with those of the ILP model. In comparison to the heuristics, FCMA achieves similar solving times while consistently delivering more cost-effective solutions.",CS,AI_ML,0.85,Extracted from log - paper 1849
Server deployment strategies considering robustness and resource constraints in edge computing,"Edge computing, as an emerging computing paradigm, efficiently offloads computationally intensive tasks to edge servers, extending services from the cloud center to the edge and significantly improving the efficiency of network data processing. However, existing research mostly focuses on the ideal deployment and optimization of edge servers, such as minimizing latency and load balancing, with little attention given to strategies for handling abnormal situations like soft attacks or sudden failures. This study delves into how to formulate server deployment strategies aiming to maximize system robustness and minimize costs when edge servers face soft attacks or sudden failures. Drawing insights from graph theory and network connectivity, we propose a server deployment strategy that enhances system robustness under resource constraints. Specifically, we introduce the concept of edge-delay-tolerant networks, ensuring the rapid establishment of a complete “backup link” in case of service interruption by constructing backup relay nodes and routing information tables to safeguard system continuity and stability. Additionally, we innovatively design adaptive edge server deployment methods in “silent” and “active” modes to cater to varying demands in different fault scenarios. To validate the effectiveness of the proposed strategies, we construct a multi-objective optimization problem through mathematical modeling and devise an hybrid optimization algorithm for solving it. Experimental simulation results demonstrate that the algorithm can provide near-optimal performance, effectively enhancing system robustness under resource constraints. This research not only offers new insights and methods for fault response mechanisms in edge computing but also provides valuable references for practical server deployment strategies, thereby promoting further development of edge computing technology.",CS,AI_ML,0.85,Extracted from log - paper 1850
Enhancing configuration security with heterogeneous read points,"Configuration files are widely used for customizing the status and behavior of cloud systems without modifying source code. The configurable system performs flexibly to meet different requirements. Several security risks come with the flexibility, since the configuration files are directly accessible to users. In this work, we propose config-flow analysis to locate suspicious usage and design three types of code-level heterogeneous operations to build security protection for related read points. The config-flow analysis can address the propagation of configuration options and further help to boost configuration security from read points to the end of usage sequence. For the three types of commonly used configuration files, i.e., key-value pairs, serialization data, and scripts, we evaluated the effectiveness of read point identification and heterogeneous operations on 14 open-source projects. The experimental results show that the overall precision of file and option read point identification is 97% and 96%, and our approach can ensure projects keep security against configuration-related vulnerabilities with acceptable performance loss.",CS,AI_ML,0.85,Extracted from log - paper 1851
Utility-driven virtual machine allocation in edge cloud environments using a partheno-genetic algorithm,"Mobile Edge Computing alleviates network congestion and reduces latency by offloading tasks to the network edge. However, fluctuating Quality of Service (QoS) and service compositions significantly challenge service reliability and utility optimization. To address these challenges, this paper proposes a novel virtual machine allocation framework designed to maximize the utility of edge cloud service provisioning under QoS constraints. First, the task processing mechanism is modeled as an M/M/m queuing system, with service loss and revenue functions defined to quantify the quality and profitability of edge services. Next, the framework dynamically reallocates virtual machines across sub-service centers, based on task arrival rates and varying QoS requirements, to optimize overall service utility. Finally, we develop a partheno-genetic algorithm based on integer coding to solve the service utility maximization (SOPGA) to determine the optimal virtual machine allocation strategy. Simulation results demonstrate that the proposed virtual machine allocation algorithm improves service utility by more than 20% compared to other virtual machine allocation algorithms, significantly enhancing service utility in edge cloud environments while maintaining robust QoS guarantees.",CS,AI_ML,0.85,Extracted from log - paper 1852
Secure and efficient ownership verification for deduplicated cloud computing systems,"Cloud storage services offer a scalable platform to store a large amount of data at a low cost. It attracts a large number of customers to outsource their data to the cloud. To manage the massive growth in the size of outsourced data, cloud service providers employ deduplication, i.e., a technique to reduce space and bandwidth requirements by eliminating the upload and storage of redundant data. However, it poses the following severe security threat: “A malicious user who learns deduplication tag of the file, i.e., a small piece of information, can convince the server to allow access to the entire file”. A proof of ownership ( POW ) concept was introduced that allows the server to challenge the user to prove that s/he owns the entire file. The existing state-of-the-art POW solutions are either not considering the complete file for determining the proof or not efficient in terms of I/O, communication, and computational overheads on the user. In this paper, we propose a secure and efficient POW scheme. The proposed scheme ensures that the user must possess the complete file to generate ownership proof. In addition, our scheme causes minimal I/O, communication, and computational overhead on the user side. We implement the proposed scheme in a real cloud scenario using Google Firebase cloud services. The performance analysis indicates that our scheme is efficient in terms of I/O, computational, and communication overheads than the existing state-of-the-art solutions.",CS,AI_ML,0.85,Extracted from log - paper 1853
Optimizing energy task offloading technique using IoMT cloud in healthcare applications,"The Internet of Medical Things (IoMT) has revolutionized patient data and healthcare surveillance, enabling continuous monitoring without costly human resources and low error rates. IoMT uses medical devices as nodes to monitor and collect patient data efficiently and cost-effectively. IoMT issues emergency alarms and monitors people in hospitals and at home to help physicians track their health. It analyzes EEGs, ECGs, blood sugar, blood pressure, and other health markers. Real-time analysis is essential in crucial situations, these latency-sensitive scenarios are suitable for cloud-based IoT platforms. This research proposes an Efficient Augmented Moth-Flame Optimization (EA-MFO) technique for task offloading. The method focuses on prioritizing critical tasks to ensure deadlines are met while optimizing energy consumption for other tasks. EA-MFO enhances the moth-flame optimization process by incorporating chaos-based initialization, adaptive position updates with weighted adjustments, and strategies to improve population diversity. The chaos-based logistic map is used to increase diversity during initialization. Simulation results reveal that EA-MFO outperforms E-PSO, GWO, MQGA, and MATO in terms of energy consumption, makespan, and total execution time (TEC). Specifically, EA-MFO achieves a total execution time of 0.63 s, a makespan of 52.13 s, and energy consumption of 592.78 kWh.",CS,AI_ML,0.85,Extracted from log - paper 1854
Virtual machine scheduling and migration management across multi-cloud data centers: blockchain-based versus centralized frameworks,"Efficiently managing virtual resources in the cloud is crucial for successful recourse utilization. Scheduling is a vital technique used to manage Virtual Machines (VMs), enabling placement and migration between hosts located in the same or different data centers. Effective scheduling not only ensures better server consolidation but also enhances hardware utilization and reduces power consumption in data centers. However, scheduling VMs across a Wide Area Network (WAN) poses considerable challenges due to connectivity issues, slower communication speeds, and concerns around data integrity and confidentiality. To enable informed scheduling decisions, it is critical to facilitate the exchange of real-time and accurate status information between cloud data centers, ensuring optimal resource allocation and minimizing latency. To address this, we propose a novel distributed cloud management solution that utilizes blockchain technology to facilitate efficient sharing of VM characteristics across multiple data centers. BigchainDB platform has been used as a blockchain-based ledger database to effectively share information required for VM scheduling and migration across different data centers. The proposed framework has been validated and compared with a Virtual Private Network (VPN)-based centralized management solution. The proposed model utilizing blockchain-based solution achieves 41.79% to 49.85% reduction in number of communication messages and 2% to 12% decrease in total communication delay comparing to the centralized model.",CS,AI_ML,0.85,Extracted from log - paper 1855
Cloud-edge hybrid deep learning framework for scalable IoT resource optimization,"In the dynamic environment of the Internet of Things (IoT), edge and cloud computing play critical roles in analysing and storing data from numerous connected devices to produce valuable insights. Efficient resource allocation and workload distribution are vital to ensuring continuous and reliable service in growing IoT ecosystems with increasing data volumes and changing application demands. This study proposes a novel optimisation approach utilising deep learning to tackle these challenges. The integration of Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO) offers a practical approach to addressing the dynamic characteristics of IoT applications. The hybrid algorithm's primary characteristic is its capacity to simultaneously fulfil multiple objectives, including reducing response times, enhancing resource efficiency, and decreasing operational costs. DQN facilitates the formulation of optimal resource allocation strategies in intricate and unpredictable environments. PPO enhances policies in continuous action spaces to guarantee reliable performance in real-time, dynamic IoT settings. This method achieves an optimal equilibrium between policy learning and optimisation, rendering it suitable for contemporary IoT systems. This method improves numerous IoT applications, including smart cities, industrial automation, and healthcare. The hybrid DQN-PPO-GNN-RL model addresses bottlenecks by dynamically managing computing and network resources, allowing for efficient operations in low-latency, high-demand environments such as autonomous systems, sensor networks, and real-time monitoring. The use of Graph Neural Networks (GNNs) improves the accuracy of resource representation, while reinforcement learning-based scheduling allows for seamless adaptation to changing workloads. Simulations using real-world IoT data on the iFogSim platform showed significant improvements: task scheduling time was reduced by 21%, operational costs by 17%, and energy consumption by 22%. The method reliably provided equitable resource distribution, with values between 0.93 and 0.99, guaranteeing efficient allocation throughout the network. This hybrid methodology establishes a novel benchmark for scalable, real-time resource management in extensive, data-centric IoT ecosystems, consequently enhancing system performance and operational efficiency.",CS,AI_ML,0.85,Extracted from log - paper 1856
Analyzing the trend of government support for cloud computing usage in e-government architecture,"The revolution in improving services to the community carried out by the current government is genuine. It is not easy for government organizations, especially local governments, to directly implement e-government services in full. One solution that is considered appropriate and can solve these problems is the application of cloud computing to support e-government services in local governments. The advantage of cloud computing for e-government is that it can increase security. In contrast, cloud-based storage can be valuable because data stored in cloud computing is guaranteed secure and various regulations and standards of information safety practices. We propose a systematic literature review approach to analyze trends in studies on cloud computing of e-government. This research is a type of descriptive qualitative research using a bibliometric approach. To assess the trend of cloud computing in e-government, we use CiteSpace’s latest bibliometric software to achieve a comprehensive knowledge overview of cloud computing in e-government. The Findings of this paper reveal a dynamic scenario influenced by advancing technological progress and administrative priorities. Across the globe, governments are progressively acknowledging the capacity of cloud computing to improve the effectiveness, accessibility, and scalability of e-government services. Overall, challenges persist, spanning from concerns regarding data safekeeping and privacy, but it also signifies a strategic transition towards harnessing digital technologies to provide more agile, citizen-focused public services.",CS,AI_ML,0.85,Extracted from log - paper 1857
Autonomous decision-making of UAV cluster with communication constraints based on reinforcement learning,"Artificial intelligence techniques are increasingly applied in the study of autonomous decision-making in unmanned clustered distributed systems. However, communication constraints has become a big bottleneck that restricts its performance. To address the need for unmanned aerial vehicles(UAVs) to execute collaborative attack missions in complex communication-constrained environments, this paper propose an autonomous decision-making method for UAVs based on Multi-Agent Reinforcement Learning (MARL). Firstly, the autonomous decision-making processes of UAV clusters are modeled as Decentralized Partially Observable Markov Decision Processes(Dec-POMDPs). Next, the algorithm is enhanced within the framework of Multi-Agent Deep Deterministic Policy Gradient(MADDPG) by designing an explicit inter-intelligent communication mechanism to achieve information exchange among UAVs. Subsequently, the algorithm utilizes Long Short-Term Memory(LSTM) networks to process the local observations of the UAVs, enhancing the effectiveness of the information sent by combining historical data with current observations. Finally, multiple rounds of experiments are conducted across various communication-constrained scenarios. Simulation results indicate that the proposed method improves the task completion capability by 46.0% and enhances stability by 24.9% compared to baseline algorithm MADDPG. Additionally, the algorithm demonstrates better generalization and exhibits good scalability, effectively adapting to varying numbers of UAVs. This research provides new theoretical insights and a technical framework for the collaboration of UAVs in environments with communication constraints, which holds great practical importance in improving the ability and application scope of UAV systems.",CS,AI_ML,0.85,Extracted from log - paper 1858
Blockchain-enabled Zero Trust-based Secure and Energy Efficient scheme for 6G-enabled UASNs,"Marine networks such as the Underwater Acoustic Sensor Network (UASN) are the backbone for the exploration and monitoring of marine environments. These sensor nodes are deployed in extremely harsh underwater conditions, which are prone to various problems, including safety threats and energy drain. However, with their dramatic significance in multiple aspects, solutions for ensuring secured and efficient communications of these networks become essential. Herein, we put forth a new scheme termed Blockchain-enabled Zero Trust-based Secure and Energy Efficient (BZTSEE), where blockchain and a zero trust notion have been worked together into UASNs whose communications are enabled through 6G. BZTSEE works on identifying security and privacy issues while providing an energy-efficient solution. Using blockchain ensures secure and trustworthy communication, transparency in data sharing, maintenance of trust among nodes, and protection of user privacy. The scheme uses a PBFT protocol to defend itself against malicious actions and keep data secured. Several simulations show how BZTSEE works well under diverse conditions, including the number of nodes and attacks. The results render it quite effective for both small and large UASNs. In short, BZTSEE considerably augments the aspects of security, privacy, trust, and energy savings-a perfect fit for future UASNs.",CS,AI_ML,0.85,Extracted from log - paper 1859
An approach for multipath optimal selection of network service combinations based on golden eagle optimizer with double learning strategies,"The traditional optimal-path algorithm can address a single constraint in small and straightforward networks. However, in complex multipath distributed cloud services, the network nodes no longer exhibit singular or deterministic path characteristics. It requires the optimal paths that not only determines the shortest routes, but also combine the safety, speed, and enhanced service quality across multiple service nodes in the network topology. The Golden Eagle Optimization Algorithm (GEO) is specialized for optimizing these network service combinations. On this basis, the Golden Eagle Optimizer with Double Learning Strategies (GEO-DLS) resolved the multipath optimal service selection issues within intricate network environments. The algorithm modeled the hunting tactics of wild golden eagles, efficiently targeting the best prey in minimal time by dynamically adjusting two critical components, such as the attack and cruising strategies. In GEO-DLS, the enhanced GEO significantly broadened the search scope for food sources by using personalized learning and mirror reflection techniques. These advancements notably enhanced the GEO search capabilities and improved the solution accuracy. Key contribution include GEO-DLS can converge to the optimal solution faster by optimizing the search strategy and parameter settings. This means that in the problem of network service composition, algorithms can quickly find the optimal path that meets the quality of service (QoS) requirements. To validate the effectiveness of GEO, a set of ten standard benchmark functions was utilized to evaluate its performance. The results from these evaluations consistently presented its superior performance in tackling optimization challenges compared to other five metaheuristic algorithms and five enhanced algorithms.",CS,AI_ML,0.85,Extracted from log - paper 1860
Compliance and feedback based model to measure cloud trustworthiness for hosting digital twins,"Cloud-based digital twins use real-time data from various data sources to simulate the behavior and performance of their physical counterparts, enabling monitoring and analysis. However, one restraining factor in the use of cloud computing for digital twins is its users’ concerns about the security of their data. This data may be located anywhere in the cloud, with very limited control of the user to ensure its security. Cloud-based digital twins provide opportunities for researchers to collaborate yet security of such digital twins requires measures specific to cloud computing. To overcome this shortcoming, we need to devise a mechanism that not only ensures essential security safeguards but also computes a Trustworthiness value for Cloud Service Providers (CSP). This would give confidence to cloud users and enable them to choose the right CSP for their data-related interaction. This research proposes a solution, whereby the Trustworthiness of CSPs is calculated based on their Compliance with data security controls, User Feedback, and Auditor Rating. Two additional factors, Accuracy of Compliance Measurement and Control Significance Factor have been built in, to cater for other nonstandard conditions. Our implementation of Data Security Compliance Monitor and Data Trust as a Service, along with three CSPs, each with ten different settings, has supported our proposition through the devised formula. Experimental outcomes show changes in the trustworthiness value with changes in compliance level, user feedback and auditor rating. CSPs with better compliance have better trustworthiness values. However, if the Accuracy of Compliance Measurement and Control Significance Factor are low the trustworthiness is also proportionately less. This creates a balance and realism in our calculations. This model is unique and will help in creating users’ trust in cloud-based digital twins.",CS,AI_ML,0.85,Extracted from log - paper 1861
Enhancing lung cancer diagnosis with data fusion and mobile edge computing using DenseNet and CNN,"The recent advancements in automated lung cancer diagnosis through the application of Convolutional Neural Networks (CNN) on Computed Tomography (CT) scans have marked a significant leap in medical imaging and diagnostics. The precision of these CNN-based classifiers in detecting and analyzing lung cancer symptoms has opened new avenues in early detection and treatment planning. However, despite these technological strides, there are critical areas that require further exploration and development. In this landscape, computer-aided diagnostic systems and artificial intelligence, particularly deep learning methods like the region proposal network, the dual path network, and local binary patterns, have become pivotal. However, these methods face challenges such as limited interpretability, data variability handling issues, and insufficient generalization. Addressing these challenges is key to enhancing early detection and accurate diagnosis, fundamental for effective treatment planning and improving patient outcomes. This study introduces an advanced approach that combines a Convolutional Neural Network (CNN) with DenseNet, leveraging data fusion and mobile edge computing for lung cancer identification and classification. The integration of data fusion techniques enables the system to amalgamate information from multiple sources, enhancing the robustness and accuracy of the model. Mobile edge computing facilitates faster processing and analysis of CT scan images by bringing computational resources closer to the data source, crucial for real-time applications. The images undergo preprocessing, including resizing and rescaling, to optimize feature extraction. The DenseNet-CNN model, strengthened by data fusion and edge computing capabilities, excels in extracting and learning features from these CT scans, effectively distinguishing between healthy and cancerous lung tissues. The classification categories include Normal, Benign, and Malignant, with the latter further sub-categorized into adenocarcinoma, squamous cell carcinoma, and large cell carcinoma. In controlled experiments, this approach outperformed existing state-of-the-art methods, achieving an impressive accuracy of 99%. This indicates its potential as a powerful tool in the early detection and classification of lung cancer, a significant advancement in medical imaging and diagnostic technology.",CS,AI_ML,0.85,Extracted from log - paper 1862
Harmfulness metrics in digital twins of social network rumors detection in cloud computing environment,"Social network rumor harm metric is a task to score the harm caused by a rumor by analyzing the spreading range of the rumor, the users affected, the repercussions caused, etc., and then the harm caused by the rumor. Rumor hazard metric models can help rumor detection digital twins to understand and analyze user behaviors and assist social network network managers to make more informed decisions. However, there is a lack of models that can quantify the harm of rumors and automated harm metric models in rumor detection digital twins. To address this issue, this paper proposes an innovative social network rumor harm metric based on rumor propagation knowledge and a large language model (LLM), RSK-T5. The method first completes the joint task of rumor comment stance detection and sentiment analysis to capture critical features of rumor propagation. Then, this knowledge is used in the pre-training process of LLM to improve the model's understanding of rumor propagation patterns. Finally, the fine-tuning phase focuses on the hazard metrics task to improve the generalization energy. We compare with some existing variants of rumor detection methods, and experimental results demonstrate that RSK-T5 achieves the lowest MSE scores on three well-known rumor detection datasets. The ablative learning work demonstrates the effectiveness of RSK-T5's knowledge of two rumor spreads.",CS,AI_ML,0.85,Extracted from log - paper 1863
Unified ensemble federated learning with cloud computing for online anomaly detection in energy-efficient wireless sensor networks,"Anomaly detection in Wireless Sensor Networks (WSNs) is critical for their reliable and secure operation. Optimizing resource efficiency is crucial for reducing energy consumption. Two new algorithms developed for anomaly detection in WSNs—Ensemble Federated Learning (EFL) with Cloud Integration and Online Anomaly Detection with Energy-Efficient Techniques (OAD-EE) with Cloud-based Model Aggregation. EFL with Cloud Integration uses ensemble methods and federated learning to enhance detection accuracy and data privacy. OAD-EE with Cloud-based Model Aggregation uses online learning and energy-efficient techniques to conserve energy on resource-constrained sensor nodes. By combining EFL and OAD-EE, a comprehensive and efficient framework for anomaly detection in WSNs can be created. Experimental results show that EFL with Cloud Integration achieves the highest detection accuracy, while OAD-EE with Cloud-based Model Aggregation has the lowest energy consumption and fastest detection time among all algorithms, making it suitable for real-time applications. The unified algorithm contributes to the system's overall efficiency, scalability, and real-time response. By integrating cloud computing, this algorithm opens new avenues for advanced WSN applications. These promising  approaches for anomaly detection in resource constrained and large-scale WSNs are beneficial for industrial applications.",CS,AI_ML,0.85,Extracted from log - paper 1864
Multiobjective trajectory optimization algorithms for solving multi-UAV-assisted mobile edge computing problem,"The Internet of Things (IoT) devices are not able to execute resource-intensive tasks due to their limited storage and computing power. Therefore, Mobile edge computing (MEC) technology has recently been utilized to provide computing and storage capabilities to those devices, enabling them to execute these tasks with less energy consumption and low latency. However, the edge servers in the MEC network are located at fixed positions, which makes them unable to be adjusted according to the requirements of end users. As a result, unmanned aerial vehicles (UAVs) have recently been used to carry the load of these edge servers, making them mobile and capable of meeting the desired requirements for IoT devices. However, the trajectories of the UAVs need to be accurately planned in order to minimize energy consumption for both the IoT devices during data transmission and the UAVs during hovering time and mobility between halting points (HPs). The trajectory planning problem is a complicated optimization problem because it involves several factors that need to be taken into consideration. This problem is considered a multiobjective optimization problem since it requires simultaneous optimization of both the energy consumption of UAVs and that of IoT devices. However, existing algorithms in the literature for this problem have been based on converting it into a single objective, which may give preference to some objectives over others. Therefore, in this study, several multiobjective trajectory planning algorithms (MTPAs) based on various metaheuristic algorithms with variable population size and the Pareto optimality theory are presented. These algorithms aim to optimize both objectives simultaneously. Additionally, a novel mechanism called the cyclic selection mechanism (CSM) is proposed to manage the population size accurately, optimizing the number of HPs and the maximum function evaluations. Furthermore, the HPs estimated by each MTPA are associated with multiple UAVs using the k-means clustering algorithm. Then, a low-complexity greedy mechanism is used to generate the order of HPs assigned to each UAV, determining its trajectory. Several experiments are conducted to assess the effectiveness of the MTPAs with variable population size and cyclic selection mechanisms. The experimental findings demonstrate that the MTPAs with the cyclic selection mechanism outperform all competing algorithms, achieving better outcomes.",CS,AI_ML,0.85,Extracted from log - paper 1865
An enhanced state-aware model learning approach for security analysis in lightweight protocol implementations,"Owing to the emergence and rapid advances of new-generation information and digitalization technologies, the concept of model-driven digital twin has received widespread attentions and is developing vigorously. Driven by data and simulators, the digital twin can create the virtual twins of physical objects to perform monitoring, simulation, prediction, optimization, and so on. Hence, the application of digital twin can increase efficiency and security of systems by providing reliable model and decision supports. In this paper, we propose a state-aware model learning method to simulate and analyze the lightweight protocol implementations in edge/cloud environments. We introduce the data flow of program execution and network interaction inputs/outputs (I/O) into the extended finite state machine (EFSM) to expand the modeling scope and insight. We aim to calibrate the states and construct an accurate state-machine model using a digital twin based layered approach to reasonably reflect the correlation of a device’s external behavior and internal data. This, in turn, improves our ability to verify the logic and evaluate the security for protocol implementations. This method firstly involves instrumenting the target device to monitor variable activity during its execution. We then employ learning algorithms to produce multiple rounds of message queries. Both the I/O data corresponding to these query sequences and the state calibration information derived from filtered memory variables are obtained through the mapper and execution monitor, respectively. These two aspects of information are combined to dynamically and incrementally construct the protocol’s state machine. We apply this method to develop SALearn and evaluate the effectiveness of SALearn on two lightweight protocol implementations. Our experimental results indicate that SALearn outperforms existing protocol model learning tools, achieving higher learning efficiency and uncovering more interesting states and security issues. In total, we identified two violation scenarios of rekey logic. These situations also reflect the differences in details between different implementations.",CS,AI_ML,0.85,Extracted from log - paper 1866
An integrated SDN framework for early detection of DDoS attacks in cloud computing,"Cloud computing is a rapidly advancing technology with numerous benefits, such as increased availability, scalability, and flexibility. Relocating computing infrastructure to a network simplifies hardware and software resource monitoring in the cloud. Software-Defined Networking (SDN)-based cloud networking improves cloud infrastructure efficiency by dynamically allocating and utilizing network resources. While SDN cloud networks offer numerous advantages, they are vulnerable to Distributed Denial-of-Service (DDoS) attacks. DDoS attacks try to stop genuine users from using services and drain network resources to reduce performance or shut down services. However, early-stage detection of DDoS attack patterns in cloud environments remains challenging. Current methods detect DDoS at the SDN controller level, which is often time-consuming. We recommend focusing on SDN switches for early detection. Due to the large volume of data from diverse sources, we recommend traffic clustering and traffic anomalies prediction which is of DDoS attacks at each switch. Furthermore, to consolidate the data from multiple clusters, event correlation is performed to understand network behavior and detect coordinated attack activities. Many existing techniques stay behind for early detection and integration of multiple techniques to detect DDoS attack patterns. In this paper, we introduce a more efficient and effectively integrated SDN framework that addresses a gap in previous DDoS solutions. Our framework enables early and accurate detection of DDoS traffic patterns within SDN-based cloud environments. In this framework, we use Recursive Feature Elimination (RFE), Density Based Spatial Clustering (DBSCAN), time series techniques like Auto Regressive Integrated Moving Average (ARIMA), Lyapunov exponent, exponential smoothing filter, dynamic threshold, and lastly, Rule-based classifier. We have evaluated the proposed RDAER model on the CICDDoS 2019 dataset, that achieved an accuracy level of 99.92% and a fast detection time of 20 s, outperforming existing methods.",CS,AI_ML,0.85,Extracted from log - paper 1867
Non-orthogonal multiple access-based MEC for energy-efficient task offloading in e-commerce systems,"Mobile edge computing (MEC) reduces the latency for end users to access applications deployed at the edge by offloading tasks to the edge. With the popularity of e-commerce and the expansion of business scale, server load continues to increase, and energy efficiency issues gradually become more prominent. Computation offloading has received widespread attention as a technology that effectively reduces server load. However, how to improve energy efficiency while ensuring computing requirements is an important challenge facing computation offloading. To solve this problem, using non-orthogonal multiple access (NOMA) to increase the efficiency of multi-access wireless transmission, MEC supporting NOMA is investigated in the research. Computing resources will be divided into separate sub-computing that will be handled via e-commerce terminals or transferred to edge sides by reutilizing radio resources, we put forward a Group Switching Matching Algorithm Based on Resource Unit Allocation (GSM-RUA) algorithm that is multi-dimensional. To this end, we first formulate this task allocation problem as a long-term stochastic optimization problem, which we then convert to three short-term deterministic sub-programming problems using Lyapunov optimization, namely, radio resource allocation in a large timescale, computation resource allocating and splitting in a small-time frame. Of the 3 short-term deterministic sub-programming problems, the first sub-programming problem can be remodeled into a 1 to n matching problem, which can be solved using the block-shift-matching-based radio resource allocation method. The latter two sub-programming problems are then transformed into two continuous convex problems by relaxation and then solved easily. We then use simulations to prove that our GSM-RUA algorithm is superior to the state-of-the-art resource management algorithms in terms of energy consumption, efficiency and complexity for e-commerce scenarios.",CS,AI_ML,0.85,Extracted from log - paper 1868
Efficiently localizing system anomalies for cloud infrastructures: a novel Dynamic Graph Transformer based Parallel Framework,"Cloud environment is a virtual, online, and distributed computing environment that provides users with large-scale services. And cloud monitoring plays an integral role in protecting infrastructures in the cloud environment. Cloud monitoring systems need to closely monitor various KPIs of cloud resources, to accurately detect anomalies. However, due to the complexity and highly dynamic nature of the cloud environment, anomaly detection for these KPIs with various patterns and data quality is a huge challenge, especially those massive unlabeled data. Besides, it’s also difficult to improve the accuracy of the existing anomaly detection methods. To solve these problems, we propose a novel Dynamic Graph Transformer based Parallel Framework (DGT-PF) for efficiently detect system anomalies in cloud infrastructures, which utilizes Transformer with anomaly attention mechanism and Graph Neural Network (GNN) to learn the spatio-temporal features of KPIs to improve the accuracy and timeliness of model anomaly detection. Specifically, we propose an effective dynamic relationship embedding strategy to dynamically learn spatio-temporal features and adaptively generate adjacency matrices, and soft cluster each GNN layer through Diffpooling module. In addition, we also use nonlinear neural network model and AR-MLP model in parallel to obtain better detection accuracy and improve detection performance. The experiment shows that the DGT-PF framework have achieved the highest F1-Score on 5 public datasets, with an average improvement of 21.6% compared to 11 anomaly detection models.",CS,AI_ML,0.85,Extracted from log - paper 1869
Distance optimization and directional overcurrent relay coordination using edge-powered biogeography-genetic algorithms,"The effective functioning and regulation of power systems crucially rely on the coordination of distance and directional overcurrent relays. Accurate fault detection and successful clearing sequences require support for each relay and the maintenance of the coordination time interval (CTI) between major distance relays, directional overcurrent relay support, and other relay zones. Efficiently initiating relays while adhering to complex coordination limitations poses a challenging task that demands innovative solutions. This study addresses the intricate problem of relay coordination by employing heuristic methods, specifically genetic algorithms (GA) and biogeography-based optimization (BBO), in both a 9-bus and 39-bus system. The primary objective is to determine the most efficient time setting factor (TSM) that minimizes the duration of relay operation. Additionally, the intelligent features of the overcurrent relay are carefully chosen to enhance the research's results. The integration of edge computing capabilities plays a significant role in advancing this coordination method. By incorporating advanced algorithms and communication technologies at the edge, the prompt activation of relays becomes possible, thereby meeting coordination demands. This study explores the combination of edge-based servers with genetic algorithms (GA) and biogeography-based optimization (BBO) techniques to enhance relay coordination. The findings indicate a notable enhancement compared to conventional approaches. However, comparative research suggests that BBO's performance is similar to GA, without a distinct advantage in achieving higher outcomes.",CS,AI_ML,0.85,Extracted from log - paper 1870
Energy-efficient virtual machine placement in distributed cloud using NSGA-III algorithm,"Cloud computing is the most widely adapted computing model to process scientific workloads in remote servers accessed through the internet. In the IaaS cloud, the virtual machine (VM) is the execution unit that processes the user workloads. Virtualization enables the execution of multiple virtual machines (VMs) on a single physical machine (PM). Virtual machine placement (VMP) strategically assigns VMs to suitable physical devices within a data center. From the cloud provider's perspective, the virtual machine must be placed optimally to reduce resource wastage to aid economic revenue and develop green data centres. Cloud providers need an efficient methodology to minimize resource wastage, power consumption, and network transmission delay. This paper uses NSGA-III, a multi-objective evolutionary algorithm, to simultaneously reduce the mentioned objectives to obtain a non-dominated solution. The performance metrics (Overall Nondominated Vector Generation and Spacing) of the proposed NSGA-III algorithm is compared with other multi-objective algorithms, namely VEGA, MOGA, SPEA, and NSGA-II. It is observed that the proposed algorithm performs 7% better that the existing algorithm in terms of ONVG and 12% better results in terms of spacing. ANOVA and DMRT statistical tests are used to cross-validate the results.",CS,AI_ML,0.85,Extracted from log - paper 1871
COPSA: a computation offloading strategy based on PPO algorithm and self-attention mechanism in MEC-empowered smart factories,"With the dawn of Industry 5.0 upon us, the smart factory emerges as a pivotal element, playing a crucial role in the realm of intelligent manufacturing. Meanwhile, mobile edge computing is proposed to alleviate the computational burden presented by substantial workloads in smart factories. Nonetheless, it is very challenging to effectively incorporate edge computing resources to improve the efficiency of resource deployment in smart factories. Accordingly, we devise a novel approach based on Proximal Policy Optimization algorithm with the Self-Attention Mechanism to implement computing resource allocation in MEC-Empowered Smart Factories. More specifically, the self-attention mechanism is incorporated to enable dynamic focus on state information, accelerates convergence and facilitates global control. A great number of experiments conducted on both simulated and real datasets have verified the superiority of our proposed approach compared to the state-of-the-art baselines.",CS,AI_ML,0.85,Extracted from log - paper 1872
Joint Autoscaling of Containers and Virtual Machines for Cost Optimization in Container Clusters,"Autoscaling enables container cluster orchestrators to automatically adjust computational resources, such as containers and Virtual Machines (VMs), to handle fluctuating workloads effectively. This adaptation can involve modifying the amount of resources (horizontal scaling) or adjusting their computational capacity (vertical scaling). The motivation for our work stems from the limitations of previous autoscaling approaches, which are either partial (scaling containers or VMs, but not both) or excessively complex to be used in real systems. This complexity arises from their use of models with a large number of variables and the addressing of two simultaneous challenges: achieving the optimal deployment for a single scheduling window and managing the transition between successive scheduling windows. We propose an Integer Linear Programming (ILP) model to address the challenge of autoscaling containers and VMs jointly, both horizontally and vertically, to minimize deployment costs. This model is designed to be used with predictive autoscalers and be solved in a reasonable time, even for large clusters. To this end, improvements and reasonable simplifications with respect to previous models have been carried out to drastically reduce the size of the resource allocation problem. Furthermore, the proposed model provides an enhanced representation of system performance in comparison to previous approaches. A tool called Conlloovia has been developed to implement this model. To evaluate its performance, we have conducted a comprehensive assessment, comparing it with two heuristic allocators with different problem sizes. Our findings indicate that Conlloovia consistently demonstrates lower deployment costs in a significant number of cases. Conlloovia has also been evaluated with a real application, using synthetic and real workload traces, as well as different scheduling windows, with deployment costs approximately 20% lower than heuristic allocators.",CS,AI_ML,0.85,Extracted from log - paper 1873
Analyzing Energy-Efficient and Kubernetes-Based Autoscaling of Microservices Using Probabilistic Model Checking,"Microservices are widely used to enable agility and scalability in modern software systems, while cloud computing offers cost-effective ways to provision computing resources on demand. However, ensuring the correctness of scaling decisions and their impact on energy consumption is a challenging problem that has not been sufficiently addressed in previous research. Thus, in this paper, we present an innovative approach for analyzing host energy consumption and energy violations influenced by microservice autoscaling policies using probabilistic model checking (PMC). We propose four variations of the Markov Decision Process (MDP) models that incorporate various scaling constraints inspired by Kubernetes-based Horizontal Pod Autoscaler, and we encode these models using two different approaches, namely, bounded-by-action (BBA) and bounded-by-state (BBS). We use PMC to verify the scaling policies in terms of host energy consumption and energy violations, and we conduct sensitivity analysis to demonstrate the effectiveness of our models in generating energy-efficient scaling policies. Our results show that the latency and energy-based MDP model offers the most suitable policies for ensuring energy efficiency in microservice systems. Additionally, the number of pods and the scale-out action significantly affect energy consumption and violations. Sensitivity analysis also reveals that incorporating latency into scaling decisions is key to energy efficiency, while variations in the maximum pod threshold significantly influence energy consumption and violation. Our approach provides a formal method for ensuring the correctness of microservice autoscaling decisions in cloud environments at design time and can help reduce energy consumption and violations while ensuring service-level objectives are met.",CS,AI_ML,0.85,Extracted from log - paper 1874
KOSMOS: Vertical and Horizontal Resource Autoscaling for Kubernetes,"Cloud applications are increasingly executed onto lightweight containers that can be efficiently managed to cope with highly varying and unpredictable workloads. Kubernetes, the most popular container orchestrator, provides means to automatically scale containerized applications to keep their response time under control. Kubernetes provisions resources using two main components: i) Horizontal Pod Autoscaler (HPA), which controls the amount of containers running for an application, and ii) Vertical Pod Autoscaler (VPA), which oversees the resource allocation of existing containers. These two components have several limitations: they must control different metrics, they use simple threshold-based rules, and the reconfiguration of existing containers requires stopping and restarting them. To overcome these limitations this paper presents KOSMOS , a novel autoscaling solution for Kubernetes. Containers are individually controlled by control-theoretical planners that manage container resources on-the-fly (vertical scaling). A dedicated component is in charge of handling resource contention scenarios among containers deployed in the same node (a physical or virtual machine). Finally, at the cluster-level a heuristic-based controller is in charge of the horizontal scaling of each application.",CS,AI_ML,0.85,Extracted from log - paper 1875
Enhancing Edge Environment Scalability: Leveraging Kubernetes for Container Orchestration and Optimization,"Kubernetes is an open‐source container orchestration platform, offers a comprehensive suite of features for managing containerized applications effectively. These features encompass horizontal scaling, per‐node‐pool cluster scaling and automated resource request adjustments. This research endeavors to harness these capabilities to address the limitations experienced by fog servers in edge environments, particularly those arising from restricted network connectivity and scalability challenges. In this research paper, the primary focus is on Kubernetes role of enhancing scalability, providing a robust framework for managing containerized applications. The proposed approach involves creating a predefined number of pods and containers within a Kubernetes cluster, specifically designed to efficiently handle incoming requests while optimizing CPU and memory usage. This method implements a microservice architecture for the web tier, with separate pods for the front end, back end and database, ensuring modular and scalable design. All pods communicate and integrate through REST APIs, facilitating seamless interaction and data exchange between the services. When handling web requests, the approach enables and controls both internal and external networks, ensuring secure and efficient communication. The analysis then examines the CPU and memory utilization of the pods, as well as node bandwidth, to provide a comprehensive evaluation of container scalability and performance within the Kubernetes cluster. These findings effectively demonstrate Kubernetes' capability in managing container scalability and optimizing resource utilization, highlighting its efficiency and robustness in a microservice environment.",CS,AI_ML,0.85,Extracted from log - paper 1876
Container Scaling Strategy Based on Reinforcement Learning,"Elasticity capability is one of the most important capabilities of cloud computing, which combines large-scale resource allocation capability to quickly achieve minute-level resource demand provisioning to meet the elasticity requirements of different scale scenarios. The elasticity capability is mainly determined by the container start-up speed and container scaling strategy together, where the container scaling strategy contains both vertical container scaling strategy and horizontal container scaling strategy. In order to make the container scaling policy more effective and improve the application service quality and resource utilization, we briefly introduce Kubernetes’ horizontal pod autoscaling (HPA) strategy, analyze the existing problem of HPA, and develop a container scaling strategy based on reinforcement learning. First, we analyze the problems of Kubernetes’ existing HPA container autoscaling strategy in the scale-up and scale-down phases, respectively. Second, the Markov decision model is used to model the container scaling problem. Then, we propose a model-based reinforcement learning algorithm to solve the container scaling problem. Finally, we compare the experimental results of the HPA scaling strategy and the model-based reinforcement learning strategy with the results from the resource utilization of the application, the change of the number of pods, and the application response time; through the experimental analysis, we verify that the reinforcement learning-based container scaling strategy can guarantee the application service quality and improve the utilization of the application resources more effectively than the HPA strategy.",CS,AI_ML,0.85,Extracted from log - paper 1877
STAM-LSGRU: a spatiotemporal radar echo extrapolation algorithm with edge computing for short-term forecasting,"With the advent of Mobile Edge Computing (MEC), shifting data processing from cloud centers to the network edge presents an advanced computational paradigm for addressing latency-sensitive applications. Specifically, in radar systems, the real-time processing and prediction of radar echo data pose significant challenges in dynamic and resource-constrained environments. MEC, by processing data near its source, not only significantly reduces communication latency and enhances bandwidth utilization but also diminishes the necessity of transmitting large volumes of data to the cloud, which is crucial for improving the timeliness and efficiency of radar data processing. To meet this demand, this paper proposes a model that integrates a spatiotemporal Attention Module (STAM) with a Long Short-Term Memory Gated Recurrent Unit (ST-ConvLSGRU) to enhance the accuracy of radar echo prediction while leveraging the advantages of MEC. STAM, by extending the spatiotemporal receptive field of the prediction units, effectively captures key inter-frame motion information, while optimizations to the convolutional structure and loss function further boost the model’s predictive performance. Experimental results demonstrate that our approach significantly improves the accuracy of short-term weather forecasting in a mobile edge computing environment, showcasing an efficient and practical solution for processing radar echo data under dynamic, resource-limited conditions.",CS,AI_ML,0.85,Extracted from log - paper 1878
ABWOA: adaptive boundary whale optimization algorithm for large-scale digital twin network construction,"Digital twin network (DTN) as an emerging network paradigm, have garnered growing attention. For large-scale networks, a crucial problem is how to effectively map physical networks onto the infrastructure platform of DTN. To address this issue, we propose a heuristic method of the adaptive boundary whale optimization algorithm (ABWOA) to solve the digital twin network construction problem, improving the efficiency and reducing operational costs of DTN. Extensive comparison experiments are conducted between ABWOA and various algorithms such as genetic algorithm, particle swarm optimization, artificial bee colony, differential evolution algorithm, moth search algorithm and original whale optimization algorithm. The experimental results show that ABWOA is superior to other algorithms in terms of solution quality, convergence speed, and time cost. It can solve the digital twin network construction problem more effectively.",CS,AI_ML,0.85,Extracted from log - paper 1879
Load balancing scheduling mechanism for OpenStack and Docker integration,"With the development of cloud-edge collaborative computing, cloud computing has become crucial in data analysis and data processing. OpenStack and Docker are important components of cloud computing, and the integration of the two has always attracted widespread attention in industry. The scheduling mechanism adopted by the existing fusion solution uses a uniform resource weight for all containers, and the computing nodes resources on the cloud platform is unbalanced under differentiated resource requirements of the containers. Therefore, considering different network communication qualities, a load-balancing Docker scheduling mechanism based on OpenStack is proposed to meet the needs of various resources (CPU, memory, disk, and bandwidth) of containers. This mechanism uses the precise limitation strategy for container resources and a centralized strategy for container resources as the scheduling basis, and it generates exclusive weights for containers through a filtering stage, a weighing stage based on weight adaptation, and a non-uniform memory access (NUMA) lean stage. The experimental results show that, compared with Nova-docker and Yun, the proposed mechanism reduces the resource load imbalance within a node by 57.35% and 59.00% on average, and the average imbalance between nodes is reduced by 53.53% and 50.90%. This mechanism can also achieve better load balancing without regard to bandwidth.",CS,AI_ML,0.85,Extracted from log - paper 1880
Comparative analysis of metaheuristic load balancing algorithms for efficient load balancing in cloud computing,"Load balancing is a serious problem in cloud computing that makes it challenging to ensure the proper functioning of services contiguous to the Quality of Service, performance assessment, and compliance to the service contract as demanded from cloud service providers (CSP) to organizations. The primary objective of load balancing is to map workloads to use computing resources that significantly improve performance. Load balancing in cloud computing falls under the class of concerns defined as ""NP-hard"" issues due to vast solution space. Therefore it requires more time to predict the best possible solution. Few techniques can perhaps generate an ideal solution under a polynomial period to fix these issues. In previous research, Metaheuristic based strategies have been confirmed to accomplish accurate solutions under a decent period for those kinds of issues. This paper provides a comparative analysis of various metaheuristic load balancing algorithms for cloud computing based on performance factors i.e., Makespan time, degree of imbalance, response time, data center processing time, flow time, and resource utilization. The simulation results show the performance of various Meta-heuristic Load balancing methods, based on performance factors. The Particle swarm optimization method performs better in improving makespan, flow time, throughput time, response time, and degree of imbalance.",CS,AI_ML,0.85,Extracted from log - paper 1881
ReactiveFnJ: A choreographed model for Fork-Join Workflow in Serverless Computing,"Function-as-a-Service (FaaS) is an event-based reactive programming model where functions run in ephemeral stateless containers for short duration. For building complex serverless applications, function composition is crucial to coordinate and synchronize the workflow of an application. Some serverless orchestration systems exist, but they are in their primitive state and do not provide inherent support for non-trivial workflows like, Fork-Join. To address this gap, we propose a fully serverless and scalable design model ReactiveFnJ for Fork-Join workflow. The intent of this work is to illustrate a design which is completely choreographed, reactive, asynchronous, and represents a dynamic composition model for serverless applications based on Fork-Join workflow. Our design uses two innovative patterns, namely, Relay Composition and Master-Worker Composition to solve execution time-out challenges. As a Proof-of-Concept (PoC), the prototypical implementation of Split-Sort-Merge use case, based on Fork-Join workflow is discussed and evaluated. The ReactiveFnJ handles embarrassingly parallel computations, and its design does not depend on any external orchestration services, messaging services, and queue services. ReactiveFnJ facilitates in designing fully automated pipelines for distributed data processing systems, satisfying the Serverless Trilemma in true essence. A file of any size can be processed using our effective and extensible design without facing execution time-out challenges. The proposed model is generic and can be applied to a wide range of serverless applications that are based on the Fork-Join workflow pattern. It fosters the choreographed serverless composition for complex workflows. The proposed design model is useful for software engineers and developers in industry and commercial organizations, total solution vendors and academic researchers.",CS,AI_ML,0.85,Extracted from log - paper 1882
Edge computing-based digital management system of game events in the era of Internet of Things,"With the great development of Internet of Things (IoT) and edge computing, the development of sports activities depends on the development of information technology and it is inevitable to pay attention to the combination and optimization of resources. The combination of IoT and edge computing will be critical in sports activities. This paper elaborates on the application of network skill in sports event information management, that is, through the effective gathering of sports event data, to realize the use of sports event information, to achieve the purpose of information and digitization. Furthermore, the goal is to investigate the effect of sports event in the era of IoT. The impact of sports events on the economy and culture of the hosting city is investigated using IoT concept of edge computing. By analyzing the advantages and disadvantages of traditional centralized optimization method, we present a series of performance indicators and utility functions and show that the method is effective and achieves the optimal purpose. Through vital research, it is found that with the development of the edge computing and IoT industry, the scale of sports events is constantly expanding. By 2019, there has been a scale of 1,271 billion yuan. An increase of 981 billion yuan, compared with 290 billion yuan in 2013. Therefore, the use of the IoT technology in combination with edge computing to manage sports events will greatly encourage the expansion of sports activities. Furthermore, the holding of sporting events reflects a city’s overall strength and enhances the city’s exposure and fame. The investigation offers a certain reference point for cities looking to increase their influence through events.",CS,AI_ML,0.85,Extracted from log - paper 1883
Memory sharing for handling memory overload on physical machines in cloud data centers,"Over-committing computing resources is a widely adopted strategy for increased cluster utilization in Infrastructure as a Service (IaaS) cloud data centers. A potential consequence of over-committing computing resources is memory overload of physical machines (PMs). Memory overload occurs if memory usage exceeds a defined alarm threshold, exposing running computation tasks at a risk of being terminated by the operating system. A prevailing measure to handle memory overload of a PM is live migration of virtual machines (VMs). However, this not only consumes network bandwidth, CPU, and other resources, but also compels a temporary unavailability of the VMs being migrated. To handle memory overload, we present a memory sharing system in this paper for PMs in cloud data centers. With memory sharing, a PM automatically borrows memory from a remote PM when necessary, and releases the borrowed memory when memory overload disappears. This is implemented through swapping inactive memory pages to remote memory resource. Experimental studies conducted on InfiniBand-networked PMs show that the memory sharing system is fully functional. The measured throughput and latency are around 929 Mbps and 1.3 $$\mu$$                   μ                 s, respectively, on average for remote memory access. They are similar to those from accessing a local-volatile memory express solid-state drive, and thus are promising in real applications.",CS,AI_ML,0.85,Extracted from log - paper 1884
A privacy protection approach in edge-computing based on maximized dnn partition strategy with energy saving,"With the development of deep neural network (DNN) techniques, applications of DNNs show state-of-art performance. In the cloud edge collaborative mode, edge devices upload the raw data, such as texts, images, and videos, to the cloud for processing. Then, the cloud returns prediction or classification results. Although edge devices take advantage of the powerful performance of DNN, there are also colossal privacy protection risks. DNN partition strategy can effectively solve the privacy problems by offload part of the DNN model to the edge, in which the encoded features are transmitted rather than original data. We explore the relationship between privacy and the intermedia result of the DNN. The more parts offloaded to the edge, the more abstract features we can have, indicating more conducive to privacy protection. We propose a privacy protection approach based on a maximum DNN partition strategy. Besides, a mix-precision quantization approach is adopted to reduce the energy use of edge devices. The experiments show that our method manages to increase at most 20% model privacy in various DNN architecture. Through the energy-aware mixed-precision quantization approach, the model’s energy consumption is reduced by at most 5x comparing to the typical edge-cloud solution.",CS,AI_ML,0.85,Extracted from log - paper 1885
Enriching computing simulators by generating realistic serverless traces,"Serverless computing is stepping forward to provide a cloud environment that mainly focuses on managing infrastructure, resources and configurations on the behalf of a user. Research in this field can’t rely on commercial providers such as AWS and Azure, as their inflexibility and cost often limits the required levels of reproducibility and scalability. Therefore, simulators have been opted as an alternative solution by the research community. They offer a reduced-cost and easy-setup environment. To get respectable precision, simulators use real traces collected and offered by commercial providers. These traces represent comprehensive information of executed tasks that reflect user behaviour. Due to serverless computing’s recency, typical workload traces employed by IaaS simulators are not well adoptable to the new computing model. In this paper, we propose an approach for generating realistic serverless traces. We enhance our previous generator approach that was based on the Azure Functions dataset. Our new, genetic algorithm based approach improves the statistical properties of the generated traces. We also enabled arbitrary scaling of the workload, while maintaining real users’ behaviour. These advances further support reproducibility in the serverless research community. We validated the results of our generator approach using the coefficient of determination (R^2), which shows that our generated workload closely matches the original dataset’s characteristics in terms of execution time, memory utilisation as well as user participation percentage. To demonstrate the benefits of the reusability of the generated traces, we applied them with a diverse set of simulators and shown that they offer reproducible results independently of the simulator used.",CS,AI_ML,0.85,Extracted from log - paper 1886
"Data pipeline approaches in serverless computing: a taxonomy, review, and research trends","Serverless computing has gained significant popularity due to its scalability, cost-effectiveness, and ease of deployment. With the exponential growth of data, organizations face the challenge of efficiently processing and analyzing vast amounts of data in a serverless environment. Data pipelines play a crucial role in managing and transforming data within serverless architectures. This paper provides a taxonomy of data pipeline approaches in serverless computing. Classification is based on architectural features, data processing techniques, and workflow orchestration mechanisms, these approaches are categorized into three primary methods: heuristic-based approach, Machine learning-based approach, and framework-based approach. Furthermore, a systematic review of existing data pipeline frameworks and tools is provided, encompassing their strengths, limitations, and real-world use cases. The advantages and disadvantages of each approach, also the challenges and performance metrics that influence their effectuality have been examined. Every data pipeline approach has certain advantages and disadvantages, whether it is framework-based, heuristic-based, or machine learning-based. Each approach is suitable for specific use cases. Hence, it is crucial assess the trade-offs between complexity, performance, cost, and scalability, while selecting a data pipeline approach. In the end, the paper highlights a number of open issues and future investigations directions for data pipeline in the serverless computing, which involve scalability, fault tolerance, data real time processing, data workflow orchestration, function state management with performance and cost in the serverless computing environments.",CS,AI_ML,0.85,Extracted from log - paper 1887
"SSF-CDW: achieving scalable, secure, and fast OLAP query for encrypted cloud data warehouse","Implementing a cloud data-warehouse to store sensitive data presents challenges, especially for OLAP queries over encrypted multidimensional data. This paper proposes SSF-CDW, a privacy-preserving scheme combining symmetric encryption and CP-ABE with a Redis-based cube-retrieval mechanism. It supports fine-grained access, accelerates ad-hoc & repeated OLAP queries, and shows significant speed-ups over baseline indexing while preserving privacy.",CS,AI_ML,0.85,Extracted from log - paper 1888
"Orchestration in the Cloud-to-Things compute continuum: taxonomy, survey and future directions","IoT use-cases require simultaneous access to heterogeneous sensors and multi-cloud resources. Extending orchestration across the Cloud-to-Things continuum raises new challenges. This survey builds a detailed taxonomy of orchestration requirements, reviews existing research/industrial solutions (e.g., AWS CloudFormation, Kubernetes, MiCADO), identifies gaps, and outlines future research challenges for automated deployment and run-time management across cloud, fog and edge layers.",CS,AI_ML,0.85,Extracted from log - paper 1889
Domain-knowledge-free cloud-IDS with lightweight embedding method,"Cloud attacks are rising as data migrates to virtualised servers. The paper introduces C-IDS, an adaptive cloud intrusion-detection system automatically deployed via Infrastructure-as-Code. Using Seq2Seq BI-LSTM with Bahdanau attention, it learns environment-specific logs and detects anomalies in real time. Experiments on Linux, Windows, Hadoop, OpenStack, Apache and CICIDS2018 datasets show 98.2 % recognition and 94.2 % detection accuracy, demonstrating effective, IaC-driven cloud security.",CS,AI_ML,0.85,Extracted from log - paper 1890
A proposed architecture for network forensic system in large-scale networks,"Cybercrime is increasing at a faster pace and sometimes causes billions of dollars of business- losses so investigating attackers after commitment is of utmost importance and become one of the main concerns of network managers. Network forensics as the process of Collecting, identifying, extracting and analyzing data and systematically monitoring traffic of network is one of the main requirements in detection and tracking of criminals. In this paper, we propose an architecture for network forensic system. Our proposed architecture consists of five main components: collection and indexing, database management, analysis component, SOC communication component and the database. The main difference between our proposed architecture and other systems is in analysis component. This component is composed of four parts: Analysis and investigation subsystem, Reporting subsystem, Alert and visualization subsystem and the malware analysis subsystem. The most important differentiating factors of the proposed system with existing systems are: clustering and ranking of malware, dynamic analysis of malware, collecting and analysis of network flows and anomalous behaviour analysis.",CS,AI_ML,0.85,Extracted from log - paper 1891
A Framework for Measuring the Quality of Infrastructure-as-Code Scripts,"Infrastructure as Code (IaC) has become integral to modern software development, enabling automated and consistent configuration of computing environments. The rapid proliferation of IaC scripts has highlighted the need for better code quality assessment methods. This paper proposes a new IaC code quality framework specifically showcased for Ansible repositories as a foundation. By analyzing a comprehensive dataset of repositories from Ansible Galaxy, we applied our framework to evaluate code quality across multiple attributes. The analysis of our code quality metrics applied to Ansible Galaxy repositories reveal trends over time indicating improvements in areas such as metadata and error handling, while highlighting declines in others such as sophistication and automation. The framework offers practitioners a systematic tool for assessing and enhancing IaC scripts, fostering standardization and facilitating continuous improvement. It also provides a standardized foundation for further work into IaC code quality.",CS,AI_ML,0.85,Extracted from log - paper 1892
Polyglot Code Smell Detection for Infrastructure as Code with GLITCH,"This paper presents GLITCH, a new technology-agnostic framework that enables automated polyglot code smell detection for Infrastructure as Code scripts. GLITCH uses an intermediate representation on which different code smell detectors can be defined. It currently supports the detection of nine security smells and nine design & implementation smells in scripts written in Ansible, Chef, Docker, Puppet, or Terraform. Studies conducted with GLITCH not only show that GLITCH can reduce the effort of writing code smell analyses for multiple IaC technologies, but also that it has higher precision and recall than current state-of-the-art tools. A video describing and demonstrating GLITCH is available at: https://youtu.be/E4RhCcZjWbk",CS,AI_ML,0.85,Extracted from log - paper 1893
A Survey on Software Defined Networking: Architecture for Next Generation Network,"The evolution of software defined networking (SDN) has played a significant role in the development of next-generation networks (NGN). SDN as a programmable network having service provisioning on the fly has induced a keen interest both in academic world and industry. In this article, a comprehensive survey is presented on SDN advancement over conventional network. The paper covers historical evolution in relation to SDN, functional architecture of the SDN and its related technologies, and OpenFlow standards/protocols, including the basic concept of interfacing of OpenFlow with network elements (NEs) such as optical switches. In addition a selective architecture survey has been conducted. Our proposed architecture on software defined heterogeneous network, points towards new technology enabling the opening of new vistas in the domain of network technology, which will facilitate in handling of huge internet traffic and helps infrastructure and service providers to customize their resources dynamically. Besides, current research projects and various activities as being carried out to standardize SDN as NGN by different standard development organizations (SODs) have been duly elaborated to judge how this technology moves towards standardization.",CS,AI_ML,0.85,Extracted from log - paper 1894
Network Function Virtualization: State-of-the-art and Research Challenges,"Network Function Virtualization (NFV) has drawn significant attention from both industry and academia as an important shift in telecommunication service provisioning. By decoupling Network Functions (NFs) from the physical devices on which they run, NFV has the potential to lead to significant reductions in Operating Expenses (OPEX) and Capital Expenses (CAPEX) and facilitate the deployment of new services with increased agility and faster time-to-value. The NFV paradigm is still in its infancy and there is a large spectrum of opportunities for the research community to develop new architectures, systems and applications, and to evaluate alternatives and trade-offs in developing technologies for its successful deployment. In this paper, after discussing NFV and its relationship with complementary fields of Software Defined Networking (SDN) and cloud computing, we survey the state-of-the-art in NFV, and identify promising research directions in this area. We also overview key NFV projects, standardization efforts, early implementations, use cases and commercial products.",CS,AI_ML,0.85,Extracted from log - paper 1895
Revolutionizing Datacenter Networks via Reconfigurable Topologies,"With the popularity of cloud computing and data-intensive applications such as machine learning, datacenter networks have become a critical infrastructure for our digital society. Given the explosive growth of datacenter traffic and the slowdown of Moore's law, significant efforts have been made to improve datacenter network performance over the last decade. A particularly innovative solution is reconfigurable datacenter networks (RDCNs): datacenter networks whose topologies dynamically change over time, in either a demand-oblivious or a demand-aware manner. Such dynamic topologies are enabled by recent optical switching technologies and stand in stark contrast to state-of-the-art datacenter network topologies, which are fixed and oblivious to the actual traffic demand. In particular, reconfigurable demand-aware and 'self-adjusting' datacenter networks are motivated empirically by the significant spatial and temporal structures observed in datacenter communication traffic. This paper presents an overview of reconfigurable datacenter networks. In particular, we discuss the motivation for such reconfigurable architectures, review the technological enablers, and present a taxonomy that classifies the design space into two dimensions: static vs. dynamic and demand-oblivious vs. demand-aware. We further present a formal model and discuss related research challenges. Our article comes with complementary video interviews in which three leading experts, Manya Ghobadi, Amin Vahdat, and George Papen, share with us their perspectives on reconfigurable datacenter networks.",CS,AI_ML,0.85,Extracted from log - paper 1896
Disruption-aware Microservice Re-orchestration for Cost-efficient Multi-cloud Deployments,"Multi-cloud environments enable a cost-efficient scaling of cloud-native applications across geographically distributed virtual nodes with different pricing models. In this context, the resource fragmentation caused by frequent changes in the resource demands of deployed microservices, along with the allocation or termination of new and existing microservices, increases the deployment cost. Therefore, re-orchestrating deployed microservices on a cheaper configuration of multi-cloud nodes offers a practical solution to restore the cost efficiency of deployment. However, the rescheduling procedure causes frequent service interruptions due to the continuous termination and rebooting of the containerized microservices. Moreover, it may potentially interfere with and delay other deployment operations, compromising the stability of the running applications. To address this issue, we formulate a multi-objective integer linear programming problem that computes a microservice rescheduling solution capable of providing minimum deployment cost without significantly affecting the service continuity. At the same time, the proposed formulation also preserves the quality of service (QoS) requirements, including latency, expressed through microservice colocation constraints. Additionally, we present a heuristic algorithm to approximate the optimal solution, striking a balance between cost reduction and service disruption mitigation. We integrate the proposed approach as a custom plugin of the Kubernetes scheduler. Results reveal that our approach significantly reduces multi-cloud deployment costs and service disruptions compared to the default Kubernetes scheduler implementation, while ensuring QoS requirements are consistently met.",CS,AI_ML,0.85,Extracted from log - paper 1897
"Portable, high-performance containers for HPC","Building and deploying software on high-end computing systems is a challenging task. High performance applications have to reliably run across multiple platforms and environments, and make use of site-specific resources while resolving complicated software-stack dependencies. Containers are a type of lightweight virtualization technology that attempt to solve this problem by packaging applications and their environments into standard units of software that are: portable, easy to build and deploy, have a small footprint, and low runtime overhead. In this work we present an extension to the container runtime of Shifter that provides containerized applications with a mechanism to access GPU accelerators and specialized networking from the host system, effectively enabling performance portability of containers across HPC resources. The presented extension makes possible to rapidly deploy high-performance software on supercomputers from containerized applications that have been developed, built, and tested in non-HPC commodity hardware, e.g. the laptop or workstation of a researcher.",CS,AI_ML,0.85,Extracted from log - paper 1898
"Serverless Computing: Architecture, Concepts, and Applications","Recently, serverless computing has gained recognition as a leading cloud computing method. Providing a solution that does not require direct server and infrastructure management, this technology has addressed many traditional model problems by eliminating them. Therefore, operational complexity and costs are reduced, allowing developers to concentrate on writing and deploying software without worrying about server management. This chapter examines the advantages, disadvantages, and applications of serverless computing, implementation environments, and reasons for its use. Additionally, integrating this computing paradigm with other technologies is examined to address the challenges of managing, securing, and implementing large amounts of data. This chapter aims to provide a comprehensive view of the potentials and limitations of serverless computing by comparing its applications in different industries and examining the future trends of this technology. Lastly, this chapter provides a comprehensive conclusion of the applications and challenges of serverless computing.",CS,AI_ML,0.85,Extracted from log - paper 1899
"AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges","Artificial Intelligence for IT operations (AIOps) aims to combine the power of AI with the big data generated by IT Operations processes, particularly in cloud infrastructures, to provide actionable insights with the primary goal of maximizing availability. There are a wide variety of problems to address, and multiple use-cases, where AI capabilities can be leveraged to enhance operational efficiency. Here we provide a review of the AIOps vision, trends challenges and opportunities, specifically focusing on the underlying AI techniques. We discuss in depth the key types of data emitted by IT Operations activities, the scale and challenges in analyzing them, and where they can be helpful. We categorize the key AIOps tasks as - incident detection, failure prediction, root cause analysis and automated actions. We discuss the problem formulation for each task, and then present a taxonomy of techniques to solve these problems. We also identify relatively under explored topics, especially those that could significantly benefit from advances in AI literature. We also provide insights into the trends in this field, and what are the key investment opportunities.",CS,AI_ML,0.85,Extracted from log - paper 1900
Cloud Resource Allocation with Convex Optimization,"We present a convex optimization framework for overcoming the limitations of Kubernetes Cluster Autoscaler by intelligently allocating diverse cloud resources while minimizing costs and fragmentation. Current Kubernetes scaling mechanisms are restricted to homogeneous scaling of existing node types, limiting cost-performance optimization possibilities. Our matrix-based model captures resource demands, costs, and capacity constraints in a unified mathematical framework. A key contribution is our logarithmic approximation to the indicator function, which enables dynamic node type selection while maintaining problem convexity. Our approach balances cost optimization with operational complexity through interior-point methods. Experiments with real-world Kubernetes workloads demonstrate reduced costs and improved resource utilization compared to conventional Cluster Autoscaler strategies that can only scale up or down existing node pools.",CS,AI_ML,0.85,Extracted from log - paper 1901
Foundational DevOps Patterns,"Adopting DevOps practices is nowadays a recurring task in the industry. DevOps is a set of practices intended to reduce the friction between the software development (Dev) and the IT operations (Ops), resulting in higher quality software and a shorter development lifecycle. Even though many resources are talking about DevOps practices, they are often inconsistent with each other on the best DevOps practices. Furthermore, they lack the needed detail and structure for beginners to the DevOps field to quickly understand them. In order to tackle this issue, this paper proposes four foundational DevOps patterns: Version Control Everything, Continuous Integration, Deployment Automation, and Monitoring. The patterns are both detailed enough and structured to be easily reused by practitioners and flexible enough to accommodate different needs and quirks that might arise from their actual usage context. Furthermore, the patterns are tuned to the DevOps principle of Continuous Improvement by containing metrics so that practitioners can improve their pattern implementations.",CS,AI_ML,0.85,Extracted from log - paper 1902
Mitigating Configuration Differences Between Development and Production Environments: A Catalog of Strategies,"Context: The Configuration Management of the development and production environments is an important aspect of IT operations. However, managing the configuration differences between these two environments can be challenging, leading to inconsistent behavior, unexpected errors, and increased downtime. Objective: In this study, we sought to investigate the strategies software companies employ to mitigate the configuration differences between the development and production environments. Our goal is to provide a comprehensive understanding of these strategies used to contribute to reducing the risk of configuration-related issues. Method: To achieve this goal, we interviewed 17 participants and leveraged the Thematic Analysis methodology to analyze the interview data. These participants shed some light on the current practices, processes, challenges, or issues they have encountered. Results: Based on the interviews, we systematically formulated and structured a catalog of eight strategies that explain how software producing companies mitigate these configuration differences. These strategies vary from 1) creating detailed configuration management plans, 2) using automation tools, and 3) developing processes to test and validate changes through containers and virtualization technologies. Conclusion: By implementing these strategies, companies can improve their ability to respond quickly and effectively to changes in the production environment. In addition, they can also ensure compliance with industry standards and regulations.",CS,AI_ML,0.85,Extracted from log - paper 1903
Lorentzian-Constrained Holographic Beamforming Optimization in Multi-user Networks with Dynamic Metasurface Antennas,"Dynamic metasurface antennas (DMAs) are promising alternatives to fully digital (FD) architectures, enabling hybrid beamforming via low-cost reconfigurable metasurfaces. In DMAs, holographic beamforming is achieved through tunable elements by Lorentzian-constrained holography (LCH), significantly reducing the need for radio-frequency (RF) chains and analog circuitry. However, the Lorentzian constraints and limited RF chains introduce a trade-off between reduced system complexity and beamforming performance, especially in dense network scenarios. This paper addresses resource allocation in multi-user multiple-input-single-output (MISO) networks under the Signal-to-Interference-plus-Noise Ratio (SINR) constraints, aiming to minimize total transmit power. We propose a holographic beamforming algorithm based on the Generalized Method of Lorentzian-Constrained Holography (GMLCH), which optimizes DMA weights, yielding flexibility for using various LCH techniques to tackle the aforementioned trade-offs. Building upon GMLCH, we further propose a new algorithm, Adaptive Radius Lorentzian Constrained Holography (ARLCH), which achieves optimization of DMA weights with additional degree of freedom in a greater optimization space, and provides lower transmitted power, while improving scalability for higher number of users. Numerical results show that ARLCH reduces power consumption by over 20% compared to benchmarks, with increasing effectiveness as the number of users grows.",CS,AI_ML,0.85,Extracted from log - paper 1904
Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks,"Sharpness-Aware Minimization (SAM) improves neural network generalization by optimizing the worst-case loss within a neighborhood of parameters, yet it perturbs parameters using the entire gradient vector, including components with low statistical significance. We introduce ZSharp, a refined sharpness-aware optimization method that incorporates layer-wise Z-score normalization followed by percentile-based filtering. This process selects only the most statistically significant gradient components-those with large standardized magnitudes-for constructing the perturbation direction. ZSharp retains the standard two-phase SAM structure of ascent and descent while modifying the ascent step to focus on sharper, curvature-relevant directions. We evaluate ZSharp on CIFAR-10, CIFAR-100, and Tiny-ImageNet using a range of models including ResNet, VGG, and Vision Transformers. Across all architectures and datasets, ZSharp consistently achieves higher test accuracy compared to SAM, ASAM, and Friendly-SAM. These results indicate that Z-score-based gradient filtering can enhance the sharpness sensitivity of the update direction, leading to improved generalization in deep neural network training.",CS,AI_ML,0.85,Extracted from log - paper 1905
Fast Sign Retrieval via Sub-band Convolution: An Elementary Extension of Binary Classification,"To efficiently compress the sign information of images, we address a sign retrieval problem for the block-wise discrete cosine transformation (DCT): reconstruction of the signs of DCT coefficients from their amplitudes. To this end, we propose a fast sign retrieval method on the basis of binary classification machine learning. We first introduce 3D representations of the amplitudes and signs, where we pack amplitudes/signs belonging to the same frequency band into a 2D slice, referred to as the sub-band block. We then retrieve the signs from the 3D amplitudes via binary classification, where each sign is regarded as a binary label. We implement a binary classification algorithm using convolutional neural networks, which are advantageous for efficiently extracting features in the 3D amplitudes. Experimental results demonstrate that our method achieves accurate sign retrieval with an overwhelmingly low computation cost.",CS,AI_ML,0.85,Extracted from log - paper 1906
On the Secrecy-Sensing Optimization of RIS-assisted Full-Duplex Integrated Sensing and Communication Network,"Integrated sensing and communication (ISAC) has recently emerged as a viable technique for establishing sensing and communication using the same resources. Nonetheless, the operation of ISAC networks is often challenged by the absence of a direct link between the sensing node and the targets, and by the risk of disclosing confidential data to malicious targets when using the same signal for both tasks. In this paper, a robust reconfigurable intelligent surface (RIS)-aided scheme for securing a full-duplex (FD) ISAC network is proposed. The considered network consists of uplink and downlink users served in FD through a multi-antenna dual-functional radar communication base station (BS), which employs co-located multi-antenna communication-radar arrays to detect multiple malicious targets while preserving communication secrecy in their presence. Additionally, the BS utilizes an optimized artificial noise (AN) that serves to disrupt the malicious targets' reception and increase the sensing power. By optimally designing the RIS phase shifts, transmit beamforming, AN covariance, and uplink users' transmit power and combining vectors using an alternating optimization-based algorithm, the network's sensing performance is maximized under secrecy and total power constraints. Numerical results present the proposed scheme's efficacy, particularly when a direct link between the BS and the various nodes/targets is absent.",CS,AI_ML,0.85,Extracted from log - paper 1907
Active Reconfigurable Intelligent Surface Assisted MIMO: Electromagnetic-Compliant Modeling with Mutual Coupling,"Reconfigurable Intelligent Surfaces (RIS) represent a transformative technology for sixth-generation (6G) wireless communications, but it suffers from a significant limitation, namely the double-fading attenuation. Active RIS has emerged as a promising solution, effectively mitigating the attenuation issues associated with conventional RIS-assisted systems. However, the current academic work on active RIS focuses on the system-level optimization of active RIS, often overlooking the development of models that are compatible with its electromagnetic (EM) and physical properties. The challenge of constructing realistic, EM-compliant models for active RIS-assisted communication, as well as understanding their implications on system-level optimization, remains an open research area. To tackle these problems, in this paper we develop a novel EM-compliant model with mutual coupling (MC) for active RIS-assisted wireless systems by integrating the developed scattering-parameter ($S$-parameter) based active RIS framework with multiport network theory, which facilitates system-level analysis and optimization. To evaluate the performance of the EM-compliant active RIS model, we design the joint optimization scheme based on the transmit beamforming at the transmitter and the reflection coefficient at the active RIS to maximize the achievable rate of EM-compliant active RIS-assisted MIMO system. To tackle the inherent non-convexity of this problem, we employ the Sherman-Morrison inversion and Neumann series (SMaN)-based alternating optimization (AO) algorithm. Simulation results verified that EM property (i.e., MC effect) is an indispensable factor in the optimization process of MIMO systems. Neglecting this effect introduces a substantial performance gap, highlighting its significance in the more pronounced the MC effect is, the greater the gap in achievable rates.",CS,AI_ML,0.85,Extracted from log - paper 1908
A Tutorial on Six-Dimensional Movable Antenna for 6G Networks: Synergizing Positionable and Rotatable Antennas,"Six-dimensional movable antenna (6DMA) is a new and revolutionary technique that fully exploits the wireless channel spatial variations at the transmitter/receiver by flexibly adjusting the three-dimensional (3D) positions and/or 3D rotations of antennas/antenna surfaces (sub-arrays), thereby improving the performance of wireless networks cost-effectively without the need to deploy additional antennas. It is thus expected that the integration of new 6DMAs into future sixth-generation (6G) wireless networks will fundamentally enhance antenna agility and adaptability, and introduce new degrees of freedom (DoFs) for system design. Despite its great potential, 6DMA faces new challenges to be efficiently implemented in wireless networks, including corresponding architectures, antenna position and rotation optimization, channel estimation, and system design from both communication and sensing perspectives. In this paper, we provide a tutorial on 6DMA-enhanced wireless networks to address the above issues by unveiling associated new channel models, hardware implementations and practical position/rotation constraints, as well as various appealing applications in wireless networks. Moreover, we discuss two special cases of 6DMA, namely, rotatable 6DMA with fixed antenna position and positionable 6DMA with fixed antenna rotation, and highlight their respective design challenges and applications. We further present prototypes developed for 6DMA-enhanced communication along with experimental results obtained with these prototypes. Finally, we outline promising directions for further investigation.",CS,AI_ML,0.85,Extracted from log - paper 1909
Semantic-aided Parallel Image Transmission Compatible with Practical System,"In this paper, we propose a novel semantic-aided image communication framework for supporting the compatibility with practical separation-based coding architectures. Particularly, the deep learning (DL)-based joint source-channel coding (JSCC) is integrated into the classical separate source-channel coding (SSCC) to transmit the images via the combination of semantic stream and image stream from DL networks and SSCC respectively, which we name as parallel-stream transmission. The positive coding gain stems from the sophisticated design of the JSCC encoder, which leverages the residual information neglected by the SSCC to enhance the learnable image features. Furthermore, a conditional rate adaptation mechanism is introduced to adjust the transmission rate of semantic stream according to residual, rendering the framework more flexible and efficient to bandwidth allocation. We also design a dynamic stream aggregation strategy at the receiver, which provides the composite framework with more robustness to signal-to-noise ratio (SNR) fluctuations in wireless systems compared to a single conventional codec. Finally, the proposed framework is verified to surpass the performance of both traditional and DL-based competitors in a large range of scenarios and meanwhile, maintains lightweight in terms of the transmission and computational complexity of semantic stream, which exhibits the potential to be applied in real systems.",CS,AI_ML,0.85,Extracted from log - paper 1910
A Reference Architecture for Autonomous Networks: An Agent-Based Approach,"The vision of autonomous systems is becoming increasingly important in many application areas, where the aim is to replace humans with agents. These include autonomous vehicles and other agents' applications in business processes and problem-solving. For networks, the increasing scale and operation and management (O&M) complexity drive the need for autonomous networks (AN). The technical objective of AN is to ensure trustworthy O&M without human intervention for higher efficiency and lower operating costs. However, realizing AN seems more difficult than autonomous vehicles. It encounters challenges of networks' structural and functional complexity, which operate as distributed dynamic systems governed by various technical and economic constraints. A key problem lies in formulating a rigorous development methodology that facilitates a seamless transition from traditional networks to AN. Central to this methodology is the definition of a reference architecture for network agents, which specifies the required functionalities for their realization, regardless of implementation choices. This article proposes a reference architecture characterizing main functional features, illustrating its application with network use cases. It shows how artificial intelligence components can be used to implement the required functionality and its coordination. The latter is achieved through the management and generation of shared domain-specific knowledge stored in long-term memory, ensuring the overall consistency of decisions and their execution. The article concludes with a discussion of architecture specialization for building network layer agents. It also identifies the main technical challenges ahead, such as satisfying essential requirements at development or runtime, as well as the issue of coordinating agents to achieve collective intelligence in meeting overall network goals.",CS,AI_ML,0.85,Extracted from log - paper 1911
Holistic Network Virtualization and Pervasive Network Intelligence for 6G,"In this tutorial paper, we look into the evolution and prospect of network architecture and propose a novel conceptual architecture for the 6th generation (6G) networks. The proposed architecture has two key elements, i.e., holistic network virtualization and pervasive artificial intelligence (AI). The holistic network virtualization consists of network slicing and digital twin, from the aspects of service provision and service demand, respectively, to incorporate service-centric and user-centric networking. The pervasive network intelligence integrates AI into future networks from the perspectives of networking for AI and AI for networking, respectively. Building on holistic network virtualization and pervasive network intelligence, the proposed architecture can facilitate three types of interplay, i.e., the interplay between digital twin and network slicing paradigms, between model-driven and data-driven methods for network management, and between virtualization and AI, to maximize the flexibility, scalability, adaptivity, and intelligence for 6G networks. We also identify challenges and open issues related to the proposed architecture. By providing our vision, we aim to inspire further discussions and developments on the potential architecture of 6G.",CS,AI_ML,0.85,Extracted from log - paper 1912
"Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities","With the increasing demand for seamless connectivity and intelligent communication, the integration of artificial intelligence (AI) and communication for sixth-generation (6G) network is emerging as a revolutionary architecture. This paper presents a comprehensive overview of AI and communication for 6G networks, emphasizing their foundational principles, inherent challenges, and future research opportunities. We commence with a retrospective analysis of AI and the evolution of large-scale AI models, underscoring their pivotal roles in shaping contemporary communication technologies. The discourse then transitions to a detailed exposition of the envisioned integration of AI within 6G networks, delineated across three progressive developmental stages. The initial stage, AI for Network, focuses on employing AI to augment network performance, optimize efficiency, and enhance user service experiences. The subsequent stage, Network for AI, highlights the role of the network in facilitating and buttressing AI operations and presents key enabling technologies, including digital twins for AI and semantic communication. In the final stage, AI as a Service, it is anticipated that future 6G networks will innately provide AI functions as services and support application scenarios like immersive communication and intelligent industrial robots. Specifically, we have defined the quality of AI service, which refers to the measurement framework system of AI services within the network. In addition to these developmental stages, we thoroughly examine the standardization processes pertinent to AI in network contexts, highlighting key milestones and ongoing efforts. Finally, we outline promising future research opportunities that could drive the evolution and refinement of AI and communication for 6G, positioning them as a cornerstone of next-generation communication infrastructure.",CS,AI_ML,0.85,Extracted from log - paper 1913
"6G Survey on Challenges, Requirements, Applications, Key Enabling Technologies, Use Cases, AI integration issues and Security aspects","Fifth-generation (5G) wireless networks will likely offer high data rates, increased reliability, and low delay for mobile, personal, and local area networks. Along with the rapid growth of smart wireless sensing and communication technologies, data traffic has increased significantly and existing 5G networks are not able to fully support future massive data traffic for services, storage, and processing. To meet the challenges ahead, research communities and industry are exploring the sixth generation (6G) Terahertz-based wireless network that is expected to be offered to industrial users in just ten years. Gaining knowledge and understanding of the different challenges and facets of 6G is crucial in meeting the requirements of future communication and addressing evolving quality of service (QoS) demands. This survey provides a comprehensive examination of specifications, requirements, applications, and enabling technologies related to 6G. It covers disruptive and innovative, integration of 6G with advanced architectures and networks such as software-defined networks (SDN), network functions virtualization (NFV), Cloud/Fog computing, and Artificial Intelligence (AI) oriented technologies. The survey also addresses privacy and security concerns and provides potential futuristic use cases such as virtual reality, smart healthcare, and Industry 5.0. Furthermore, it identifies the current challenges and outlines future research directions to facilitate the deployment of 6G networks.",CS,AI_ML,0.85,Extracted from log - paper 1914
"6G: The Intelligent Network of Everything -- A Comprehensive Vision, Survey, and Tutorial","The global 6G vision has taken its shape after years of international research and development efforts. This work culminated in ITU-R's Recommendation on ""IMT-2030 Framework"". While the definition phase of technological requirements is currently ongoing, 3GPP's standardization process on 6G networks is expected to start in 2025 and worldwide commercialization around 2030. This article serves as a comprehensive guide to 6G by providing an overall vision, a contemporary survey of the main literature, and an informative tutorial-type presentation style. In our vision, 6G will be based on three fundamental elements: wireless, artificial intelligence (AI), and the Internet of Everything (IoE). Consequently, 6G can ultimately become the Intelligent Network of Everything while serving as an enabling platform for the next major disruption in mobile communication, called mobile intelligence. The potential of mobile intelligence is that anything can be made connected, intelligent, and aware of its environment. This will revolutionize the way how devices, systems, and applications are designed; how they operate and interact with humans and each other; and how they can be used for the benefit of people, society, and the world in general. After high-level visioning, the main details of 6G are discussed, including fundamental elements, disruptive applications, key use cases, main performance requirements, potential technologies, and defining features. A special focus is given to a comprehensive set of potential 6G technologies, each of which is introduced in a tutorial manner. Finally, we speculate on what comes after 6G and sketch the first high-level vision of 7G. All in all, the objective of this article is to provide a thorough guide to 6G in order to serve as a source of knowledge and inspiration for further research and development work in academia, industry, and standardization bodies.",CS,AI_ML,0.85,Extracted from log - paper 1915
O-RAN and 6G: The Future of Wireless Innovation?,"The emergence of 6G technology represents a significant advancement in wireless communications, providing unprecedented speed, extremely low latency, and pioneering applications. In light of this development, an important question arises: Can the Open Radio Access Network (O-RAN), with its emphasis on openness, flexibility, RAN slicing, RAN Intelligent Controller (RIC), and cost-effectiveness, fulfill the complex requirements of 6G? This paper delves into the potential synergy between O-RAN and 6G, illustrating how O-RAN can facilitate customization, reduce expenses, and stimulate innovation in next-generation networks. We also tackle the challenges associated with 6G, such as the need for exceptional performance, integration with non-terrestrial networks, and heightened security. By examining the interaction between O-RAN and 6G, we underscore their joint role in shaping the future of wireless communication. Lastly, we demonstrate the potential of O-RAN through a unique, learning-based spectrum-sharing solution that aligns with the objectives of 6G for efficient spectrum usage.",CS,AI_ML,0.85,Extracted from log - paper 1916
6G: the Wireless Communications Network for Collaborative and AI Applications,"At the dawn of 5G, we take a leap forward and present an original vision of wireless communication beyond its horizon towards 6G: a paradigm-shifting perspective of wireless networks on the cusp of an AI revolution.",CS,AI_ML,0.85,Extracted from log - paper 1917
An In-Depth Survey on Virtualization Technologies in 6G Integrated Terrestrial and Non-Terrestrial Networks,"6G networks are envisioned to deliver a large diversity of applications and meet stringent quality of service (QoS) requirements. Hence, integrated terrestrial and non-terrestrial networks (TN-NTNs) are anticipated to be key enabling technologies. However, the TN-NTNs integration faces a number of challenges that could be addressed through network virtualization technologies such as Software-Defined Networking (SDN), Network Function Virtualization (NFV) and network slicing. In this survey, we provide a comprehensive review on the adaptation of these networking paradigms in 6G networks. We begin with a brief overview on NTNs and virtualization techniques. Then, we highlight the integral role of Artificial Intelligence in improving network virtualization by summarizing major research areas where AI models are applied. Building on this foundation, the survey identifies the main issues arising from the adaptation of SDN, NFV, and network slicing in integrated TN-NTNs, and proposes a taxonomy of integrated TN-NTNs virtualization offering a thorough review of relevant contributions. The taxonomy is built on a four-level classification indicating for each study the level of TN-NTNs integration, the used virtualization technology, the addressed problem, the type of the study and the proposed solution, which can be based on conventional or AI-enabled methods. Moreover, we present a summary on the simulation tools commonly used in the testing and validation of such networks. Finally, we discuss open issues and give insights on future research directions for the advancement of integrated TN-NTNs virtualization in the 6G era.",CS,AI_ML,0.85,Extracted from log - paper 1918
Towards Organic 6G Networks: Virtualization and Live Migration of Core Network Functions,"In the context of Industry 4.0, more and more mobile use cases are appearing on industrial factory floors. These use cases place high demands on various quantitative requirements, such as latency, availability, and more. In addition, qualitative requirements such as flexibility are arising. Since virtualization technology is a key enabler for the flexibility that is required by novel use cases and on the way to organic networking as it is addressed by 6G, we investigate container virtualization technology in this paper. We focus on container technology since OS-level virtualization has multiple benefits compared to hardware virtualization, such as VMs. Thus, we discuss several aspects of container based virtualization, e.g. selection of suitable network drivers and orchestration tools, with respect to most important 5GC functions. In addition, the functions have different quantitative or qualitative requirements depending on whether they are stateless or stateful, and whether the specific function is located at either the control or user plane. Therefore, we also analyze the aforementioned live migration concepts for the 5GC functions and evaluate them based on well-defined metrics, such as migration time and process downtime.",CS,AI_ML,0.85,Extracted from log - paper 1919
Secure Virtual Mobile Small Cells: A Stepping Stone Towards 6G,"As 5th Generation research reaches the twilight, the research community must go beyond 5G and look towards the 2030 connectivity landscape, namely 6G. In this context, this work takes a step towards the 6G vision by proposing a next generation communication platform, which aims to extend the rigid coverage area of fixed deployment networks by considering virtual mobile small cells (MSC) that are created on demand. Relying on emerging computing paradigms such as NFV (Network Function Virtualization) and SDN (Software Defined Networking), these cells can harness radio and networking capability locally reducing protocol signaling latency and overhead. These MSCs constitute an intelligent pool of networking resources that can collaborate to form a wireless network of MSCs providing a communication platform for localized, ubiquitous and reliable connectivity. The technology enablers for implementing the MSC concept are also addressed in terms of virtualization, lightweight wireless security, and energy efficient RF. The benefits of the MSC architecture towards reliable and efficient cell offloading are demonstrated as a use-case.",CS,AI_ML,0.85,Extracted from log - paper 1920
Towards Sustainability in 6G and beyond: Challenges and Opportunities of Open RAN,"The transition to 6G is expected to bring significant advancements, including much higher data rates, enhanced reliability and ultra-low latency compared to previous generations. Although 6G is anticipated to be 100 times more energy efficient, this increased efficiency does not necessarily mean reduced energy consumption or enhanced sustainability. Network sustainability encompasses a broader scope, integrating business viability, environmental sustainability, and social responsibility. This paper explores the sustainability requirements for 6G and proposes Open RAN as a key architectural solution. By enabling network diversification, fostering open and continuous innovation, and integrating AI/ML, Open RAN can promote sustainability in 6G. The paper identifies high energy consumption and e-waste generation as critical sustainability challenges and discusses how Open RAN can address these issues through softwarisation, edge computing, and AI integration.",CS,AI_ML,0.85,Extracted from log - paper 1921
A Survey of AIOps for Failure Management in the Era of Large Language Models,"As software systems grow increasingly intricate, Artificial Intelligence for IT Operations (AIOps) methods have been widely used in software system failure management to ensure the high availability and reliability of large-scale distributed software systems. However, these methods still face several challenges, such as lack of cross-platform generality and cross-task flexibility. Fortunately, recent advancements in large language models (LLMs) can significantly address these challenges, and many approaches have already been proposed to explore this field. However, there is currently no comprehensive survey that discusses the differences between LLM-based AIOps and traditional AIOps methods. Therefore, this paper presents a comprehensive survey of AIOps technology for failure management in the LLM era. It includes a detailed definition of AIOps tasks for failure management, the data sources for AIOps, and the LLM-based approaches adopted for AIOps. Additionally, this survey explores the AIOps subtasks, the specific LLM-based approaches suitable for different AIOps subtasks, and the challenges and future directions of the domain, aiming to further its development and application.",CS,AI_ML,0.85,Extracted from log - paper 1922
AIOps Solutions for Incident Management: Technical Guidelines and A Comprehensive Literature Review,"The management of modern IT systems poses unique challenges, necessitating scalability, reliability, and efficiency in handling extensive data streams. Traditional methods, reliant on manual tasks and rule-based approaches, prove inefficient for the substantial data volumes and alerts generated by IT systems. Artificial Intelligence for Operating Systems (AIOps) has emerged as a solution, leveraging advanced analytics like machine learning and big data to enhance incident management. AIOps detects and predicts incidents, identifies root causes, and automates healing actions, improving quality and reducing operational costs. However, despite its potential, the AIOps domain is still in its early stages, decentralized across multiple sectors, and lacking standardized conventions. Research and industrial contributions are distributed without consistent frameworks for data management, target problems, implementation details, requirements, and capabilities. This study proposes an AIOps terminology and taxonomy, establishing a structured incident management procedure and providing guidelines for constructing an AIOps framework. The research also categorizes contributions based on criteria such as incident management tasks, application areas, data sources, and technical approaches. The goal is to provide a comprehensive review of technical and research aspects in AIOps for incident management, aiming to structure knowledge, identify gaps, and establish a foundation for future developments in the field.",CS,AI_ML,0.85,Extracted from log - paper 1923
AIOps-Driven Enhancement of Log Anomaly Detection in Unsupervised Scenarios,"Artificial intelligence operations (AIOps) play a pivotal role in identifying, mitigating, and analyzing anomalous system behaviors and alerts. However, the research landscape in this field remains limited, leaving significant gaps unexplored. This study introduces a novel hybrid framework through an innovative algorithm that incorporates an unsupervised strategy. This strategy integrates Principal Component Analysis (PCA) and Artificial Neural Networks (ANNs) and uses a custom loss function to substantially enhance the effectiveness of log anomaly detection. The proposed approach encompasses the utilization of both simulated and real-world datasets, including logs from SockShop and Hadoop Distributed File System (HDFS). The experimental results are highly promising, demonstrating significant reductions in pseudo-positives. Moreover, this strategy offers notable advantages, such as the ability to process logs in their raw, unprocessed form, and the potential for further enhancements. The successful implementation of this approach showcases a remarkable reduction in anomalous logs, thus unequivocally establishing the efficacy of the proposed methodology. Ultimately, this study makes a substantial contribution to the advancement of log anomaly detection within AIOps platforms, addressing the critical need for effective and efficient log analysis in modern and complex systems.",CS,AI_ML,0.85,Extracted from log - paper 1924
Data Silos A Roadblock for AIOps,"Using artificial intelligence to manage IT operations, also known as AIOps, is a trend that has attracted a lot of interest and anticipation in recent years. The challenge in IT operations is to run steady-state operations without disruption as well as support agility"" can be rephrased as ""IT operations face the challenge of maintaining steady-state operations while also supporting agility [11]. AIOps assists in bridging the gap between the demand for IT operations and the ability of humans to meet that demand. However, it is not easy to apply AIOps in current organizational settings. Data Centralization is a major obstacle for adopting AIOps, according to a recent survey by Cisco [1]. The survey, which involved 8,161 senior business leaders from organizations with more than 500 employees, found that 81% of them acknowledged that their data was scattered across different silos within their organizations. This paper illustrates the topic of data silos, their causes, consequences, and solutions.",CS,AI_ML,0.85,Extracted from log - paper 1925
A Systematic Mapping Study in AIOps,"IT systems of today are becoming larger and more complex, rendering their human supervision more difficult. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to AI and Big Data. However, past AIOps contributions are scattered, unorganized and missing a common terminology convention, which renders their discovery and comparison impractical. In this work, we conduct an in-depth mapping study to collect and organize the numerous scattered contributions to AIOps in a unique reference index. We create an AIOps taxonomy to build a foundation for future contributions and allow an efficient comparison of AIOps papers treating similar problems. We investigate temporal trends and classify AIOps contributions based on the choice of algorithms, data sources and the target components. Our results show a recent and growing interest towards AIOps, specifically to those contributions treating failure-related tasks (62%), such as anomaly detection and root cause analysis.",CS,AI_ML,0.85,Extracted from log - paper 1926
MapReduce: simplified data processing on large clusters,"MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.",CS,AI_ML,0.85,Extracted from log - paper 1927
Dynamo: amazon's highly available key-value store,"Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems. This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core services use to provide an ""always-on"" experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.",CS,AI_ML,0.85,Extracted from log - paper 1928
Incremental View Maintenance for Property Graph Queries,"Graph processing challenges are common in modern database systems, with the property graph data model gaining widespread adoption. Due to the novelty of the field, graph databases and frameworks typically provide their own query language, such as Cypher for Neo4j, Gremlin for TinkerPop and GraphScript for SAP HANA. These languages often lack a formal background for their data model and semantics. To address this, the openCypher initiative aims to standardise a subset of the Cypher language, for which it currently provides grammar specification and a set of acceptance tests to allow vendors to implement their openCypher compatible engine.",CS,AI_ML,0.85,Extracted from log - paper 1929
Nimrod/G: An Architecture of a Resource Management and Scheduling System in a Global Computational Grid,"The availability of powerful microprocessors and high-speed networks as commodity components has enabled high performance computing on distributed systems (wide-area cluster computing). In this environment, as the resources are usually distributed geographically at various levels (department, enterprise, or worldwide) there is a great challenge in integrating, coordinating and presenting them as a single resource to the user; thus forming a computational grid. Another challenge comes from the distributed ownership of resources with each resource having its own access policy, cost, and mechanism. The proposed Nimrod/G grid-enabled resource management and scheduling system builds on our earlier work on Nimrod and follows a modular and component-based architecture enabling extensibility, portability, ease of development, and interoperability of independently developed components. It uses the Globus toolkit services and can be easily extended to operate with any other emerging grid middleware services. It focuses on the management and scheduling of computations over dynamic resources scattered geographically across the Internet at department, enterprise, or global level with particular emphasis on developing scheduling schemes based on the concept of computational economy for a real test bed, namely, the Globus testbed (GUSTO).",CS,AI_ML,0.85,Extracted from log - paper 1930
Replica Selection in the Globus Data Grid,"The Globus Data Grid architecture provides a scalable infrastructure for the management of storage resources and data that are distributed across Grid environments. These services are designed to support a variety of scientific applications, ranging from high-energy physics to computational genomics, that require access to large amounts of data (terabytes or even petabytes) with varied quality of service requirements. By layering on a set of core services, such as data transport, security, and replica cataloging, one can construct various higher-level services. In this paper, we discuss the design and implementation of a high-level replica selection service that uses information regarding replica location and user preferences to guide selection from among storage replica alternatives. We first present a basic replica selection service design, then show how dynamic information collected using Globus information service capabilities concerning storage system properties can help improve and optimize the selection process. We demonstrate the use of Condor's ClassAds resource description and matchmaking mechanism as an efficient tool for representing and matching storage resource capabilities and policies against application requirements.",CS,AI_ML,0.85,Extracted from log - paper 1931
Economic Models for Management of Resources in Grid Computing,"The accelerated development in Grid and peer-to-peer computing has positioned them as promising next generation computing platforms. They enable the creation of Virtual Enterprises (VE) for sharing resources distributed across the world. However, resource management, application development and usage models in these environments is a complex undertaking. This is due to the geographic distribution of resources that are owned by different organizations. The resource owners of each of these resources have different usage or access policies and cost models, and varying loads and availability. In order to address complex resource management issues, we have proposed a computational economy framework for resource allocation and for regulating supply and demand in Grid computing environments. The framework provides mechanisms for optimizing resource provider and consumer objective functions through trading and brokering services. In a real world market, there exist various economic models for setting the price for goods based on supply-and-demand and their value to the user. They include commodity market, posted price, tenders and auctions. In this paper, we discuss the use of these models for interaction between Grid components in deciding resource value and the necessary infrastructure to realize them. In addition to normal services offered by Grid computing systems, we need an infrastructure to support interaction protocols, allocation mechanisms, currency, secure banking, and enforcement services. Furthermore, we demonstrate the usage of some of these economic models in resource brokering through Nimrod/G deadline and cost-based scheduling for two different optimization strategies on the World Wide Grid (WWG) testbed.",CS,AI_ML,0.85,Extracted from log - paper 1932
Dynamic Management of Virtual Machine and Container Scheduling in Multi-Data Center Cloud Environments,"Efficiently managing virtual resources is a critical component of server virtualization technology. The scheduler is crucial in strategically distributing Virtual Machines (VMs) and containers across diverse computing nodes, responsible for the allocation and the placement of VMs and containers on different computing nodes, and the migration of deployed ones between different nodes. In this thesis, we propose novel solutions in scheduling virtual resources, particularly in the management of VMs and containers deployed across multi-data center cloud environments. The proposed solutions leverage mathematical models, machine learning techniques, and blockchain technology to optimize scheduling decisions, enhance server consolidation, minimize energy consumption, and secure container scheduling. We introduce mathematical models for live VM migration techniques used in simulating and studying live VM migration in cloud systems environments. We present a novel distributed scheduling model that leverages blockchain technology to facilitate efficient sharing of VM status across multiple data centers. This enables prompt Local Area Network (LAN) or Wide Area Network (WAN) scheduling decisions for VMs. Additionally, we employ machine and deep learning techniques in a VM migration prediction service to identify the most suitable live migration method for each VM based on its unique characteristics. Our blockchain-based model reduces the total messages exchanged for the VM migration with percentages ranging from 0.5% to 22% and the total communication delay by 8% to 72% compared to a REST-based distributed model. The proposed blockchain-based distributed model also reduces the number of communication messages by 41.79% to 49.85% and total delay by 2% to 12% compared to a VPN-based centralized model. The Service Level Agreement (SLA) compliance rate of the proposed VM migration prediction service ranges from 18% to 94.9% for different machine learning algorithms and SLA policies. The proposed solution reduces the total migration time by 14% to 79% and the downtime by 64% to 99%. Furthermore, we present a novel two-stage container scheduling solution that addresses node imbalances and efficiently deploys containers as an optimization problem, integrating various objective functions and constraints to enhance server consolidation and minimize energy consumption. The confidentiality of migrated containers is ensured through encryption, and the associated costs of the proposed attributes-based encryption model are incorporated into the optimization constraints. The proposed solution's efficacy is demonstrated in its ability to efficiently deploy containers in multi-data center cloud environments and seamlessly migrate them between hosts within the same data center or across different data centers. The results show optimal consolidation with a reduction in the number of running hosts, ranging from 4% to over 18%. Additionally, the solution promotes minimal total power consumption with savings ranging from 3.5 to 16.25 megawatts, while also ensuring balanced server loads, highlighting the effectiveness of the proposed container scheduling approach.",CS,AI_ML,0.85,Extracted from log - paper 1933
A systematic review on recent methods of scheduling and load balancing in containerized cloud environments,"Containers running microservices must handle many requests, tolerate network failures, and ensure scalability, availability, and high performance. Scheduling and load balancing for containers are two closely related activities. In distributed environments, various methods are used to solve the problems of container scheduling and load balancing. These methods encompass heuristic approaches, including traditional methods, bioinspired optimizations, and learning-based approaches. Learning-based approaches leverage artificial intelligence (AI) and related technologies to comprehend runtime situations and make well-informed decisions during the scheduling process. Additionally, load balancing facilitates resource optimization, leading to improved infrastructure efficiency. Scheduling and load balancing can encompass larger contexts in distributed computing environments. The paper evaluates host provisioning in Open Stack and the impact of cloud computing. Kubernetes outperforms Docker Swarm, and even though Kubernetes installation is complex, it is worth the effort. The proposed approach improves resource allocation, focusing on placement strategies for all three entities. The approach, employing integer linear programming, offers cost savings and easy integration into container orchestration frameworks-a novel algorithm that addresses various costs, contributing to containerized application scheduling in the cloud.",CS,AI_ML,0.85,Extracted from log - paper 1934
Energy efficient virtual machines placement in cloud datacenters using genetic algorithm and adaptive thresholds,"Cloud computing platform provides on-demand IT services to users and advanced the technology. The purpose of virtualization is to improve the utilization of resources and reduce power consumption. Energy consumption is a major issue faced by data centers management. Virtual machine placement is an effective technique used for this purpose. Different algorithms have been proposed for virtual machine placement in cloud environments. These algorithms have considered different parameters. It is obvious that improving one parameter affects other parameters. There is still a need to reduce energy consumption in cloud data centers. Data centers need solutions that reduce energy consumption without affecting other parameters. There is a need to device solutions to effectively utilize cloud resources and reduce energy consumption. In this article, we present an algorithm for Virtual Machines (VMs) placement in cloud computing. The algorithm uses adaptive thresholding to identify over utilized and underutilized hosts to reduce energy consumption and Service Level Agreement (SLA) violations. The algorithm is validated with simulations and comparative results are presented. In cloud data centers, energy consumption can be reduced by increasing the number of VMs and decreasing physical machines. Activation of the active and idle modes of VMs also decreases energy consumption and enhances resource utilization. Sorting of tasks according to the schedule of processing needs to avoid overload or underload hosts in data centers and reduce energy consumption. Mapping of the task from one VM to other VMs and the assignment of the physical machine to VMs also consumes energy and needs to be addressed. In this article, we present a novel algorithm for VM placement in cloud computing environments. The algorithm is based on Genetic Algorithm (GA) with adaptive threshold. Adaptive thresholds are used to detect over-utilized and underutilized hosts in cloud data centers. Setting static upper and lower thresholds may lead to imbalance load and thus cause more energy consumption and low QoS. The proposed algorithm uses an intermediate value as the average of the upper and lower thresholds to identify hosts with utilization near the lower threshold. The resources of these hosts are allocated to VMs for better resource utilization. Furthermore, GA is used to optimize the allocation process. The objectives are to reduce energy consumption with low SLA violations for better QoS. The algorithm is validated by experiments with different configurations and comparative results are presented.",CS,AI_ML,0.85,Extracted from log - paper 1935
Xen and the Art of Virtualization,"Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service. This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe, resource-managed fashion without sacrificing performance or functionality.",CS,AI_ML,0.85,Extracted from log - paper 1936
Live Migration of Virtual Machines,"Migrating operating system instances across distinct physical hosts is a useful tool for administrators of data centers and clusters: it allows a clean separation between hardware and software, and facilitates fault management, load balancing, and low-level system maintenance. By carrying out the majority of migration while OSes continue to run, we achieve impressive performance with minimal service downtimes.",CS,AI_ML,0.85,Extracted from log - paper 1937
OpenFlow: Enabling Innovation in Campus Networks,This whitepaper proposes OpenFlow: a way for researchers to run experimental protocols in the networks they use every day. OpenFlow is based on an Ethernet switch with an internal flow-table and a standardized interface to add and remove flow entries.,CS,AI_ML,0.85,Extracted from log - paper 1938
Bigtable: A Distributed Storage System for Structured Data,"Bigtable is a distributed storage system for managing structured data that is designed to scale to very large size – petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance.",CS,AI_ML,0.85,Extracted from log - paper 1939
My VM is Lighter (and Safer) Than Your Container,"Containers are in great demand because they are lightweight compared to virtual machines. On the downside, containers offer weaker isolation than VMs, to the point that people often run containers inside VMs to achieve proper isolation. In this paper, we ask whether there is truly a strict trade-off between isolation (VMs) and efficiency (containers).",CS,AI_ML,0.85,Extracted from log - paper 1940
DevOps: A Systematic Literature Review,The DevOps approach is a connection of development and operation teams in software development to supply IT solutions faster to the market. This systematic literature review selected 58 out of 842 publications.,CS,AI_ML,0.85,Extracted from log - paper 1941
Gray Failure: The Achilles' Heel of Cloud-Scale Systems,"Cloud scale provides vast resources to replace failed components, but only if those failures can be detected. Major availability breakdowns in cloud environments tend to be caused by subtle underlying faults – so-called gray failures – rather than obvious fail-stop failures.",CS,AI_ML,0.85,Extracted from log - paper 1942
Onix: A Distributed Control Platform for Large-Scale Networks,"Computer networks lack a general control paradigm, as traditional networks provide no network-wide management abstractions. As a result, each new function (such as routing) must implement its own state distribution, discovery, and failover mechanisms. We believe this lack of a common control platform has hindered development of flexible, reliable, feature-rich control planes.",CS,AI_ML,0.85,Extracted from log - paper 1943
"“Hey, You, Get Off of My Cloud!”: Exploring Information Leakage in Third-Party Clouds","Third-party cloud computing offers the promise of outsourcing computation. Services like Microsoft Azure and Amazon EC2 allow users to instantiate virtual machines (VMs) on demand and purchase precisely the capacity needed when required. In turn, virtualization allows cloud providers to maximize utilization by multiplexing many customer VMs across shared physical infrastructure. However, in this paper, we show that this approach can introduce new vulnerabilities. Using Amazon EC2 as a case study, we demonstrate that it is possible to map the internal cloud infrastructure, identify where a particular target VM is likely located, and then instantiate new VMs until one is co-resident with the target. We then explore how such placement can be used to mount cross-VM side-channel attacks to extract information from a target VM on the same physical server.",CS,AI_ML,0.85,Extracted from log - paper 1944
"Ceph: A Scalable, High-Performance Distributed File System","We have developed Ceph, a distributed file system that provides excellent performance, reliability, and scalability. Ceph maximizes separation of data and metadata management by replacing allocation tables with a pseudo-random data distribution function (CRUSH), designed for heterogeneous and dynamic clusters of unreliable object storage devices (OSDs). We leverage device intelligence by distributing data replication, failure detection, and recovery to semi-autonomous OSDs. A dynamic distributed metadata cluster provides extremely efficient metadata management and adapts seamlessly to a wide range of workloads. Performance measurements under varied workloads show that Ceph delivers excellent I/O performance and scalable metadata management, supporting more than 250,000 metadata operations per second.",CS,AI_ML,0.85,Extracted from log - paper 1945
Remus: High Availability via Asynchronous VM Replication,"Allowing applications to survive hardware failure is an expensive undertaking, generally involving complex recovery software or redundant fail-over hardware. This paper presents Remus, a virtualization-based system that provides high availability by asynchronously replicating the entire state of a running virtual machine to a backup. Remus significantly reduces HA cost by isolating an OS and its applications from hardware failures transparently. It runs a primary VM with a backup VM such that if the primary fails, the backup seamlessly takes over. The system asynchronously propagates state changes from primary to backup at high frequency, buffering network output until state is safely replicated to the backup. Our approach provides OS- and application-agnostic high availability on commodity hardware, incurring only modest performance overhead for many workloads.",CS,AI_ML,0.85,Extracted from log - paper 1946
Pivot Tracing: Dynamic Causal Monitoring for Distributed Systems,"Monitoring and troubleshooting large distributed systems is notoriously difficult – problems are complex, varied, and unpredictable. The tools used today (logs, counters, metrics) have two key limitations: what gets recorded is defined a priori, and data is recorded in a component-centric way, making it hard to correlate cross-component events. This paper presents Pivot Tracing, a monitoring framework that addresses both limitations by combining dynamic instrumentation with a novel relational operator called the happened-before join. Pivot Tracing lets users, at runtime, define arbitrary metrics at one point in the system, while filtering and grouping events meaningful at other parts, even across component boundaries. We implemented a Pivot Tracing prototype for Java and evaluated it on a heterogeneous Hadoop cluster (HDFS, HBase, MapReduce, YARN). We show that Pivot Tracing can effectively identify diverse root causes such as software bugs, misconfiguration, and “limping” hardware, enabling dynamic cross-tier analysis with low overhead.",CS,AI_ML,0.85,Extracted from log - paper 1947
Power Provisioning for a Warehouse-Sized Computer,"Large-scale Internet services require a computing infrastructure that can be described as a warehouse-sized computing system. The cost of building datacenter facilities to deliver a given power capacity can rival the ongoing energy costs. Thus, there are strong economic incentives to operate facilities close to maximum capacity so that non-recurring facility costs are well amortized. In practice this is difficult due to uncertainties in equipment power ratings and highly variable power usage under real workloads. Effective power provisioning strategies are needed to determine how much equipment can be safely and efficiently hosted within a given power budget. In this paper we present aggregate power usage characteristics of large server populations (up to 15,000 servers) for different application classes over ~6 months. These observations let us evaluate opportunities to maximize utilization of deployed power capacity, and assess risks of over-subscribing it. We find even well-tuned applications have a noticeable gap (7–16%) between achieved and theoretical peak cluster power usage – growing to ~40% at whole-datacenter scale. This “headroom” can be used to deploy extra equipment within the same power budget with minimal risk. We use our model to estimate potential power management savings, finding significant opportunities (greater at cluster-level than rack-level). Finally, we argue systems must be power-efficient across the activity range, not just at peak load.",CS,AI_ML,0.85,Extracted from log - paper 1948
"Cloud Computing: Vision, Hype, and Reality for the 5th Utility (2009)","With significant advances in ICT over the last half-century, there is a growing vision that computing will one day be the “5th utility” (after water, electricity, gas, and telephony). This computing utility, like the other four, would provide a basic level of service considered essential to meet everyday needs of the general community. To deliver this vision, numerous computing paradigms have been proposed – the latest being Cloud Computing. Hence, in this paper, we define cloud computing and provide an architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as virtual machines. We also provide insights on market-based resource management strategies that encompass customer-driven service management and computational risk management to sustain SLA-oriented resource allocation. In addition, we offer early thoughts on interconnecting Clouds for dynamically creating global cloud exchanges and markets. We then present representative Cloud platforms (especially industrial offerings), along with our current work toward realizing market-oriented Cloud resource allocation in the Aneka enterprise Cloud technology. Furthermore, we highlight the differences between High-Performance Computing (HPC) workloads and Internet-based service workloads, describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing “Storage Clouds” for high-performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st-century vision.",CS,AI_ML,0.85,Extracted from log - paper 1949
The Vision of Autonomic Computing (2003),"A 2001 IBM manifesto observed that a looming software complexity crisis – caused by applications and environments comprising tens of millions of lines of code – threatened to halt computing progress. The manifesto noted the almost impossible difficulty of managing current and planned systems, which require integrating heterogeneous environments into corporate-wide systems extending into the Internet. Autonomic computing, perhaps the most attractive approach to this problem, envisions systems that can manage themselves given high-level objectives from administrators. Systems will self-manage according to administrator goals: new components integrate effortlessly, like a new cell in a body. These ideas are not science fiction, but elements of the grand challenge to create self-managing computing systems.",CS,AI_ML,0.85,Extracted from log - paper 1950
Ethane: Taking Control of the Enterprise (2007),"This paper presents Ethane, a new network architecture for the enterprise. Ethane allows managers to define a single network-wide, fine-grained policy, which is then enforced directly. Ethane couples extremely simple flow-based Ethernet switches with a centralized controller that manages flow admission and routing. While radical, the design is backwards-compatible with existing hosts and switches. We implemented Ethane in both hardware and software, supporting wired and wireless hosts. Our operational Ethane network has supported 300+ hosts for four months in Stanford’s Computer Science network, and this deployment experience significantly affected Ethane’s design.",CS,AI_ML,0.85,Extracted from log - paper 1951
Chubby: The Lock Service for Loosely-Coupled Systems (2006),"We describe our experiences with the Chubby lock service, which provides coarse-grained locking and reliable (low-volume) storage for a loosely-coupled distributed system. Chubby’s interface is like a distributed file system with advisory locks, but the design emphasis is on availability and reliability rather than high performance. Many Chubby instances have been used for over a year, with several each handling tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modified to accommodate the differences. (For example, Chubby was intended to help with coarse-grained leader election and meta-data storage; systems like GFS and Bigtable use Chubby for master election and as a well-known root for their distributed data structures.) Our experience shows Chubby greatly improved availability in systems that previously required human intervention on faults.",CS,AI_ML,0.85,Extracted from log - paper 1952
ZooKeeper: Wait-free Coordination for Internet-Scale Systems (2010),"In this paper, we describe ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper is part of critical infrastructure, it aims to provide a simple and high-performance kernel for building more complex coordination primitives at the client. It incorporates elements from group messaging, shared registers, and distributed lock services in a replicated, centralized service. The interface exposed by ZooKeeper has the wait-free properties of shared registers, combined with an event-driven mechanism (analogous to cache invalidations in file systems) to provide a simple yet powerful coordination service. The ZooKeeper interface enables a high-performance service implementation. In addition to the wait-free property, ZooKeeper provides each client with FIFO ordering of its requests and linearizability for all updates to the ZooKeeper state. These design decisions enable a high-throughput pipeline where read requests are often served by local replicas. We show that for typical workloads (read:write ratios of 2:1 up to 100:1), ZooKeeper can handle tens to hundreds of thousands of transactions per second, allowing it to be used extensively by client applications.",CS,AI_ML,0.85,Extracted from log - paper 1953
Dremel: Interactive Analysis of Web-Scale Datasets (2010),"Dremel is a scalable, interactive ad hoc query system for analysis of read-only nested data. By combining multilevel execution trees and a columnar data layout, it can run aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe Dremel’s architecture and implementation, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on multi-thousand-node instances of the system.",CS,AI_ML,0.85,Extracted from log - paper 1954
Failure Trends in a Large Disk Drive Population (2007),"Over 90% of new information produced globally is stored on magnetic media, most of it on hard disk drives. Despite their importance, relatively little published work exists on disk drive failure patterns and factors affecting drive lifetime. Most available data come either from small accelerated-aging tests or modest field studies; large-population studies seldom collect health signals from in-service drives, which are critical for detailed failure analysis. We present data from detailed observations of a very large disk drive population in a production Internet services deployment – an order of magnitude larger than previous studies. In addition to failure statistics, we analyze correlation between failures and several parameters commonly thought to impact drive longevity. Our analysis identifies several Self-Monitoring (SMART) parameters (e.g. scan errors, reallocation counts, offline reallocation counts, probational counts) that correlate highly with failures. Despite this high correlation, we conclude that models based on SMART parameters alone are unlikely to accurately predict individual drive failures. Surprisingly, we found that temperature and activity levels were much less correlated with drive failures than previously reported.",CS,AI_ML,0.85,Extracted from log - paper 1955
Cloud Computing: State-of-the-Art and Research Challenges (2010),"Cloud computing has recently emerged as a new paradigm for hosting and delivering services over the Internet. Cloud computing is attractive to business owners because it eliminates the need to plan ahead for provisioning and allows enterprises to start small and increase resources only when demand rises. However, despite the huge opportunities cloud computing offers the IT industry, the technology is still in its infancy with many issues to address. In this paper, we present a survey of cloud computing, highlighting key concepts, architectural principles, state-of-the-art implementations, and research challenges. The aim is to better understand cloud design challenges and identify important research directions in this increasingly important area.",CS,AI_ML,0.85,Extracted from log - paper 1956
Disaster Recovery in Cloud Computing Systems: An Overview,"Over the years, there has been a heavy reliance on cloud computing as IT has innovated through time. In recent times cloud computing has grown monumentally. Many organizations rely on this technology to perform their business as usual and use it as a backbone of their companies’ IT infrastructure. This paper investigates the organizational adaptation for cloud computing technology – reviewing case studies from various institutions and companies worldwide to provide a detailed analysis of innovative techniques with cloud computing. We investigate the features and delivery approaches cloud computing offers and the potential challenges and constraints we face when adopting cloud computing into the business setting. We also explore the cybersecurity elements associated with cloud computing, focusing on intrusion detection and prevention and understanding how that can be applied in the cloud. Finally, we investigate the future research directions for cloud computing and expand this paper into further articles with experiments and results.",CS,AI_ML,0.85,Extracted from log - paper 1957
Container Security in Cloud Environments: A Comprehensive Analysis and Future Directions for DevSecOps,"In recent years, the security of containers has become a crucial aspect of modern software applications’ security and integrity. Containers are extensively used due to their lightweight and portable nature, allowing swift and agile deployment across different environments. However, the increasing popularity of containers has led to unique security risks, including vulnerabilities in container images, misconfigured containers, and insecure runtime environments. Containers are often built using public repository images and base image vulnerability is inherited by containers. Container images may contain outdated components or services, including system libraries and dependencies and known vulnerabilities from these components can be exploited. Images downloaded from untrusted sources may include malicious code that compromises other containers running in the same network or the host system. Base images may include unnecessary software or services that increase the attack surface and potential vulnerabilities. Several security measures have been implemented to address these risks, such as container image scanning, container orchestration security, and runtime security monitoring. Implementing a solid security policy and updating containers with the latest patches can significantly improve container security. Given the increasing adoption of containers, organizations must prioritize container security to protect their applications and data. This work presents automated, robust security techniques for continuous integration and continuous development pipelines, and the added overhead is empirically analyzed. Then, we nail down specific research and technological problems the DevSecOps community encounters and appropriate initial fixes. Our results will make it possible to make judgments that are enforced when using DevSecOps techniques in enterprise security and cloud-native applications.",CS,AI_ML,0.85,Extracted from log - paper 1958
Infrastructure as Code: Technology Review and Research Challenges,"The quality of software management in infrastructure operations for application software is important as automation in software operations continues to grow. Infrastructure as Code (IaC) refers to a systematic, technology-supported approach to manage deployment infrastructure for software applications. Sample contexts are general software automation, but also cloud and edge and various software-defined networking applications. DevOps (development and operations) practices, which are already applied in the Infrastructure as Code (IaC) context, need to be extended to cover the whole IaC life cycle from code generation to dynamic, automated control. The ultimate objective would range from IaC generation to full self-adaptation of IaC code in an automated setting. We review available IaC technologies based on a comprehensive comparison framework to capture the state-of-the-art. We also introduce an IaC-specific DevOps process. This serves as a basis to identify open research challenges. A discussion of defect categories is at the centre of this process.",CS,AI_ML,0.85,Extracted from log - paper 1959
Autonomic Cloud Computing: Research Perspective,"As the cloud infrastructure grows, it becomes more challenging to manage resources in such a massive, diverse, and distributed setting, despite the fact that cloud computing provides computational capabilities on-demand. Due to resource variability and unpredictability, resource allocation issues arise in a cloud setting. A Quality of Service (QoS) based autonomic resource management strategy automates resource management, delivering trustworthy, dependable, and cost-effective cloud services that efficiently execute workloads. Autonomic cloud computing aims to understand how computing systems may autonomously accomplish user-specified “control” objectives without the need for an administrator and without violating the Service Level Agreement (SLA) in dynamic cloud computing environments. This article presents a research perspective and analysis on autonomous resource allocation in cloud computing, with a focus on QoS- and SLA-aware autonomous resource management. The study also discusses the current status of autonomic resource management in the cloud and highlights key next-generation research directions.",CS,AI_ML,0.85,Extracted from log - paper 1960
AI-Powered Cloud Orchestration: Automating Multi-Cloud & Hybrid Cloud Workloads,"AI-powered cloud orchestration revolutionizes how enterprises manage and optimize their multi-cloud and hybrid cloud environments. Integrating artificial intelligence into cloud management addresses complexity, manual intervention, and reactive problem-solving challenges that plague traditional orchestration methods. By implementing intelligent algorithms for resource allocation, workload balancing, predictive scaling, security enhancement, and self-healing capabilities, organizations can transform their cloud operations from manually-defined workflows to autonomous systems capable of continuous optimization. These advanced orchestration technologies enable dynamic resource distribution based on usage patterns and forecasted demand while simultaneously identifying cost-saving opportunities through workload consolidation and intelligent scheduling. Security frameworks are significantly strengthened through anomaly detection, predictive threat intelligence, and adaptive access control policies that evolve with changing organizational needs. Perhaps most transformative is the ability of self-healing infrastructure to automatically detect, diagnose, and remediate issues before they cause service disruptions, dramatically reducing the operational burden on technical teams and allowing them to focus on innovation rather than troubleshooting. This technological shift represents a fundamental evolution in cloud management, offering enterprises unprecedented efficiency, reliability, and cost optimization across their distributed computing environments.",CS,AI_ML,0.85,Extracted from log - paper 1961
"A Multivocal Review of MLOps Practices, Challenges and Open Issues","MLOps has emerged as a key solution to address many socio-technical challenges of bringing ML models to production, such as integrating ML models with non-ML software, continuous monitoring, maintenance, and retraining of deployed models. Despite the utility of MLOps, an integrated body of knowledge regarding MLOps remains elusive because of its extensive scope due to the diversity of ML productionalization challenges it addresses. Whilst the existing literature reviews provide valuable snapshots of specific practices, tools, and research prototypes related to MLOps at various times, they focus on particular facets of MLOps, thus fail to offer a comprehensive and invariant framework that can weave these perspectives into a unified understanding of MLOps. This paper presents a Multivocal Literature Review that systematically analyzes a corpus of 150 peer-reviewed and 48 grey literature to synthesize a unified conceptualization of MLOps and develop a snapshot of its best practices, adoption challenges, and solutions.",CS,AI_ML,0.85,Extracted from log - paper 1962
Cost modelling and optimisation for cloud: a graph-based approach,"Cloud computing has become popular among individuals and enterprises due to its convenience, scalability, and flexibility. However, a major concern for many cloud service users is the rising cost of cloud resources. Since cloud computing uses a pay-per-use model, costs can add up quickly, and unexpected expenses can arise from a lack of visibility and control. The cost structure gets even more complicated when working with multi-cloud or hybrid environments. Businesses may spend much of their IT budget on cloud computing, and any savings can improve their competitiveness and financial stability. Hence, an efficient cloud cost management is crucial. To overcome this difficulty, new approaches and tools are being developed to provide greater oversight and command over cloud a graph-based approach for modelling cost elements and cloud resources and a potential way to solve the resulting constraint problem of cost optimisation. In this context, we primarily consider utilisation, cost, performance, and availability. The proposed approach is evaluated on three different user scenarios, and results indicate that it could be effective in cost modelling, cost optimisation, and scalability. This approach will eventually help organisations make informed decisions about cloud resource placement and manage the costs of software applications and data workflows deployed in single, hybrid, or multi-cloud environments.",CS,AI_ML,0.85,Extracted from log - paper 1963
Optimization of datacenter selection through a genetic algorithm-driven service broker policy,"Establishing an optimal datacenter selection policy within the cloud environment is paramount to maximize the performance of the cloud services. Service broker policy governs the selection of datacenters for user requests. In our research, we introduce an innovative approach incorporating the genetic algorithm with service broker policy to assist cloud services in identifying the most suitable datacenters for specific userbases. The effectiveness of our proposed genetic algorithm was rigorously evaluated through experiments conducted on CloudAnalyst platform. The results clearly indicate that our proposed algorithm surpasses existing service broker policies and previous research works done in this field in terms of reducing response time and data processing time. The results analysis validates its efficacy and potential for enhancing cloud service performance and reducing the cost of overall cloud infrastructure.",CS,AI_ML,0.85,Extracted from log - paper 1964
Navigating intent-based networking: from user descriptions to deployable configurations,"Network automation development has accompanied network evolution due to its significant role in speeding up and simplifying network operations. Emerging networking and computing paradigms such as information-centric networks, next-generation networks, cloud, and edge computing and recent innovative technologies, such as the Internet of things (IoT), enabled novel network services (such as the Internet of Vehicles (IoV), context-aware applications, virtual reality, and augmented reality) that demand complex configurations and management. Intent-based networking (IBN) is a promising networking paradigm that provides abstract and autonomous network management. IBN promises to simplify configuring networking devices, allowing network engineers and service providers to focus on providing the expected services and continuously verifying that the network operates within the desired status. An IBN process starts by expressing the user requirement in a high-level descriptive format. Then, the IBN system translates these requirements to a low-level deployable format in a process called intent translation. In this work, we formally define the intent translation process and propose a generic intent translation system. Furthermore, we review the research on intent translation published between 2018 and 2022. We analyze and classify the proposed intent translation schemes and discuss the challenges and recent trends in intent translation.",CS,AI_ML,0.85,Extracted from log - paper 1965
Chaos Engineering: A Multi-Vocal Literature Review,"Organizations, particularly medium and large enterprises, typically today rely heavily on complex, distributed systems to deliver critical services and products. However, the growing complexity of these systems poses challenges in ensuring service availability, performance, and reliability. Traditional resilience testing methods often fail to capture modern systems’ intricate interactions and failure modes. Chaos Engineering addresses these challenges by proactively testing how systems in production behave under turbulent conditions, allowing developers to uncover and resolve potential issues before they escalate into outages. Though chaos engineering has received growing attention from researchers and practitioners alike, we observed a lack of a comprehensive literature review. Hence, we performed a Multivocal Literature Review (MLR) on chaos engineering to fill this research gap by systematically analyzing 88 academic and grey literature sources published from January 2019 to April 2024. We first used the selected sources to derive a unified definition of chaos engineering and to identify key capabilities, components, and adoption drivers. We also developed a taxonomy for chaos engineering and compared the relevant tools using it. Finally, we analyzed the state of the current chaos engineering research and identified several open research issues.",CS,AI_ML,0.85,Extracted from log - paper 1966
kvm: the Linux Virtual Machine Monitor,"Virtualization is a hot topic in operating systems these days. It is useful in many scenarios: server consolidation, virtual test environments, and for Linux enthusiasts who still cannot decide which distribution is best. Recently, hardware vendors of commodity x86 processors have added virtualization extensions to the instruction set that can be utilized to write relatively simple virtual machine monitors. The Kernel-based Virtual Machine, or kvm, is a new Linux subsystem which leverages these virtualization extensions to add a virtual machine monitor (or hypervisor) capability to Linux. Using kvm, one can create and run multiple virtual machines. These virtual machines appear as normal Linux processes and integrate seamlessly with the rest of the system.",CS,AI_ML,0.85,Extracted from log - paper 1967
Impacts of data consistency levels in cloud-based NoSQL for data-intensive applications,"When using database management systems (DBMSs), it is common to distribute instance replicas across multiple locations for disaster recovery and scaling purposes. To efficiently geo-replicate data, it is crucial to ensure the data and its replicas remain consistent with the same and the most up-to-date data. However, DBMSs’ inner characteristics and external factors, such as the replication strategy and network latency, can affect system performance when dealing with data replication, especially when the replicas are deployed far apart. Thus, it is essential to comprehend how achieving high data consistency levels in geo-replicated systems can impact systems performance. This work analyzes various data consistency settings for the widely used NoSQL DBMSs, namely MongoDB, Redis, and Cassandra. The analysis is based on real-world experiments in which DBMS nodes are deployed on cloud platforms in different locations, considering single and multiple region deployments. Based on the results of the experiments, we provide a comprehensive analysis regarding the system throughput and response time when executing reading and writing operations, pointing out scenarios where each DBMS could be better employed. Some of our findings include, for instance, that opting for strong data consistency significantly impacts Cassandra’s reading operations in the single-region deployment, while MongoDB writing operations are most affected in a multi-region scenario. Additionally, all of these DBMSs exhibit statistically significant variations across all scenarios in the multi-region setup when the data consistency is switched from weak to stronger level.",CS,AI_ML,0.85,Extracted from log - paper 1968
Functionality-aware offloading technique for scheduling containerized edge applications in IoT edge computing,"Edge computing (EC) represents a basic functionality to support the efficiency of the future Internet of intelligent things. The EC has promoted container adoption for deploying and managing applications. Current container scheduling techniques on edge infrastructures show multiple limitations. The design scheduler of container applications execute workflow as specified by the container cloud workflow engine assisted by the Kubernetes platform. We model dependency workflow of containerized applications built using microservices as directed acyclic graph (DAG). The structure of DAG allows the system to prepare the scheduling order list of microservices. The Argo workflow is used to prepare the sequence to deploy containerized applications. In addition, edge worker nodes’ resource utilization data enabled assists to select on which edge worker nodes the scheduling will take place. By combining the two mechanism, we termed the scheduling as functionality-aware offloading on scheduling containerized edge applications. We implemented the orchestration prototype and evaluate the performance of the proposed technique under extensive simulations using the ContainerCloudSim simulator with a module that models a lightweight Kubernetes platform in the context of the edge computing infrastructure. To validate our containerized edge inference service and collect data for the simulation setup, we used Raspberry Pis 4, and the cloud core was set up on Amazon Web Services. The workload in the pre-defined workflow using Argo K8s native was performed by calling the pre-trained model (downloaded and stored locally) and then executing the prediction microservice running on Raspberry Pis. The results demonstrate that our proposal outperforms the baseline scheduling offloading technique in edge computing by decreasing the average scheduling time of containerized edge applications by 15%.",CS,AI_ML,0.85,Extracted from log - paper 1969
Explainable AI-based innovative hybrid ensemble model for intrusion detection,"Cybersecurity threats have become more worldly, demanding advanced detection mechanisms with the exponential growth in digital data and network services. Intrusion Detection Systems (IDSs) are crucial in identifying illegitimate access or anomalous behaviour within computer network systems, consequently opposing sensitive information. Traditional IDS approaches often struggle with high false positive rates and the ability to adapt embryonic attack patterns. This work asserts a novel Hybrid Adaptive Ensemble for Intrusion Detection (HAEnID), an innovative and powerful method to enhance intrusion detection, different from the conventional techniques. HAEnID is composed of a string of multi-layered ensemble, which consists of a Stacking Ensemble (SEM), a Bayesian Model Averaging (BMA), and a Conditional Ensemble method (CEM). HAEnID combines the best of these three ensemble techniques for ultimate success in detection with a considerable cut in false alarms. A key feature of HAEnID is an adaptive mechanism that allows ensemble components to change over time as network traffic patterns vary and new threats appear. This way, HAEnID would provide adequate protection as attack vectors change. Furthermore, the model would become more interpretable and explainable using Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME). The proposed Ensemble model for intrusion detection on CIC-IDS 2017 achieves excellent accuracy (97-98%), demonstrating effectiveness and consistency across various configurations. Feature selection further enhances performance, with BMA-M (20) reaching 98.79% accuracy. These results highlight the potential of the ensemble model for accurate and reliable intrusion detection and, hence, is a state-of-the-art choice for accuracy and explainability.",CS,AI_ML,0.85,Extracted from log - paper 1970
Privacy-enhanced distributed revocable identity management scheme based self-sovereign identity,"In recent years, the rapid proliferation of digital services and resources on Industrial Internet has imposed higher demands on universality and privacy of identity management. Particularly with the advent of the digital economy, prudent users are urged to maintain control over their digital identity credentials. However, traditional identity management methods have failed to meet this requirement and thus have been prone to raise users’ concerns about potential financial loss. Specifically, conventional identity management systems (IDMS) have been plagued by imperceptible privacy disclosure, which derives from the flaws in single points of failure, excessive disclosure, correlation analysis, traceability, and revocation. The emerging Self-Sovereign Identity (SSI) architecture aims to tackle these issues and is propelling the evolution of privacy-enhanced distributed identity management. To this end, we proposed a privacy-enhanced distributed identity management scheme with sequential aggregate issuance, threshold traceability and revocability in the setting of multiple credential issuers and regulators. We adopted the Decentralized identifiers (DIDs) and verifiable credentials (VCs) based on the SSI architecture to ensure the hierarchical identity authentication. The security and performance analysis shows that our proposal achieves the desired design goals and is feasible for distributed Industrial Internet.",CS,AI_ML,0.85,Extracted from log - paper 1971
Seagull: Intelligent Cloud Bursting for Enterprise Applications,"Enterprises with existing IT infrastructure are beginning to employ a hybrid cloud model where the enterprise uses its own private resources for the majority of its computing, but then “bursts” into the cloud when local resources are insufficient. However, current approaches to cloud bursting cannot be effectively automated because they heavily rely on system administrator knowledge to make decisions. In this paper we describe Seagull, a system designed to facilitate cloud bursting by determining which applications can be transitioned into the cloud most economically, and automating the movement process at the proper time. We further optimize the deployment of applications into the cloud using an intelligent precopying mechanism that proactively replicates virtualized applications, lowering the bursting time from hours to minutes. Our evaluation illustrates how our prototype can reduce cloud costs by more than 45% when bursting to the cloud, and the incremental cost added by precopying applications is offset by a burst time reduction of nearly 95%.",CS,AI_ML,0.85,Extracted from log - paper 1972
Maglev: A Fast and Reliable Software Network Load Balancer,"Maglev is Google’s network load balancer. It is a large distributed software system that runs on commodity Linux servers. Unlike traditional hardware network load balancers, it does not require a specialized physical rack deployment, and its capacity can be easily adjusted by adding or removing servers. Network routers distribute packets evenly to the Maglev machines via Equal Cost Multipath (ECMP); each Maglev machine then matches the packets to their corresponding services and spreads them evenly to the service endpoints. To accommodate high and ever-increasing traffic, Maglev is specifically optimized for packet processing performance. A single Maglev machine is able to saturate a 10Gbps link with small packets. Maglev is also equipped with consistent hashing and connection tracking features, to minimize the negative impact of unexpected faults and failures on connection-oriented protocols. Maglev has been serving Google’s traffic since 2008. It has sustained the rapid global growth of Google services, and it also provides network load balancing for Google Cloud Platform.",CS,AI_ML,0.85,Extracted from log - paper 1973
Monarch: Google’s Planet-Scale In-Memory Time Series Database,"Monarch is a globally-distributed in-memory time series database system in Google. Monarch runs as a multi-tenant service and is used mostly to monitor the availability, correctness, performance, load, and other aspects of billion-user-scale applications and systems at Google. Every second, the system ingests terabytes of time series data into memory and serves millions of queries. Monarch has a regionalized architecture for reliability and scalability, and global query and configuration planes that integrate the regions into a unified system. On top of its distributed architecture, Monarch has flexible configuration, an expressive relational data model, and powerful queries. This paper describes the structure of the system and the novel mechanisms that achieve a reliable and flexible unified system on a regionalized distributed architecture. We also share important lessons learned from a decade’s experience of developing and running Monarch as a service in Google.",CS,AI_ML,0.85,Extracted from log - paper 1974
Cassandra - A Decentralized Structured Storage System,"Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write throughput while not sacrificing read efficiency.",CS,AI_ML,0.85,Extracted from log - paper 1975
Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency,"Windows Azure Storage (WAS) is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data for any duration of time. WAS customers have access to their data from anywhere at any time and only pay for what they use and store. In WAS, data is stored durably using both local and geographic replication to facilitate disaster recovery. Currently, WAS storage comes in the form of Blobs (files), Tables (structured storage), and Queues (message delivery). In this paper, we describe the WAS architecture, global namespace, and data model, as well as its resource provisioning, load balancing, and replication systems.",CS,AI_ML,0.85,Extracted from log - paper 1976
FAWN: A Fast Array of Wimpy Nodes,"This paper presents a new cluster architecture for low-power data-intensive computing. The design couples low-power CPUs with flash storage to create a fast array of wimpy nodes. FAWN reduces per-node power consumption while still achieving high performance by parallelism across many nodes. We prototype a FAWN key-value storage system and show it yields significantly more queries per joule than traditional disk-based clusters. FAWN demonstrates that efficiency at scale can be improved by using large numbers of modest, energy-efficient nodes for certain workloads, highlighting an alternative approach to building energy-proportional data centers.",CS,AI_ML,0.85,Extracted from log - paper 1977
Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing,"We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are immutable, partitioned collections of records. RDDs can be rebuilt efficiently if partitions are lost, using lineage information. We implement RDDs in Spark, which provides a high-level programming interface for cluster computing. Using RDDs, many iterative algorithms and interactive data analysis tasks run 10× faster than Hadoop MapReduce by avoiding repeated disk I/O. RDDs enable a wide range of applications while retaining fault tolerance. We evaluate Spark on real cluster workloads and show it achieves excellent performance and recovery properties.",CS,AI_ML,0.85,Extracted from log - paper 1978
CloudSim: a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms,"CloudSim is a toolkit that provides a generalized and extensible simulation framework to model Cloud infrastructure and application services. It supports system and behavior modeling of cloud components (data centers, virtual machines, resource provisioning policies, etc.) and allows evaluation of resource allocation and scheduling algorithms in a repeatable manner under different configurations. This paper describes CloudSim’s architecture and capabilities, including modeling of multi-tenant data centers, energy-aware simulations, network topologies, and dynamic workloads. We demonstrate CloudSim’s use by simulating a scheduling algorithm and analyzing its performance. As an open-source tool, CloudSim has become popular for researchers to test new cloud management strategies (e.g., load balancing, VM consolidation, power management) without the expense of real deployments.",CS,AI_ML,0.85,Extracted from log - paper 1979
Apollo: Scalable and Coordinated Scheduling for Cloud-Scale Computing,"This paper presents Apollo, a highly scalable and coordinated scheduling framework, which has been deployed on production clusters at Microsoft. Apollo dynamically schedules hundreds of thousands of jobs across tens of thousands of machines while meeting diverse scheduling objectives. It combines distributed local schedulers for low-latency decisions with a periodic global coordination mechanism to optimize overall placement. Experiments on Microsoft’s production workload show Apollo achieves near-optimal scheduling quality with sub-second average decision time, significantly outperforming prior centralized and decentralized schedulers in both fairness and efficiency. Apollo demonstrates that even at cloud scale, coordinated scheduling can provide excellent performance, challenging the conventional trade-off between scale and optimality.",CS,AI_ML,0.85,Extracted from log - paper 1980
"Firmament: Fast, Centralized Cluster Scheduling at Scale","This paper describes Firmament, a centralized scheduler that scales to over 10,000 machines while making sub-second placement decisions. Firmament formulates scheduling as a min-cost max-flow problem, enabling global optimization of task assignments. By using incremental graph optimization and multiple optimizations, Firmament achieves fast, high-quality scheduling. Simulation results on Google cluster trace workloads show Firmament improves job turnaround time and fairness compared to state-of-the-art schedulers. A prototype integrated with Kubernetes confirms Firmament can feasibly handle production-scale cluster management. Firmament’s results suggest that contrary to conventional assumptions, centralized scheduling can meet the latency and scale demands of large clusters, paving the way for more intelligent resource management in datacenters.",CS,AI_ML,0.85,Extracted from log - paper 1981
The Vision of Autonomic Computing,"In mid-October 2001, IBM released a manifesto observing that the main obstacle to further progress in the IT industry is a looming software complexity crisis. The company cited applications and environments that weigh in at tens of millions of lines of code and require skilled IT professionals to install, configure, tune, and maintain. The manifesto pointed out that the difficulty of managing today's computing systems goes well beyond the administration of individual software environments. The need to integrate several heterogeneous environments into corporate-wide computing systems, and to extend that beyond company boundaries into the Internet, introduces new levels of complexity. Computing systems' complexity appears to be approaching the limits of human capability, yet the march toward increased interconnectivity and integration rushes ahead unabated. This march could turn the dream of pervasive computing—trillions of computing devices connected to the Internet—into a nightmare. Programming language innovations have extended the size and complexity of systems that architects can design, but relying solely on further innovations in programming methods will not get us through the present complexity crisis. As systems become more interconnected and diverse, architects are less able to anticipate and design interactions among components, leaving such issues to be dealt with at runtime. Soon systems will become too massive and complex for even the most skilled system integrators to install, configure, optimize, maintain, and merge. And there will be no way to make timely, decisive responses to the rapid stream of changing and conflicting demands. The only option remaining is autonomic computing—computing systems that can manage themselves given high-level objectives from administrators. When IBM's senior vice president of research, Paul Horn, introduced this idea to the National Academy of Engineers at Harvard University in a March 2001 keynote address, he deliberately chose a term with a biological connotation. The autonomic nervous system governs our heart rate and body temperature, thus freeing our conscious brain from the burden of dealing with these and many other low-level, yet vital, functions. The term autonomic computing is emblematic of a vast and somewhat tangled hierarchy of natural self-governing systems, many of which consist of myriad interacting, self-governing components that in turn comprise large numbers of interacting, autonomous, self-governing components at the next level down. The enormous range in scale, starting with molecular machines within cells and extending to human markets, societies, and the entire world socioeconomy, mirrors that of computing systems, which run from individual devices to the entire Internet. Thus, we believe it will be profitable to seek inspiration in the self-governance of social and economic systems as well as purely biological ones. Clearly then, autonomic computing is a grand challenge that reaches far beyond a single organization. Its realization will take a concerted, long-term, worldwide effort by researchers in a diversity of fields. A necessary first step is to examine this vision: what autonomic computing systems might look like, how they might function, and what obstacles researchers will face in designing them and understanding their behavior.",CS,AI_ML,0.85,Extracted from log - paper 1982
f4: Facebook’s Warm BLOB Storage System,"f4 is a new system that lowers the effective replication factor of warm BLOBs (binary large objects) while remaining fault tolerant and able to support their lower throughput demands. Facebook’s photo storage stack originally replicated all images three times for reliability. As images age (“warm” data), f4 replaces one replica with erasure-coded fragments, reducing storage overhead significantly (e.g., 2.1× instead of 3× replication) while tolerating disk, host, rack, and even datacenter failures. f4 consists of heterogeneous storage nodes and a software stack that handles encoding, decoding, and repair. Deployed across Facebook’s data centers, f4 saves dozens of petabytes of storage and seamlessly serves warm photo traffic with acceptable latency. This paper details f4’s design, including its Reed-Solomon encoding parameters, failure domain design, and the caching and tiering mechanisms that ensure user experience is not impacted. f4 demonstrates how large-scale storage systems can exploit erasure coding for substantial cost savings.",CS,AI_ML,0.85,Extracted from log - paper 1983
Finding a needle in Haystack: Facebook’s photo storage,"This paper describes Haystack, an object storage system optimized for Facebook’s Photos. Facebook currently stores over 260 billion images, with millions of new photos uploaded every hour. Traditional storage architectures incur excessive metadata overhead and disk seeks per photo. Haystack collapses the storage hierarchy: it stores many photos in one large file (a “haystack”) on each storage volume and maintains an in-memory index mapping photo IDs to byte offsets. This removes almost all disk seeks for reads and writes. In production, Haystack reduced average photo read latency from several hundred milliseconds (previous NFS-based system) to under 50 ms, and cut the number of I/O operations per read from 3+ to 1. Haystack’s simplified design and efficient use of extents enabled Facebook to scale photo storage to petabytes on commodity hardware with minimal operational complexity. The system has been running in production for over two years, serving billions of images daily with high reliability and efficiency.",CS,AI_ML,0.85,Extracted from log - paper 1984
DeathStarBench: Open-Source Benchmark Suite for Cloud Microservices,"We introduce DeathStarBench, a comprehensive benchmark suite to evaluate end-to-end performance of cloud microservice applications. It includes five real-world inspired application stacks (e.g., social network, media streaming, e-commerce) built with dozens of microservices using popular frameworks. We utilize DeathStarBench to study microservices’ impact on cloud systems. Our analysis reveals that microservices incur increased communication (e.g., 60% of requests are remote calls) and exhibit highly variable tail latencies due to fan-out and inter-service dependencies. We evaluate how modern RPC frameworks and kernel network stacks handle these workloads, finding existing systems often underperform (e.g., kernel network overhead contributes significantly to 99th percentile latency). We discuss optimizations such as user-space networking and improved concurrency control. DeathStarBench provides a realistic and reproducible way to drive research on microservice-aware operating systems, network optimizations, and scheduling policies, ultimately guiding the design of next-generation cloud platforms better suited for microservices.",CS,AI_ML,0.85,Extracted from log - paper 1985
Google Cluster Workload Traces: Analysis and Lessons,"Google’s publicly released cluster workload traces provide a rare glimpse into large-scale data center operations. In this study, we analyze a month-long Google trace (May 2011) to characterize workload patterns and draw implications for cluster management. We observe that the workload is highly heterogeneous: a mix of long-running services and short batch jobs, with arrival rates varying diurnally. Resource utilization is low on average (CPU ~40%) but bursts to high levels, indicating significant headroom for consolidation. Task durations are heavy-tailed, with many very short tasks and a few extremely long ones. Failures and interruptions (preemptions) are frequent – over 10% of tasks were evicted or failed – underscoring the need for resilient scheduling. Based on these findings, we discuss lessons: e.g., schedulers should incorporate priority preemption to accommodate urgent jobs, and resource allocation should account for usage variance. Our analysis provides a foundation for researchers to develop more efficient, robust cluster schedulers and resource management policies that reflect production realities.",CS,AI_ML,0.85,Extracted from log - paper 1986
Site Reliability Engineering: Google’s Approach to Service Reliability,"Google’s Site Reliability Engineering (SRE) is a discipline that applies software engineering to operations, aiming to create ultra-reliable software systems. This article provides an overview of SRE practices that keep Google’s services running at scale. Key concepts include the use of Service Level Objectives (SLOs) and error budgets to balance reliability with feature velocity – if a service meets its reliability target, developers can take more risks until the error budget is exhausted. SRE teams focus on eliminating toil (repetitive manual work) through automation and have authority to improve systems’ reliability by refactoring or optimizing code. We describe Google’s incident management process (emphasizing blameless post-mortems), how SREs conduct capacity planning and demand forecasting, and techniques for testing reliability (like chaos engineering). SRE has a unique culture where risk is managed proactively and failure is embraced as an opportunity to learn. The result is services that achieve high uptime despite continuous deployment of changes. The paper concludes with lessons for organizations adopting SRE, including hiring profiles (software engineers with ops mindset) and the importance of management support for reliability investments.",CS,AI_ML,0.85,Extracted from log - paper 1987
Fog Computing and Its Role in the Internet of Things,"Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. De ning characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution; c) Mobility; d) Very large number of nodes, e) Predominant role of wireless access, f) Strong presence of streaming and real time applications, g) Heterogeneity. In this paper we argue that the above characteristics make the Fog the appropriate platform for a number of critical Internet of Things (IoT) services and applications, namely, Connected Vehicle, Smart Grid , Smart Cities, and, in general, Wireless Sensors and Actuators Networks (WSANs).",CS,AI_ML,0.85,Extracted from log - paper 1988
I-MPaFS: enhancing EDoS attack detection in cloud computing through a data-driven approach,"Cloud computing offers cost-effective IT solutions but is susceptible to security threats, particularly the Economic Denial of Sustainability (EDoS) attack. EDoS exploits cloud elasticity and the pay-per-use billing model, forcing users to incur unnecessary costs. This research introduces the Integrated Model Prediction and Feature Selection (I-MPaFS) framework to address EDoS attacks. I-MPaFS framework enhances an existing dataset to improve performance, using the generated data to build a Random Forest model for EDoS detection. Our investigation employs the UNSW-NB15, CSE-CIC-IDS18 and NSL-KDD datasets, demonstrating the proposed method’s superiority over existing techniques. The model achieved recall scores of 99.45% on the UNSW-NB15 dataset, 98.19% on the CSE-CIC-IDS18 dataset, and 99.82% on the NSL-KDD dataset, highlighting its reliability and efficacy in safeguarding cloud users from financial exploitation. This study contributes to the field by evaluating current EDoS detection methods, introducing the I-MPaFS framework, validating its performance with benchmark datasets, and comparing its effectiveness against state-of-the-art techniques. The findings affirm the significant potential of I-MPaFS in enhancing cloud security and protecting users from EDoS attacks.",CS,AI_ML,0.85,Extracted from log - paper 1989
PowerNap: Eliminating Server Idle Power,"Cloud data centers often operate servers at 10-50% average utilization, yet servers consume nearly full power even when doing little work. This wastes energy and reduces efficiency. We propose PowerNap, an approach where servers transition rapidly to a low-power state when idle and resume full-power operation when load returns. We present a PowerNap-enabled server prototype and show it can enter a near-zero-power sleep state in milliseconds. To complement PowerNap, we introduce RAILS (Redundant Array of Inexpensive Load Sharing), a power supply design that efficiently delivers power across the wide dynamic range from full load to sleep with minimal conversion loss. Evaluation using real-world traces indicates that a cluster of PowerNap servers can maintain required performance while eliminating almost all idle power consumption, achieving energy proportionality. For example, a web cluster with PowerNap reduces energy use by 74% compared to today’s always-on servers. We discuss software implications, such as ensuring quick state save/restore and modifying operating system timers. PowerNap demonstrates a practical path to energy-proportional computing by tackling idle power waste through fast, fine-grained power management.",CS,AI_ML,0.85,Extracted from log - paper 1990
The Hadoop Distributed File System,"The Hadoop Distributed File System (HDFS) is designed to reliably store very large files across machines in a large cluster. HDFS stores file data as blocks on multiple DataNodes for fault tolerance, with an overarching NameNode managing namespace metadata and block mapping. This article outlines HDFS’s architecture and features. Key design points include: high throughput rather than low latency, streaming data access patterns (write-once, read-many), and simple coherency model (no updates to files once written). HDFS ensures durability through replication (default 3x copies) and automatically re-replicates blocks on DataNode failure. A secondary NameNode periodically checkpoints metadata for recovery. We highlight HDFS’s approach to rack-aware placement, which improves data reliability and network bandwidth utilization by spreading replicas across racks. HDFS has proven to scale to petabytes of storage and thousands of nodes in production at Yahoo! and Facebook. Its success has made it the foundation of the Apache Hadoop ecosystem, demonstrating how a carefully engineered file system can enable big data processing on commodity hardware.",CS,AI_ML,0.85,Extracted from log - paper 1991
Apache Hadoop YARN: yet another resource negotiator,"The initial design of Apache Hadoop [1] was tightly focused on running massive, MapReduce jobs to process a web crawl. For increasingly diverse companies, Hadoop has become the data and computational agorá---the de facto place where data and computational resources are shared and accessed. This broad adoption and ubiquitous usage has stretched the initial design well beyond its intended target, exposing two key shortcomings: 1) tight coupling of a specific programming model with the resource management infrastructure, forcing developers to abuse the MapReduce programming model, and 2) centralized handling of jobs' control flow, which resulted in endless scalability concerns for the scheduler. In this paper, we summarize the design, development, and current state of deployment of the next generation of Hadoop's compute platform: YARN. The new architecture we introduced decouples the programming model from the resource management infrastructure, and delegates many scheduling functions (e.g., task fault-tolerance) to per-application components. We provide experimental evidence demonstrating the improvements we made, confirm improved efficiency by reporting the experience of running YARN on production environments (including 100% of Yahoo! grids), and confirm the flexibility claims by discussing the porting of several programming frameworks onto YARN viz. Dryad, Giraph, Hoya, Hadoop MapReduce, REEF, Spark, Storm, Tez.",CS,AI_ML,0.85,Extracted from log - paper 1992
Infrastructure as Code for Dynamic Deployments,"Modern DevOps organizations require a high degree of automation to achieve software stability at frequent changes. Further, there is a need for flexible, timely reconfiguration of the infrastructure, e.g., to use pay-per-use infrastructure efficiently based on application load. Infrastructure as Code (IaC) is the DevOps tool to automate infrastructure. However, modern static IaC solutions only support infrastructures that are deployed and do not change afterward. To implementinfrastructures that change dynamically over time, static IaC programs have to be (updated and) re-run, e.g., in a CI/CD pipeline, or configure an external orchestrator that implements the dynamic behavior, e.g., an autoscaler or Kubernetes operator. Both do not capture the dynamic behavior in the IaC program and prevent analyzing and testing the infrastructure configuration jointly with its dynamic behavior. To fill this gap, we envision dynamic IaC, which augments static IaC with the ability to define dynamic behavior within the IaC program. In contrast to static IaC programs, dynamic IaC programs run continuously. They re-evaluate program parts that depend on external signals when these change and automatically adjust the infrastructure accordingly. We implement DIaC as the first dynamic IaC solution and demonstrate it in two realistic use cases of broader relevance. With dynamic IaC, ensuring the program’s correctness is even harder than for static IaC because programs may define many target configurations in contrast to only a few. However, for this reason, it is also more critical. To solve this issue, we propose automated, specialized property-based testing for IaC programs and implement it in ProTI.",CS,AI_ML,0.85,Extracted from log - paper 1993
"DevSecOps: A Multivocal Literature Review on Current State, Challenges, Practices, Tools, and Metrics","Security appears as a key non-functional requirement of software development but is often ignored and devalued, because of regarding security as an inhibitor to high velocity required in DevOps implementation. DevSecOps approach is created as a security-orientation expansion to DevOps, aimed to integrate security into DevOps by promoting collaboration among development, operation and security teams.This paper aims to analyze the current state of DevSecOps in the literature; and investigate the application of DevSecOps in Global Software Engineering contexts. A Multi-vocal Literature Review on DevSecOps and its global application was conducted, including white (104) and grey (43) literature from 2012 to 2021. A Thematic Analysis was performed to identify, synthesize and analyze the themes within data for reporting the results.Results identify five aspects of DevSecOps in the existing literature: Definitions, Challenges, Practices, Tools/Technologies, and Metrics/Measurement; collect related themes of each aspect; further generate a Challenge-Practice-Tool-Metric (CPTM) model by integrating the themes of latter four aspects. This model captures the current status and existing experience of DevSecOps. Moreover, a missing global dimension of DevSecOps has been identified. This reveals the need for future research on exploring how DevSecOps operates globally, and how it may differ in local or global settings.",CS,AI_ML,0.85,Extracted from log - paper 1994
The Google File System,"We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.",CS,AI_ML,0.85,Extracted from log - paper 1995
"Service Mesh: Architectures, Applications, and Implementations","The scalability and flexibility of microservice architecture have led to major changes in cloud-native application architectures. However, the complexity of managing thousands of small services written in different languages and handling the exchange of data between them have caused significant management challenges. Service mesh is a promising solution that could mitigate these problems by introducing an overlay layer on top of the services. In this paper, we first study the architecture and components of service mesh architecture. Then, we review two important service mesh implementations and discuss how the service mesh could be helpful in other areas, including 5G.",CS,AI_ML,0.85,Extracted from log - paper 1996
Enterprise Architecture Components for Cloud Service Consumers,Enterprise Architecture (EA) and appropriate governance enables cloud computing adoption by consumer organisations. EA is gaining acceptability as an approach for strategic alignment of business and IT and as key enabler for cloud computing. EA practices consist of a range of activities and covers many of the elements necessary for enabling cloud computing. This paper discusses the key architectural components necessary from the perspective of a consumer organization for the adoption of cloud computing and discusses these elements in the context of EA frameworks and governance. The ability to use maturity assessments on these architectural components to determine organizational readiness to achieve cloud benefits is introduced.,CS,AI_ML,0.85,Extracted from log - paper 1997
An Updated Performance Comparison of Virtual Machines and Linux Containers,"Cloud computing makes extensive use of virtual machines because they permit workloads to be isolated from one another and for the resource usage to be somewhat controlled. In this paper, we explore the performance of traditional virtual machine (VM) deployments, and contrast them with the use of Linux containers. We use KVM as a representative hypervisor and Docker as a container manager. Our results show that containers result in equal or better performance than VMs in almost all cases. Both VMs and containers require tuning to support I/Ointensive applications. We also discuss the implications of our performance results for future cloud architectures.",CS,AI_ML,0.85,Extracted from log - paper 1998
Spanner: Google’s Globally-Distributed Database,"Spanner is Google’s scalable, multi-version, globallydistributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This paper describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free read-only transactions, and atomic schema changes, across all of Spanner.",CS,AI_ML,0.85,Extracted from log - paper 1999
Ananta: Cloud Scale Load Balancing,"Layer-4 load balancing is fundamental to creating scale-out web services. We designed and implemented Ananta, a scale-out layer-4 load balancer that runs on commodity hardware and meets the performance, reliability and operational requirements of multi-tenant cloud computing environments. Ananta combines existing techniques in routing and distributed systems in a unique way and splits the components of a load balancer into a consensus-based reliable control plane and a decentralized scale-out data plane. A key component of Ananta is an agent in every host that can take over the packet modification function from the load balancer, thereby enabling the load balancer to naturally scale with the size of the data center. Due to its distributed architecture, Ananta provides direct server return (DSR) and network address translation (NAT) capabilities across layer-2 boundaries. Multiple instances of Ananta have been deployed in the Windows Azure public cloud with combined bandwidth capacity exceeding 1Tbps. It is serving traffic needs of a diverse set of tenants, including the blob, table and relational storage services. With its scale-out data plane we can easily achieve more than 100Gbps throughput for a single public IP address. In this paper, we describe the requirements of a cloud-scale load balancer, the design of Ananta and lessons learnt from its implementation and operation in the Windows Azure public cloud.",CS,AI_ML,0.85,Extracted from log - paper 2000
X-Trace: A Pervasive Network Tracing Framework,"Modern Internet systems often combine different applications (e.g., DNS, web, and database), span different administrative domains, and function in the context of network mechanisms like tunnels, VPNs, NATs, and overlays. Diagnosing these complex systems is a daunting challenge. Although many diagnostic tools exist, they are typically designed for a specific layer (e.g., traceroute) or application, and there is currently no tool for reconstructing a comprehensive view of service behavior. In this paper we propose X-Trace, a tracing framework that provides such a comprehensive view for systems that adopt it. We have implemented X-Trace in several protocols and software systems, and we discuss how it works in three deployed scenarios: DNS resolution, a three-tiered photo-hosting website, and a service accessed through an overlay network.",CS,AI_ML,0.85,Extracted from log - paper 2001
The Tail at Scale,Software techniques that tolerate latency variability are vital to building responsive large-scale Web services.,CS,AI_ML,0.85,Extracted from log - paper 2002
A Virtual Machine Introspection Based Architecture for Intrusion Detection,"Today's architectures for intrusion detection force the IDS designer to make a difficult choice. If the IDS resides on the host, it has an excellent view of what is happening in that host's software, but is highly susceptible to attack. On the other hand, if the IDS resides in the network, it is more resistant to attack, but has a poor view of what is happeninginside the host, making it more susceptible to evasion. In this paper we present an architecture that retains the visibility of a host-based IDS, but pulls the IDS outside of the host for greater attack resistance. We achieve this through the use of a virtual machine monitor. Using this approach allows us to isolate the IDS from the monitored host but still retain excellent visibility into the host's state. The VMM also offers us the unique ability to completely mediate interactions between the host software and the underlying hardware. We present a detailed study of our architecture, including Livewire, a prototype implementation. We demonstrate Livewire by implementing a suite of simple intrusion detection policies and using them to detect real attacks.",CS,AI_ML,0.85,Extracted from log - paper 2003
Disaster Recovery as a Cloud Service: Economic Benefits & Deployment Challenges,"Many businesses rely on Disaster Recovery (DR) services to prevent either manmade or natural disasters from causing expensive service disruptions. Unfortunately, current DR services come either at very high cost, or with only weak guarantees about the amount of data lost or time required to restart operation after a failure. In this work, we argue that cloud computing platforms are well suited for offering DR as a service due to their pay-as-you-go pricing model that can lower costs, and their use of automated virtual platforms that can minimize the recovery time after a failure. To this end, we perform a pricing analysis to estimate the cost of running a public cloud based DR service and show significant cost reductions compared to using privately owned resources. Further, we explore what additional functionality must be exposed by current cloud platforms and describe what challenges remain in order to minimize cost, data loss, and recovery time in cloud based DR services.",CS,AI_ML,0.85,Extracted from log - paper 2004
Software-Defined Networking: A Comprehensive Survey,"The Internet has led to the creation of a digital society, where (almost) everything is connected and is accessible from anywhere. However, despite their widespread adoption, traditional IP networks are complex and very hard to manage. It is both difficult to configure the network according to predefined policies, and to reconfigure it to respond to faults, load and changes. To make matters even more difficult, current networks are also vertically integrated: the control and data planes are bundled together. Software-Defined Networking (SDN) is an emerging paradigm that promises to change this state of affairs, by breaking vertical integration, separating the network’s control logic from the underlying routers and switches, promoting (logical) centralization of network control, and introducing the ability to program the network. The separation of concerns introduced between the definition of network policies, their implementation in switching hardware, and the forwarding of traffic, is key to the desired flexibility: by breaking the network control problem into tractable pieces, SDN makes it easier to create and introduce new abstractions in networking, simplifying network management and facilitating network evolution. In this paper we present a comprehensive survey on SDN. We start by introducing the motivation for SDN, explain its main concepts and how it differs from traditional networking, its roots, and the standardization activities regarding this novel paradigm. Next, we present the key building blocks of an SDN infrastructure using a bottom-up, layered approach. We provide an in-depth analysis of the hardware infrastructure, southbound and northbound APIs, network virtualization layers, network operating systems (SDN controllers), network programming languages, and network applications. We also look at cross-layer problems such as debugging and troubleshooting. In an effort to anticipate the future evolution of this new paradigm, we discuss the main ongoing research efforts and challenges of SDN. In particular, we address the design of switches and control platforms– with a focus on aspects such as resiliency, scalability, performance, security and dependability– as well as new opportunities for carrier transport networks and cloud providers. Last but not least, we analyze the position of SDN as a key enabler of a software-defined environment.",CS,AI_ML,0.85,Extracted from log - paper 2005
PeerPressure: Automated Diagnosis of Misconfigurations in Large-Scale Systems,"Technical support contributes 17% of the total cost of ownership of today’s desktop PCs . An important element of technical support is troubleshooting misconf igured applications. Misconfiguration troubleshooting is particularly challenging, because configuration information is shared and altered by multiple applications. In this paper, we present a novel troubleshooting system: PeerPressure, which uses statistics from a set of sample machines to diagnose the root-cause misconfigurations on a sick machine. This is in contrastwithmethods that require manual identification on a healthy machine for diagnosing misconfigurations . The elimination of this manual operation makes a significant step towards automated misconfiguration troubleshooting. In PeerPressure, we introduce a ranking metric for misconfiguration candidates. This metric is based on empirical Bayesian estimation. We have prototyped a PeerPressure troubleshootingsystem anduseda databaseof87machine configurationsnapshots to evaluate its performance. With 20 real-world troubleshooting cases, PeerPressure can effectively pinpoint the root-cause misconfigurations for 12 of these cases. For the remaining cases, PeerPressure significantly narrows down the number of root-cause candidates by three orders of magnitude.",CS,AI_ML,0.85,Extracted from log - paper 2006
Bubble-up: Increasing utilization in modern warehouse scale computers via sensible co-locations,"As much of the world's computing continues to move into the cloud, the overprovisioning of computing resources to ensure the performance isolation of latency-sensitive tasks, such as web search, in modern datacenters is a major contributor to low machine utilization. Being unable to accurately predict performance degradation due to contention for shared resources on multicore systems has led to the heavy handed approach of simply disallowing the co-location of high-priority, latency-sensitive tasks with other tasks. Performing this precise prediction has been a challenging and unsolved problem. In this paper, we present Bubble-Up, a characterization methodology that enables the accurate prediction of the performance degradation that results from contention for shared resources in the memory subsystem. By using a bubble to apply a tunable amount of “pressure” to the memory subsystem on processors in production datacenters, our methodology can predict the performance interference between co-locate applications with an accuracy within 1% to 2% of the actual performance degradation. Using this methodology to arrive at “sensible” co-locations in Google's production datacenters with real-world large-scale applications, we can improve the utilization of a 500-machine cluster by 50% to 90% while guaranteeing a high quality of service of latency-sensitive applications.",CS,AI_ML,0.85,Extracted from log - paper 2007
DCTCP: Efficient Packet Transport for the Commoditized Data Center,"Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry ""background"" flows build up queues at the switches, and thus impact the performance of latency sensitive ""foreground"" traffic. To address these problems, we propose DCTCP, a variant of TCP for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to a simple control mechanism implemented in the host OS. We evaluate DCTCP at 1 and 10Gbps speeds, through benchmark experiments and analysis. In the data center, operating with commodity, shallow buffered switches, we find DCTCP delivers the same or better throughput than TCP, while using 90% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.",CS,AI_ML,0.85,Extracted from log - paper 2008
Prominent Security Vulnerabilities in Cloud Computing,"This research study examines the significant security vulnerabilities and threats in cloud computing, analyzes their potential consequences for enterprises, and proposes effective solutions for mitigating these vulnerabilities. This paper discusses the increasing significance of cloud security in a time characterized by rapid data expansion and technological progress. The paper examines prevalent vulnerabilities in cloud computing, including cloud misconfigurations, data leakage, shared technology threats, and insider threats. It emphasizes the necessity of adopting a proactive and comprehensive approach to ensure cloud security. The report places significant emphasis on the shared responsibility paradigm, adherence to industry laws, and the dynamic nature of cybersecurity threats. The situation necessitates the cooperation of researchers, cybersecurity professionals, and enterprises to proactively address these difficulties. This partnership aims to provide a thorough manual for organizations aiming to bolster their cloud security measures and safeguard valuable data in an ever-evolving digital landscape.",CS,AI_ML,0.85,Extracted from log - paper 2009
On Scalable Integrity Checking for Secure Cloud Disks,"Merkle hash trees are the standard method to protect the integrity and freshness of stored data. However, hash trees introduce additional compute and I/O costs on the I/O critical path, and prior efforts have not fully characterized these costs. In this paper, we quantify performance overheads of storage-level hash trees in realistic settings. We then design an optimized tree structure called Dynamic Merkle Trees (DMTs) based on an analysis of root causes of overheads. DMTs exploit patterns in workloads to deliver up to a 2.2x throughput and latency improvement over the state of the art. Our novel approach provides a promising new direction to achieve integrity guarantees in storage efficiently and at scale.",CS,AI_ML,0.85,Extracted from log - paper 2010
Secure Data Sharing in Cloud Computing: A Comprehensive Survey of Two-Factor Authentication and Cryptographic Solutions,"Cloud computing has emerged as a pivotal trend across both commercial and academic sectors, offering substantial storage capabilities to service providers and end-users. However, the security of data within cloud environments remains a paramount concern, primarily due to insufficient access controls. This concern is addressed herein through a systematic literature review, which underscores the shared responsibility model, the ubiquitous nature of data access, and the associated breach risks. The importance of enforcing rigorous security protocols to maintain data integrity, confidentiality, and availability is emphasized, as these are vital to stakeholders relying on cloud-based services. The current work presents an analytical review of two-factor authentication (2FA) and cryptographic measures, advocating for their combined implementation to bolster the security frameworks of cloud systems. The survey meticulously examines existing research on cloud computing vulnerabilities, security mechanisms, and the underlying challenges. It is posited that ongoing enhancements and innovations in security practices are critical for countering evolving threats and safeguarding data in an increasingly digital landscape. An exhaustive evaluation of the latest advancements in cryptography is conducted, aiming to ensure secure and authenticated access control for outsourced and encrypted cloud data across diverse user groups. The findings herein serve as a foundation for researchers to refine and develop robust cloud data storage systems. The survey underscores the need for the creation of advanced encryption algorithms, authentication protocols, and intrusion detection systems. Such developments are instrumental in mitigating risks, establishing trust, and preserving the confidentiality and integrity of data stored in cloud infrastructures.",CS,AI_ML,0.85,Extracted from log - paper 2011
Categorizing Defects in Infrastructure as Code,"Infrastructure as code (IaC) scripts are used to automate the maintenance and con guration of software development and deployment infrastructure. IaC scripts can be complex in nature and can contain hundreds of lines of code, leading to defects that can be di cult to debug and to wide-scale system discrepancies, such as service outages. The use of IaC scripts is getting increasingly popular, yet the nature of defects that occur in these scripts have not been systematically categorized. The goal of this paper is to help software practitioners improve their development process of infrastructure as code (IaC) scripts by analyzing the defect categories in IaC scripts based upon a qualitative analysis of commit messages and issue report descriptions. We collect 12,875 commits that map to 2,424 IaC scripts from four organizations, namely Mirantis, Mozilla, Openstack, and Wikimedia Commons. With 89 raters, we apply the defect type attribute of the orthogonal defect classi cation (ODC)",CS,AI_ML,0.85,Extracted from log - paper 2012
"Space Shuffle: A Scalable, Flexible, and High-Performance Data Center Network","The increasing need of cloud and big data applications requires data center networks to be scalable and bandwidth-rich. Current data center network architectures often use rigid topologies to increase network bandwidth. A major limitation is that they can hardly support incremental network growth. Recent work has been investigating new network architecture and protocols to achieve three important design goals of large data centers, namely, high throughput, routing and forwarding scalability, and flexibility for incremental growth. Unfortunately, existing data center network architectures focus on one or two of the above properties and pay little attention to the others. In this paper, we design a novel flexible data center network architecture, Space Shuffle (S2), which applies greedy routing on multiple ring spaces to achieve high-throughput, scalability, and flexibility. The proposed greedy routing protocol of S2 effectively exploits the path diversity of densely connected topologies and enables key-based routing. Extensive experimental studies showthat S2 provides high bisectional bandwidth and throughput, near-optimal routing path lengths, extremely small forwarding state, fairness among concurrent data flows, and resiliency to network failures.",CS,AI_ML,0.85,Extracted from log - paper 2013
DEFENSE-IN-DEPTH METHODS IN MICROSERVICES ACCESS CONTROL,"More and more application deployments are moving towards leveraging the microservice paradigm in hopes of increased efficiency of operations and more flexible software development. Microservices are not a straightforward successor of existing methods and they introduce a lot of new complexity. Especially security concerns lack analysis in academic literature and new developments have mostly been assessed in grey literature. The thesis explores the solutions to increase the security of microservice applications hosted in virtual private clouds. We start with the assumption that the networking security controls have been bypassed and the adversary is inside the network. We look at the situation through a holistic lens to identify the biggest gaps and how they can be filled in REST service-to-service communications. The solutions are platform agnostic to support the multi-cloud paradigm to reduce operational costs and increase global coverage. Defense-in-depth methods proposed are establishing mutually authenticated TLS connections between services comprising an application and introducing granular access control using cryptographically secure methods. The industry state of the art ways to achieve these are assessed and analyzed comparatively and against good security engineering design principles. Both methodologies and their practical implementations are explored. We assess two distinct models for reference use for secure architecture design in microservices. These models piece lower level pieces into a comprehensive idea of what good microservice security looks like. The architectures can be used as is, as a basis for designing secure application architectures. The thesis introduces security analysis of existing methods of deploying and establishing secure microservice applications, from container level orchestration to high level architectural choices. The work adds to the existing body of knowledge by assessing some of the security concerns enterprises moving towards microservice deployments are facing and by providing a new analysis of industry developments that have not been looked at thoroughly through a security lens in scientific literature.",CS,AI_ML,0.85,Extracted from log - paper 2014
"A survey on cloud computing security: Issues, threats, and solutions","Over the internet, the cloud computing reveals a remarkable potential to provide on-demand services to consumers with greater flexibility in a cost effective manner. While moving towards the concept of on-demand service, resource pooling, shifting everything on the distributive environment, security is the major obstacle for this new dreamed vision of computing capability. This survey present a comprehensive overview of the security issues for different factors affecting cloud computing. Furthermore, a detailed discussion on several key topics regarding embedded system, application, storage system, clustering related issues and many more. This paper works on some public cloud and private cloud authorities as well as related security concerns. Additionally, it encompasses the requirements for better security management and suggests 3-tier security architecture. Open issues with discussion in which some new security concepts and recommendations are also provided.",CS,AI_ML,0.85,Extracted from log - paper 2015
Performance Evaluation of Genetic Algorithm Selection Methods in Outlier Detection: Further Analysis,"Feature selection is very crucial in the activities of soft computing algorithms for quality, precision and accuracy. This paper evaluates the performance of some feature selection methods of Genetic Algorithm in outlier detection on fingerprint images. Roulette wheel, Rank and Tournament methods were considered for feature selection and selected features were enhanced using histogram equalization. K-nearest neighbor algorithm was employed for classification to detect outliers. The implementation of the experiment was carried out in Matrix laboratory environment. Performance of the selection methods were evaluated based on metrics of accuracy, specificity, precision and computation time.",CS,AI_ML,0.85,Extracted from log - paper 2016
An algorithm for sizing software products,This paper reports on efforts to develop a cost forecasting scheme based on a Function Metric called System BANG. A Function Metric is a quantifiable indication of system system size and complexity derived directly from a formal statement of system requirement. Conclusions from a small sample of projects are presented.,CS,AI_ML,0.85,Extracted from log - paper 2017
"Performance Evaluation of Capuchin Search Algorithm Through Non-linear Problems, and Optimization of Gear Train Design Problem","The purpose of this paper is to demonstrate the superiority of the Capuchin Search Algorithm (CapSA), a metaheuristic, in competitive environments and its advantages in optimizing engineering design problems. To achieve this, the CEC 2019 function set was used. Due to the challenging characteristics of the CEC 2019 function set in reaching a global solution, it effectively showcases the algorithm's quality. For this comparison, sea-horse optimizer (SHO), grey wolf optimizer (GWO), sine-cosine algorithm (SCA), and smell agent optimization (SAO) were chosen as current and effective alternatives to the CapSA algorithm. Furthermore, the gear train design problem (GTD) was selected as an engineering design problem. In addition to the CapSA algorithm, a hybrid of SCA and GWO algorithm (SC-GWO) and genetic algorithm (GA) were chosen as alternatives for optimizing this problem. The performance superiority and optimization power of the CapSA algorithm were assessed using statistical metrics and convergence curves, then compared with alternative algorithms. Experimental results conclusively demonstrate the significant effectiveness and advantages of the CapSA algorithm.",CS,AI_ML,0.85,Extracted from log - paper 2018
Performance Evaluation of Genetic Algorithm-Driven Blockchain Encryption for EHR Management and Validation,"In the realm of electronic health record (EHR) management, ensuring robust security and validation mechanisms is paramount due to the sensitive nature of healthcare data. This research focuses on the performance evaluation of a genetic algorithm-driven blockchain encryption approach for enhancing EHR security and validation. The proposed method leverages genetic algorithms to optimize encryption parameters within a blockchain framework, aiming to safeguard patient privacy and prevent unauthorized access. By integrating advanced cryptographic techniques like Elliptic Curve Cryptography (ECC) and Keyed-Hash Message Authentication Code (HMAC)-based authentication, along with machine learning for data classification. The evaluation of the approach holds significant promise in advancing secure EHR management practices, addressing critical challenges in data privacy and integrity within healthcare environments. Finally, as a result, this study presents a comparative analysis of cryptographic systems genetic algorithm-driven blockchain encryption (GADBE)+ECC and GADBE+ Advanced Encryption Standard (AES), focusing on the scaling of encryption and decryption times relative to key sizes and data volumes. Results show that both systems exhibit increasing times with larger key sizes and data sizes. ECC consistently demonstrates superior speed over AES, with decryption times ranging from 0.4 to 3.5 seconds for key sizes from 128 to 512 bits, indicating potential performance advantages of ECC in cryptographic applications.",CS,AI_ML,0.85,Extracted from log - paper 2019
Application of ID3 Algorithm in Logistics Performance Evaluation,"It is of vital importance to evaluate logistics performance of logistics company along with fiercer competition. ID3 algorithm is an effective method. In this paper, fisrtly we introduce ID3 algorithm. Then we apply it to evaluate logistics performance of a logistics company.",CS,AI_ML,0.85,Extracted from log - paper 2020
Performance analysis of an algorithm for concurrency control in replicated database systems,"In this paper, we analyze the performance of a concurrency control algorithm for replicated database systems. We present a model of a distributed database system which provides a framework to study the performance of different concurrency control algorithms. We discuss performance criteria to evaluate different algorithms. We use the model to analyze the performance of an algorithm for concurrency control in replicated database systems. The technique used in analysis is iterative and approximate. We plot a set of performance measures for several values of the model parameters. The results of analysis are compared against a simulation study.",CS,AI_ML,0.85,Extracted from log - paper 2021
A time-stamping algorithm for efficient performance estimation of superscalar processors,"The increasing complexity of modern superscalar microprocessors makes the evaluation of new designs and techniques much more difficult. Fast and accurate methods for simulating program execution on realistic and hypothetical processor models are of great interest to many computer architects and compiler writers. There are many existing techniques, from profile based runtime estimation to complete cycle-level simulations. Many researchers choose to sacrifice the speed of profiling for the accuracy obtainable by cycle-level simulators. This paper presents a technique that provides accurate performance predictions, while avoiding the complexity associated with a complete processor emulator. The approach augments a fast in-order simulator with a time-stamping algorithm that provides a very good estimate of program running time. This algorithm achieves an average accuracy that is within 7.5% of a cycle-level out-of-order simulator in approximately 41% of the running time on the eight SPECInt95 integer benchmarks.",CS,AI_ML,0.85,Extracted from log - paper 2022
The k-means Algorithm: A Comprehensive Survey and Performance Evaluation,"The k-means clustering algorithm is considered one of the most powerful and popular data mining algorithms in the research community. However, despite its popularity, the algorithm has certain limitations, including problems associated with random initialization of the centroids which leads to unexpected convergence. Additionally, such a clustering algorithm requires the number of clusters to be defined beforehand, which is responsible for different cluster shapes and outlier effects. A fundamental problem of the k-means algorithm is its inability to handle various data types. This paper provides a structured and synoptic overview of research conducted on the k-means algorithm to overcome such shortcomings. Variants of the k-means algorithms including their recent developments are discussed, where their effectiveness is investigated based on the experimental analysis of a variety of datasets. The detailed experimental analysis along with a thorough comparison among different k-means clustering algorithms differentiates our work compared to other existing survey papers. Furthermore, it outlines a clear and thorough understanding of the k-means algorithm along with its different research directions.",CS,AI_ML,0.85,Extracted from log - paper 2023
Fuzzy Comprehensive Evaluation Algorithm and Application of Supply Chain Performance,"This article embarks from the connotation of supply chain performance and its evaluation indexes’ present research situation, selects the key indexes of supply chain performance evaluation, constructs indexes system of supply chain performance evaluation, and uses fuzzy comprehensive evaluation algorithm to make appraisal on the key indexes. Finally through computation and analysis of specific cases to show this appraisal method is helpful for enterprise improving their overall supply chain performance.",CS,AI_ML,0.85,Extracted from log - paper 2024
Quantitative Performance Evaluation Technology Based on Sparse Oblique Trees Algorithm in Mobile Internet,"The quantitative performance evaluation is a method for evaluating an employee based on measurable factors directly related to their job. Many professionals in the modern workforce are no longer able to easily and consistently access desktop computers due to their increasingly itinerant lifestyle. Employees must be able to access contemporary employee performance management programs via mobile internet for them to be completely inclusive. Problems with the performance evaluation include the subjective one-sidedness of the evaluation indicators and the challenging measurement of the indicators. To address this drawback, the methodology used in this research was provided in order to classify employee data more precisely. The data is gathered from 227 enterprises in South Korea. At the pre-processing stage, the adaptive robust cubature kalman filtering is used to pre-process the data. The employee discipline, employee attitude, employee effort are successfully classified using sparse oblique trees algorithm (SOTA).To increase the SOTA, the neural network's weight parameter is optimized using the Sea Lion optimization (SLO).The proposed QPET-SOTA-SLO applied in MATLAB/Simulink platform. The proposed method was calculated using performance measures like accuracy, precision, sensitivity, computation time, and recall. Higher accuracy of 16.65%, 18.85%, and 17.89%, as well as higher sensitivity of 16.34%, 12.23%, and 18.54%, are achieved by the suggested QPET-SOTA-SLO approach. In comparison to the current approach, there are 14.89%, 16.89%, and 18.23% as well as 82.37%, 94.47%, and 87.76% less computing time.",CS,AI_ML,0.85,Extracted from log - paper 2025
Biclustering Performance Evaluation of Cheng and Church Algorithm and Iterative Signature Algorithm,"Biclustering has been widely applied in recent years. Various algorithms have been developed to perform biclustering applied to various cases. However, only a few studies have evaluated the performance of bicluster algorithms. Therefore, this study evaluates the performance of biclustering algorithms, namely the Cheng and Church algorithm (CC algorithm) and the Iterative Signature Algorithm (ISA). Evaluation of the performance of the biclustering algorithm is carried out in the form of a comparative study of biclustering results in terms of membership, characteristics, distribution of biclustering results, and performance evaluation. The performance evaluation uses two evaluation functions: the intra-bicluster and the inter-bicluster. The results show that, from an intra-bicluster evaluation perspective, the optimal bicluster group of the CC algorithm produces bicluster quality which tends to be better than the ISA. The biclustering results between the two algorithms in inter-bicluster evaluation produce a deficient level of similarity (20-31 percent). This is indicated by the differences in the results of regional membership and the characteristics of the identifying variables. The biclustering results of the CC algorithm tend to be homogeneous and have local characteristics. Meanwhile, the results of biclustering ISA tend to be heterogeneous and have global characteristics. In addition, the results of biclustering ISA are also robust.",CS,AI_ML,0.85,Extracted from log - paper 2026
Comprehensive evaluation measures of nonlinear estimation algorithm performance,"Although many scholars say that their algorithms are better than others in the state estimation problem, only a fewer convincing algorithms were applied to engineering practices. The reason is that their algorithms outperform others only in some aspects such as the estimation accuracy or the computation load. To solve the problem of performance evaluation of state estimation algorithms, in this paper, the comprehensive evaluation measures (CEM) for evaluating the nonlinear estimation algorithm (NEA) is proposed, which can comprehensively reflect the performance of the NEAs. First, we introduce three types of the NEAs. Second, the CEM combining the flatness, estimation accuracy and computation time of the NEAs, is designed to evaluate the above NEAs. Finally, the superiority of the CEM is verified by a numerical example, which helps decision makers of nonlinear estimation algorithms theoretically and technically.",CS,AI_ML,0.85,Extracted from log - paper 2027
Research on human resource management performance evaluation based on BP algorithm,"Abstract With economic globalisation and in the face of increasingly fierce market competition, enterprises must establish an evaluation mechanism that can promote the company’s development and motivate employees to improve performance in order to achieve the company’s strategic goals. In view of the characteristics and problems of enterprise performance appraisal, a performance evaluation method based on artificial neural network (ANN) technology is proposed. This study uses BP algorithm to comprehensively evaluate the performance of enterprises and construct an evaluation network. According to the statistics of 75 power companies in the province from 2018 to 2021, the training was carried out in batches, and the 10-fold cross-validation method was used to find the smallest optimisation value of the error term (average overall relative error) of the test set. The training set is set as 70% and the test set as 30%, and the termination condition is used to end the training process when the training error is &lt;0.0001. This proves that the use of BP neural network for performance evaluation can effectively avoid the influence of subjective factors on the evaluation results, so as to establish a more objective comprehensive evaluation system.",CS,AI_ML,0.85,Extracted from log - paper 2028
Numerical Evaluation of the MU-MIMO Beamforming Performance with User Selection Algorithm,This paper presents the numerical evaluation of the ZF beamforming algorithm using the user selection in the multiuser multiantenna (MU-MIMO) downlink system. Two user selection algorithm – semiorthogonal user selection and greedy user selection are numerically evaluated based on the open source MIMO channel model. The sum rate performance depending on number of users are presented. The arising inter user correlation degrades the sum rate (spectral efficiency) performance of multiuser MIMO system especially in scenarios where the number of users is larger than the number of antennas at the BS. The selection of users is based on the orthogonality of the channels among selected users. For MIMO channel simulation the QUADRIGA channel model reflecting the real propagation conditions is used. The obtained performance of MU-MIMO ZF precoding in spatially correlated channel are compared based on the empirical cumulative density function of the sum rate of multiple users. Numerical results show that the ZF precoder using user selection (G ZF) outperforms the ZF precoder with random user selection in spectral efficiency. The greedy user selection in spatially correlated channel has advantage to semi-orthogonal user selection. It isobserved that as the increasing the number of served users used for selection the greedy user selection gives better performance than semi-orthogonal algorithm.,CS,AI_ML,0.85,Extracted from log - paper 2029
Performance evaluation of low resolution lip recognition algorithm,"Lip print recognition technology originated in the field of forensic medicine, and convolutional neural network has made breakthrough achievements in the field of pattern recognition and machine vision. Convolutional neural network (CNN) algorithm is rarely used in lip pattern recognition. Further exploration and research on the network model suitable for lip pattern recognition. Lip print recognition algorithm based on depth convolution neural network aims to solve the problems of complex image preprocessing, difficult feature extraction and low recognition efficiency in traditional lip print recognition algorithms. It includes collecting lip print images to establish data sets, selecting different CNN models to conduct performance evaluation experiments on low resolution lip print data sets, and analyzing the experimental results with model evaluation indicators.",CS,AI_ML,0.85,Extracted from log - paper 2030
Credit Fraud Recognition Based on Performance Evaluation of Deep Learning Algorithm,"Over time, the growth of credit cards and the financial data need credit models to support banks in making financial decisions. So, to avoid fraud in internet transactions which increased with the growth of technology it is crucial to develop an efficient fraud detection system. Deep Learning techniques are superior to other Machine Learning techniques in predicting the customer behavior of credit cards depending on the missed payments probability of customers. The BiLSTM model proposed to train on Taiwanese non-transactional dataset for bank credit cards to decrease the losses of banks. The Bidirectional LSTM reached 98% accuracy in fraud credit detection compared with other Machine Learning techniques.",CS,AI_ML,0.85,Extracted from log - paper 2031
Strategic Financial Performance Evaluation of the Iranian automotive industry Using Imperialist competitive Algorithm,"Purpose. This paper evaluates strategic financial performance of 10 Iranian stock exchange listed automotive companies over the period 2005-2009 at the hand of value-creating performance indicators and the Free Cash Flow derived value indicators.Design. To this effect, profiting from the Imperialist Competitive Algorithm (ICA), the understudy companies were assigned to three clusters in terms of debt structure, firm size, and growth opportunitiesFindings. The results, in general, indicate a significant correlation between value-based indicators Economic Value Added (EVA) and True Value Added (TVA) and the FCF-derived indicators Created Value from Free Cash Flow to Firm (CVFCFF) and Created Value from Free Cash Flow to Equity (CVFCFE), and between Market Value Added (MVA) and CVFCFF (one of the two FCF-derived indicators), while, according to the results, there is no significant correlation between the value-driven performance indicators Refined Economic Value Added (REVA) and Equity Economic Value Added (EEVA) and either of the FCF-derived indicators CVFCFF and CVFCFE.Originality. Present Paper, by company clustering in ICA environment, the companies are clustered based on their close similarity in all three criteria and subsequently for examination of each strategic performance indicator were subjected to correlation and Fisher (F) tests.",CS,AI_ML,0.85,Extracted from log - paper 2032
Market Network Marketing Performance Evaluation in a Smart City Based on Fuzzy Algorithm,"ABSTRACT In order to overcome the problems of long time-consuming evaluation results generation processes, low accuracy of evaluation results, and poor self-adaptability existing in traditional market network marketing performance evaluation models, this study designed a market network marketing performance evaluation model based on fuzzy algorithms. First, complete the design of the overall deployment architecture of the model by designing the terminal layer, server layer, and data layer. Among them, the work efficiency of the model is improved by setting a web server. Then, based on the analysis and evaluation principles, four first-level evaluation indicators and 18 second-level indicators are constructed, and then the evaluation of market network marketing performance is completed by designing fuzzy rules and the operation process of fuzzy algorithms, and combining the index weights. Experimental verification shows that the generation process of the evaluation results of this model is less time-consuming, the evaluation results are highly accurate, and the adaptive ability is strong, which proves that it has achieved the design expectations.",CS,AI_ML,0.85,Extracted from log - paper 2033
Theory building for empirical software engineering in qualitative research: Operationalization,"Context: This work is part of a research project whose ultimate goal is to systematize theory building in qualitative research in the field of software engineering. The proposed methodology involves four phases: conceptualization, operationalization, testing, and application. In previous work, we performed the conceptualization of a theory that investigates the structure of IT departments and teams when software-intensive organizations adopt a culture called DevOps. Objective: This paper presents a set of procedures to systematize the operationalization phase in theory building and their application in the context of DevOps team structures. Method: We operationalize the concepts and propositions that make up our theory to generate constructs and empirically testable hypotheses. Instead of using causal relations to operationalize the propositions, we adopt logical implication, which avoids the problems associated with causal reasoning. Strategies are proposed to ensure that the resulting theory aligns with the criterion of parsimony. Results: The operationalization phase is described from three perspectives: specification, implementation, and practical application. First, the operationalization process is formally defined. Second, a set of procedures for operating both concepts and propositions is described. Finally, the usefulness of the proposed procedures is demonstrated in a case study. Conclusions: This paper is a pioneering contribution in offering comprehensive guidelines for theory operationalization using logical implication. By following established procedures and using concrete examples, researchers can better ensure the success of their theory-building efforts through careful operationalization.",CS,AI_ML,0.85,Extracted from log - paper 2034
Evaluation Algorithm of TV Program Host Performance Based on Emotion Recognition,"With the rapid development of information technology and system norms and the incongruity of personnel training speed, the development of news media reports has caused a large number of negative effects. From the current situation of the development of news media reports, due to the lack of perfect management, the difference in media literacy between users and enterprises, entertainment to death, rampant consumer culture, and many other factors, the network media reports are chaotic. In the proposed model Fuzzy Fredholm Integral Market Efficiency (FFI-ME) for the media coverage. With the FFI-ME model, the information technology computes the news media data for the estimation of the coverage to achieve market efficiency. The FFI-ME model uses the Fredholm Integral model to compute the market efficiency for the media data in China. The FFI-ME model computes the news data in China and clusters the data for the classification and detection of the instances in the equation. Through the Integral Fredholm model, the features of the news are estimated to compute the media coverage and efficiency of the news media data. The model uses the Deep learning model for the classification of the data instance in the media data. The simulation analysis expressed that the proposed FFI-ME model achieves a higher classification data accuracy of the 98%.",CS,AI_ML,0.85,Extracted from log - paper 2035
Performance evaluation of M-ary algorithm using reprogrammable hardware,"Se han encontrado diversas formas de realizar cifrado de datos, y una de las funciones involucradas en algoritmos estándar como el RSA es la exponencial modular. Básicamente, el algoritmo RSA utiliza algunas propiedades de la aritmética modular para cifrar y descifrar textos planos, con cierta dependencia en la longitud del texto. El crecimiento en la capacidad de cómputo ha creado la necesidad de utilizar sistemas robustos que puedan realizar cálculos con números significativamente grandes, y la formulación de procedimientos enfocados en mejorar la velocidad para lograrlo. Uno de éstos es el algoritmo M-ary para la ejecución de la función exponencial modular. Este artículo describe una implementación de este algoritmo en hardware reprogramable (FPGA) para evaluar su desempeño.La primera sección introduce el algoritmo M-ary. La segunda, usa descripción en bloques para comprender la implementación. La tercera, muestra los resultados en diagramas de tiempo, y finalmente, la última sección expone conclusiones.",CS,AI_ML,0.85,Extracted from log - paper 2036
Application performance pitfalls and TCP's Nagle algorithm,"Performance improvements to networked applications can have unintended consequences. In a study of the performance of the Network News Transport Protocol (NNTP), the initial results suggested it would be useful to disable TCP's Nagle algorithm for this application. Doing so significantly improved latencies. However, closer observation revealed that with the Nagle algorithm disabled, the application was transmitting an order of magnitude more packets. We found that proper application buffer management significantly improves performance, but that the Nagle algorithm still slightly increases mean latency. We suggest that modifying the Nagle algorithm would eliminate this cost.",CS,AI_ML,0.85,Extracted from log - paper 2037
Evaluation of corporate financial performance based on bionic algorithm and biomechanical analysis,"The evaluation of corporate financial performance plays a critical role in driving enterprise transformation and fostering industrial development. To enhance the accuracy of financial performance evaluation, this study integrates knowledge from biomechanics and bioinformatics, exploring the application of a bio-inspired immune algorithm-optimized convolutional neural network (CNN) in financial performance evaluation. A biomechanics-based model is constructed using CNN to simulate the “mechanical response” of financial performance evaluation. By simulating the structure of biological visual systems, CNNs can effectively extract local features from input data, enabling efficient classification and recognition. During the optimization process, the biological immune algorithm adjusted hyperparameters such as the learning rate and kernel size through mechanisms of selection, reproduction, and mutation. The application of biologically inspired algorithms in deep learning effectively enhanced the model’s adaptability and robustness, providing new ideas and methods for financial performance evaluation and validating the effectiveness of bionic algorithms in complex tasks. In the experiments, a GRA-Entropy-SOM-CNN model was constructed, with initial test results showing an accuracy of 97.18% in the task. However, by introducing the biological immune algorithm to optimize the CNN, the final model achieved an accuracy of 98.5% on the test set, demonstrating significant performance improvement.",CS,AI_ML,0.85,Extracted from log - paper 2038
Comparison and Performance Evaluation of Boundary Fill and Flood Fill Algorithm,"In computer graphics, there are many polygon filling algorithms available like as inside and outside test, scan line method, boundary fill, flood fill, edge fill and fence fill algorithm. In this paper we focus on the flood fill and boundary fill algorithms for both four connected as well as eight connected pixels. This research paper shows the comparison and performance evaluation of boundary fill and flood fill algorithms with consideration of running time in C-language. And also shows the how stack affects the performance of the system due to overflow of buffer later on JAVA implementation with queue improves the system performance for large polygon. It also shows the literature review of various papers use the fill algorithm on different applications, some proposed the new and enhanced polygon fill algorithms.",CS,AI_ML,0.85,Extracted from log - paper 2039
Study on the Algorithm for Evaluation of Converged Network Performance Based on QoE,"In converged networking, the evaluation of network performance is completed by analyzing the key performance indexes which can reflect the states of network from the point of user view and integrated results should be evaluated by user or quality of experiences. This paper proposes the algorithm for evaluation of network performance based on the fuzzy analytic hierarchy process that utilizes strategy data and key performance indexes to calculate the integrated results of evaluation. The result provides the necessary reference for the real-time monitoring of network.",CS,AI_ML,0.85,Extracted from log - paper 2040
Research and performance analysis of random forest-based feature selection algorithm in sports effectiveness evaluation,"AbstractThe rapid progress in fields such as data mining and machine learning, as well as the explosive growth of sports big data, have posed new challenges to the research of sports big data. Most of the available sports data mining techniques concentrates on extracting and constructing effective features for basic sports data, which cannot be achieved simply by using data statistics. Especially in the targeted mining of sports data, traditional mining techniques still have shortcomings such as low classification accuracy and insufficient refinement. In order to solve the problem of low accuracy in traditional mining methods, the study combines the random forest algorithm with the artificial raindrop algorithm, and adopts a sports data mining method based on feature selection to achieve effective analysis of sports big data. This study is based on the evaluation method of motion effects using random forests, and uses feature extraction algorithms to study the motion effect impacts. It uses the information gain index to rank the importance of features and accurately gain the degree of influence of exercise on various indicators of the human body. Through simulation verification, the algorithm proposed by the research institute performs the best in accuracy and FI scores on the training and testing sets, with accuracies of 0.849 ± 0.021 and 0.819 ± 0.022, respectively, and F1 scores of 0.837 ± 0.020 and 0.864 ± 0.021, respectively. This indicates that the algorithm proposed by the research institute has high classification accuracy and performance proves that the Random Forest-based feature selection algorithm established in this study is superior to the existing traditional feature extraction and extraction methods in terms of both performance and accuracy. The proposal of this data analysis method has achieved accurate and efficient utilization of sports big data, which is of great significance for the development of the sports education industry.",CS,AI_ML,0.85,Extracted from log - paper 2041
Performance evaluation of RUNT algorithm,"This paper evaluates the performance of Reduction to Uniprocessor Transformation (RUNT) with Voltage and Frequency Scaling, called Static RUNT (S-RUNT) and Dynamic RUNT (D-RUNT), respectively. Simulation results show that how to assign tasks to servers in RUNT influences energy consumption and the worst-fit heuristic is the best in many cases. In addition, the idle task assignment policy saves more energy consumption in D-RUNT and D-RUNT outperforms S-RUNT if the actual case execution time of each task is shorter than its worst case execution time.",CS,AI_ML,0.85,Extracted from log - paper 2042
Performance Evaluation Model of Corporate Financial Sustainability based on Swarm Algorithm,"In traditional financial performance evaluation models, parameter settings are often too large or too small, resulting in significant model errors. To address this issue, an improved artificial bee colony algorithm was proposed and applied to optimize the parameters of performance evaluation models. This method first constructs a corporate financial performance evaluation system, and then improves the artificial bee colony algorithm with differential evolution algorithm to optimize the parameters of the long short-term memory network, in order to improve the accuracy of the long short-term memory network in corporate financial performance evaluation. The results showed that the improvement of the ABC algorithm was effective. The improved ABC algorithm converged on the Ackley function in the 800th iteration, and the ABC algorithm converged in the 1400th iteration. The evaluation error of the proposed method is the lowest, with the algorithm having the lowest four errors of -0.0121, 0.0453, 0.0683, and 0.0047, respectively. Among the other algorithms, the comprehensive error of the financial performance evaluation model based on Long Short Term Memory (LSTM) network is relatively low, but still lower than the algorithm proposed in the study. The research proposes a long short-term memory network optimized based on improved artificial bee colony algorithm, which can accurately evaluate the financial performance of enterprises, help them review their own development level, and clarify their future development direction.",CS,AI_ML,0.85,Extracted from log - paper 2043
E-COMMERCE INFORMATION SYSTEM USING TECHNOLOGY ACCEPTANCE MODEL APPROACH,"E-commerce websites are one of the most accessed in Indonesia during the covid-19 pandemic. In research conducted for clothing product brands from PT. Mong Kreasi Indonesia, the problem that occurs is the company has not made full use of the information system. The company only uses media social like Instagram and WhatsApp for sales and communication order. Processing and recording of data and transactions are also still manually. The purpose of this research is to design an e-commerce sales information system, that is useful for solving existing problems. To measure the achievement of the designed system, a questionnaire was distributed to the respondents. Total of 70 questionnaires were answered and then tested using the Technology Acceptance Model method using the SMART PLS application. The results is variable Attention to Use on Behavioral Intention to Use has an influence level of 5.063, Perceived Ease of Use on Attention to Use has an influence level of 2.569, Perceived Usefulness on Attention to Use has an influence level of 3.382, and Perceived Usefulness on Behavioral Intention to Use has an influence level of 2.352. Each value is above 1.996, this means each variable has a positive and significant effect in benefits, easy to use, gives satisfaction to users and users has the intention to recommend this system to another.",CS,AI_ML,0.85,Extracted from log - paper 2044
An algorithm for approximate counting using limited memory resources,"This paper describes a randomized algorithm for approximate counting that preserves the same modest memory requirements of log(log n) bits per counter as the approximate counting algorithm introduced in the seminal paper of R. Morris (1978), and in addition, is characterized by (i) lower expected number of memory accesses and (ii) lower standard error on more than 99 percent of its counting range. An exact analysis of the relevant statistical properties of the algorithm is carried out. Performance evaluation via simulations is also provided to validate the presented theory. Given its properties, the presented algorithm is suitable as a basic building block of data streaming applications having a large number of simultaneous counters and/or operating at very high speeds. As such, it is applicable to a wide range of measurement and monitoring operations, including performance monitoring of communication hardware, measurements for optimization in large database systems, and gathering statistics for data compression.",CS,AI_ML,0.85,Extracted from log - paper 2045
"Performance evaluation of black hole algorithm, gravitational search algorithm and particle swarm optimization","Particle Swarm Optimization (PSO) and Gravitational Search Algorithm are a well known population-based heuristic optimization techniques. PSO is inspired from a motion flock of birds searching for a food. In PSO, a bird adjusts its position according to its own ‘‘experience’’ as well as the experience of other birds. Tracking and memorizing the best position encountered build bird’s experience which will leads to optimal solution. GSA is based on the Newtonian gravity and motion laws between several masses. In GSA, the heaviest mass presents an optimum solution in the search space. Other agents inside the population are attracted to heaviest mass and will finally converge to produce best solution. Black Hole Algorithm (BH) is one of the optimization technique recently proposed for data clustering problem. BH algorithm is inspired by the natural universe phenomenon called ""black hole”. In BH algorithm, the best solution is selected to be the black hole and the rest of candidates which are called stars will be drawn towards the black hole. In this paper, performance of BH algorithm will be analyzed and reviewed for continuous search space using CEC2014 benchmark dataset against Gravitational Search Algorithm (GSA) and Particle Swarm Optimization (PSO). CEC2014 benchmark dataset contains 4 unimodal, 7 multimodal and 6 hybrid functions. Several common parameters has been chosen to make an equal comparison between these algorithm such as size of population is 30, 1000 iteration, 30 dimension and 30 times of experiment.",CS,AI_ML,0.85,Extracted from log - paper 2046
Performance Evaluation of Machine Learning Algorithm in Various Datasets,"Machine learning is one of the fast-growing areas of computer science, with far-reaching applications. There are several applications for machine learning. The most significant of which is supervised learning. Supervised learning is common in classification problems. In this study, frequently used twelve machine learning algorithms are considered: NB, LDA, LR, ANN, SVM, K-NN, HT, DT, C4.5, CART, RF and BB. We apply these algorithms on seven datasets. The main goal of this study was to evaluate the performance of the machine learning algorithms on both binary and multiple classification problems using a variety of performance metrics: accuracy, kappa statistic, precision, recall, specificity, F-measure, MAE, RMSE and MCC. Here, we found that RF algorithm proved to have the best performance in three out of seven datasets. But the other four algorithms: NN, NB, BB and LR also performed well.",CS,AI_ML,0.85,Extracted from log - paper 2047
Performance evaluation of proposed load balancing algorithm with unstable concurrent programs,"&lt;span&gt;IoT is the succeeding cohort of the digital computing environment. A swift progression in the IoT deployment and its applications are on the rise. Improving load balancing mechanisms induces healthier performance of the internet based computing as higher number of users can be comfortable. Implementing full services for tasks with unstable concurrency is an uphill process. One of the encounters allied with this administration is the task partition among the applications, regularly referred as concurrent programs. Through load balancing not only resources are equally utilized but also concurrent job’s response time can be promoted. Therefore, in this paper the widely used load balancing algorithms are investigated and yet the proposed algorithm is introduced. Simulation is employed in order to compare the performance metrics such as mean queue length, utilization and throughput between the recommended and existing algorithms. The proposed algorithm confirms the load balancing and outperforms when processing unstable concurrent programs.&lt;/span&gt;",CS,AI_ML,0.85,Extracted from log - paper 2048
Distribution of Consensus in a Broadcasting-based Consensus-forming Algorithm,"The consensus achieved in the consensus-forming algorithm is not generally a constant but rather a random variable, even if the initial opinions are the same. In the present paper, we investigate the statistical properties of the consensus in a broadcasting-based consensus-forming algorithm. We focus on two extreme cases: consensus forming by two agents and consensus forming by an infinite number of agents. In the two-agent case, we derive several properties of the distribution function of the consensus. In the infinite-numberof- agents case, we show that if the initial opinions follow a stable distribution, then the consensus also follows a stable distribution. In addition, we derive a closed-form expression of the probability density function of the consensus when the initial opinions follow a Gaussian distribution, a Cauchy distribution, or a L´evy distribution.",CS,AI_ML,0.85,Extracted from log - paper 2049
Performance Evaluation of Lane Detection and Tracking Algorithm Based on Learning-Based Approach for Autonomous Vehicle,"Disruptive technology, especially autonomous vehicles, is predicted to provide higher safety and reduce road traffic emissions. Lane detection and tracking are critical building blocks for developing autonomous or intelligent vehicles. This study presents a lane detecting algorithm for autonomous vehicles on different road pavements (structured and unstructured roads) to overcome challenges such as the low detection accuracy of lane detection and tracking. First, datasets for performance evaluation were created using an interpolation method. Second, a learning-based approach was used to create an algorithm using the steering angle, yaw angle, and sideslip angle as inputs for the adaptive controller. Finally, simulation tests for the lane recognition method were carried out by utilising a road driving video in Melbourne, Australia, and the BDD100K dataset created by the Berkeley DeepDrive Industrial Consortium. The mean detection accuracy ranges from 97% to 99%, and the detection time ranges from 20 to 22 ms under various road conditions with our proposed algorithm. This lane detection algorithm outperformed conventional techniques in terms of accuracy and processing time, as well as efficiency in lane detection and overcoming road interferences. The proposed algorithm will contribute to advancing the lane detection and tracking of intelligent-vehicle driving assistance and help further improve intelligent vehicle driving safety.",CS,AI_ML,0.85,Extracted from log - paper 2050
Image Fusion Algorithm Based on Contrast Pyramid and its Performance Evaluation,"Contrast pyramid algorithm is put forward in this paper. The human visual system is sensitive to contrast information of image, so contrast pyramid algorithm would outstanding the contrast of image. The algorithm consists of creation process of Gauss Pyramid, the process of creating contrast Pyramid and reconstruction process of clear image. Simulation by MATLAB was completed in multi-focus image, multi-modality image and color image. Objective evaluation index such as mean, standard deviation, entropy and average gradient was calculated Simulation results and index show that the contrast pyramid algorithm has advantage of projecting the contrast of image, especially in color image fusion.",CS,AI_ML,0.85,Extracted from log - paper 2051
Performance evaluation and design of B‐128 modified Blowfish algorithm,"AbstractThe field of information security has many uses in the modern day and beyond. Encryption is a method used to secure information from unauthorized access. Since symmetric key algorithms can decrypt data much more quickly than asymmetric key algorithms, the former are more popular. Blowfish is an unpatented, freely useable, compact, quick, and efficient symmetric key encryption technique. Additionally, this method has a high level of security. The size of its blocks (64 bits) is limiting its use, though. The paper aims to propose a modified version of the Blowfish algorithm that performs high‐speed encryption with high throughput and supports 128‐bit block size, enhancing its applicability in various areas. The algorithm can be an alternative to the AES algorithm with limited power consumption. The proposed algorithm is compared with the original Blowfish algorithm based on execution speed, throughput, and the avalanche effect. The algorithm's performance is also evaluated on images based on diffusion properties, image histogram, entropy, and correlation coefficient.",CS,AI_ML,0.85,Extracted from log - paper 2052
Correlation Between Scientific and Technological Innovation and Financial Performance of Seed Enterprises: Application of Principal Component Analysis Algorithm in Performance Evaluation,"This study explores the relationship between scientific and technological innovations and the financial performance of startups, focusing on a representative Artificial Intelligence (AI) healthcare startup. Utilizing Principal Component Analysis (PCA), the research aims to dissect the complex interplay between innovation-related metrics—such as R&amp;D spending, patent counts, and technology adoption rates—and financial outcomes like revenue growth, profitability, and market share. The PCA methodology enabled the reduction of high-dimensional data into PCA, which clearly illustrates how various dimensions of innovation impact financial metrics. The approach used in the investigation helps people comprehend more thoroughly how development benefits business viability in several different manners and provides startups with practical advice to use when preparing their revolutionary investments. The objective of the research is to assist the ecosystem of startup consumers (including investors, business owners, and policymakers) in making better decisions that balance technological progress with economic objectives.",CS,AI_ML,0.85,Extracted from log - paper 2053
Evaluation of Guidance Laws Performance Sing Genetic Algorithm,Several conditions affect the performance of guidance law like target parameters or delayed line of sight rate. A variable navigation ratio is used to enhance the performance of guidance law. In this paper a Genetic Algorithm is used to formulate different forms of variable gains and measure the miss distance. An optimization process is running to find the minimum miss distance. The average values and standard deviation of miss distance for all genetic algorithm individuals are calculated to measure the performance and robustness of guidance law. Two guidance laws are considered proportional navigation (PN) and differential geometry (DG). The simulation results show that the proportional navigation is superior to differential geometry performance in the presence of delayed line of sight rate.,CS,AI_ML,0.85,Extracted from log - paper 2054
Performance Evaluation of the MSMPS Algorithm under Different Distribution Traffic,"In this paper, the Maximal Size Matching with Permanent Selection (MSMPS) scheduling algorithm and its performance evaluation, under different traffic models, are described. In this article, computer simulation results under nonuniformly, diagonally and lin-diagonally distributed traffic models are presented. The simulations was performed for different switch sizes: 4×4, 8×8 and 16×16. Results for MSMPS algorithm and for other algorithms well known in the literature are discussed. All results are presented for 16×16 switch size but simulation results are representative for other switch sizes. Mean Time Delay and efficiency were compared and considered. It is shown that our algorithm achieve similar performance results like another algorithms, but it does not need any additional calculations. This information causes that MSMPS algorithm can be easily implemented in hardware.",CS,AI_ML,0.85,Extracted from log - paper 2055
Performance Evaluation Based on Information Theory for Multi-Target Tracking Algorithm,"This paper is concerned with the performance evaluation of algorithm of multi-target and target types tracking. Performance evaluation is based on information theory, Kullback-Leibler measure is used to discriminate information provided by algorithm. Through simulations, algorithm of multi-target tracking was evaluated in term of information (localization, classification, and target number components) the algorithm provide about the actual state of ground truth.",CS,AI_ML,0.85,Extracted from log - paper 2056
Evaluation of several initialization methods on arithmetic optimization algorithm performance,"Abstract Arithmetic optimization algorithm (AOA) is one of the recently proposed population-based metaheuristic algorithms. The algorithmic design concept of the AOA is based on the distributive behavior of arithmetic operators, namely, multiplication (M), division (D), subtraction (S), and addition (A). Being a new metaheuristic algorithm, the need for a performance evaluation of AOA is significant to the global optimization research community and specifically to nature-inspired metaheuristic enthusiasts. This article aims to evaluate the influence of the algorithm control parameters, namely, population size and the number of iterations, on the performance of the newly proposed AOA. In addition, we also investigated and validated the influence of different initialization schemes available in the literature on the performance of the AOA. Experiments were conducted using different initialization scenarios and the first is where the population size is large and the number of iterations is low. The second scenario is when the number of iterations is high, and the population size is small. Finally, when the population size and the number of iterations are similar. The numerical results from the conducted experiments showed that AOA is sensitive to the population size and requires a large population size for optimal performance. Afterward, we initialized AOA with six initialization schemes, and their performances were tested on the classical functions and the functions defined in the CEC 2020 suite. The results were presented, and their implications were discussed. Our results showed that the performance of AOA could be influenced when the solution is initialized with schemes other than default random numbers. The Beta distribution outperformed the random number distribution in all cases for both the classical and CEC 2020 functions. The performance of uniform distribution, Rayleigh distribution, Latin hypercube sampling, and Sobol low discrepancy sequence are relatively competitive with the Random number. On the basis of our experiments’ results, we recommend that a solution size of 6,000, the number of iterations of 100, and initializing the solutions with Beta distribution will lead to AOA performing optimally for scenarios considered in our experiments.",CS,AI_ML,0.85,Extracted from log - paper 2057
Application and algorithm optimization of music emotion recognition in piano performance evaluation,"In the current research, we integrate distinct learning modalities—Curriculum Learning (CL) and Reinforcement Learning (RL)—in an attempt to develop and optimize Music Emotion Recognition (MER) in piano performance. Classical approaches have never been successful when applied in the field of determining the degree of emotion in the music of the piano, owing to the substantial complexity required. Addressing this particular issue is the primary motivation for the present endeavour. In an approach that’s comparable to how human beings acquire information, it trains the RL agent CL in phases; such an approach improves the student’s learning model in understanding the diverse emotions expressed by musical compositions. A higher rating of performance can be achieved after learning the model to recognize emotions more effectively and precisely. A set of piano melodies with emotional content notes has been included in the EMOPIA repository for use when conducting the process of evaluation. In order to benchmark the proposed approach with different models, parameters including R2, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE) were deployed. Studies indicate that the recommended approach accurately recognizes the emotions expressed by piano-playing music. In challenging tasks like MER, the significance of implementing the CL paradigm with the RL has been emphasized using the outcomes mentioned earlier.",CS,AI_ML,0.85,Extracted from log - paper 2058
Implementation of Frequent Pattern (FP)-Growth Algorithm for Comparative Analysis on Teacher's Performance Evaluation Results,"The performance of teachers in a class has been the center of attention for the most educational sector. The primary purpose of this project was to design, develop and implement a system for comparative analysis on teacher's performance evaluation results for Saint Michael College of Caraga. The algorithm should be applied for the accurate generation and comparison of results on the existing performance evaluation system. Based on the development phase, the system was able to achieve its objective. The administrator created add user that allowed them to log in. The users are allowed to view evaluation results from certain school year and semester. It also allows users to compare results from the past academic year to the present and generate printed reports. The user can modify his/her account and set school year dates of evaluation. It was recommended that the instructor, program head, department dean, and the head of the Human Resource Development Office of the Saint Michael College of Caraga, are encouraged to implement and utilize the system. In addition, before the system should be put into full operation, training for the users should be conducted. Moreover, the system needs statistical features and a high-end support framework for data analysis to guarantee the correctness of data and rationality of evaluation results.",CS,AI_ML,0.85,Extracted from log - paper 2059
Performance Evaluation of Support Vector Machine Algorithm for Human Gesture Recognition,"Research on human motion gesture recognition has been widely used for several technological devices to support monitoring of human-computer interaction, elderly people and so forth. This research area can be observed by conducting experiments for several body movements, such as hand movements, or body movements as a whole. Many methods have been used for human motion gesture recognition in previous studies. This paper attempted to collect data of performance evaluation of support vector machine algorithms for human motion recognition. We developed research methodology that is adapted PRISMA. This methodology is consisted of four main steps for reviewing scientific articles, including identification, screening, eligibility and inclusion criteria. After we obtained result of systematic literature review. We also conducted pilot study of SVM implementation for human gesture recognition. Based on the previous study result, the accuracy performance of vector machine algorithms for body gesture dataset is between 82.88% - 99.92% and hand gesture dataset 88.24% - 95.42%. Based on our pilot experiment, recognition accuracy with the SVM algorithm for human gesture recognition achieved 94,50% (average) accuracy.",CS,AI_ML,0.85,Extracted from log - paper 2060
Performance Evaluation of Classification Algorithm for Movie Review Sentiment Analysis,"The majority of the current research on sentiment analysis, which covers topics like political reviews, movie reviews, and product reviews, has developed quickly. The classification and clustering stage of sentiment analysis research involves a number of subjects. Some of them cover text classification comparison research and algorithm performance optimization. An intricate issue in sentiment analysis research is dealing with unstructured or semi-structured data. The sentiment analysis procedure and improving the efficacy of the classifier’s algorithm are both hampered by unstructured data. In order to manage unstructured data successfully and provide accurate and relevant information, unique strategies are required. The proposed classification model performance evaluation using Support Vector Machine, Naive Bayes, K-Nearest Neighbor, and Decision Tree is specifically covered in this paper. According to the study’s findings, SVM has an accuracy rate of 96% and Naive Bayes is 86%. While the decision tree’s gain accuracy is 78 percent and the kNN classification model’s gain accuracy is 78 percent, respectively. The test results demonstrate that SVM is superior to other classification models in terms of accuracy performance.",CS,AI_ML,0.85,Extracted from log - paper 2061
Image Fusion Algorithm Based on Gradient Pyramid and its Performance Evaluation,"Image fusion algorithm based on gradient pyramid is one of the multi-scale, multi-resolution decomposition algorithms. Original image was decomposed into Gauss pyramid, after that, gradient decomposition was completed on each layer in four directions, and fusion effect was evaluated by taking using of entropy, average gradient, mean and standard deviation. Simulation results show that gradient pyramid algorithm is effective to multi-focus image and color image.",CS,AI_ML,0.85,Extracted from log - paper 2062
Scalable and Distributed Mathematical Modeling Algorithm Design and Performance Evaluation in Heterogeneous Computing Clusters,"A growing number of scalable and distributed methods are required to effectively simulate complicated events as computing needs in the research and industrial sectors keep growing. A novel approach for developing and accessing mathematically modeled methods in heterogeneous computing clusters is proposed in this study to meet this difficulty. The suggested methodology uses DRL based Parallel Computational model for the evaluation of Heterogenous computing clusters. The algorithms makes use of parallelization methods to split up the processing burden among several nodes, supporting the variety of topologies seen in contemporary computing clusters. Through the utilization of heterogeneous hardware parts such as CPUs, GPUs, and acceleration devices, the architecture seeks to maximize speed and minimize resource usage. To evaluate the effectiveness of the proposed approach, a comprehensive performance assessment is conducted. The evaluation encompasses scalability analysis, benchmarking, and comparisons against traditional homogeneous computing setups. The research investigates the impact of algorithm design choices on the efficiency and speed achieved in diverse computing environments.",CS,AI_ML,0.85,Extracted from log - paper 2063
Deep learning algorithm performance compared to experts in visual evaluation of interior vena cava collapse on ultrasound to determine intravenous ﬂuid need in dehydration management,"Objectives: To create a deep learning (DL) algorithm capable of analyzing real time ultrasound video of the inferior vena cava (IVC) for complete collapse in pediatric patients being evaluated for intravenous fluid (IVF) resuscitation. Methods: Researchers employed a VGG-16 based DL architecture, running inside a Long Short Term Memory algorithm design, to analyze prospectively obtained ultrasound video from pediatric patients presenting with dehydration to a busy urban ED, obtained for a prior clinical study. All videos were de-identified and no patient information was available. A total of 184 patient IVC ultrasound videos were used in the study. All videos were previously reviewed and graded by two blinded POCUS experts (PedEM fellow and PedEM attending with 20 years experience) and split into two categories, those showing complete (95 patients) and those with incomplete (89 patients) IVC collapse. Approximately 10% (9) patient videos were randomly removed from each original data groups to be used for algorithm testing after training was completed. A standard 80%/20% training and validation split was used on the remaining 166 patient videos for algorithm training. Training accuracy, losses and learning curves were tracked and various training parameters such as learning rates and batch sizes were optimized throughout training. As a final real world test, the DL algorithm was tasked with analyzing the 18 previously unseen, randomly selected IVC videos. Cohen’s kappa was calculated for each of the blinded POCUS reviewers and DL algorithm. Results: This DL algorithm completed analysis of each previously unseen real world test video and is the first such algorithm to analyze IVC collapse through visual estimation in real-time. The algorithm was able to deliver a collapse result prediction for all 18 test IVC videos and there were no failures. Algorithm agreement with PedEM POCUS attending was substantial with a Cohen’s kappa of 0.78 (95% CI 0.49 to 1.0). Algorithm agreement with PedEM Fellow was substantial with Cohen’s kappa of 0.66 (95% CI 0.31 to 1.0). The PEM fellow and PEM POCUS attending also had substantial agreement, yielding a Cohen’s kappa of 0.66 (95% CI 0.32 to 1.0). Conclusions: This DL algorithm developed on prospectively acquired IVC video data from patients being studied for an IVF resuscitation study proved accurate at identifying when the IVC collapsed completely in real time. There was substantial agreement with POCUS reviewers of the same videos. Such an algorithm could allow novice clinicians to rapidly identify complete IVC collapse in children and the need for IVF administration. This could expand patient access to point of care technology by enabling novices with little training to use the diagnostic tool at bedside and decide if patients require intravenous fluid administration.",CS,AI_ML,0.85,Extracted from log - paper 2064
Research on the Evaluation Index System of ERP Implementation Performance Based on the Fuzzy Algorithm,"Based on the BSC (balanced score card) and its expansion theory, this paper regards the fuzzy algorithm as the means and the empirical research of sampling of the large and medium-size enterprises as the basis, then combines the characteristics of the enterprise sectors and activities to build a set of evaluation index system of ERP implementation performance, which is objective, valid and integral. This aims to improve the success rate to implement ERP for enterprises and promote their business processes as well as the performance.",CS,AI_ML,0.85,Extracted from log - paper 2065
OPTIMIZATION OF NAÃVE BAYES BASED ON GENETIC ALGORITHM FOR PERFORMANCE EVALUATION OF LECTURERS OF PGRI WIRANEGARA UNIVERSITY,"The evaluation of educators' performance, particularly that of professors, is crucial for maintaining the high quality of instruction at the college. This study employed Bayes Methods as the basis for performance assessment, using the criteria set forth by the quality assurance unit's standards of conformity. The results of the Bayes Method will aid the institution in evaluating professors' performance, with the goal of enhancing their skills and serving as a decision-making tool. A Genetic Algorithm (GA) was proposed to optimize the NaÃ¯ve Bayes parameter value and improve accuracy. The results showed that the accuracy of the Naive Bayes was 89.93%, while the accuracy of the Naive Bayes with Genetic Algorithm was 95%, indicating that the latter method is more effective",CS,AI_ML,0.85,Extracted from log - paper 2066
Applications of picture fuzzy filters: performance evaluation of an employee using clustering algorithm,"&lt;abstract&gt;&lt;p&gt;This article defines the concepts of picture fuzzy filter, picture fuzzy grill, picture fuzzy section, picture fuzzy base, picture fuzzy subbase, picture fuzzy ultrafilter, as well as their fundamental features. Characteristics of the aforementioned concepts are addressed, and equivalence between the picture fuzzy filter and picture fuzzy grills is established. Real-world examples are offered to demonstrate the advantages of picture fuzzy filters in the classification of sets using a clustering technique. Illustration is provided to show the advantages of picture fuzzy sets and the results are compared with intuitionistic fuzzy sets. Clustering technique is applied to the picture fuzzy filter collection reduces the computational process which helps the decision makers to classify the sets with fewer iterations.&lt;/p&gt;&lt;/abstract&gt;",CS,AI_ML,0.85,Extracted from log - paper 2067
An Online Algorithm for Smoothed Online Convex Optimization,"We consider Online Convex Optimization (OCO) in the setting where the costs are m-strongly convex and the online learner pays a switching cost for changing decisions between rounds. We show that the recently proposed Online Balanced Descent (OBD) algorithm is constant competitive in this setting, with competitive ratio 3+O(1/m), irrespective of the ambient dimension. We demonstrate the generality of our approach by showing that the OBD framework can be used to construct competitive a algorithm for LQR control.",CS,AI_ML,0.85,Extracted from log - paper 2068
Performance evaluation of parallel multithreaded A* heuristic search algorithm,"Heuristic search is used in many problems and applications, such as the 15 puzzle problem, the travelling salesman problem and web search engines. In this paper, the A* heuristic search algorithm is reconsidered by proposing a parallel generic approach based on multithreading for solving the 15 puzzle problem. Using multithreading, sequential computers are provided with virtual parallelization, yielding faster execution and easy communication. These advantageous features are provided through creating a dynamic number of concurrent threads at the run time of an application. The proposed approach is evaluated analytically and experimentally and compared with its sequential counterpart in terms of various performance metrics. It is revealed by the experimental results that multithreading is a viable approach for parallel A* heuristic search. For instance, it has been found that the parallel multithreaded A* heuristic search algorithm, in particular, outperforms the sequential approach in terms of time complexity and speedup.",CS,AI_ML,0.85,Extracted from log - paper 2069
Tail Optimality of the Nudge-M Scheduling Algorithm,"Recently it was shown that the response time of First-Come- First-Served (FCFS) scheduling can be stochastically and asymptotically improved upon by the Nudge scheduling algorithm in case of light-tailed job size distributions. Such improvements are feasible even when the jobs are partitioned into two types and the scheduler only has information about the type of incoming jobs (but not their size). In this paper we introduce Nudge-M scheduling, where basically any incoming type-1 job is allowed to pass any type-2 job that is still waiting in the queue given that it arrived as one of the last M jobs. We prove that Nudge- M has an asymptotically optimal response time within a large family of Nudge scheduling algorithms when job sizes are light-tailed. Simple explicit results for the the prefactor of Nudge-M are derived as well as explicit results for the optimal parameter M. An expression for the prefactor that only depends on the type-1 and type-2 mean job sizes and the fraction of type-1 jobs is presented in the heavy traffic setting.",CS,AI_ML,0.85,Extracted from log - paper 2070
Performance Evaluation of Wavelet-Based Algorithm for Printed Circuit Board (PCB) Inspection,"Operasi perbezaan imej seringkali digunakan dalam pemeriksaan papan litar bercetak secara automatik dan tidak terkecuali juga dalam banyak aplikasi pemprosesan imej yang lain. Pelaksanaan sistem pemeriksaan ini sangat bergantung kepada kelajuan operasi ini, di mana ia adalah masalah umum berkaitan dengan pembezaan imej. Matlamat teknik kami adalah untuk memperoleh pemeriksaan secara waktu nyata dengan menggunakan ubahan wavelet. Kertas kerja ini membentangkan satu algoritma baru berasaskan wavelet untuk pembezaan imej, dimana pembezaan imej dilakukan ke atas keluaran ubahan wavelet. Keputusan pelaksanaan teknik ini ke atas imej–imej papan litar bercetak menunjukkan pembaikan yang ketara berbanding dengan pembezaan imej secara tradisional. Kata kunci: Pembezaan imej; ubahan wavelet; papan litar bercetak; masa pemeriksaan Image difference operation is frequently used in automated printed circuit board (PCB) inspection system as well as in many other image processing applications. The inspection system performance depends critically on the speed of this operation, which is a common problem related to the image difference. The goal of our technique is to achieve real time inspection using wavelet transform. This paper presents a new wavelet–based algorithm for image difference, which computes image difference to the output of the wavelet transform. The results of applying the technique to PCB images showed significant improvement on the traditional image differencing. Key words: Image difference; wavelet transform; printed circuit board; inspection time",CS,AI_ML,0.85,Extracted from log - paper 2071
Performance Evaluation of MRMED Algorithm by Monitoring BER,"In this research work, we have developed a communication system (transmitter / receiver) to control peak to average power (PAPR) with small bit error rate (BER) for a 4G system called multicode code division multiple access (MCCDMA). Proposed communication system works on modified Reed Muller encoded data (MRMED) string. In MRMED data is first encoded with Reed Muller (RM) code. Thereafter, encoded RM message is XORed with optimal binary string, which results lower Peak to Average Power ratio (PAPR). A well-known fact is that, bit error rate (BER) is the best performance measurement tool for a communication system. To check the integrity of our communication system, we have run the simulation for monitoring BER using MRMED sequence. Simulation work conducted, with multipath Rayleigh fading, Minimum Shift Keying (MSK) modulation and several orders of RM codes. Our results show that implementing MRMED sequences of the suggested MCCDMA communication structure, returns noticeable lower BER. For instance, in case of RM(1,4), that has error improvement proficiency of 3 (three) errors , returns BER = 8.2x10−5 adopting MSK, at SNR = 12dB. Similarly, for RM(2,3), which has error improvement efficiency of 0 error and shows distinct BER of 4.9x10−4 at 12dB (SNR).In addition to using simulation for checking BER performance of our communication system, we have also shown in our results that, as the error improvement capability of different RM codes surges, correspondingly we get a lower BER.",CS,AI_ML,0.85,Extracted from log - paper 2072
"An experimental determination of the ""purity"" of a trivial algorithm","Recent work in an area which might be designated as Software Physics [1,2,3,4,5,6] has suggested that the basic structure of algorithms may offer an interesting field for experimental research. Such an experiment is reported here. In an earlier paper [2], it was suggested that a ""Second Law"" might be stated as:""The internal quality, LV, of a pure algorithm is independent of the language in which it is expressed.""",CS,AI_ML,0.85,Extracted from log - paper 2073
The Transient Performance-Evaluation Model for Desalination of Seawater Based on Matrix Algorithm – a Requirement for a Ported Arabic Smart City,"ABSTRACT Based on the transient performance model for desalination of seawater based on the matrix algorithm, the desalination device is designed according to the evaporation condensation mechanism of falling film in horizontal tube, and its structure operation and operation principle are analyzed. Then, the differential equation of the transient heat-conduction problem of the seawater desalination device is constructed. On the basis of experimental data or numerical calculation results, the temperature field of structure at some time is selected to form the snapshots matrix, and a set of optimal eigenorthogonal decomposition bases is obtained by eigenorthogonal decomposition of the snapshots matrix. Finally, the finite-element discrete scheme of the nonlinear transient heat-conduction problem is reduced by using eigenorthogonal decomposition bases of the linear problem, and the lower-order ordinary differential equations of the reduced-order model for the nonlinear transient heat-conduction problem of the seawater desalination device are obtained. The ordinary differential equations of the reduced-order model are solved by solving the ordinary differential equations of the reduced-order model. The distribution of the temperature field at each time of the structure is obtained. The performance of the model is verified by numerical examples under the conditions of two-stage temperature-difference heating and only the top-level heating. The results show that the model has good accuracy and efficiency.",CS,AI_ML,0.85,Extracted from log - paper 2074
An efficient algorithm for semi-homogeneous queueing network models,"The class of product-form semi-homogeneous queueing networks is introduced as a generalization of the class of homogeneous networks, which has been considered by Balbo et al for the performance modeling of local area networks. In semi-homogeneous networks, the relative traffic intensity at the various shared resources may depend on the routing chain to which a customer belongs. We develop an efficient algorithm for the exact analysis of this class of networks. It is based on the equations which form the foundation of RECAL, a general purpose exact algorithm for multiple-chain closed queueing networks. The complexity of the algorithm is shown to be of order less than exponential in (P-1) 1/2 , where P is the number of processors (workstations) in the network. It is therefore, in general, more efficient than a direct application of either convolution, MVA or RECAL to the class of semi-homogeneous queueing networks. The algorithm presented here may be situated between the algorithms of Balbo et al and the general purpose algorithms, both in terms of its generality and efficiency.",CS,AI_ML,0.85,Extracted from log - paper 2075
Evaluation of KDP Estimation Algorithm Performance in Rain Using a Known-Truth Framework,"AbstractAccurate estimation of specific differential phase (KDP) is necessary for rain rate estimation, attenuation correction, and hydrometeor classification algorithms. There are numerous published methods to process polarimetric radar observations of propagation differential phase shift (ΦDP) and estimate KDP, but the corresponding KDP estimate uncertainty is unquantified. This study provides guidance on how commonly used KDP estimation algorithms perform in various environments. We create numerous synthetic (“true”) KDP profiles, integrate over them to obtain “smoothed” ΦDP, and then add noise typical of S-band operational weather radar measurements. Each algorithm is applied to our noisy ΦDP profiles and compared to the true KDP profile such that the errors and uncertainty are quantified. The synthetic KDP profiles are Gaussian in shape, which allows systematic variations in their magnitude and width to determine how each algorithm performs in smooth, slowly changing KDP profiles, as well as steep profiles. Results demonstrate that algorithm performance is dependent on the ΦDP field received. These results are further supported by an error analysis of each algorithm for two more complicated synthetic KDP profiles. Some KDP algorithms allow users to change various tuning parameters; a subset of these tuning parameters is tested to provide guidance on how changing these parameters impacts algorithm performance. We then provide evidence that our known-truth framework provides insight into algorithm performance in observed data through two case studies.",CS,AI_ML,0.85,Extracted from log - paper 2076
A novel algorithm combined X-ray fluorescence and Neural Network (XRF-NN) for coal ash content prediction: Algorithm design and performance evaluation,"This paper investigated a precise algorithm combining X-ray Fluorescence and Neural Network (XRF-NN) for predicting ash content. The 261 sets of XRF tests show that the 34 elements in the chosen Guqiao coal can be categorized as major, secondary and tiny elements, whose cumulative ratio were ~95%, 4-5% and &lt;1%, respectively. Referring to the machine learning theory, the construction strategy of Element-Ash dataset was determined viz. value determination → standardization → division → optimization of generalization ability. Then, the hyperparameters optimization displays that XRF-NN model with 34 inputs and 1 output was suitable to predict coal ash content, where the activation function, loss function, and optimizer were ReLU, MAE, and Momentum, respectively. After iterative training, the new XRF-NN model provides the precise prediction of coal ash content with the absolute errors between -2.0% and 2%. Moreover, the prediction accuracy rose from 57.69% to 100%, as the expected relative error increased from 1% to 5%. Furthermore, the comparisons between different prediction methods reveal that the minimum MRE of 1.22% can be obtained by XRF-NN with total elements, which was only half of those given by the conventional Multiple Linear Regression and Partial Least Squares Regression. Besides, XRF-NN model presents the root mean squared error 0.797%, the mean absolute error 0.625% and the coefficient of determination 0.999, which were significantly superior to those calculated by Dual-Energy Gamma-ray Transmission, Ploy, RFR, XGBoost and DNN model. The results of this study suggest the excellent performance of the new XRF-NN model in predicting ash content.",CS,AI_ML,0.85,Extracted from log - paper 2077
The Evaluation of Online Education Course Performance Using Decision Tree Mining Algorithm,"With the continuous development of “Internet + Education”, online learning has become a hot topic of concern. Decision tree is an important technique for solving classification problems from a set of random and unordered data sets. Decision tree is not only an effective method to generate classifier from data set, but also an active research field in data mining technology. The decision tree mining algorithm can classify the data, grasp the teaching process of the teacher, and analyze the overall performance of the students, so as to realize the dynamic management of the educational administration and help the educational administration personnel to make the right decision, with more reasonable allocation of resources. This paper evaluates students’ academic performance based on the learning behavior data of online learning, so as to intervene in students’ learning in advance, which is the key problem that needs to be solved at present. Taking students’ learning attitude, completion of homework, and attendance as factors, the paper uses decision tree technology to analyze the factors affecting students’ performance, and evaluates students’ performance. Firstly, this paper collects the high‐dimensional behavioral characteristic data of students’ online learning and conducts correlation analysis after preprocessing the behavioral characteristic data. Then, the decision tree C4.5 algorithm is used to construct a performance evaluation model. Students’ performance is evaluated by the model, and the evaluation accuracy is about 88% compared with actual performance. Finally, through the model analysis, it is concluded that the video task point completion is the most influential in students’ achievement, followed by chapter test completion and chapter test average score, and the course interaction amount and homework average score are the least influential in students’ achievement, which has a practical reference value for effectively serving online learning and teachers’ teaching.",CS,AI_ML,0.85,Extracted from log - paper 2078
Performance Evaluation of Dynamic QoS-Aware CAC (DQA-CAC) Algorithm for Broadband Networks,"Call admission control (CAC) is a technique deployed in the management of network resources in mobile broadband networks. Despite its importance, the WiMAX technology like most mobile broadband networks does not make provision for CAC, making it an open area of research. A dynamic QoS-Aware Call Admission Control (DQACAC) scheme was recently proposed to improve resource utilization and ensure QoS. Several simulation experiments were conducted to evaluate the performance of the DQA-CAC against other CAC algorithms. The results of the simulation indicated that the DQA-CAC outperformed the existing CAC schemes in terms of reduced new connection blocking rate. However, the evaluation failed to consider other performance metrics like resource utilization and throughput that are also very crucial in the real-world scenario. This paper therefore attempts to improve the performance of the DQA-CAC by investigating its performance in terms of additional metrics like the average system throughput and resource utilization that further mimic the real-world experience. A Java based discrete event simulator designed for the study was used and the simulation results indicate that the DQA-CAC algorithm outperforms the existing CAC algorithms in terms of improved per-flow throughput, average system throughput and resource utilization in addition to reducing the connection blocking rate.",CS,AI_ML,0.85,Extracted from log - paper 2079
Performance evaluation model and algorithm of green supply chain management based on sustainable computing,"Abstract How to facilitate collaborative development between the enterprise and the environment under the dual constraints of resources and the environment is the focus of today's green supply chain management system research. Through the performance evaluation of the green supply chain, we can understand the operation of the whole supply chain and its shortcomings, provide a basis for improving related processes, and have important practical significance for improving the competitiveness and protection of its products. First of all, by summarising and analysing the research status of sustainable supply chain management in different countries, the research idea and overall background of this paper are proposed. It discusses the theory of sustainable supply chain management and the performance evaluation system and calculation types of sustainable supply chain management. Finally, the relative weight of each index is determined based on the sustainability calculation method, and then the decentralisation degree of the index is constructed. During this period, the fuzzy comprehensive evaluation method is used to evaluate the performance of sustainable supply chain, conduct case analysis and summary, and evaluate the performance of green supply chain component in economic, social, environmental and other aspects. In this paper, representative companies are selected as examples to evaluate their green supply chain management performance, and the evaluation algorithm is studied based on sustainable calculation method. The results show that a reasonable and effective evaluation of the enterprise performance of green supply chain management and a sustainable algorithm study can effectively identify potential problems in the operation of the company and improve the overall operation of the company at this stage.",CS,AI_ML,0.85,Extracted from log - paper 2080
Evaluation of the Performance of Algorithm-Based Methods for Subjective Refraction,"Objective: To evaluate the performance of two subjective refraction measurement algorithms by comparing the refraction values, visual acuity, and the time taken by the algorithms with the standard subjective refraction (SSR). Methods: The SSR and two semi-automated algorithm-based subjective refraction (SR1 and SR2) in-built in the Vision-R 800 phoropter were performed in 68 subjects. In SR1 and SR2, the subject’s responses were recorded in the algorithm which continuously modified the spherical and cylindrical component accordingly. The main difference between SR1 and SR2 is the use of an initial fogging step in SR1. Results: The average difference and agreement limits intervals in the spherical equivalent between each refraction method were smaller than 0.25 D, and 2.00 D, respectively. For the cylindrical components, the average difference was almost zero and the agreement limits interval was less than 0.50 D. The visual acuities were not significantly different among the methods. The times taken for SR1 and SR2 were significantly shorter, and SR2 was on average was three times faster than SSR. Conclusions: The refraction values and the visual acuity obtained with the standard subjective refraction and algorithm-based methods were similar on average. The algorithm-based methods were significantly faster than the standard method.",CS,AI_ML,0.85,Extracted from log - paper 2081
Performance Evaluation of Priority Load-Aware Scheduling (PLAS) Algorithm for IEEE802.16 Networks,"In recent years, there has been an increase in the transmission of multimedia services due to the emergence of wireless broadband (WiBB) technologies such as WiMAX. WiMAX supports multiple QoS classes for transmission of different multimedia applications. However, satisfying the requirements of these applications with the often-limited resources has been a major challenge, which requires an efficient scheduling scheme such as PLAS. PLAS is a variant of LAWRR designed to reduce queuing delay of real-time traffics in WiBB networks. Several simulation experiments were conducted to evaluate the performance of PLAS against the LAWRR algorithm. The results revealed that PLAS outperforms the other scheme in terms of average delay for low input traffic. However, the metric (delay) used and the traffics generated for the simulation are not adequate to realistically evaluate the performance of a scheduling algorithm in a typical metropolitan area network. In this study, we further evaluated the performance of PLAS and LAWRR under varying higher input traffics, and an additional performance metric, using discrete event simulation. The results demonstrated that the PLAS achieved better results in terms of queuing delay and throughput compared to LAWRR. The improved performance will lead to better user experience, which will increase the number of subscribers and consequently increase revenue for service providers.",CS,AI_ML,0.85,Extracted from log - paper 2082
Performance Evaluation of Hybrid Photovoltaic Thermal Thermoelectric Collector Using Grasshopper Optimization Algorithm With Simulated Annealing,"Abstract In this paper, a mathematical model of a single-channel photovoltaic thermal (PVT) air collector incorporated with a thermoelectric (TE) module has been presented. The overall electrical energy obtained from the photovoltaic thermal-thermoelectric (PVT-TE) collector is 5.78% higher than the PVT collector. Further, the grasshopper optimization algorithm (GOA) and hybrid grasshopper optimization algorithm with simulated annealing (GOA-SA) have been proposed and implemented to optimize the parameters of opaque PVT-TE collector. Although there are different parameters that influence the performance of PVT-TE system, yet in this study only four parameters, viz., length of the channel (L), width of the channel (b), mass flowrate of air in the channel (mair), and temperature of air at the inlet of channel (Tair,i) are considered for optimization. The simulation result demonstrates that the hybrid GOA-SA algorithm turned out to be an exceptionally effective method for optimal tuning of the parameters of the PVT-TE system. The result explicitly shows that the average value of overall electrical efficiency and exergy gain are 15.27% and 27.0565 W, respectively, when the parameters are optimized by the suggested GOA-SA algorithm which is way ahead with respect to the outcomes obtained with that of the calculated values or using GOA algorithm alone.",CS,AI_ML,0.85,Extracted from log - paper 2083
Influence of the enterprise’s intelligent performance evaluation model using neural network and genetic algorithm on the performance compensation of the merger and acquisition parties in the commitment period,"The purpose is to study the performance compensation of the bid purchased during the mergers and acquisitions (M&amp;A) process. An intelligent model of enterprise performance appraisal is built to analyze the performances of the acquired enterprises. First, the evaluation indicators of enterprise performance are selected from both financial and non-financial aspects. An enterprise performance appraisal model is established based on the neural networks and optimized by the factor analysis method and Genetic Algorithm (GA). The principal factors affecting enterprise performance are analyzed. Then the M&amp;A parties’ performances during the M&amp;A commitment period under the earnings compensation mechanism are analyzed quantitatively. Corresponding hypotheses and evaluation indicators are established. Mean test results and regression analyses demonstrate that the hypotheses proposed are valid under particular circumstances. Introducing the earnings compensation mechanism during the M&amp;A process can improve the enterprise performance effectively so that the earnings forecasted in the commitment period are significantly higher than the historical profitability. Hence, the earnings compensation mechanism plays a positive role in guiding enterprise performance. Comparison with models proposed in previous research reveals that the output error ratio of the designed corporate performance evaluation model is 1.16%, which can effectively evaluate corporate performance. The above results provide a reference for studying the impact of the earnings compensation mechanism on enterprise performance during the M&amp;A process.",CS,AI_ML,0.85,Extracted from log - paper 2084
A multi-level solution algorithm for steady-state Markov chains,"A new iterative algorithm, the multi-level algorithm, for the numerical solution of steady state Markov chains is presented. The method utilizes a set of recursively coarsened representations of the original system to achieve accelerated convergence. It is motivated by multigrid methods, which are widely used for fast solution of partial differential equations. Initial results of numerical experiments are reported, showing significant reductions in computation time, often an order of magnitude or more, relative to the Gauss-Seidel and optimal SOR algorithms for a variety of test problems. It is shown how the well-known iterative aggregation-disaggregation algorithm of Takahashi can be interpreted as a special case of the new method.",CS,AI_ML,0.85,Extracted from log - paper 2085
Prediction of Pile Bearing Capacity Using XGBoost Algorithm: Modeling and Performance Evaluation,"The major criteria that control pile foundation design is pile bearing capacity (Pu). The load bearing capacity of piles is affected by the various characteristics of soils and the involvement of multiple parameters related to both soil and foundation. In this study, a new model for predicting bearing capacity is developed using an extreme gradient boosting (XGBoost) algorithm. A total of 200 driven piles static load test-based case histories were used to construct and verify the model. The developed XGBoost model results were compared to a number of commonly used algorithms—Adaptive Boosting (AdaBoost), Random Forest (RF), Decision Tree (DT) and Support Vector Machine (SVM) using various performance measure metrics such as coefficient of determination, mean absolute error, root mean square error, mean absolute relative error, Nash–Sutcliffe model efficiency coefficient and relative strength ratio. Furthermore, sensitivity analysis was performed to determine the effect of input parameters on Pu. The results show that all of the developed models were capable of making accurate predictions however the XGBoost algorithm surpasses others, followed by AdaBoost, RF, DT, and SVM. The sensitivity analysis result shows that the SPT blow count along the pile shaft has the greatest effect on the Pu.",CS,AI_ML,0.85,Extracted from log - paper 2086
Performance evaluation of similarity measures for K-means clustering algorithm,"Clustering is a useful technique that organizes a large quantity of unordered datasets into a small number of meaningful and coherent clusters. Every clustering method is based on the index of similarity or dissimilarity between data points. However, the true intrinsic structure of the data could be correctly described by the similarity formula defined and embedded in the clustering criterion function. This paper uses squared Euclidean distance and Manhattan distance to investigates the best method for measuring similarity between data objects in sparse and high-dimensional domain which is fast, capable of providing high quality clustering result and consistent. The performances of these two methods were reported with simulated high dimensional datasets.",CS,AI_ML,0.85,Extracted from log - paper 2087
Performance Evaluation of Novel Random Biased-Genetic Algorithm (NRB-GA): A Hybrid Load Balancing Algorithm in a Cloud Computing Environment,"A novel random biased-genetic algorithm (NRB-GA) load-balancing algorithm that exhibits the characteristics of both genetic algorithms and biased random algorithms is designed and developed to improve the processing time and response time metrics of the cloud computing environment. The NRB-GA is designed to discover a virtual machine with fewer loads by applying a genetic algorithm with a fitness function that is inversely proportional to the average load over a period of time for each virtual machine and with biased parent selection to maximize the fitness values of offspring. The developed NRB-GA load-balancing algorithm is evaluated by analysing its performance for various simulated scenarios in a cloud computing environment with different user bases and data center configurations. The analysis of the experimental results of NRB-GA indicates that the average response time is reduced by 27.22%, 21.15%, and 22.34%, and the processing time is reduced by 25.73%, 16.14%, and 18.82% for one, two, and three data centers, respectively. It is evident that the proposed NRB-GA algorithm for load balancing outperforms other existing algorithms significantly.",CS,AI_ML,0.85,Extracted from log - paper 2088
Performance Evaluation of Cluster Head Selection Algorithm for Heterogeneous Wireless Sensor Networks,"In this article, an Improved Energy-Efficient Cluster Head Selection (IEECHS) algorithm for implementation in heterogeneous Wireless Sensor Networks (WSNs) is proposed. The IEECHS algorithm addresses the key issues of energy-efficiency and security, and combines the rotation-based clustering and energy-saving mechanisms to enhance network performance and prolong the WSN’s lifetime. For the heterogeneous network model, using the Full Duplex (FD) features of nodes and cutthrough mechanism, a FD Medium Access Control Protocol (FDMAC) is implemented, which (i) is a contention window-based mechanism for collision detection, and (ii) provides channel access prioritization to different traffic classes. To evaluate IEECHS performance, simulation experiments are conducted considering key performance metrics viz., throughput, energy consumption, and packet delivery ratio. The results demonstrate that through IEECHS, after the completion of 10,000 iterations, the energy depletion rate lowers thereby, resulting in increased network lifetime and higher stored energy.",CS,AI_ML,0.85,Extracted from log - paper 2089
Performance Evaluation of New Joint EDF-RM Scheduling Algorithm for Real Time Distributed System,"In Real Time System, the achievement of deadline is the main target of every scheduling algorithm. Earliest Deadline First (EDF), Rate Monotonic (RM), and least Laxity First are some renowned algorithms that work well in their own context. As we know, there is a very common problem Domino's effect in EDF that is generated due to overloading condition (EDF is not working well in overloading situation). Similarly, performance of RM is degraded in underloading condition. We can say that both algorithms are complements of each other. Deadline missing in both events happens because of their utilization bounding strategy. Therefore, in this paper we are proposing a new scheduling algorithm that carries through the drawback of both existing algorithms. Joint EDF-RM scheduling algorithm is implemented in global scheduler that permits task migration mechanism in between processors in the system. In order to check the improved behavior of proposed algorithm we perform simulation. Results are achieved and evaluated in terms of Success Ratio (SR), Average CPU Utilization (ECU), Failure Ratio (FR), and Maximum Tardiness parameters. In the end, the results are compared with the existing (EDF, RM, and D_R_EDF) algorithms. It has been shown that the proposed algorithm performs better during overloading condition as well in underloading condition.",CS,AI_ML,0.85,Extracted from log - paper 2090
An analytical study of CANIT algorithm in TCP protocol,"CANIT (Congestion Avoidance with Normalized Interval of Time) algorithm is a new policy for TCP congestion avoidance which is proposed in order to improve TCP fairness over long delay links. CANIT uses a new parameter refered to NIT ( Normalized Interval of Time ), which is the key of this algorithm. In former works, we showed by simulations of some configuration with various value of NIT parameter, that using our algorithm instead of the standard one, improves the TCP fairness as well as the utilisation of network ressources. In this work, we propose an analytical study and we give the basic equations in order to find the optimal value of NIT parameter which provides more fairness and better bandwidth utilisation.",CS,AI_ML,0.85,Extracted from log - paper 2091
Efficient Dynamic Scheduling Algorithm for Performance evaluation of Workflow in cloud Environment,"As of late, distributed computing has been under a creating innovation as a potential answer for giving a flexible, on request figuring foundation for applications. The most testing issue looked by distributed computing is Workflow Scheduling. As, workflow planning is actualized on networks and groups however it shows significant downside, for example, virtual machine execution inconstancy and occasion securing delay. The common framework additionally neglects to either meet the client's Quality of Service (QoS) necessities or to incorporate some essential standards of Cloud registering like flexibility and heterogeneity of the figuring assets. The proposed framework comprises of a dynamic scheduling algorithm for calculation for planning a work process in an open Cloud. This procedure comprises of the preferences offered by the distributed computing for asset the board in IT division by actualizing this calculation to oversee and allot the assets by planning the cut-off times of changed undertakings of significant tasks in IT industry, for example, examination , structure ,usage ,testing, sending and upkeep with less cost .The proposed technique likewise gives the presentation assessment of a representative by breaking down his participation, input from administrator and their partners upheld his work.",CS,AI_ML,0.85,Extracted from log - paper 2092
Performance Evaluation of a Multi-Hop Cluster Based Algorithm for Vehicular Ad-hoc Networks,"Abstract: Vehicular ad hoc networks are characterized as the ad hoc networks with dynamic and dense network topology which faces issues like routing, data congestion, and overhead. One technique which has proved to be useful in managing VANETs is clustering. Clustering is a technique to divide the network into smaller, distributed and more stable hierarchical structure. The parameters like speed, position, distance, direction and mobility are used for clustering the networks. Clustering helps in load balancing, improving scalability, efficient resource allocation and reducing overhead. In this paper a multi-hop cluster-based algorithm (MhCA) for VANET is proposed which uses Fuzzy TOPSIS for CH selection based on Rank Index of nodes. The flowchart of the algorithm along with the description of the algorithm is given below in the paper. Extensive simulation experiments are run using the ns3 and SUMO to evaluate &amp; compare the performance of proposed algorithm with the existing multi-hop algorithms like VMaSC and n-hop. Keywords: CH, CM, CH Change Duration, CH Duration, OSM, NS3.",CS,AI_ML,0.85,Extracted from log - paper 2093
A recursive random search algorithm for network parameter optimization,"This paper proposes a new heuristic search algorithm, Recursive Random Search(RRS), for black-box optimization problems. Specifically, this algorithm is designed for the dynamical parameter optimization of network protocols which emphasizes on obtaining good solutions within a limited time frame rather than full optimization. The RRS algorithm is based on the initial high-efficiency property of random sampling and attempts to maintain this high-efficiency by constantly ""restarting"" random sampling with adjusted sample spaces. Due to its basis on random sampling, the RRS algorithm is robust to the effect of random noises in the objective function and it performs especially efficiently when handling the objective functions with negligible parameters. These properties have been demonstrated with the tests on a suite of benchmark functions. The RRS algorithm has been successfully applied to the optimal configuration of several network protocols. One application to a network routing algorithm is presented.",CS,AI_ML,0.85,Extracted from log - paper 2094
An EM algorithm for the model fitting of Markovian binary trees (abstract only),"Markovian binary trees are a special class of branching processes in which the lifetime of an individual is controlled by a transient Markovian arrival process. A Markovian binary tree is characterized by the 4-tuple ( ? ,D0,B, d ), where ? is the vector of initial phase distribution of the first individual, D0 is the matrix of phase transition rates between birth and death events, B is the matrix of birth rates and d is the vector of death rates.   In order to use the Markovian binary tree to model the evolution of a real population, we need to determine the parameters ( ? ,D0,B, d ) from observations of that population. In the absence of migration, the only observable changes in a population are those associated with a birth or a death event; no phase transition in the underlying process can actually been seen. We are thus dealing with a problem of parameter estimation from incomplete data, and one way to solve this statistical problem is to make use of the EM algorithm. Our purpose here is thus to specify this algorithm to the Markovian binary tree setting. In the first part of this paper, we introduce a discrete time terminating marked Markov arrival process (MMAP), based on which a class of discrete multivariate phase-type (MPH) distributions is defined. The discrete MPH-distributions hold many of the properties possessed by continuous MPH-distributions (Assaf, et al. (1983), Kulkarni (1988), and O'Cinneide (1990)). It is known that the joint distribution functions of continuous MPH are fairly complicated and difficult to calculate. In contrast, for the discrete MPH introduced here, we provide recursive formulas the joint probabilities and explicit expressions for means, variances, and co-variances.",CS,AI_ML,0.85,Extracted from log - paper 2095
The analysis and performance evaluation of the pheromone‐Q‐learning algorithm,"Abstract:  The paper presents the pheromone‐Q‐learning (Phe‐Q) algorithm, a variation of Q‐learning. The technique was developed to allow agents to communicate and jointly learn to solve a problem. Phe‐Q learning combines the standard Q‐learning technique with a synthetic pheromone that acts as a communication medium speeding up the learning process of cooperating agents. The Phe‐Q update equation includes a belief factor that reflects the confidence an agent has in the pheromone (the communication medium) deposited in the environment by other agents. With the Phe‐Q update equation, the speed of convergence towards an optimal solution depends on a number of parameters including the number of agents solving a problem, the amount of pheromone deposit, the diffusion into neighbouring cells and the evaporation rate. The main objective of this paper is to describe and evaluate the performance of the Phe‐Q algorithm. The paper demonstrates the improved performance of cooperating Phe‐Q agents over non‐cooperating agents. The paper also shows how Phe‐Q learning can be improved by optimizing all the parameters that control the use of the synthetic pheromone.",CS,AI_ML,0.85,Extracted from log - paper 2096
A recursive random search algorithm for large-scale network parameter configuration,"Parameter configuration is a common procedure used in large-scale network protocols to support multiple operational goals. It can be formulated as a black-box optimization problem and solved with an efficient search algorithm. This paper proposes a new heuristic search algorithm, Recursive Random Search(RRS), for large-scale network parameter optimization. The RRS algorithm is based on the initial high-efficiency feature of random sampling and it attempts to maintain this high efficiency by constantly ""restarting"" random sampling with adjusted sample spaces. Besides the high efficiency, the RRS algorithm is robust to the effect of random noise and trivial parameters in the objective function because of its root in random sampling. These features are very important for the efficient optimization of network protocol configuration. The performance of RRS is demonstrated with the tests on a suite of benchmark functions. The algorithm has been applied to the configuration of several network protocols, such as RED, OSPF and BGP. One example application in OSPF routing algorithm is presented.",CS,AI_ML,0.85,Extracted from log - paper 2097
SCAT,"This paper presents a new algorithm for the approximate analysis of closed product-form queueing networks with fixed-rate, delay (infinite-server), and load-dependent queues. This algorithm has the accuracy, speed, small memory requirements, and simplicity necessary for inclusion in a general network analysis package. The algorithm allows networks with large numbers of queues, job classes, and populations to be analyzed interactively even on microcomputers with very limited memory.",CS,AI_ML,0.85,Extracted from log - paper 2098
Performance Evaluation of Novel Dynamic Data Replication Algorithm under Optical Burst Switching,"Abstract The database replication mechanism ensures the duplication of data from one geographical location to another. In recent past, PDDRA (pre-fetching-based dynamic data replication algorithm) is investigated heavily under various classes of request arrivals. The request arriving at a particular server for replication depends on many factors such as bandwidth supported by link, its geographical location and routing algorithms. The number of served requests depends on the processing speed and buffering capacity of a particular server. If a request cannot be served at the server, it is dropped. If a bunch of requests arrive at a server, then traffic modelled as bursty traffic. The objective of this paper is to investigate the performance of dynamic replication algorithm under bursty request arrival. In a similar context, a mathematical model is detailed for the estimation of request burst length, such that both buffering capacity and transmission delay can be kept minimal. Finally, simulation results are present to evaluate the fraction of successful request under diverse conditions.",CS,AI_ML,0.85,Extracted from log - paper 2099
EVALUATION OF INITIALIZATION METHODS FOR THE PERFORMANCE OF THE K-MEANS ALGORITHM,"The K-means algorithm is one of the most widely used unsupervised machine learning methods; it helps to sort data clusters into a given number of groups with a pattern association that identifies relevant information in research domains. The heuristics of the algorithm are adaptable and easy to implement; however, one of its most notorious weaknesses is the poor assignment of K groups. This paper aims to analyze the different means of initialization and performance of the algorithm, as well as some applications of K-means in different industry sectors through a literature review, addressing relevant aspects to conclude in which cases transcendent results are obtained. Keywords: K-means, heuristic, clustering, performance, initialization, algorithm, centroids, metaheuristic, accuracy, evaluation, methods, applications.",CS,AI_ML,0.85,Extracted from log - paper 2100
Performance Evaluation of English Part-of-Speech Tagging Based on Typical Parameter Smoothing Algorithm,"Abstract Part-of-speech (POS) tagging for English is the basis for implementing English automatic correction. Although researchers have done a lot of useful studies on English POS tagging, most of them are aimed at users with English as the first language, while studies for users with English as the second language are few. For this purpose, manual tagging is performed based on the typical parameter smoothing algorithm. On this basis, a performance evaluation method for English part-of-speech tagging is proposed, which integrates the features of term clustering, non-tagged corpus statistics, word pronunciation, etc. The experimental results show that the algorithm can improve the performance of POS tagging effectively, with the tagging accuracy improved from 94.49% to 97.07%",CS,AI_ML,0.85,Extracted from log - paper 2101
PERFORMANCE EVALUATION OF ROUTING ALGORITHM FOR MANET BASED ON THE MACHINE LEARNING TECHNIQUES,"The rapid advances in wireless communication technology has led to an extraordinary progress in the adhoc type of networking. The mobile adhoc networks being a subtype of the adhoc network almost poses the same characteristics of the adhoc network, presenting multiple challenges in framing a route for the transmission of the information from the source to the destination. So the paper proposes a routing method developed based on the reinforcement learning, exploiting the node information’s to establish a route that is short and stable. The proposed method scopes to minimize the energy consumption, transmission delay, and improve the delivery ratio of the packets, enhancing the throughput. The efficiency of the proposed method is determined by validating its performance in the network simulator-II, in terms of the energy consumption, delay in the transmission and the packet delivery ratio.",CS,AI_ML,0.85,Extracted from log - paper 2102
PERFORMANCE EVALUATION OF ORTHOGONAL-DIAMOND SEARCH OF BLOCK MATCHING ALGORITHM FOR VIDEO CODING,"Several drawbacks of established fast Block Matching Algorithm (BMA) are the reasons why new fast BMAs are being developed and proposed in these recent years in order to reduce the computational cost while maintaining the quality of the video signal. In this paper, a new algorithm is proposed, namely Orthogonal-Diamond Search (ODS) which employs an orthogonal-shaped search pattern in the first step and then is switched into diamond-shaped search pattern for the next step. Few established algorithm, namely Orthogonal Search (OS), Full Search (FS), Diamond Search (DS) and Hexagon-Diamond Search (HDS) are implemented using MATLAB along with the ODS and their performance are being compared and analyzed in terms of computational complexity, peak signal-to-noise ratio (PSNR), and number of search points. Simulation result shows that the proposed algorithm can find motion vector with fewer number of search points while maintains close performance of video quality with other selected algorithms.",CS,AI_ML,0.85,Extracted from log - paper 2103
Solution properties and convergence of an approximate mean value analysis algorithm,"We present the solution properties and convergence results of an approximate Mean Value Analysis (MVA) algorithm, the Queue Line (QL) algorithm, for solving separable queueing networks. We formally prove that the QL algorithm is always more accurate than, and yet has the same computational complexity as the Bard-Schweitzer Proportional Estimation algorithm, the most popular approximate MVA algorithm for solving this type of queueing networks.",CS,AI_ML,0.85,Extracted from log - paper 2104
Evaluation of Factors Affecting Employees&amp;#39; Performance Using Artificial Neural Networks Algorithm: The Case Study of Fajr Jam,"Human resources are the most valuable assets of any organization. Therefore, human resources performance has the greatest impact on the organization&amp;#39;s performance and its ability to operate. Many factors affect the performance of employees in organizations. In this research, we seek to evaluate the factors affecting the performance of Fajr Jam refinery employees. For this purpose, firstly, the literature of the research, the indicators affecting the performance of employees were identified and the conceptual model of the problem was formed. Then, the required data were collected using a standard questionnaire based on the conceptual model of the problem among employees of FJG Company. After assessing the validity and reliability of the collected data, it is time to evaluate the performance of the indicators. For this purpose, an artificial neural network algorithm was used to estimate the efficiency boundary values. After calculating the efficiency values in the presence of all the indices, each indices were eliminated from the conceptual model and again the efficiency values were estimated. Now, by comparing the performance statistics in the state before and after the removal of each indicator from the conceptual model, the degree and the mode of its effect are determined. The results of this study indicate that the &amp;quot;payroll&amp;quot; indicators, &amp;quot;environmental conditions&amp;quot; and &amp;quot;reporting culture&amp;quot; are the strengths of the system under review, and are now at an appropriate level. Also, the results indicate a negative impact on the indicators of &amp;quot;awareness&amp;quot;, &amp;quot;system planning and preparation for critical situations,&amp;quot; &amp;quot;amenities,&amp;quot; &amp;quot;training,&amp;quot; and &amp;quot;job security&amp;quot; in the system under review.",CS,AI_ML,0.85,Extracted from log - paper 2105
Performance Evaluation Of SVM and ANN for Cancer Classification and Designing Algorithm for Arrhythmia Prediction in Cancer Patient,"Classifying the cancer based on the age and predicting the arrhythmia in cancer patient is necessary to determine the next steps in dealing with the patients. This prediction can be done by using multiple algorithms of machine learning such as SVM, Linear classifier, neural network. Machine learning, interpretability refers to understand the underlying behaviour of the prediction of a model in order to identify diagnosis criteria and/or new rules from its output. Interpretability contributes to increase the usability of the method. Also, it is relevant in decision support systems, such as in medical applications. Using multiple algorithm on big data set and predicting the arrhythmia cases from early age to old age.Apache (Acute Physiology, Age and Chronic Health Evaluation) and SOFA (Sequential Organ Failure Assessment) score are the important factor in critically ill patients. The number of ICU (intensive care unit) admission will be depending on these two scores. Analyzing Apache and SOFA scores will be helpful for intensivist.[4]",CS,AI_ML,0.85,Extracted from log - paper 2106
Application of a Performance Evaluation Model to the Paper and Paper Products Printing Sector: The DEA-AHP Hybrid Algorithm,"The paper and paper products printing sector plays a crucial role in generating income, creating employment opportunities, and supporting exports and various industries. Measuring the efficiency of companies operating in this sector is important in identifying areas for improvement and enhancing overall performance. In this study, a two-stage DEA (data envelopment analysis)-AHP (analytic hierarchy process) approach is proposed to analyze the efficiency of twelve paper and paper products printing companies traded on Borsa Istanbul. The modified DEA method is employed to make pairwise comparisons of the companies. Total assets, total equity, and the number of employees are selected as inputs, while revenue and net profit are considered as outputs. The AHP method prioritizes the companies by considering the outputs of the mathematical models constructed via DEA. The proposed framework presents a different view because it contributes to identifying the most efficient company, benchmarking company performance, and determining areas for improvement.",CS,AI_ML,0.85,Extracted from log - paper 2107
Evaluation and Performance Analysis using ANP and TOPSIS Algorithm,"Abstract Suppliers play an important role in supporting the supply chain within the company. Therefore, evaluation of suppliers in the company is a problem that is studied to improve the performance of the company’s supply chain. The purpose of this study is to evaluate the performance of green suppliers using the Analytic Network Process (ANP)-Technique for order performance by similarity to ideal solution (TOPSIS). ANP is used to weigh each factor and criteria, and TOPSIS is used to evaluate the performance of green suppliers. This research is a case study on the construction industry in Indonesia. Six factors and fourteen sub-criteria are used to evaluate the performance of green suppliers. Based on the ANP method, the cost factor is the criterion that has the highest weight, which is 0.44318. The sub-criteria that has the highest weight is product price, with a weight of 0.2681. Furthermore, the order of green supplier performance based on TOPSIS shows that supplier B ranks first, followed by suppliers A, D, and C.",CS,AI_ML,0.85,Extracted from log - paper 2108
Optimization and Performance Evaluation of Deep Learning Algorithm in Medical Image Processing,"In this paper, the optimization and performance evaluation of deep learning algorithm in medical image processing are studied. Firstly, the paper introduces the importance and challenges of medical image processing, and expounds the application prospect of deep learning in this field. Subsequently, this paper discusses the optimization methods of deep learning algorithm in detail, including model structure design, data preprocessing, super parameter adjustment and so on. In terms of performance evaluation, this study selected classic models such as U-Net, DeepLab and DenseNet, and compared them with ROC curve and AUC value to evaluate their predictive ability in medical image classification. The results show that the DenseNet model shows high performance in prediction accuracy, while the performance of U-Net and DeepLab models is slightly average. Finally, the advantages and disadvantages of each model are analyzed, and the future research direction is prospected. This study is of great significance to promote the development and application of medical image processing technology, and provides important theoretical and technical support for medical diagnosis and treatment.",CS,AI_ML,0.85,Extracted from log - paper 2109
Performance Evaluation of Map Reduce vs. Spark framework on Amazon Machine Image for TeraSort Algorithm,"TeraSort is one of Hadoop’s widely used benchmarks. Hadoop’s distribution contains both the input generator and sorting implementations: the TeraGen generates the input and TeraSort conducts the sorting. We focus on the comparison of TeraSort algorithm on the different distributed platforms with different configurations of the resources. We have considered the parameters of measure of efficiency as Compute Time, Data Read, Data Write, Compute Time, and Speedup. We have conducted experiments using Hadoop map reduce and Spark (Java). We empirically evaluate the performance of TeraSort algorithm on Amazon EC2 Machine Images, and demonstrate that it achieves 3.95 × - 2.4 × speedup, compared with TeraSort, for typical settings of interest.",CS,AI_ML,0.85,Extracted from log - paper 2110
Study on Evolutionary Algorithm Online Performance Evaluation Visualization Based on Python Programming Language,"Abstract Evolutionary computations are kinds of random searching algorithms derived from natural selection and biological genetic evolution behavior. Evaluating the performance of an algorithm is a fundamental task to track and find the way to improve the algorithm, while visualization technique may play an important act during the process. Based on current existing algorithm performance evaluation criteria and methods, a Python-based programming tracking strategy, which employs 2-D graphical library of python matplotlib for online algorithm performance evaluation, is proposed in this paper. Tracking and displaying the performance of genetic algorithm (GA) and particle swarm optimization (PSO) optimizing two typical numerical benchmark problems are employed for verification and validation. Results show that the tracking strategy based on Python language for online performance evaluation of evolutionary algorithms is valid, and can be used to help researchers on algorithms’ performance evaluation and finding ways to improve it.",CS,AI_ML,0.85,Extracted from log - paper 2111
Construction of Financial Performance Evaluation System based on Principal Component Analysis Algorithm and Its Application in Digital Transformation Enterprises,"In the context of a strong national push toward the growth of the """"digital economy"""", traditional manufacturing companies are increasingly turning to new digital technologies for their digital transformation. This paper aims to investigate the suitability of using a financial performance evaluation system for assessing the success of digital transformation strategies employed by these enterprises. The application of principal component analysis in digital transformation enterprises involves repeatedly selecting the main indicators in the financial performance evaluation index system of manufacturing enterprises. Finally, a financial performance evaluation index system suitable for analyzing digital transformation enterprises is constructed. The differences in financial performance before and after transformation are analyzed, and a comprehensive evaluation and comparative analysis are conducted on the financial performance of digital transformation enterprises and non-digital transformation enterprises. The experimental results show that the average growth rate of total assets of enterprises is 7.07%. The average growth rate of operating revenue is 20.99%. The standard deviations are 17.42% and 235.9%. There is a significant difference between the maximum and minimum values of these two indicators, indicating that the average dispersion of these two indicators is relatively high. In the initial phases of digital transformation implementation, enterprises that adopt digital technology experience a certain level of profitability improvement, as shown by the results. Compared to businesses that have not undergone a digital transformation, digitally transformed enterprises possess greater advantages and flexibility in digital operations. Digital transformation has important theoretical and practical value in improving the financial management level of digital transformation enterprises.",CS,AI_ML,0.85,Extracted from log - paper 2112
An efficient algorithm for the exact analysis of multiclass queueing networks with large population sizes,"We introduce an efficient algorithm for the exact analysis of closed multiclass product-form queueing network models with large population sizes. We adopt a novel approach, based on linear systems of equations, which significantly reduces the cost of computing normalizing constants. With the proposed algorithm, the analysis of a model with N circulating jobs of multiple classes requires essentially the solution of N linear systems with order independent of population sizes.A distinguishing feature of our approach is that we can immediately apply theorems, solution techniques, and decompositions for linear systems to queueing network analysis. Following this idea, we propose a block triangular form of the linear system that further reduces the requirements, in terms of both time and storage, of an exact analysis. An example illustrates the efficiency of the resulting algorithm in presence of large populations.",CS,AI_ML,0.85,Extracted from log - paper 2113
A distributed data streaming algorithm for network-wide traffic anomaly detection,"Nowadays, Internet has serious security problems and network failures that are hard to resolve, for example, botnet attacks, polymorphic worm/virus spreading, DDoS, and flash crowds. To address many of these problems, we need to have a network-wide view of the traffic dynamics, and more importantly, be able to detect traffic anomaly in a timely manner. To our knowledge, Principle Component Analysis (PCA)is the best-known spatial detection method for the network-wide traffic anomaly. However, existing PCA-based solutions have scalability problems in that they require O ( m 2 n )running time and O ( mn )space to analyze traffic measurements from m aggregated traffic flows within a sliding window of the length n . We propose a novel data streaming algorithm for PCA-based network-wide traffic anomaly detection in a distributed fashion. Our algorithm can archive O ( wn log n )running time and O ( wn )space at local monitors,and O ( m 2 log n )running time and O ( m log n ) space at Network Operation Center (NOC), where w denotes the maximum number of traffic flows at a local monitor.",CS,AI_ML,0.85,Extracted from log - paper 2114
"Performance Evaluation and Comparison of Different Noise, Apply on PNG Image Format Used in Deconvolution Wiener Filter (FFT) Algorithm","Image Restoration is a field of Image Processing. This deals with recovering an original and sharp image from a degraded image using degradation &amp; restoration function. This study focus on restoration of degraded images which have been blurred by known degradation function. PNG (Tag Index Format) are considered for analyzing the image restoration techniques deconvolution using wiener filter (FFT) algorithm with an information of the Point Spread Function (PSF) corrupted blurred image and then corrupted by Different noise. Performance analysis is done to measure the efficiency by which image is recovered. The analysis is done on the basis of various performance metrics like Peak Signal to Noise Ratio (PSNR), Mean Square Error(MSE),Root Mean Square Error (RMSE), Mean Absolute Error (MAE).",CS,AI_ML,0.85,Extracted from log - paper 2115
Comparative performance evaluation of routing algorithm and topology size for wireless network-on-chip,"Wireless Network-on-Chip or WiNoC is an alternative to traditional planar on-chip networks. On-chip wireless links are utilized to reduce latency between distant nodes due to its capability to communicate with far-away node within a single hop. This paper analyzes the impact of various routing schemes and the effect of WiNoC sizes on network traffic distributions compared to conventional mesh NoC. Radio hubs (4×4) are evenly placed on WiNoC to analyze global average delay, throughput, energy consumption and wireless utilization. For validation, three various network sizes (8×8, 16×16 and 32×32) of mesh NoC and WiNoC architectures are simulated on cycle-accurate Noxim simulator under numerous traffic load distributions. Simulation results show that WiNoC architecture with the 16×16 network size has better average speedup (∼1.2×) and improved network throughputs by 6.36% in non-uniform transpose traffic distribution. As the trade-off, WiNoC requires 63% higher energy consumption compared to the classical wired NoC mesh.",CS,AI_ML,0.85,Extracted from log - paper 2116
Performance Evaluation of Hybrid Method for Securing and Compressing Images,"Security is a most important field of research work for sending and receiving of data in secret way over the network. Cryptographyis a method for securing transformation like image, audio, video, text without any hacking problem. Encryption and Decryption are two methods used to secure the data. Image compression technique used to reducing the size of an image for effective data communication. There are variety of algorithms has been proposed in the literature for securing images using encryption/decryption techniques and reduce the size of images using image compression techniques. These techniques still need improvement to overcome issues, challenges and its limitations. Hence in this research work a hybrid method which combines securing image using RSA, hill cipher and 2bit rotation and compressing of images using lossless compression algorithm has been proposed. This method compared to execution time of existing method. This method secures the image and reduces the size of the image for data communication over the internet. This method is suitable for various applications uses images like remote sensing, medical and Spatio-temporal.",CS,AI_ML,0.85,Extracted from log - paper 2117
A parallel branch-and-bound algorithm for MIN-based multiprocessors,"A parallel ""Decomposite Best-First"" search Branch-and-Bound algorithm ( pdbsbb ) for MIN-based multiprocessor systems is proposed in this paper. A conflict free mapping scheme, known as step-by-step spread , is used to map the algorithm efficiently on to a MIN-based system for reducing communication overhead. It is shown that the proposed algorithm provides better speed-up than other reported schemes when communication overhead is taken into consideration.",CS,AI_ML,0.85,Extracted from log - paper 2118
Evaluation and Characterization of the Influence of Solar Position Algorithm on the Performance of Parabolic Trough Solar System,"For the open loop control system based on the solar position algorithm (SPA), without sensor correction, the error of SPA will bring the tracking error directly to decrease the efficiency of the solar system. By comparing with SPA proposed by NREL, this paper first evaluates main SPAs with different accuracy and presents the error of SPA on tracking error. Based on the annual average efficiency of the solar trough system, this paper evaluates the impact of the tracking error caused by SPA on the solar trough system, and proposes that the average SPA calculation error can be applied to characterize the impact of SPA on the trough solar system. By making a comparison of solar trough system efficiency calculated with fixed annual average tracking error and normal constantly changing tracking error within a year, respectively, the evaluation results show that for most SPAs, the introduced error is less than 0.05%, and only a few empirical algorithms with large tracking errors introduce larger errors, but they are not suitable for the trough solar system. Therefore, the SPA evaluation method proposed in this paper is applicable to the solar trough system.",CS,AI_ML,0.85,Extracted from log - paper 2119
Research on Human Resource Management Performance Evaluation Method Based on Chaos Optimization Algorithm,"All enterprise components and business activities are centered on the customer. Whether human resource management is reasonable and effective not only is a matter of human resource management, but also directly influences whether other resources can be used reasonably and effectively, and determines the efficiency of business operations. In human resource management, perfect performance evaluation criteria are developed based on the actual situation in order to evaluate employee productivity and work ethic. Nonetheless, in the process of actual enterprise HR management performance evaluation work, there are not only imperfect performance evaluation standards, but also relatively objective evaluation standards, and HR cannot conduct a comprehensive analysis of each position, which has a negative impact on the quality of enterprise HR management work. This paper improves the performance evaluation model for the original data mining and proposes a human resource management performance evaluation method based on chaotic optimization algorithm, which generates the initial values of evaluation data through chaotic logistic mapping, bringing the initial data closer to the optimal value, reducing the impact of random initialization on the algorithm’s performance, and when the model is trained, it can make the algorithm’s performance more stable. Capability thereby addresses the flaws in the original evaluation method. Experiments demonstrate that our method significantly improves the precision of model training and prediction.",CS,AI_ML,0.85,Extracted from log - paper 2120
Performance Evaluation of Various Scheduling Algorithm Based on Cloud Computing System,"&#x0D; &#x0D; &#x0D; &#x0D; Cloud computing is an information technology archetype which has been used significantly for providing various services through Internet. It ensures easier access to resources and high-level services. The working procedure of cloud systems must be scheduled, so as to efficiently provide services to people. The goal of task scheduling is to acquire best system throughput and to allocate various computing resources to applications. The unpredictable situation increases with the size of the task and becomes high potential to solve effectively. Numerous intellectual methods are recommended to clarify this situation in the territory of scheduling of cloud computing. In this research, a comparative analysis has been conducted for different types of existing scheduling algorithms in the cloud environment with their respective parameters.&#x0D; &#x0D; &#x0D; &#x0D; &#x0D;",CS,AI_ML,0.85,Extracted from log - paper 2121
Performance evaluation of a hybridized simulated annealing algorithm for flow shop scheduling under a dynamic environment,"Purpose– The purpose of this paper is to propose a hybrid-simulated annealing algorithm to address the lacunas in production logistics. The primary focus is laid on the basic understanding of the critical quandary occurring in production logistics, and subsequently research attempts are undertaken to resolve the issue by developing a hybrid algorithm. A logistics problem associated with a flow shop (FS) having a string of jobs which need to be scheduled on m number of machines is considered.Design/methodology/approach– An attempt is made here to introduce and further establish a hybrid-simulated annealing algorithm (NEHSAO) with a new scheme for neighbourhood solutions generation, outside inverse (OINV). The competence in terms of performance of the proposed algorithm is enhanced by incorporating a fast polynomial algorithm, NEH, which provides the initial seed. Additionally, a new cooling scheme (Ex-Log) is employed to enhance the capacity of the algorithm. The algorithm is tested on the benchmark problems of Carlier and Reeves and subsequently validated against other algorithms reported in related literature.Findings– It is clearly observed that the performance of the proposed algorithm is far superior in most of the cases when compared to the other conventionally used algorithms. The proposed algorithm is then employed to a FS under dynamic conditions of machine breakdown, followed by formulation of three cases and finally identification of the best condition for scheduling under dynamic conditions.Originality/value– This paper proposes an hybrid algorithm to reduce makespan. Practical implementation of this algorithm in industries would lower the makespan and help the organisation to increse their profit",CS,AI_ML,0.85,Extracted from log - paper 2122
Performance evaluation of interference aware topology power and flow control channel assignment algorithm,"Multi-Radio Multi-Channel Wireless Mesh Network (MRMC-WMN) has been considered as one of the key technology for the enhancement of network performance. It is used in a number of real-time applications such as disaster management system, transportation system and health care system. MRMC-WMN is a multi-hop network and allows simultaneous data transfer by using multiple radio interfaces. All the radio interfaces are typically assigned with different channels to reduce the effect of co-channel interference. In MRMC-WMN, when two nodes transmit at the same channel in the range of each other, generates co-channel interference and degrades the network throughput. Co-channel interference badly affects the capacity of each link that reduces the overall network performance. Thus, the important task of channel assignment algorithm is to reduce the co-channel interference and enhance the network performance. In this paper, the problem of channel assignment has been addressed for MRMC-WMN. We have proposed an Interference Aware, Topology, Power and Flow Control (ITPFC) Channel Assignment algorithm for MRMC-WMN. This algorithm assignes the suitable channels to nodes, which provides better link capacity and reduces the co-channel interference. In the previous work performance of the proposed algorithm has been evaluated for a network of 30 nodes. The aim of this paper is to further evaluate the performance of proposed channel assignment algorithm for 40 and 50 nodes network. The results obtained from these networks show the consistent performance in terms of throughput, delay, packet loss and number of channels used per node as compared to LACA, FCPRA and IATC Channel Assignment algorithms.",CS,AI_ML,0.85,Extracted from log - paper 2123
DECOMPOSITION ALGORITHM FOR PERFORMANCE EVALUATION OF AGV SYSTEMS,"This paper proposes a realistic queueing model of automated guided vehicle (agv) systems in just‐in‐time production systems. The model takes into consideration return paths, Erlang distributed service times, and pull‐type dispatching rule, assuming finite buffer capacities. Since it has no product‐form solution and natural decomposability due to complex nontree fork‐cum‐join architecture and dynamic dispatching rules, we propose a machine‐based decomposition algorithm for the performance evaluation of the model. Each decomposed module consists of the processing machine and its dispatching station. Three flow probabilities, derived from flow conservation analysis, relate the modules, which are updated iteratively until the parameters converge. The numerical results from a real‐life Agv system application show that the algorithm is reasonably accurate.",CS,AI_ML,0.85,Extracted from log - paper 2124
Heavy-Traffic Behavior of the MaxWeight Algorithm in a Switch with Uniform Traffic,"We consider a switch with uniform traffic operating under the MaxWeight scheduling algorithm. This traffic pattern is interesting to study in the heavy-traffic regime since the queue lengths exhibit a multi-dimensional state-space collapse. We use a Lyapunov-type drift technique to characterize the heavy-traffic behavior of the expectation of the sum queue lengths in steady-state. Specifically, in the case of Bernoulli arrivals, we show that the heavy-traffic scaled queue length is ( n -- 3/2 + 1/2 n ). Our result implies that the MaxWeight algorithm has optimal queue-length scaling behavior in the heavy-traffic regime with respect to the size of a switch with a uniform traffic pattern. This settles the heavy-traffic version of an open conjecture.",CS,AI_ML,0.85,Extracted from log - paper 2125
Performance Evaluation of a Topology Control Algorithm for Wireless Sensor Networks,"A main design challenge in the area of sensor networks is energy efficiency to prolong the network operable lifetime. Since most of the energy is spent for radio communication, an effective approach for energy conservation is scheduling sleep intervals for extraneous nodes, while the remaining nodes stay active to provide continuous service. Assuming that node position information is unavailable, we present a topology control algorithm, termed OTC, for sensor networks. It uses two-hop neighborhood information to select a subset of nodes to be active among all nodes in the neighborhood. Each node in the network selects its own set of active neighbors from among its one-hop neighbors. This set is determined such that it covers all two-hop neighbors. OTC does not assume the network graph to be a Unit Disk Graph; OTC also works well on general weighted network graphs. OTC is evaluated against two well-known algorithms from the literature, namely, Span and GAF through realistic simulations using TOSSIM. In terms of operational lifetime, load balancing and Spanner property OTC shows promising results. Apart from being symmetric and connected, the resulting graph when employing OTC shows good spanner properties.",CS,AI_ML,0.85,Extracted from log - paper 2126
Research and Development of Machine Vision Algorithm Performance Evaluation System in Complex Scenes,"Abstract In recent years, deep learning technology has gained widespread attention and has been successfully applied in various applications, including machine vision for automatic driving. However, one of the major challenges in this field is the ability of AI models to perform well in complex scenes with varying weather conditions and lighting conditions. The proposed scheme uses VR technology to simulate different weather conditions and lighting conditions and to evaluate the performance of AI products in such complex scenes. To further enhance the generalization capabilities of AI models, the Software Development Kit has been updated with complex scene data with configurable parameters, and training data with typical characteristics according to machine vision tasks. This approach has the potential to significantly improve the accuracy and reliability of machine vision systems for automatic driving, ultimately enhancing their safety and effectiveness.",CS,AI_ML,0.85,Extracted from log - paper 2127
Accurate modeling of the hybrid hash join algorithm,"The join of two relations is an important operation in database systems. It occurs frequently in relational queries, and join performance is a significant factor in overall system performance. Cost models for join algorithms are used by query optimizers to choose efficient query execution strategies. This paper presents an efficient analytical model of an important join method, the hybrid hash join algorithm, that captures several key features of the algorithm's performance—including its intra-operator parallelism, interference between disk reads and writes, caching of disk pages, and placement of data on disk(s). Validation of the model against a detailed simulation of a database system shows that the response time estimates produced by the model are quite accurate.",CS,AI_ML,0.85,Extracted from log - paper 2128
An Optimal Randomized Online Algorithm for QoS Buffer Management,"The QoS buffer management problem, with significant and diverse computer applications, e.g., in online cloud resource allocation problems, is a classic online admission control problem in the presence of resource constraints. In its basic setting, packets with different values, arrive in online fashion to a switching node with limited buffer size. Then, the switch needs to make an immediate decision to either admit or reject the incoming packet based on the value of the packet and its buffer availability. The objective is to maximize the cumulative profit of the admitted packets, while respecting the buffer constraint. Even though the QoS buffer management problem was proposed more than a decade ago, no optimal online solution has been proposed in the literature. This paper proposes an optimal randomized online algorithm for this problem.",CS,AI_ML,0.85,Extracted from log - paper 2129
Study on Employee Performance Evaluation Based on Adaptive Feature Selection Fuzzy Algorithm,"In our study, in terms of performance evaluation methods, the performance evaluation algorithm based on genetic algorithm and fuzzy comprehensive performance evaluation algorithm is introduced, their advantages and disadvantages are compared and analyzed, and the design idea of fuzzy performance evaluation algorithm based on compound elements is proposed. It can be divided into seven steps: first, clarify the evaluation purpose and object; second, select the optimal evaluation mode and method; third, compile the evaluation index system; fourth, it is to collect information extensively; fifth, the evaluation adopts a variety of methods, multiple angles, and multiple sides to collect materials so that the conclusion of the evaluation has sufficient factual basis; the sixth is to process the information and make a comprehensive evaluation; the seventh is to analyze the results and write an evaluation report. Based on the existing algorithms, the fuzzy performance evaluation algorithm based on compound elements is studied, and the detailed design of the algorithm is presented. Through the comparison and analysis of the performance evaluation algorithm based on compound elements and the performance evaluation algorithm of specific elements, the superiority of the performance evaluation algorithm based on compound elements is experimentally verified by comparing the operation time and classification accuracy. The performance evaluation system based on the performance evaluation algorithm designed in this study, combined with the official business of the performance evaluation system, modularizes the administrative management activities of the enterprise, digitizes the electronic office information, and conducts in-depth exploration of the unstructured natural language. A real-time performance evaluation system based on the arrangement of corporate administrative activities has been established. By designing and implementing a performance-assisted analysis system based on text content analysis, which is suitable for performance evaluation systems, it solves the imperfect problem of performance evaluation based on electronic enterprise administrative management.",CS,AI_ML,0.85,Extracted from log - paper 2130
Performance Evaluation of VM Placement Using Classical Bin Packing and Genetic Algorithm for Cloud Environment,"In current era, the trend of cloud computing is increasing with every passing day due to one of its dominant service i.e. Infrastructure as a service (IAAS), which virtualizes the hardware by creating multiple instances of VMs on single physical machine. Virtualizing the hardware leads to the improvement of resource utilization but it also makes the system over utilized with inefficient performance. Therefore, these VMs need to be migrated to another physical machine using VM consolidation process in order to reduce the amount of host machines and to improve the performance of system. Thus, the idea of placing the virtual machines on some other hosts leads to the proposal of many new algorithms of VM placement. However, the reduced set of physical machines needs the lesser amount of power consumption therefore; in current work the authors have presented a decision making VM placement system based on genetic algorithm and compared it with three predefined VM placement techniques based on classical bin packing. This analysis contributes to better understand the effects of the placement strategies over the overall performance of cloud environment and how the use of genetic algorithm delivers the better results for VM placement than classical bin packing algorithms.",CS,AI_ML,0.85,Extracted from log - paper 2131
Performance Evaluation of Ultrasound Images Using Non-Local Means Algorithm with Adaptive Isotropic Search Window for Improved Detection of Salivary Gland Diseases: A Pilot Study,"Speckle noise in ultrasound images (UIs) significantly reduces the accuracy of disease diagnosis. The aim of this study was to quantitatively evaluate its feasibility in salivary gland ultrasound imaging by modeling the adaptive non-local means (NLM) algorithm. UIs were obtained using an open-source device provided by SonoSkills and FUJIFILM Healthcare Europe. The adaptive NLM algorithm automates optimization by modeling the isotropic search window, eliminating the need for manual configuration in conventional NLM methods. The coefficient of variation (COV), contrast-to-noise ratio (CNR), and edge rise distance (ERD) were used as quantitative evaluation parameters. UIs of the salivary glands revealed evident visualization of the internal echo shape of the malignant tumor and calcification line using the adaptive NLM algorithm. Improved COV and CNR results (approximately 4.62 and 2.15 times, respectively) compared with noisy images were achieved. Additionally, when the adaptive NLM algorithm was applied to the UIs of patients with salivary gland sialolithiasis, the noisy images and ERD values were calculated almost similarly. In conclusion, this study demonstrated the applicability of the adaptive NLM algorithm in optimizing search window parameters for salivary gland UIs.",CS,AI_ML,0.85,Extracted from log - paper 2132
Parallelization and Performance Evaluation of an Edge Detection Algorithm on a Streaming Multi-Core Engine,"In the world of multi-core processors, the STI Cell Broadband Engine (BE) stands out as a heterogeneous 9-core processor with a PowerPC host processor (PPE) and 8 synergic processor engines (SPEs). The Cell BE architecture is designed to improve upon conventional processors in graphics and related areas by integrating 8 computation engines each with multiple execution units and large register sets to achieve a high performance per area return. In this paper, we discuss the parallelization, implementation and performance evaluation of an edge detection image processing application based on the Roberts edge detector on the Cell BE. The authors report the edge detection performance measured on a computer with one Cell processor and with varying numbers of synergic processor engines enabled. These results are compared to the results obtained on the Cell’s single PPE with all 8 SPEs disabled. The results indicate that edge detection performs 10 times faster on the Cell BE than on modern RISC processors.",CS,AI_ML,0.85,Extracted from log - paper 2133
Evaluation of the Performance of Technology Companies using VIKOR Algorithm,"Technology sector plays a central role in a country nowadays. The development of the current era where the contribution of technology cannot be denied and should be greatly concerned by the government indeed. The financial structure of the technology companies is needed to be improved so that they are more resistant to economic fluctuations. Moreover, the technology sector constitutes an essential part of global employment, and it can also contribute to world economic growth in an effective way. For nowadays’ trends, the competitive environment forced companies to utilize their financial resources effectively. As a result, the financial performance (FP) of the technology companies is investigated in this study. The FP is assessed by the important financial factors, which are current assets, total assets, current liabilities, total liabilities, revenue and net income. The purpose of this research is to propose a research framework to determine the FP and ranking of Malaysia’s technology companies using VIKOR algorithm. According to the results of this research, the five top-performing companies in terms of FP are MYEG, INARI, VSTECS, VITROX and PENTA. Based on the optimal solution of VIKOR algorithm, OMESTI is not able to show good financial performance as compared to other technology companies. This paper is also capable to provide insight into the technology companies for benchmarking in the future based on the ranking and current financial status of the companies. This study is significant to investigate the FP and ranking of technology companies with the proposed VIKOR algorithm.",CS,AI_ML,0.85,Extracted from log - paper 2134
Acceptance Analysis of School DAPODIK Information System Using the Technology Acceptance Model (TAM),"The purpose of this study was to determine user acceptance of the Basic Education Data system (DAPODIK) with the Technology Acceptance Model (TAM) based on five constructs. The five constructs are Perceived Usefulness, Perceived Ease Of Use, Attitude Towards Behavior, Behavioral Intention, and Actual Technology Use. This study will analyze the relationship between constructs that affect the acceptance and use of DAPODIK in Elementary School District Binuang.&#x0D; The type of research used in this research is explanatory research with data analysis techniques using the SEM-PLS approach, analyzed using SmartPLS 3.0 software . There are 62 samples of respondents from teachers at the District Binuang Elementary School in Indonesia.&#x0D; The conclusion of this study is that of the seven hypotheses proposed, there are six accepted hypotheses and one rejected hypothesis, namely: Perceived Ease Of Use affects Perceived Usefulness, Perceived Ease Of Use has an effect on Attitude Towards Behavior, Perceived Usefulness has an effect on Attitude Towards Behavior, Perceived Usefulness has no effect on Behavioral Intention, Perceived Usefulness has an effect on Actual Technology Use, Attitude Towards Behavior has an effect on Behavioral Intention, and Behavioral Intention has an effect on Actual Technology Use.",CS,AI_ML,0.85,Extracted from log - paper 2135
Cat Swarm Optimization Algorithm: A Survey and Performance Evaluation,"This paper presents an in-depth survey and performance evaluation of cat swarm optimization (CSO) algorithm. CSO is a robust and powerful metaheuristic swarm-based optimization approach that has received very positive feedback since its emergence. It has been tackling many optimization problems, and many variants of it have been introduced. However, the literature lacks a detailed survey or a performance evaluation in this regard. Therefore, this paper is an attempt to review all these works, including its developments and applications, and group them accordingly. In addition, CSO is tested on 23 classical benchmark functions and 10 modern benchmark functions (CEC 2019). The results are then compared against three novel and powerful optimization algorithms, namely, dragonfly algorithm (DA), butterfly optimization algorithm (BOA), and fitness dependent optimizer (FDO). These algorithms are then ranked according to Friedman test, and the results show that CSO ranks first on the whole. Finally, statistical approaches are employed to further confirm the outperformance of CSO algorithm.",CS,AI_ML,0.85,Extracted from log - paper 2136
Enhancing Algorithm Selection through Comprehensive Performance Evaluation: Statistical Analysis of Stochastic Algorithms,"Analyzing stochastic algorithms for comprehensive performance and comparison across diverse contexts is essential. By evaluating and adjusting algorithm effectiveness across a wide spectrum of test functions, including both classical benchmarks and CEC-C06 2019 conference functions, distinct patterns of performance emerge. In specific situations, underscoring the importance of choosing algorithms contextually. Additionally, researchers have encountered a critical issue by employing a statistical model randomly to determine significance values without conducting other studies to select a specific model for evaluating performance outcomes. To address this concern, this study employs rigorous statistical testing to underscore substantial performance variations between pairs of algorithms, thereby emphasizing the pivotal role of statistical significance in comparative analysis. It also yields valuable insights into the suitability of algorithms for various optimization challenges, providing professionals with information to make informed decisions. This is achieved by pinpointing algorithm pairs with favorable statistical distributions, facilitating practical algorithm selection. The study encompasses multiple nonparametric statistical hypothesis models, such as the Wilcoxon rank-sum test, single-factor analysis, and two-factor ANOVA tests. This thorough evaluation enhances our grasp of algorithm performance across various evaluation criteria. Notably, the research addresses discrepancies in previous statistical test findings in algorithm comparisons, enhancing result reliability in the later research. The results proved that there are differences in significance results, as seen in examples like Leo versus the FDO, the DA versus the WOA, and so on. It highlights the need to tailor test models to specific scenarios, as p-value outcomes differ among various tests within the same algorithm pair.",CS,AI_ML,0.85,Extracted from log - paper 2137
Performance Evaluation of Network Intrusion Detection System for Detecting Zero-Day Attacks: SNORT-XSS Algorithm,"The main objective of Intrusion Detection and Prevention Systems is to provide a method of detecting and preventing malicious behaviors in a network system to minimize the harm caused by attackers. In this article, a survey of the techniques applied for the identification and classification of attacks based on KDD Cup’99 and DARPA data set is discussed, and from the open issues a new and a proficient method called SNORT-XSS algorithm is anticipated and implemented that can recognize and classify real time intrusions including zero day attacks. For this research, the SNORT open source tool developed by CISCO Systems was used to describe rules from the existing data collected from DARPA and KDD Cup’99 dataset. Fuzzy Reasoning system is applied to organize the rules into fuzzy sets that reduces true negative and false positive rate. The advantage of Feed Forward Neural Network with Back Propagation of Errors from Artificial Neuron Networks is considered for training, validating and testing the proposed system. The experimental results achieved by preprocessing anomalous behaviors in a network and the detection rate of zero-day attacks or novel attacks were very promising and were beyond expectations. The precision values of the proposed model were 98.93% and 98.89% respectively, and detection rate of Probe and DoS attacks were greater than 98%. The false positive and true negative rate is almost negligible. It was noticed that the best categorization was acquired at epoch numbers from 50 to 55 with a mean squared error of 0.004.",CS,AI_ML,0.85,Extracted from log - paper 2138
Design of performance evaluation model for warm mix recycled asphalt pavement based on fuzzy decision algorithm,"In order to improve the accuracy and comprehensiveness of asphalt pavement performance evaluation, a performance evaluation model based on warm mixed recycled asphalt pavement was proposed. Based on the evaluation indexes such as pavement quality, pavement damage, pavement structure bearing capacity, pavement skid resistance, pavement rutting and pavement comprehensive performance, the performance evaluation system of warm mixed recycled asphalt pavement was established. According to the standard numerical characteristics of asphalt pavement performance evaluation index, the subjection function of asphalt pavement performance is constructed. The trapezoidal and semi-trapezoidal distribution functions are obtained, and the membership functions of each distribution section are established. Considering the comprehensiveness, fuzziness and complexity of the evaluation, the fuzzy decision algorithm is introduced to determine the interval weight of each index, and the structure is quantified according to the evaluation index. The membership function and fuzzy subset are established. The evaluation standard and evaluation level are determined based on fuzzy relation matrix, and the pavement performance is evaluated. The experimental results show that the evaluation results of this model are accurate and reliable, and it has obvious advantages in application.",CS,AI_ML,0.85,Extracted from log - paper 2139
Impact of Experimental Learning on Graduates Success in Engineering Education,"Experimental Learning is an Important learning approach in which students learn by doing it. This research analysed effective learning approach for success of graduates in engineering education. For this experiment 168 students of computer science and engineering from college A and B are considered, where college A adopted Experimental Learning and college B adopted Traditional learning approach. The result is significantly difference in success rate of college A than college B. The course delivery in college A has done by project-based learning which concept of course has been implemented by students by doing projects, solving problems, mini projects for real world problems to find solutions and completing course by doing it practically with involvement of latest technologies. This Experience develops problem solving, creative thinking, leadership qualities, teamwork and ready for industry. Students success is measured with semester end exam (SEE) scores and Placements. This shows college A graduates are successes then college B and also a positive change in individual growth and Organizational. A conclusion is done that Experimental Learning Improve Students� performance and can be implemented in other disciplines.",CS,AI_ML,0.85,Extracted from log - paper 2140
EVALUATION OF PERFORMANCE EFFICIENCY OF PACKING A GROUP OF PRODUCTS IN THE WORKPLACE OF ADDITIVE MACHINE USING A GENETIC ALGORITHM,"Research results of possibilities of packing a group of 3D-models of products in a layered build space using a genetic algorithm are presented. It is proposed to determine the efficiency of the optimization problem of rational arrangement of 3D-models group in the workspace of additive machines depending on the number of loaded products. Condition for efficient use of the layered build workspace is the minimum number of layers per product and the largest relative filling. Such criteria are important, for example, for SLS/SLM technologies. Examples of evaluation based on the analysis of derived voxel 3D model of the workspace with located products are considered. Industrial products with different geometrical complexity were selected as test 3D models. This approach allowed to perform a comparative analysis of the results depending on the design features of products. The practical realization was performed in the subsystem of packing 3D-models in a workspace, which is part of the technological preparation system for the manufacture of complex products by additive methods. This system was developed at the Department of ""Integrated Technologies of Mechanical Engineering"" named after M. Semko of NTU ""KhPI"".",CS,AI_ML,0.85,Extracted from log - paper 2141
An experimental study in Real-time Facial Emotion Recognition on new 3RL dataset,"Although real-time facial emotion recognition is a hot topic research domain in the field of human-computer interaction, state-ofthe-art available datasets still suffer from various problems, such as some unrelated photos such as document photos, unbalanced numbers of photos in each class, and misleading images that can negatively affect correct classification. The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems. The 3RL dataset is labelled with five basic emotions: happiness, fear, sadness, disgust, and anger. Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we applied the most commonly used algorithms in previous works, SVM and CNN. The results show a noticeable improvement in generalization on the 3RL dataset. Experiments have shown an accuracy of up to 91.4% on 3RL dataset using CNN where results on FER2013, CK+ are, respectively (approximately from 60% to 85%).",CS,AI_ML,0.85,Extracted from log - paper 2142
EXPERIMENTAL ANALYSIS OF MULTINATIONAL GENETIC ALGORITHM AND ITS MODIFICATIONS,"Context. Niching genetic algorithms are one of the most popular approaches to solve multimodal optimization problems. When classifying niching genetic algorithms it is possible to select algorithms explicitly analyzing topography of fitness function landscape; multinational genetic algorithm is one of the earliest examples of these algorithms. Objective. Development and analysis of the multinational genetic algorithm and its modifications to find all maxima of a multimodal function. Method. Experimental analysis of algorithms is carried out. Numerous runs of algorithms on well-known test problems are conducted and performance criteria are computed, namely, the percentage of convergence, real (global, local) and fake peak ratios; note that peak rations are computed only in case of algorithm convergence. Results. Software implementation of a multinational genetic algorithm has been developed and experimental tuning of its parameters has been carried out. Two modifications of hill-valley function used for determining the relative position of individuals have been proposed. Experimental analysis of the multinational genetic algorithm with classic hill-valley function and with its modifications has been carried out. Conclusions. The scientific novelty of the study is that hill-valley function modifications producing less number of wrong identifications of basins of attraction in comparison with classic hill-valley function are proposed. Using these modifications yields to performance improvements of the multinational genetic algorithm for a number of test functions; for other test functions improvement of the quality criteria is accompanied by the decrease of the convergence percentage. In general, the convergence percentage and the quality criterion values demonstrated by the algorithm studied are insufficient for practical use in comparison with other known algorithms. At the same time using modified hill-valley functions as a post-processing step for other niching algorithms seems to be a promising improvement of performance of these algorithms.",CS,AI_ML,0.85,Extracted from log - paper 2143
Performance Analysis of Quicksort Algorithm: An Experimental Study of Its variants,"The Quicksort algorithm is often the best practice choice for sorting due to its remarkable efficiency on average cases, small constant factors hidden in the θ(n log n) notation, and its in-place sorting nature. This paper provides a comprehensive study and empirical results of the Quicksort algorithm and its variants. The study encompasses all Quicksort variants from 1961 to the present. Additionally, the paper compares the performance of different versions of Quicksort in terms of running time on integer arrays that are sorted, reversed, and randomly generated. Our work will be invaluable to anyone interested in studying and understanding the Quicksort algorithm and its various versions.",CS,AI_ML,0.85,Extracted from log - paper 2144
An Experimental Investigation of the Use of Computer-Based Graphics in Decision Making,"This paper presents the results of an experiment designed to investigate the impact of computer-based graphics on decision making. The experimental task consisted of selecting quarterly reorder quantities for an importer under condition of uncertain demand. Subjects in the experiment were participants in an executive program for middle and upper level managers. Each subject received information on the cumulative probability distribution of demand and had an opportunity to run up to eight trial simulations with past demand data using his or her order quantities. After completing the trial simulations, the subjects made quarterly ordering decisions for one year in which the quantities demanded were drawn from the demand distribution.  Treatments included the use of a hard copy terminal and five different types of displays on a CRT. The results of the experiment provide limited support for the use of graphics presentation in an information system. Decision or cognitive style also appears to be an important variable influencing the performance of an individual and the reaction to an information system. The implications of the findings for the design of information systems are discussed.",CS,AI_ML,0.85,Extracted from log - paper 2145
USE OF TAM FOR EVALUATION OF INTERNSHIP INFORMATION SYSTEM,"This study aims to evaluate the internship information system at a Vocational High School. To achieve this goal, it is necessary to perform statistical analysis and calculations on the influence of the relationship between each variable in the Technology Acceptance Model (TAM) with the addition of one external variable, namely self-efficacy variable. The approach used in this research is quantitative. The source of the data used is primary data, derived from the users of the internship information system through a questionnaire containing eighteen questions with five alternative answers using a Likert scale as a point determination. Respondents targeted in this study amounted to 202 respondents, but one other thing was constrained, only 180 respondents filled out the questionnaire. refers to the analysis obtained using WarpPLS, data obtained that: (1) the convenience factor has an influence on the benefits, in using the information system, (2) the use benefit factor has a real influence on the intensity and use of the information system, (3) the attitude of use has influence on the intensity of the use of information systems, (4) the intensity has an effect on the use of information systems in real or implementation. (5) self-efficacy has an influence on the ease of use of information systems.",CS,AI_ML,0.85,Extracted from log - paper 2146
Experimental interpretation of adequate weight-metric combination for dynamic user-based collaborative filtering,"Recommender systems include a broad scope of applications and are associated with subjective preferences, indicating variations in recommendations. As a field of data science and machine learning, recommender systems require both statistical perspectives and sufficient performance monitoring. In this paper, we propose diversified similarity measurements by observing recommendation performance using generic metrics. Considering user-based collaborative filtering, the probability of an item being preferred by any user is measured. Having examined the best neighbor counts, we verified the test item bias phenomenon for similarity equations. Because of the statistical parameters used for computing in a global scope, there is implicit information in the literature, whether those parameters comprise the focal point user data statically. Regarding each dynamic prediction, user-wise parameters are expected to be generated at runtime by excluding the item of interest. This yields reliable results and is more compatible with real-time systems. Furthermore, we underline the effect of significance weighting by examining the similarities between a user of interest and its neighbors. Overall, this study uniquely combines significance weighting and test-item bias mitigation by inspecting the fine-tuned neighborhood. Consequently, the results reveal adequate similarity weight and performance metric combinations. The source code of our architecture is available at https://codeocean.com/capsule/1427708/tree/v1.",CS,AI_ML,0.85,Extracted from log - paper 2147
Recruiting a Probability-Based Online Panel via Postal Mail: Experimental Evidence,"Once recruited, probability-based online panels have proven to enable high-quality and high-frequency data collection. In ever faster-paced societies and, recently, in times of pandemic lockdowns, such online survey infrastructures are invaluable to social research. In absence of email sampling frames, one way of recruiting such a panel is via postal mail. However, few studies have examined how to best approach and then transition sample members from the initial postal mail contact to the online panel registration. To fill this gap, we implemented a large-scale experiment in the recruitment of the 2018 sample of the German Internet Panel (GIP) varying panel recruitment designs in four experimental conditions: online-only, concurrent mode, online-first, and paper-first. Our results show that the online-only design delivers higher online panel registration rates than the other recruitment designs. In addition, all experimental conditions led to similarly representative samples on key socio-demographic characteristics.",CS,AI_ML,0.85,Extracted from log - paper 2148
Global cost diversity aware dispatch algorithm for heterogeneous data centers (abstracts only),"Large, Internet based companies service user requests from multiple data centers located across the globe. These data centers often house a heterogeneous computing infrastructure and draw electricity from the local electricity market.Reducing the electricity costs of operating these data centers is a challenging problem, and in this work, we propose a novel solution which exploits both the data center heterogeneity and global electricity market diversity to reduce data center operating cost. We evaluate our solution in our test-bed that simulates a heterogeneous data center, using real-world request workload and real-world electricity prices. We show that our strategies achieve cost and energy saving of at least 21% over a naive load balancing scheme that distributes requests evenly across data centers, and outperform existing solutions which either do not exploit the electricity market diversity or do not exploit data center hardware diversity.",CS,AI_ML,0.85,Extracted from log - paper 2149
Analysis of a transmission scheduling algorithm for supporting bandwidth guarantees in bufferless networks,"In a network of bufferless packet multiplexers, the user-perceived capacity of an ingress-egress tunnel (connection) may degrade quickly with increasing path length. This is due to the compounding of transmission blocking probabilities along the path of the connection, even when the links are not overloaded. In such an environment, providing users (e.g., client ISPs) with tunnels of statistically guaranteed bandwidth may limit the network's connection-carrying capacity.In this paper, we introduce and analyze a transmission-scheduling algorithm that employs randomization and traffic regulation at the ingress, and batch scheduling at the links. The algorithm ensures that a fraction of transmissions from each connection is consistently subject to small blocking probability at every link, so that these transmissions are likely to survive long paths. For this algorithm, we obtain tight bounds on the expectation and tail probability of the blocking rate of any ingress-egress connection. We compare the bounds to those obtained using the FCFS link-scheduling rule. We find that the proposed scheduling algorithm significantly improves the network's connection-carrying capacity.In deriving the desired bounds, we develop an analytic framework for stochastically comparing network-wide routing and bandwidth allocation scenarios with respect to blocking in a packet multiplexer. The framework enables us to formally characterize the routing and bandwidth allocation scenarios that maximize the expected blocking rate along the path of a tagged connection.",CS,AI_ML,0.85,Extracted from log - paper 2150
Performance evaluation and comparative analysis of CrowWhale-energy and trust aware multicast routing algorithm,"Multipath routing helps to establish various quality of service parameters, which is significant in helping multimedia broadcasting in the Internet of Things (IoT). Traditional multicast routing in IoT mainly concentrates on ad hoc sensor networking environments, which are not approachable and vigorous enough for assisting multimedia applications in an IoT environment. For resolving the challenging issues of multicast routing in IoT, CrowWhale-energy and trust-aware multicast routing (CrowWhale-ETR) have been devised. In this research, the routing performance of CrowWhale-ETR is analyzed by comparing it with optimization-based routing, routing protocols, and objective functions. Here, the optimization-based algorithm, namely the Spider Monkey Optimization algorithm (SMO), Whale Optimization Algorithm (WOA), Dolphin Echolocation Optimization (DEO) algorithm, Water Wave Optimization (WWO) algorithm, Crow Search Algorithm (CSA), and, routing protocols, like Ad hoc On-Demand Distance Vector (AODV), CTrust-RPL, Energy-Harvesting-Aware Routing Algorithm (EHARA), light-weight trust-based Quality of Service (QoS) routing, and Energy-awareness Load Balancing-Faster Local Repair (ELB-FLR) and the objective functions, such as energy, distance, delay, trust, link lifetime (LLT) and EDDTL (all objectives) are utilized for comparing the performance of CrowWhale-ETR. In addition, the performance of CrowWhale-ETR is analyzed in terms of delay, detection rate, energy, Packet Delivery Ratio (PDR), and throughput, and it achieved better values of 0.539 s, 0.628, 78.42%, 0.871, and 0.759 using EDDTL as fitness.",CS,AI_ML,0.85,Extracted from log - paper 2151
An Observation and Experimental Evaluation of Image Spam Detection,"In belonging to other supports duel beside researchers of image spam detections, unsolicited mail have newly developed the image based spam dodge to construct the investigation of e-mails’ content of text unsuccessful. To avoid signature based recognition, it involves in implanting the unsolicited text or message into an appendage image, which is frequently arbitrarily customized. Identifying image based spam emails tries out to be an motivating illustration of the problem text embedded in images were subjected to noise such as background pattern, color, font variations and imperfections in a font size so as to eliminate the chances of being identified as unsolicited e-mail by classification techniques. In this research paper we spring a exhaustive review and categorization of machine learning and classification systems suggested so far in contradiction of image based spam email, and make an empirical investigation and correlation of few of them on real, widely accessible data sets.",CS,AI_ML,0.85,Extracted from log - paper 2152
Performance Evaluation of Ingenious Crow Search Optimization Algorithm for Protein Structure Prediction,"Protein structure prediction is one of the important aspects while dealing with critical diseases. An early prediction of protein folding helps in clinical diagnosis. In recent years, applications of metaheuristic algorithms have been substantially increased due to the fact that this problem is computationally complex and time-consuming. Metaheuristics are proven to be an adequate tool for dealing with complex problems with higher computational efficiency than conventional tools. The work presented in this paper is the development and testing of the Ingenious Crow Search Algorithm (ICSA). First, the algorithm is tested on standard mathematical functions with known properties. Then, the application of newly developed ICSA is explored on protein structure prediction. The efficacy of this algorithm is tested on a bench of artificial proteins and real proteins of medium length. The comparative analysis of the optimization performance is carried out with some of the leading variants of the crow search algorithm (CSA). The statistical comparison of the results shows the supremacy of the ICSA for almost all protein sequences.",CS,AI_ML,0.85,Extracted from log - paper 2153
Performance Evaluation of Load Balancing Algorithm for Virtual Machine in Data Centre in Cloud Computing,"Cloud computing has become biggest buzz in the computer era these days. It runs entire operating systems on the cloud and do everything on cloud to store data off-site. Cloud computing is primarily based on grid computing, but itâ€™s a new computational model. Cloud computing has emerged into a new opportunity to further enhance way of hosting data centre and provide services. The primary substance of cloud computing is to deal the computing power, storage, different sort of stages and services which assigned to the external users on demand through the internet. Task scheduling in cloud computing is vital role optimisation and effective dynamic resource allocation for load balancing. In cloud, the issue focused is under utilisation and over utilisation of the resources to distribute workload of multiple network links for example, when cloud clients try to access and send request to the same cloud server while the other cloud server remain idle at that moment, leads to the unbalanced of workload on cloud data centers. Thus, load balancing is to assign tasks to the individual cloud data centers of the shared system so that no single cloud data centers is overloaded or under loaded. A Hybrid approach of Honey Bee (HB) and Particle Swarm Optimisation (PSO) load balancing algorithm is combined in order to get effective response time. The proposed hybrid algorithm has been experimented by using CloudSim simulator. The result shows that the hybrid load balancing algorithm improves the cloud system performance by reducing the response time compared to the Honey Bee (HB) and Particle Swarm Optimisation (PSO) load balancing algorithm.Â Â",CS,AI_ML,0.85,Extracted from log - paper 2154
Performance Evaluation of Decision Trees with Machine Learning Algorithm,"Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. Decision trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition and Data Mining have dealt with the issue of growing a decision tree from available data. Decision trees in machine learning will be used for classification problems, to categorize objects to gain an understanding of similar features. Decision trees helps in decision-making by representing complex choices in a hierarchical structure. Every node in decision tree verifies specific attributes, guiding decisions based on different data values in the dataset. Leaf nodes provide final outcomes and result which gives a clear and interpretable path for decision analysis in machine learning. Therefore implementation of Decision tree algorithm using python is presented in this paper Keywords— Decision Trees, Classification. Machine learning, statistics, regression",CS,AI_ML,0.85,Extracted from log - paper 2155
Application of Nave Bayes Algorithm for Security Performance Evaluation at PT. Sei Mangke Nusantara 3,"Security serves as a security guard in an agency. In carrying out his duties a security must have a balance and functions in achieving the needs of an agency itself. In the world of work at an agency, especially security, which plays a role in maintaining the security of the agency. In the current era of security, there are those who are not responsible for their duties, so that an agency does not feel comfortable with the security. The purpose of this study to evaluate the performance of security at PT. Sei Mangke Nusantara Tiga and to create a safe and orderly atmosphere. In this study, the researchers used a Data Mining technique using the Naïve Bayes algorithm. Sources of research data obtained from the provision of questionnaires or questionnaires to danton PT. Sei Mangke Nusantara Tiga. The variables of the research used are discipline, attendance, honesty, communication skills and responsibility. In this study, the alternative used as a sample is security at PT. Sei Mangke Nusantara Tiga. The number of data tested is 5 security with two classes. From the results of the calculation of the Naïve Bayes Algorithm, it is obtained that there are 3 classes of good security and 2 security classes that are not good. The results of this study found that the level of accuracy of 100.00%.",CS,AI_ML,0.85,Extracted from log - paper 2156
Experimental evaluation of the performance of coherent digital communications algorithm in littoral ocean,"Underwater acoustic communications data using phase modulated signals were collected during the Littoral Warfare Advanced Development 98-1 experiment, conducted in the Gulf of Mexico in Nov. 1998. CW pulse and quadrature phase shifted keying (QPSK) signals were projected from a towed source. Data of three baud rates were collected to study the temporal and spatial variation of the acoustic impulse response of the ocean and to evaluate the performance of a phase coherent digital communication algorithm. Post-experiment analysis examined the Doppler shift, signal amplitude fluctuation, signal-to-noise ratio of the received signal, channel impulse response, temporal correlation function of the received signals, and signal fluctuation statistics during several segments of the experiment. The objective is to determine the characteristics of signal propagation in littoral environment, and to determine whether and how these characteristics affect the bit-error-rate of a coherent acoustic communication algorithm. The analysis results serve as guidance in the design of a reliable coherent digital communication system. [This work is supported by the Office of Naval Research.]",CS,AI_ML,0.85,Extracted from log - paper 2157
Decision Tree Algorithm in the Performance Evaluation of School-Enterprise Cooperation for Higher Vocational Education,"With the development of China’s economy and the internet, machine learning has become one of the people’s favorite ways of working. This study aims to solve the problems of brain drain, inefficiency, and injustice in the performance appraisal of school-enterprise cooperation. Decision tree technology is used to establish an assessment system. After the assessment index data are segmented, and the fuzzy version of the C4.5 algorithm is used to calculate and count different data segments. Finally, different data types are used to classify and construct decision trees. The school-enterprise cooperation performance appraisal system has been established and perfected for higher vocational education. In this way, the performance appraisal system is optimized in the school-enterprise cooperation. The system improves work efficiency while reducing manual labor and improves the problems in the performance appraisal of other colleges and universities’ cooperation. Facts have proved that the establishment of decision trees can effectively solve the problems of duplication of indicators and complex calculations in performance appraisal. After being optimized, the C4.5 algorithm will increase the calculation accuracy to 95%. The overall speed of establishment is increased by 5% based on the algorithm before optimization. This breaks the traditional performance appraisal system and promotes the performance appraisal of school-enterprise cooperation to be more open, transparent, fair, and equal.",CS,AI_ML,0.85,Extracted from log - paper 2158
Experimental study of Elementary Cellular Automata dynamics using the density parameter,"Classifying cellular automata in order to capture the notion of chaos algorithmically is a challenging problem than can be tackled in many ways.We here give a classification based on the computation of a macroscopic parameter, the $d$-spectrum, and show how our classifying scheme can be used to separate the chaotic ECA from the non-chaotic ones.",CS,AI_ML,0.85,Extracted from log - paper 2159
Analisis Technology Acceptance Model (TAM) Terhadap Penerimaan Mahasiswa Pada Portal Akademik,"The purpose of this study was to evaluate the use of academic information systems among students. Seeing how student admissions use academic information systems for future improvements. This study uses the Technology Acceptance Model (TAM) method which is related to the content design and content quality of academic information systems. Data collection from this study was carried out by using a questionnaire. The questionnaire was filled in by ≥152 respondents consisting of students from various faculties and departments and at different semester levels. By obtaining construct reliability cut off ≥ 0.7 and Average Variance Extracted with cut off ≥ 0.5, the indicator value has good internal consistency. From the results of the analysis conducted, it shows that the significant influence of content quality on content design, quality, and content design on the ease and benefits felt by students. Although there is no significant correlation between ease of use of the portal and its perceived benefits.",CS,AI_ML,0.85,Extracted from log - paper 2160
On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study,"Safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions. This is especially true for complex AI-enabled and software intensive systems such as Autonomous Drive (AD). System-Theoretic Process Analysis (STPA) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry. However, STPA assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels. This would inhibit software developers from using STPA to analyze their software as part of a bigger system, resulting in a lack of traceability. This can be seen as a maintainability challenge in continuous development and deployment (DevOps). In this paper, STPA's different guidelines for the automotive industry, e.g. J31887/ISO21448/STPA handbook, are firstly compared to assess their applicability to the distributed development of complex AI-enabled systems like AD. Further, an approach to overcome the challenges of using STPA in a multi-level design context is proposed. By conducting an interview study with automotive industry experts for the development of AD, the challenges are validated and the effectiveness of the proposed approach is evaluated.",CS,AI_ML,0.85,Extracted from log - paper 2161
PRODUCT CLUSTERING USING K-MEANS METHOD IN CV. JAYA ABADI,"CV. Jaya Abadi is a company engaged in the distribution of goods, or, to be more precise, a distributor of raw materials for making bread. The fluctuating number of requests from consumers results in stock that must be prepared to be unstable. In addition, many types of products make stock management inaccurate. Sometimes we do not want to have a shortage of stock, goods, or certain products when consumer demand is high. The purpose of this research is to analyse sales data mining through the clustering method with the k-means algorithm. The method used for this analysis uses clustering with the k-means algorithm. So that this research is not subjective, the authors also use research methods in the form of observation, interviews, and documentation. The results of this study were carried out by analysing sales by applying data mining through the clustering method with the k-means algorithm. Data mining that has been going on for a long time can be used as a reference in the management of CV Jaya Abadi. Apart from being used for administrative purposes each period, it can also be used as a decision-making solution in order to maximize existing products and sales at CV Jaya Abadi.",CS,AI_ML,0.85,Extracted from log - paper 2162
Experimental Analysis of Covid 19 Spread Predictor using Linear Regression Algorithm,"In the past few years, people’s life is affecting badly by the spread of coronavirus due to a lack of information about the spread of the virus and proper management to control it. The government is also looking for ways to get information that how beneficial is their preventive measures. So, that they can Know that whether their preventive measures need to be modified or not. The effect of coronavirus can be seen by the number of people affected, the number of people being treated, and the number of people dead. These are the data based on which our application will make a prediction. The goal of this paper is to make a model that will give us a good prediction based on other variables. In most cases, we use linear regression for data because linear regression gives good accuracy. This paper will be helpful for both people and the government, they will be able to predict the number of cases in the next month so that they can prepare themselves to face the problem and control it from further spreading.",CS,AI_ML,0.85,Extracted from log - paper 2163
Identifying Factors Affecting Acceptance of Virtual Reality in Classrooms Based on Technology Acceptance Model (TAM),"Technology Acceptance Model (TAM) has received great recognition through the various research conducted on determining users’ acceptance of relevant technology innovation. Past researches have focused on technology innovation in education such as e-learning, Learning Management Systems and online applications. The 21st century teaching and learning framework has identified the relevance of the Internet of Things (IoT) and online applications as part of the teaching and learning process. Besides e-learning, MOOCs, Virtual and Augmented Reality have also found their place in the emerging teaching and learning platforms. As Virtual Reality only became popularized in classrooms in the recent years, not much is known about users’ acceptance of this technology innovation in the classroom. This paper, which is based on the TAM, attempted to identify the factors that could affect the respondents’ acceptance of Virtual Reality (VR) in classrooms. Factors on the perceived ease of use (PEoU) and perceived usefulness (PU) affecting the respondents’ attitude and intention to use VR in their classrooms were studied. Employing a quantitative research design, a set of questionnaire based on constructs adapted by Davis (1989) and adapted from past researches (Ngai et al, 2005; Weng et al, 2018, Muhamad Sufi, 2019) was distributed to a group of in-service teachers who were pursuing their postgraduate studies in one of the faculties in Universiti Teknologi MARA. The data was analyzed using SPSS in determining the relationships between the independent variables and the dependent variables. The analysis has further confirmed past research findings. However, in the context of VR, some suggestions to improve current practice are suggested. Policymakers and decision-makers could be enlightened by the present study’s findings. Likewise, teachers may find VR a more convincing platform to be integrated in their classrooms.",CS,AI_ML,0.85,Extracted from log - paper 2164
Experimental analysis of some computation rules in a simple parallel reasoning system for the ALC description logic,"Experimental analysis of some computation rules in a simple parallel reasoning system for theALCdescription logicA computation rule determines the order of selecting premises during an inference process. In this paper we empirically analyse three particular computation rules in a tableau-based, parallel reasoning system for theALCdescription logic, which is built in the relational programming model in the Oz language. The system is constructed in the lean deduction style, namely, it has the form of a small program containing only basic mechanisms, which assure soundness and completeness of reasoning. In consequence, the system can act as a convenient test-bed for comparing various inference algorithms and their elements. We take advantage of this property and evaluate the studied methods of selecting premises with regard to their efficiency and speedup, which can be obtained by parallel processing.",CS,AI_ML,0.85,Extracted from log - paper 2165
An experimental evaluation of refinement techniques for the subgraph isomorphism backtracking algorithms,"AbstractIn this paper, we study a well-known computationally hard problem, called the subgraph isomorphism problem where the goal is for a given pattern and target graphs to determine whether the pattern is a subgraph of the target graph. Numerous algorithms for solving the problem exist in the literature and most of them are based on the backtracking approach. Since straightforward backtracking is usually slow, many algorithmic refinement techniques are used in practical algorithms. The main goal of this paper is to study such refinement techniques and to determine their ability to speed up backtracking algorithms. To do this we use a methodology of experimental algorithmics. We perform an experimental evaluation of the techniques and their combinations and, hence, demonstrate their usefulness in practice.",CS,AI_ML,0.85,Extracted from log - paper 2166
Experimental study on dominant sets clustering,"Based on a graph‐theoretic concept of a cluster, dominant sets clustering has been shown to be an attractive clustering algorithm with many useful properties. In this study, the authors conduct a comprehensive study of related issues in dominant sets clustering, in an endeavour to explore the potential of this algorithm and obtain the best clustering results. Specifically, they empirically investigate how similarity parameters, similarity measures and game dynamics influence the dominant sets clustering results. From experiments on eight datasets, they conclude that distance‐based similarity measures perform evidently better than cosine and histogram intersection similarity measures potentially, and they need to find the best‐performing similarity parameter to make use of this advantage. They then study the effect of similarity parameter on dominant sets clustering results and induce the range of the best‐performing similarity parameters. Furthermore, they find that the recently proposed infection and immunisation dynamics performs better than the replicator dynamics in most cases while being much more efficient than the latter. These observations are helpful in applying dominant sets clustering to practical problems, and also indicate directions for further improvement of this algorithm.",CS,AI_ML,0.85,Extracted from log - paper 2167
An Experimental Study of Polynomial Time Algorithms for Minimum 2-Edge Connected Subgraph Problem,"In this paper, we focus on the problem of finding a minimum-sized directed 2-edge-connected subgraph, a problem classified as NP-Complete [8], which plays a critical role in various practical applications. We present approximation algorithms aimed at finding efficient, high-quality solutions within polynomial time. These algorithms are based on a comprehensive analysis of the problem of finding a directed 2-edge-connected subgraph, with performance evaluated in terms of the number of edges. The results of our experiments demonstrate that the proposed algorithms effectively reduce the number of remaining edges across different graph scenarios, particularly in high-density graphs. Moreover, they maintain strong connectivity even in the event of edge failures, ensuring the continuity of network operations in the face of faults and disasters.",CS,AI_ML,0.85,Extracted from log - paper 2168
Computer‐aided skin prick test,"Abstract:  The interpretation of the skin prick test is subject to inter‐observer variation. To remove this variation, a computerized procedure for the skin prick test is suggested. Instead of manually measuring the emerging wheals, a series of photographs is automatically taken of the forearm. The photographs thus taken are then analyzed with a digital image processing algorithm to give the measurement results. The computerized test has the added benefit of being able to produce a time series for the wheal size. This makes it possible to see the onset time of the reaction in addition to the size of the wheal. Preliminary feasibility test suggests that the simple setup described in this letter is able to perform the skin prick test automatically and to show the kinetic behaviour of the wheal. The main challenges with the test setup are related to illumination and wheal detection algorithms.",CS,AI_ML,0.85,Extracted from log - paper 2169
Artificial Intelligence-Based color Reconstruction of Mogao Grottoes Murals Using Computer Vision Techniques,"The Mogao Grottoes murals have deteriorated over centuries due to environmental exposure, pigment degradation, and natural ageing, making cultural heritage preservation difficult. AI and computer vision can identify, classify, and reconstruct faded pigments, revolutionizing color restoration. This reconstructs faded mural sections using deep learning, image processing, and pigment data implemented through TensorFlow, PyTorch and OpenCV. The study uses high-resolution Digital Dunhuang database images of Mogao Grottoes murals and 50 pigments categorized by color, stability, and chemical composition. CNNs and deep learning-based color mapping algorithms detect fading and suggest color restorations of pigments. AI reconstructions along with history accuracy through expert evaluations and pigment records. Artificial intelligence-driven mural conservation detects faded pigments, precisely reconstructs missing sections, and matches restored colors to historical authenticity, improving accuracy, efficiency, and scalability. Scientifically, AI-based digital heritage conservation outperforms manual restoration. AI preserves and faithfully reconstructs cultural heritage sites using historical artworks using global digital pigment database and deep learning-driven restoration models. The first reproducible and scientific model (CNN, GAN and deep learning-based color mapping algorithms) using AI-based color restoration and historical pigment analysis in Mogao Grottoes murals was created.",CS,AI_ML,0.85,Extracted from log - paper 2170
ANALISIS KESUKSESAN PENERAPAN SISTEM INFORMASI AKADEMIK MENGGUNAKAN MODEL DELONE-MCLEAN DAN TECHNOLOGY ACCEPTANCE MODEL (TAM),"ABSTRAK&#x0D; Penelitian ini bertujuan untuk menganalisis kesuksesan penerapan Sistem Informasi Akademik dengan &#x0D; memadukan model Delone Mclean dan Technology Acceptance Model (TAM). Populasi penelitian ini &#x0D; adalah Dosen, Karyawan dan mahasiswa yang berjumlah 359 responden, metode sampling menggunakan &#x0D; purposive sampling, sedangkan sampel yang memenuhi kriteria berjumlah 139 responden. Kuesioner diuji &#x0D; dengan uji reliabilitas dan uji validitas, selanjutnya dilakukan pengujian menggunakan pemodelan &#x0D; Struktural Equation Model (SEM) dengan software SPSS Amos 22. Hasil penelitian menunjukkan bahwa &#x0D; pengguna menerima dengan baik untuk penggunaan Sistem Informasi Akademik yang dapat dilihat bahwa &#x0D; hampir semua variabel (Kualitas sistem, kualitas layanan, persepsi kemudahan penggunaan, persepsi &#x0D; kemanfaatan, niat perilaku penggunaan dan kepuasan pengguna) berpengaruh secara signifikan. Dengan &#x0D; demikian penggunaan Sistem Informasi Akademik dapat diterima untuk proses kegiatan belajar mengajar. &#x0D; Kata kunci : Kesuksesan Sistem Informasi, Sistem Informasi Akademik&#x0D; ABSTRACT&#x0D; This study aims to analyze the success of the application of the Academic Information System by combining &#x0D; the Delone Mclean model and the Technology Acceptance Model (TAM). The population of this study were &#x0D; 359 lecturers, employees and students, the sampling method used purposive sampling, while the sample that &#x0D; met the criteria was 139 respondents. The questionnaire was tested with reliability and validity tests, then &#x0D; tested using the Structural Equation Model (SEM) modeling with SPSS Amos 22 software. service quality, &#x0D; perceived ease of use, perceived usefulness, intended use behavior and user satisfaction) have a significant &#x0D; effect. Thus the use of Academic Information Systems can be accepted for the process of teaching and &#x0D; learning activities.&#x0D; Keywords: Information Systems Success, Academic Information Systems",CS,AI_ML,0.85,Extracted from log - paper 2171
Automated Security Findings Management: A Case Study in Industrial DevOps,"In recent years, DevOps, the unification of development and operation workflows, has become a trend for the industrial software development lifecycle. Security activities turned into an essential field of application for DevOps principles as they are a fundamental part of secure software development in the industry. A common practice arising from this trend is the automation of security tests that analyze a software product from several perspectives. To effectively improve the security of the analyzed product, the identified security findings must be managed and looped back to the project team for stakeholders to take action. This management must cope with several challenges ranging from low data quality to a consistent prioritization of findings while following DevOps aims. To manage security findings with the same efficiency as other activities in DevOps projects, a methodology for the management of industrial security findings minding DevOps principles is essential. In this paper, we propose a methodology for the management of security findings in industrial DevOps projects, summarizing our research in this domain and presenting the resulting artifact. As an instance of the methodology, we developed the Security Flama, a semantic knowledge base for the automated management of security findings. To analyze the impact of our methodology on industrial practice, we performed a case study on two DevOps projects of a multinational industrial enterprise. The results emphasize the importance of using such an automated methodology in industrial DevOps projects, confirm our approach's usefulness and positive impact on the studied projects, and identify the communication strategy as a crucial factor for usability in practice.",CS,AI_ML,0.85,Extracted from log - paper 2172
Research on Academic Information System Unnes Using Technology Acceptance Model (TAM),"Online assessment has been carried out research on academic information system Unnes using Technology Acceptance Model (TAM) whose goal is to achieve accountability in academic application by taking the Graduate Program Unnes sampling. The method used to carry out this research through interviews/in dept interviews, documentation and questionnaires. Analyses were performed using analysis of Structural Equation Modeling (SEM) based covariance AMOS (Analysis of Moment Structures). The results obtained overall, Knowledge of Search Domain has no effect on Perceived Ease of Use. However, CMIN / DF qualiﬁed, because it has a value of≤2.",CS,AI_ML,0.85,Extracted from log - paper 2173
Elaborasi Model Technology Acceptance Model (TAM) dan DeLone &amp; McLean Untuk Mengukur Faktor Penggunaan ShopeePay,"The use of digital wallets in Indonesia is increasing both by online shopping or shopping directly. There are various factors that behind the use of digital wallets by the Indonesian people. This study seek to find out the influence of the factors that are the background for using digital wallets which in this case is ShopeePay, by knowing the effect of Easy of Use to Intention to Use and User Satisfaction and impact it has on the Net Benefit. This study combined two forms of information system research model that is the Technology Acceptance Model (TAM) and the Delone and McLean model. This research is carried out with a quantitative approach through the asosiatif method using by the likert scale. The sampling technique used is nonprobability sampling, with a total sample is 100 respondents from ShopeePay user. The results of this research support all of hypotheses, showing that there are positive and significant effects of the Easy of Use to Intention to Use and User Satisfaction and their impact on the Net Benefits. The highest relationship rate is in the Easy of Use variable to Intention to Use variable with a original value of samples by 0.696 or 69%.",CS,AI_ML,0.85,Extracted from log - paper 2174
Penggunaan Model TAM (Technology Acceptance Model) untuk Mengukur Penerimaan Layanan Google Classroom,"Abstrak - Pesatnya perkembangan teknologi saat ini cukup mempengaruhi aktivitas belajar di bidang pendidikan. Di lingkungan universitas, teknologi ini juga mengekspresikan keunggulan mereka ketika dikombinasikan dengan metode pengajaran tradisional di kelas. Menyadari manfaat yang signifikan tersebut, penerapan sistem e-learning untuk mahasiswa telah menjadi kecenderungan. Universitas AMIKOM juga memanfaatkan perkembangan teknologi pada system pembelajaranya. Salah satu sistem informasi yang digunakan adalah google classroom. Pada penelitian ini peneliti ingin mengukur seberapa besar manfaat yang dirasakan oleh mahasiswa dan kesiapan menggunakan system yang sebelumnya manual dibandingkan dengan sekarang yang memanfaatkan teknologi informasi pada proses pembelajaran. Penelitian ini menggunakan model TAM (Technology Acceptance Model) untuk mengukur penerimaan layanan google classroom teknologi informasi, untuk mengukur keberhasilan penelitian menggunakan instrumen kuesioner atau angket untuk memperoleh hasil informasi yang relevan dan untuk memperoleh tingkat keandalan (reliability) dan keabsahan (validity) setinggi mungkin. Peneliti menggunakan kuesioner dengan skala Likert (Likert Scale). Hasil dari penelitian ini berupa Analisis , adapun analisisnya dalam penelitian ini meliputi uji hipotesis dengan alat analisis regresi linier berganda yang terdiri dari uji T, Uji F dan koefisien Determinasi, menunjukkan bahwa mahasiswa di lingkungan Universitas AMIKOM Yogyakarata pada tiap fakultas yang berbeda memiliki tingkat kesiapan sistem teknologi informasi yang baik dan mengetahui manfaat yang didapat apabila menggunakan teknologi informasi atau sistem informasi.Kata kunci: teknologi informasi, skala Likert , sistem informasi, TAM (Technology Acceptance Model) Abstract - The rapid development of technology is currently sufficient to affect learning activities in the field of education. In the university environment, In the university environment, this technology also expresses their superiority when combined with traditional teaching methods in the classroom. Recognizing these significant benefits, the application of e-learning systems for students has become a trend. AMIKOM University also utilizes technological developments in its learning systems. One of the information systems used is google classroom. In this study, researchers wanted to measure how much the benefits felt by students and the readiness to use a system that was previously manual compared to now that utilizes information technology in the learning process. This study uses the TAM (Technology Acceptance Model) model to measure the acceptance of information technology google classroom services, to measure the success of research using a questionnaire or questionnaire to obtain the results of relevant information and to obtain the highest level of reliability and validity. Researchers used a questionnaire with a Likert scale (Likert Scale). The results of this study are in the form of Analysis, while the analysis in this study includes hypothesis testing with multiple linear regression analysis tools consisting of T test, F Test and Determination coefficient, shows that students at the University of AMIKOM Yogyakarata at each different faculty have a good level of information technology system readiness and know the benefits obtained when using information technology or information systems.Keywords: information technology, Likert scale, information systems, TAM (Technology Acceptance Model)",CS,AI_ML,0.85,Extracted from log - paper 2175
Alternative Layouts for Grid Questions in PC and Mobile Web Surveys: An Experimental Evaluation Using Response Quality Indicators and Survey Estimates,"The grid question refers to a table layout for a series of survey question items (i.e., sub-questions) with the same introduction and identical response categories. Because of their complexity, concerns have already been raised about grids in web surveys on PCs, and these concerns have heightened regarding mobile devices. Some studies suggest decomposing grids into item-by-item layouts, while others argue that this is unnecessary. To address this challenge, this paper provides a comprehensive evaluation of the grid layout and four item-by-item alternatives, using 10 response quality indicators and 20 survey estimates. Results from the experimental web survey ( n = 4644) suggest that item-by-item layouts (unfolding or scrolling) should be used instead of grids, not only on mobile devices but also on PCs. While the former justifies the already increasing use of item-by-item layouts on mobile devices in survey practice, the latter implies that the prevailing routine of using grids on PCs should be reconsidered.",CS,AI_ML,0.85,Extracted from log - paper 2176
DevServOps: DevOps For Product-Oriented Product-Service Systems,"For companies developing web-based applications, the Dev and the Ops refer to different groups with either operational or development focus. Therefore, DevOps help these companies streamline software development and operations activities by emphasizing the collaboration between the two groups. However, for companies producing software-intensive products, the Ops would refer to customers who use and operate the product. In addition, companies producing software-intensive products do not only offer products to customers but rather Product Service Systems (PSS), where product-related services play a key role in ensuring customer satisfaction besides their significant revenue contribution. Thus, the context of product-oriented PSS is very different from web-based applications, making it difficult to apply DevOps without considering the role of the services. Therefore, based on a two years participant observation case study conducted at a multinational telecommunications systems provider, we propose a new and novel approach called Development-Services-Operations (DevServOps) which incorporates services as a key player facilitating an end-to-end software flow toward customers in one direction and feedback toward developers in the other direction. Services become the glue that connects the Dev and the Ops, achieved by providing internal services to increase the precision of the development organization and external services to increase the speed of deployment and new content adoption on the customers' side.",CS,AI_ML,0.85,Extracted from log - paper 2177
Computer Aided Pressure Transient Analysis of a Layered Reservoir System with a Constant Pressure Boundary,"A Layered reservoir system with a constant pressure bottom boundary has immense potential to produce oil and gas, using horizontal wells may further increase productivity because it offers a larger surface area for fluid withdrawal. Test analysis in this system is tedious, time consuming and interpretation may be confusing or erroneous. Hence an efficient algorithm is needed to generate accurate pressure response of the wells to facilitate quick and easy analysis and prediction. This paper presents an interactive computer program that computes the dimensionless pressure and dimensionless pressure derivatives of horizontal wells in a two layered reservoir system with a constant pressure boundary. Program’s codes and structure of the reservoir model solution were written utilising the exponential integral solution, Gauss Laguerre and Gauss Lagrange numerical methods. The program has features that gave visual interpretation (graphical user interfaces (GUI)) as it computes pressure and derivative values when the interface between the layers is either permeable (crossflow reservoir) or impermeable (no crossflow reservoir). Examples illustrating its use in model identification and well test analysis of layered system with constant pressure boundary are also included in this work. Hence it will serve as a good learning tool because it is user friendly, fast and results have acceptable level of accuracy.",CS,AI_ML,0.85,Extracted from log - paper 2178
Experimental and Comparison Based Study on Diabetes Prediction Using Artificial Neural Network,"Background: Diabetes is spreading in the entire world. In a survey, it is observed that every generation from child to old age people are suffering from diabetes. If diabetes is not identified in time, it may lead to deadliest disease. Prediction of diabetes is of the utmost challenging task by machines. In the human body, diabetes is one of the perilous maladies that creates depended disease such as kidney disease, heart attack, blindness etc. Thus it is very important to diagnose diabetes in time.   Objective: Our target is to develop a system using Artificial Neural Network (ANN), with the ability to predict whether a patient suffers from diabetes or not.   Methods: This paper illustrates various machine learning techniques in form of literature review; such as Support Vector Machine, Naïve Bayes, K Nearest Neighbor, Decision Tree, Random Forest, etc. We applied ANN to predict diabetes. In this paper, the architecture of ANN consists of four hidden layers each of six neurons and one output layer with one neuron. Optimizer used for the architecture is ‘Adam’.   Results: We have Pima Indian diabetes dataset of sufficient number of patients with nine different symptoms with respect to the patients and nine different features in connection with the mathematical computation/prediction. Hence we bifurcate the dataset into training and testing set in majority and minority ratio of 80:20 respectively. It facilitates us the majority patient’s data to be used as training set and minority data to be used as testing set. We train our network for multiple epoch with different activation function. We used four hidden layers with six neurons in each hidden layer and one output layer. On the hidden layer, we used multiple activation functions such as sigmoid, ReLU etc. and obtained beat accuracy (88.71%) in 600 epochs with ReLU activation function. On the output layer, we used only sigmoid activation function because we have only two classes in our dataset.   Conclusion: Diabetes prediction by machine is a challenging task. So many machine learning algorithms exist to predict the diabetes such as Naïve Bayes, decision tree, K nearest neighbor, support vector machine etc. This paper presents a novel approach to predict whether a patient has diabetes or not based on Pima Indian diabetes dataset. In this paper, we used artificial neural network to train out network and it is observed that artificial neural network approach performs better than all other classifiers.",CS,AI_ML,0.85,Extracted from log - paper 2179
An experimental evaluation of localization methods used in wireless sensor networks,"&lt;span class=""fontstyle0""&gt;The problem of localization in wireless sensor networks has received considerable attention from researchers over the past decades. Several methods and algorithms have been proposed to solve this problem. The effectiveness of these algorithms depends on the accuracy of the estimated positions and the information required to calculate the coordinates. In this paper, we propose to evaluate four of the most commonly used localization methods in sensor networks. Our study considers a mathematical description of the studied methods in order to evaluate their complexity, and then a practical implementation on the simulation tool Cooja. We evaluate the performance of the studied methods as a function of the number of deployed sensor nodes and their degree of mobility in terms of several performance metrics. The objective is to reveal the most suitable localization method for a particular case of deployment. Improvement proposals are also provided to improve the most relevant localization method for the investigated study.&lt;/span&gt; &lt;br /&gt;&lt;br /&gt;",CS,AI_ML,0.85,Extracted from log - paper 2180
Navigating through CS1: The Role of Self-Regulation and Supervision in Student Progress,"The need for students' self-regulation for fluent transitioning to university studies is known. Our aim was to integrate study-supportive activities with course supervision activities within CS1. We educated TAs to pay attention to students' study ability and self-regulation. An interview study ($N=14$) was undertaken to investigate this approach. A thematic analysis yielded rather mixed results in light of our aims. Self-regulation was underpinned by the influences external to our setting, including labor market-related needs, earlier crises in study habits, and personal characteristics such as passion, grit, creativity, and valuation of utility. Safety in one-to-one supervision was considered essential, while shyness, fear, and even altruism caused self-handicapping during the course. Students were aware of their learning styles and need for self-regulation, while did not always know how to self-regulate or preferred to externalize it. The results highlight that supporting self-regulation should be integrated with students' personal histories and experiences, and thereby calls attention to transformative learning pedagogies. The thematization can help to understand CS1 students' self-regulation processes and improve CS1 support practices.",CS,AI_ML,0.85,Extracted from log - paper 2181
Exclusive use and evaluation of inheritance metrics viability in software fault prediction—an experimental study,"Software Fault Prediction (SFP) assists in the identification of faulty classes, and software metrics provide us with a mechanism for this purpose. Besides others, metrics addressing inheritance in Object-Oriented (OO) are important as these measure depth, hierarchy, width, and overriding complexity of the software. In this paper, we evaluated the exclusive use, and viability of inheritance metrics in SFP through experiments. We perform a survey of inheritance metrics whose data sets are publicly available, and collected about 40 data sets having inheritance metrics. We cleaned, and filtered them, and captured nine inheritance metrics. After preprocessing, we divided selected data sets into all possible combinations of inheritance metrics, and then we merged similar metrics. We then formed 67 data sets containing only inheritance metrics that have nominal binary class labels. We performed a model building, and validation for Support Vector Machine(SVM). Results of Cross-Entropy, Accuracy, F-Measure, and AUC advocate viability of inheritance metrics in software fault prediction. Furthermore, ic, noc, and dit metrics are helpful in reduction of error entropy rate over the rest of the 67 feature sets.",CS,AI_ML,0.85,Extracted from log - paper 2182
One-Stage Logo Detection Framework using AdaBoost Resnet50 Backbone,"Logo is an important asset as it is designed to express identity or character of the company or organization that owns the logo. The advent of deep learning methods and proliferated of logo images sample dataset in the past decade has made automated logo detection from digital images or video an interesting computer vision problem with wide potential applications. This paper presents a novel one-stage logo detector framework in which the backbone of the proposed logo detector is a deep learning model which is trained supervisedly using gradient descent training algorithm and the target logo classes as input dataset. The experiment results showed that AdaBoost Resnet50 (0.58 MAP) as the logo detector backbone outperforms Resnet50 (0.56 MAP), VGG19 (0.32 MAP), and AdaBoost VGG19 (0.56 MAP).",CS,AI_ML,0.85,Extracted from log - paper 2183
Experimental research on text CAPTCHA of fine-grained security features,"CAPTCHA is a cybersecurity measure that distinguishes between humans and automated scripts. Researchers have employed various security features to thwart automated program identification by hackers. However, previous research on the attack resistance of CAPTCHAs has used roughly quantitative analysis instead of a fine-grain quantitative study. This study implemented comparative experiments based on CAPTCHA recognition algorithms to find the best-mixed security features. A multi-stage best parameter selection (MBPS) mechanism was proposed in this study. Experiment results indicated that mixed security features of “overlap + scale + rotate + bg (background)” were the best, with an average machine recognition accuracy of only 4.81%. The contrast experiment result illustrated that the anti-attack ability of mixed security features was better than adding adversarial noise, with machine recognition accuracy decreased by 2.2%. Moreover, by investigating the efficacy of security feature parameters, this study provides practical guidelines for designing robust CAPTCHAs. Furthermore, this study also presents valuable insights into the security of image generation technology.",CS,AI_ML,0.85,Extracted from log - paper 2184
Towards Intersectional Moderation: An Alternative Model of Moderation Built on Care and Power,"Shortcomings of current models of moderation have driven policy makers, scholars, and technologists to speculate about alternative models of content moderation. While alternative models provide hope for the future of online spaces, they can fail without proper scaffolding. Community moderators are routinely confronted with similar issues and have therefore found creative ways to navigate these challenges. Learning more about the decisions these moderators make, the challenges they face, and where they are successful can provide valuable insight into how to ensure alternative moderation models are successful. In this study, I perform a collaborative ethnography with moderators of r/AskHistorians, a community that uses an alternative moderation model, highlighting the importance of accounting for power in moderation. Drawing from Black feminist theory, I call this ""intersectional moderation."" I focus on three controversies emblematic of r/AskHistorians' alternative model of moderation: a disagreement over a moderation decision; a collaboration to fight racism on Reddit; and a period of intense turmoil and its impact on policy. Through this evidence I show how volunteer moderators navigated multiple layers of power through care work. To ensure the successful implementation of intersectional moderation, I argue that designers should support decision-making processes and policy makers should account for the impact of the sociotechnical systems in which moderators work.",CS,AI_ML,0.85,Extracted from log - paper 2185
Enhancing Focus Topic Findings of Discussion Forum through Corpus Classifier Algorithm,"In learning management system, a discussion forum, in which the students and lecturers are involved actively as part of the learning method, enriches the context of communication, thereby enhancing the students’ learning and performance. The aim of this paper was to determine the appropriate topics for a discussion forum for learning management systems through enhanced probabilistic latent semantic analysis (PLSA) with the corpus classifier algorithm. In preparing the paper, the methods used were PLSA and the classifying process, which classifies the documents to become a corpus based on the similarity word approach. The similarity word is influenced by the term-frequency of the word in the document. The novel concept in this paper is the corpus classifier algorithm. The experiment was conducted using three approaches to discover the topic, and it used 4,868 distinct words from 234 documents. The documents were contained in three threads subject. The post of the discussion forum is the text document. The performance of the result was measured by the f-measure, which was calculated for each thread subject. The corpus classifier algorithm was used in the second approach, and third approach increased the average f-measure values for the second and third thread subjects by approximately 24 and 17%, respectively.",CS,AI_ML,0.85,Extracted from log - paper 2186
AN EXPERIMENTAL STUDY OF FACE RECOGNITION METHOD,"The increased use of face recognition techniques leads to the development of improved methods with higher accuracy and efficiency. Currently, there are various face recognition techniques based on different algorithm. In this study, a new method of face recognition is proposed based on the idea of wavelet operators for creating spectral graph wavelet transformation. The proposed idea relies on the spectral graph wavelet kernel procedure. In this proposed method, feature extraction is based on transformation into SGWT by means of spatial domain. For recognition purpose, the feature vectors are used for computation of selected training samples which makes the classification. The decomposition of face image is done using the SGWT. The system identifies the test image by calculating the Euclidean distance. Finally, the study conducted an experiment using the ORL face database. The result states that the recognition accuracy is higher in the proposed system which can be further improved using the number of training images. Overall, the result shows that the proposed method has good performance in terms of accuracy of the face recognition",CS,AI_ML,0.85,Extracted from log - paper 2187
THE PREDICTIONS OF PERFORMANCE METRICS IN INFORMATION RETRIEVAL: AN EXPERIMENTAL STUDY,"Information retrieval systems are widely used by people from all walks of life to meet diverse user needs. Hence, the ability of these retrieval systems to return the relevant information in response to user queries has been a matter of concern to the information retrieval research community. To address this concern, evaluations of these retrieval systems is extremely critical and the most popular way is the approach that employs test collections. This approach has been the popular evaluation approach in information retrieval for several decades. However, one of the limitations of this evaluation approach concerns the costly creation of relevance judgments. In recent research, this limitation was addressed by predicting performance metrics at the high cut-off depths of documents by using performance metrics computed at low cut-off depths. However, the challenge the research community is faced with is how to predict the precision and the non-cumulative gain performance metrics at the high cut-off depths of documents while using other performance metrics computed at the low cut-off depths of at most 30 documents. This study addresses this challenge by investigating the predictability of performance metrics and proposing two approaches that predict the precision and the non-cumulative discounted gain performance metrics. This study has shown that there exist dataset shifts in the performance metrics computed from different test collections. Furthermore, the proposed approaches have demonstrated better results of the ranked correlations of the predictions of performance metrics than existing research.",CS,AI_ML,0.85,Extracted from log - paper 2188
Proxy Design: A Method for Involving Proxy Users to Speak on Behalf of Vulnerable or Unreachable Users in Co-Design,"Designing digital artifacts is not a linear, straightforward process. This is particularly true when applying a user-centered design approach, or co-design, with users who are unable to participate in the design process. Although the reduced participation of a particular user group may harm the end result, the literature on solving this issue is sparse. In this article, proxy design is outlined as a method for involving a user group as proxy users to speak on behalf of a group that is difficult to reach. We present a design ethnography spanning three years at a cancer rehabilitation clinic, where digital artifacts were designed to be used collaboratively by nurses and patients. The empirical data were analyzed using content analysis and consisted of 20 observation days at the clinic, six proxy design workshops, 21 telephone consultations between patients and nurses, and log data from the digital artifact. We show that simulated consultations, with nurses roleplaying as proxies for patients ignited and initiated the design process and enabled an efficient in-depth understanding of patients. Moreover, we reveal how proxy design as a method further expanded the design. We illustrate: (1) proxy design as a method for initiating design, (2) proxy design as an embedded element in co-design and (3) six design guidelines that should be considered when engaging in proxy design. The main contribution is the conceptualization of proxy design as a method that can ignite and initiate the co-design process when important users are unreachable, vulnerable or unable to represent themselves in the co-design process. Based on the empirical findings from a design ethnography that involved nurses as proxy users speaking on behalf of patients, the article shows that roleplaying in proxy design is a fitting way of initiating the design process, outlining proxy design as an embedded element of co-design.",CS,AI_ML,0.85,Extracted from log - paper 2189
Development and large scale benchmark testing of the PROSPECTOR_3 threading algorithm,"AbstractThis article describes the PROSPECTOR_3 threading algorithm, which combines various scoring functions designed to match structurally related target/template pairs. Each variant described was found to have a Z‐score above which most identified templates have good structural (threading) alignments, Zstruct (Zgood). ‘Easy’ targets with accurate threading alignments are identified as single templates with Z &gt; Zgood or two templates, each with Z &gt; Zstruct, having a good consensus structure in mutually aligned regions. ‘Medium’ targets have a pair of templates lacking a consensus structure, or a single template for which Zstruct &lt; Z &lt; Zgood. PROSPECTOR_3 was applied to a comprehensive Protein Data Bank (PDB) benchmark composed of 1491 single domain proteins, 41–200 residues long and no more than 30% identical to any threading template. Of the proteins, 878 were found to be easy targets, with 761 having a root mean square deviation (RMSD) from native of less than 6.5 Å. The average contact prediction accuracy was 46%, and on average 17.6 residue continuous fragments were predicted with RMSD values of 2.0 Å. There were 606 medium targets identified, 87% (31%) of which had good structural (threading) alignments. On average, 9.1 residue, continuous fragments with RMSD of 2.5 Å were predicted. Combining easy and medium sets, 63% (91%) of the targets had good threading (structural) alignments compared to native; the average target/template sequence identity was 22%. Only nine targets lacked matched templates. Moreover, PROSPECTOR_3 consistently outperforms PSIBLAST. Similar results were predicted for open reading frames (ORFS) ≤200 residues in the M. genitalium, E. coli and S. cerevisiae genomes. Thus, progress has been made in identification of weakly homologous/analogous proteins, with very high alignment coverage, both in a comprehensive PDB benchmark as well as in genomes. Proteins 2004;55:000–000. © 2004 Wiley‐Liss, Inc.",CS,AI_ML,0.85,Extracted from log - paper 2190
Knowns and Unknowns: An Experience Report on Discovering Tacit Knowledge of Maritime Surveyors,"Context: Requirements elicitation is an essential activity to ensure that systems provide the necessary functionality to users, and that they are fit for purpose. In addition to traditional `reductionist' techniques, the use of observations and ethnography-style techniques have been proposed to identify requirements. Research Problem: One frequently heard issue with observational techniques is that they are costly to use, as developers would lose considerable time to partake, and also depend on luck in identifying requirements. Very few experience reports exist to evaluate observational techniques in practice. Results: In this experience report, we draw on several data sources, covering insights from both developers and users. The data were collected through 9 interviews with users and developers, and over 80 hours of observation of prospective users in the maritime domain. We capture `knowns' and `unknowns' from both developers and users, and highlight the importance of observational studies. Contribution: While observational techniques are costly to use, we conclude that essential information is uncovered, which is key for developers to understand system users and their concerns.",CS,AI_ML,0.85,Extracted from log - paper 2191
"Optimisation, benchmark testing and comparison of droop control variants in microgrids","AbstractGrid‐forming inverter control is recently discussed for bulk power systems and is already in use for islanded microgrids. A common control type is the droop control. Numerous variants of the basic droop control have been proposed. However, there is lack of performance comparison of the droop variants in literature. Their superiority has only been demonstrated for some specific microgrid scenarios. This work composes benchmark scenarios to assess and compare the applicability of droop control variants and also their combination with virtual impedances under practical conditions. A number of microgrid topologies and the interaction with synchronous machines are considered to benchmark the performance. Static criteria, such as the steady‐state power sharing, as well as dynamic stability criteria, are taken into account for modal analysis. To guarantee a meaningful comparison, a genetic algorithm tailored to the problem is used to optimise controller parameters for each controller type. Results indicate that the combination with virtual impedance has a more decisive effect on stability than the droop variant. The outcome is relevant for microgrid stability analysis in numerous contexts, such as optimal placement of inverters or topology optimisation, where the choice of the most suitable controller type with optimised parameter sets is key.",CS,AI_ML,0.85,Extracted from log - paper 2192
Evaluasi Penggunaan Flowgorithm dalam Pembelajaran Algoritma Pemrograman menggunakan Technology Acceptance Model (TAM),"be able to automate work and software development. However, a programmer must understand the basics of computational thinking that often causes a problem for students who are desirous of becoming programmers. This extensive research aims to evaluate student's acceptance of the use of flow Flogorithm using the Technological Acceptance Model (TAM) method. This type of research is quantitative and is conducted by using a survey method which involves distributing questionnaires to around 96 respondents who have particularly studied programming algorithms. Furthermore, the random sampling technique was used in the sampling. The analysis was carried out using the PLS-SEM method which consists of a measurement model and structural model with five variables, namely ease of use, perceived usefulness, attitude towards use, intention, and actual use. The result of the research shows that the acceptance of the Flowgorithm has a positive influence on all the set of variables, except for the perceived ease of use on attitude toward using.",CS,AI_ML,0.85,Extracted from log - paper 2193
Analisis Deskriptif Perilaku Konsumen Shopee: Technology Acceptance Model (TAM),"Shopee merupakan salah satu perusahaan e-commerce dari Singapura. Shopee sebagai salah satu apliasi e-commerce mobile dimana para pengguna aplikasi dapat melakukan kegiatan menjelajah, jual maupun beli secara praktis dan cepat serta bisa dilakukan kapanpun dan dimanapun melalui aplikasi yang berada didalam perangkat mobile yang dimiliki. Ada beberapa faktor yang wajib dipenuhi oleh aplikasi agar tercipta layanan lingkungan berbelanja yang mudah dan aman. Walaupun nama Shopee sendiri bisa dibilang terkenal dan sudah memiliki nama yang besar, tentunya akan tetap ada kemungkinan pengguna aplikasi merasa dikecewakan oleh layanan yang diberikan oleh aplikasi. Dalam usaha untuk meningkatkan kenyaman serta keamanan dalam menggunakan aplikasi mobile Shopee maka perusahaan perlu melakukan pengukuran untuk mengetahui faktor mana yang akan memberikan pengaruh terhadap sikap penerimaan para pengguna aplikasi. Jenis penelitian ini deskriptif kuantitatif, pengambilan sampel secara random sampling sehingga jumlah sampel sebanyak 41 responden dan data penelitian ini merupakan data sekunder. Hasil penelitian ini perilaku pengguna Shopee masuk dalam katagori yang sangat bagus meliputi Perceived ease to use (85,73%), Perceived usefulness (84,55%), Trust (81,14%), Behaviour intention to use (80,98%), Attitude toward using (83,17%).",CS,AI_ML,0.85,Extracted from log - paper 2194
Reinforcement Learning-supported AB Testing of Business Process Improvements: An Industry Perspective,"In order to better facilitate the need for continuous business process improvement, the application of DevOps principles has been proposed. In particular, the AB-BPM methodology applies AB testing and reinforcement learning to increase the speed and quality of improvement efforts. In this paper, we provide an industry perspective on this approach, assessing requirements, risks, opportunities, and more aspects of the AB-BPM methodology and supporting tools. Our qualitative analysis combines grounded theory with a Delphi study, including semi-structured interviews and multiple follow-up surveys with a panel of ten business process management experts. The main findings indicate a need for human control during reinforcement learning-driven experiments, the importance of aligning the methodology culturally and organizationally with the respective setting, and the necessity of an integrated process execution platform.",CS,AI_ML,0.85,Extracted from log - paper 2195
THE DESIGN OF MANAGEMENT INFORMATION SYSTEMS AT JAMI KAUTSAR MOSQUE,"This research is motivated by the management of the Jami Al-Kautsar Mosque in Depok which has not been properly recorded and the data storage is scattered. These constraints have the potential for data loss and inaccurate information and a process taking a long time to produce such information. In this study, the data collection techniques used are observation, interviews and literature study. The information system built uses the waterfall method as a software development or Software Development Life Cycle (SDLC) using object-oriented design of the Unified Modeling Language (UML) and testing using Black Box. The process in this study includes the process of managing activities, managing the mosque, inventory, finances and goods (income and expenditure). The results of research implemented by the author on the Management Information System of the Jami Al-Kautsar Mosque is a system that can provide information on the management of activities, mosque administrators (human resources), inventory, finances and goods (income and expenditure) as well as communicative and informative reports so that they can make evaluation materials and transparent accountability reports from the management to all congregations of the Jami Al-Kautsar Mosque. User satisfaction with the evaluation information system uses the five-category Black Box test and produces a good information system. Three categories of Black Box testing are no errors in functions, database access and structure and system performance above 60%.",CS,AI_ML,0.85,Extracted from log - paper 2196
Machine learning-based feature selection and classification for cerebral infarction screening: an experimental study,"Cerebral infarction screening (CIS) is critical for timely intervention and improved patient outcomes. We investigate the application of machine learning techniques for feature selection and classification of speech and cognitive function assessments to enhance cerebral infarction screening. We analyze a dataset containing 117 patients (95 patients were diagnosed with cerebral infarction, and 54 were identified as lacunar cerebral infarction of them) comprising speech and cognitive function features from patients with lacunar and non-lacunar cerebral infarction, as well as healthy controls. In this article, we present a framework called CIS which comprises a cerebral infarction screening model to identify cerebral infarction from populations and a diagnostic model to classify lacunar infarction, non-lacunar infarction, and healthy controls. Feature selection method, Recursive Feature Elimination with Cross-Validation (RFECV), is employed to identify the most relevant features. Various classifiers, such as support vector machine, K-nearest neighbor, decision tree, random forest, logistic regression, and eXtreme gradient boosting (XGBoost), were evaluated for their performance in binary and ternary classification tasks. The CIS based on XGBoost classifier achieved the highest accuracy of 88.89% in the binary classification task (i.e., distinguishing cerebral infarction from healthy controls) and 77.78% in the ternary classification task (i.e., distinguishing lacunar infarction, non-lacunar infarction, and healthy controls). The selected features significantly contributed to the classification performance, highlighting their potential in differentiating cerebral infarction subtypes. We develop a comprehensive system to effectively assess cerebral infarction subtypes. This study demonstrates the efficacy of machine learning methods in cerebral infarction screening through the analysis of speech and cognitive function features. These findings suggest that incorporating these techniques into clinical practice could improve early detection and diagnosis of cerebral infarction. Further research with larger and more diverse datasets is warranted to validate and extend these results.",CS,AI_ML,0.85,Extracted from log - paper 2197
Adopting Microservices and DevOps in the Cyber-Physical Systems Domain: A Rapid Review and Case Study,"The domain of cyber-physical systems (CPS) has recently seen strong growth, e.g., due to the rise of the Internet of Things (IoT) in industrial domains, commonly referred to as ""Industry 4.0"". However, CPS challenges like the strong hardware focus can impact modern software development practices, especially in the context of modernizing legacy systems. While microservices and DevOps have been widely studied for enterprise applications, there is insufficient coverage for the CPS domain. Our goal is therefore to analyze the peculiarities of such systems regarding challenges and practices for using and migrating towards microservices and DevOps. We conducted a rapid review based on 146 scientific papers, and subsequently validated our findings in an interview-based case study with 9 CPS professionals in different business units at Siemens AG. The combined results picture the specifics of microservices and DevOps in the CPS domain. While several differences were revealed that may require adapted methods, many challenges and practices are shared with typical enterprise applications. Our study supports CPS researchers and practitioners with a summary of challenges, practices to address them, and research opportunities.",CS,AI_ML,0.85,Extracted from log - paper 2198
Consumer Acceptance Analysis of Purchase Interest Using Live Features on The Marketplace with Technology Acceptance Model (TAM) Method Case Study : Shopee,"The development of technology has become one of the instant lifestyles of the community because it has high mobility in carrying out daily activities. The use of the internet for commercial transactions is known as electronic commerce (e-Commerce). In 2021 Shopee launched a new feature before other competitors had the Shopee live feature. This research is more focused on analyzing buyers using the Shopee live feature on the Shopee marketplace using the Technology Acceptance Model (TAM). The purpose of this study is to analyze how the influence of consumers in buying interest using the Shopee live feature and to determine the effect of each variable used. This type of research is quantitative research. The research sample was 255 UNNES student respondents from 2015-2019. The data analysis method used is descriptive analysis using SEM with SmartPLS 3.0 software. The data that has been obtained is done data cleaning. It was found that the valid data were 203 people. The results of the research based on what has been done on the outer model, there are three indicators that are deleted, namely PEOU 2, PEOU 4, and PU 5, because the results obtained are still below the set value &lt;0.7. In the inner model research, there is one hypothesis that is rejected, namely the t-statistics test, namely PEOU &gt; BI because this hypothesis has a value less than the standard value of the t-test so it is rejected. However, there are six accepted hypotheses, namely PE &gt; PU, PE &gt; PEOU, PE &gt; BI, PT &gt; PU, PT &gt; BI, and PU &gt; BI. Based on the results of this analysis, the six variables have factors that influence the purchase intention using the Shopee live feature.",CS,AI_ML,0.85,Extracted from log - paper 2199
"Virtual Tools for Testing Autonomous Driving: A Survey and Benchmark of Simulators, Datasets, and Competitions","Traditional road testing of autonomous vehicles faces significant limitations, including long testing cycles, high costs, and substantial risks. Consequently, autonomous driving simulators and dataset-based testing methods have gained attention for their efficiency, low cost, and reduced risk. Simulators can efficiently test extreme scenarios and provide quick feedback, while datasets offer valuable real-world driving data for algorithm training and optimization. However, existing research often provides brief and limited overviews of simulators and datasets. Additionally, while the role of virtual autonomous driving competitions in advancing autonomous driving technology is recognized, comprehensive surveys on these competitions are scarce. This survey paper addresses these gaps by presenting an in-depth analysis of 22 mainstream autonomous driving simulators, focusing on their accessibility, physics engines, and rendering engines. It also compiles 35 open-source datasets, detailing key features in scenes and data-collecting sensors. Furthermore, the paper surveys 10 notable virtual competitions, highlighting essential information on the involved simulators, datasets, and tested scenarios involved. Additionally, this review analyzes the challenges in developing autonomous driving simulators, datasets, and virtual competitions. The aim is to provide researchers with a comprehensive perspective, aiding in the selection of suitable tools and resources to advance autonomous driving technology and its commercial implementation.",CS,AI_ML,0.85,Extracted from log - paper 2200
MAGIC: Multi-Agent Argumentation and Grammar Integrated Critiquer,"Automated Essay Scoring (AES) and Automatic Essay Feedback (AEF) systems aim to reduce the workload of human raters in educational assessment. However, most existing systems prioritize numeric scoring accuracy over the quality of feedback. This paper presents Multi-Agent Argumentation and Grammar Integrated Critiquer (MAGIC), a framework that uses multiple specialized agents to evaluate distinct writing aspects to both predict holistic scores and produce detailed, rubric-aligned feedback. To support evaluation, we curated a novel dataset of past GRE practice test essays with expert-evaluated scores and feedback. MAGIC outperforms baseline models in both essay scoring , as measured by Quadratic Weighted Kappa (QWK). We find that despite the improvement in QWK, there are opportunities for future work in aligning LLM-generated feedback to human preferences.",CS,NLP,90,Clear CS paper focusing on NLP applications in educational assessment. Uses multi-agent framework for automated essay scoring and feedback generation, with strong natural language processing components.
